;;/**********************************************************************************
;;    Copyright (C) 2008 Investment Science Corp.
;;
;;    This program is free software: you can redistribute it and/or modify
;;    it under the terms of the GNU General Public License as published by
;;    the Free Software Foundation, either version 3 of the License, or
;;    any later version.
;;
;;    This program is distributed in the hope that it will be useful,
;;    but WITHOUT ANY WARRANTY; without even the implied warranty of
;;    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
;;    GNU General Public License for more details.
;;
;;    You should have received a copy of the GNU General Public License
;;    along with this program.  If not, see <http://www.gnu.org/licenses/>.
;;
;;***********************************************************************************/
;;
;;*************************************
;;*************************************
;; Exported Lambda File Cabinet Document
;;*************************************
;;*************************************

;;**EXPORTKEY**:%MACROS(esm)
;; ********************************************************************
;; summary:  Esm User defined macros
;;
;; 			 Compile time child Lambdas for the selector compiler.
;; 			 These child Lambdas support the selector compilation process.
;;
;; Notes:    Requires the browseLib, the ParseLib, and the compiler
;;           definition source must be checked into the file cabinet 
;;           under the key: |esm.selector:%DEFINITION|.
;;
;; ********************************************************************

;; *************************************
;; Number Validation Macro
;; *************************************
(defmacro VALIDATE(x) (list (symbol "if") 
                            (list |<|: 'BIGPOSNUM x) 
                                BIGPOSNUM
                                (list (symbol "if") 
                                      (list |>|: 'BIGNEGNUM x) 
                                           BIGNEGNUM
			                               (list (symbol "if")
			                                     (list and: 
			                                           (list |<|: 'LOWNEGNUM x) 
			                                           (list |>|: 'LOWPOSNUM x)) 
			                                           0.0 
			                                           x)))
         ) ; end VALIDATE










































;;**EXPORTKEY**:esm
(defun esm(ObjVector:X NumVector:Y Integer:G Number:S ...)
;; *******************************************************************
;;
;; WARNING: This Lambda complex must be compiled at least TWICE so that the VALIDATE
;;          macro defined in esm:selector:%COMPILER_USERFUNCTIONS and the validate 
;;          macro defined in the math Lambda will both be in place when the main body 
;;          of this code is compiled.
;;
;; summary: The Evolutionary Sequencing Machine Lambda (ESM) is a learning machine which learns to
;;          select and score the best individuals from a universe of individuals over time. Over  
;;          a series of discrete time steps, a universe of individuals is collected for each time
;;          step. The individuals are things such as Stocks, People, Cities, etc. The discrete time
;;          steps are weeks, days, seconds, years, microseconds, etc.
;;       
;;          Each individual followed by the system is given a unique identifier which remains
;;          unique across all time periods studied (no two individuals ever have the same identifier).
;;          Furthermore, each time period studied is given a unique ascending integer index (i.e. week 1,
;;          week 2, etc.). So, for a series of time periods, historical information about groups of
;;          individuals is collected for each time period. The historical information collected for each
;;          individual for each time period is stored in a Number Vector and includes: the time period index;
;;          the unique identifier of the individual; and other numeric information about the individual
;;          pertinent to the current investigation. Finally, each individual in each time period is given
;;          a numeric "score" which determines the value of the individual in that time period. The "best"
;;          individuals have the highest "score" values.
;;
;;          During training, the ESM is given historical information for time periods 0 through T for
;;          all individuals. The ESM is also given the "score" values for each individual in each training
;;          time period from 0 through T. During training the ESM attempts to "learn" any patterns in 
;;          the available historical data. The machine (ESM) is free to discover static as well as time
;;          varying patterns.
;;
;;          During forward prediction, the ESM is given new information for time period T+1 for
;;          all individuals. The ESM is NOT given the "score" values for each individual in the new
;;          time period T+1. During prediction the ESM attempts to use any patterns it has learned to 
;;          select and score the best individuals from the universe of individuals seen in time period T+1. The 
;;          machine (ESM) is free to select no individuals in time period T+1 (an "I am uncertain" response). 
;;          Once the machine selects and scores a set of individuals, the accuracy of the machine is determined by
;;          least squares error on the selected individuals, averaging the actual "score" values for the selected
;;          individuals (which the machine has never seen) with the average "score" for all individuals 
;;          in time period T+1, or by other appropriate methods.
;;
;; Args:    X:         - The N by M vector array representing the training data in the form of:
;;                                         x x ... x
;;                                         x x ... x
;;                                             ... 
;;                                         x x ... x
;;          Y   	   - The N Vector of score variables for each training example.
;;          G   	   - The maximum number of evolutionary generations to run.
;;          S          - The halting "score" which, if achieved, will halt the training run (as a percent of all elements 0% thru 100%).
;;          Ck         - (Optional)Checkpoint command (a)false if checkpointing is NOT desired, 
;;                                                    (b) checkpoint: if checkpointing is desired, 
;;                                                    (c) reuse: if reuse of initial checkpoint and checkpointing is desired, 
;;                                                    (d) restart: if checkpoint and restart is desired.
;;          seed       - (Optional-Number)User specified seed number for the pseudo random number generator (each seed starts a different but completely deterministic training run).
;;
;; Return:  Rf:        - A new selector Lambda in the form of: 
;;                                         Rf(Xt) ==> Ey {Xt}
;;                       where (before invoking Rf) Xt is the N by M vector array representing the testing data in the form of:
;;                       in the form of:   x x ... x
;;                                         x x ... x
;;                                             ... 
;;                                         x x ... x
;;                       and (after invoking Rf) Xt is the N by B vector array representing the "best" B vector elements
;;                       selected from the testing data by Rf
;;                       and where Ey is the B vector (returned from the Rf invocation) representing the estimated scores for the selected B elements.
;;
;; Depends:  browseLib 
;;           math 
;;           ParseLib 
;;           rulesLib 
;;
;; *******************************************************************
  pvars:(;; Public Variables

        ;; Begin User Defined Learning Options
        (myBestAverage 1)  	            ;; The count of best selector champions to average in creating the final myBest selector Lambda.
        (Integer:myBestOfBest 10)       ;; The maximum number of best of the best selector champions to save.
        (Integer:myBGMMaximum 0003)     ;; The maximum number of chromosomes to allowed in any one BGM regression.
        (Integer:myChromosomeGEN 0000)  ;; The maximum random root-chromosome genomes from different columns to grow during each generation step.
        (Integer:myChromosomeINIT 0000) ;; The maximum random root-chromosome genomes from different columns to grow during the initialization step.
	    (myChromosomeRule MVL:)         ;; The rule used for root-chromosome search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
        (myCrossChromosomeSW false)  	;; The use of chromosomes from different columns during cross over operations.
        (Number:myCrossColPct 0.00)  	;; The probability of cross over in each column island during each generation step.
        (myCrossLinearSW false)  	    ;; The use of a linear grammar during cross over operations.
        (myCrossMarrySW true)  	        ;; The use of expression marrying during cross over operations.
        (Number:myCrossRegPct 0.00)  	;; The probability of cross over in the best-regressor island during each generation step.
        (Number:myCrossSelPct 1.00)  	;; The probability of cross over in the best-of-breed island during each generation step.
        (myCrossSpliceSW true)  	    ;; The use of expression splicing during cross over operations.
        (Integer:myENNLayers 3)         ;; The maximum number of hidden layers to grow in each ENN machine.
        (Integer:myFRMMaximum 1000)     ;; The maximum number of chromosomes to allowed in any one FRM regression.
        (Integer:myGreedyDepth 00)    	;; The depth of greedy search exploration of each chosen column island  during each generation step. 
        (Integer:myGreedyGEN  000)    	;; The number of greedy search WFFs the system will grow in the best-of-breed island  during each generation step. 
        (Integer:myGreedyINIT 0000)    	;; The number of greedy search WFFs the system will grow in the best-of-breed island  during the initialization step. 
        (myGreedyWidth narrow)    	    ;; The width of each greedy search WFF the system will grow in the best-of-breed island (base, full, narrow, random). 
        (myGreedyRule MVL:)          	;; The rule used for greedy search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
        (Integer:myGrowColINIT 00)   	;; The number of random WFFs the system will grow in each column island during the initialization step.
        (Integer:myGrowColGEN 00)    	;; The number of random WFFs the system will grow in each column island during each generation step. 
        (myGrowColRule REG:)         	;; The rule used for random WFFs grown in each column island (BGM, FRM, MVL, ENN, REG, or SVM). 
        (Integer:myGrowSelINIT 0000)   	;; The number of random WFFs the system will grow in the best-of-breed island during the initialization step.
        (Integer:myGrowSelGEN 00)    	;; The number of random WFFs the system will grow in the best-of-breed island during each generation step. 
        (myGrowSelRule MVL:)         	;; The rule used for random WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
        (myGrowWFFStyle root:)       	;; The style used for random WFFs grown in the best-of-breed island (full, or root). 
        (Integer:myMaxColWFFLen 05)  	;; The maximum length of a single WFF for each column.
        (Number:myMigratePct 0.00)   	;; The probability of Column island mutation during each generation step.
        (Number:myMutateColPct 0.00) 	;; The probability of mutation in each column island during each generation step.
        (Number:myMutateRegPct 0.00) 	;; The probability mutation in the best-regressor island during each generation step.
        (Number:myMutateSelPct 1.00) 	;; The probability mutation in the best-of-breed island during each generation step.
        (myMutateSpliceSW true)  	    ;; The use of expression splicing during mutation operations.
        (Integer:myMVLMaximum 1000)     ;; The maximum number of chromosomes to allowed in any one MVL regression.
        (myParetoErrorSW false)         ;; The switch to support pareto front as an error fitness measure ( true or false).
        (myRandomError .00)          	;; The amount of random error to add to each selfTest test case (0.00 thru .50).
        (myREGMultiple true)            ;; Support multiple columns when growing REG regression expressions.
        (myREGOperatorJoin "*")         ;; The operator used to join multiple columns when growing REG regression expressions ("+" or "*").
        (Integer:myREGMaximum 1000)     ;; The maximum number of chromosomes to allowed in any one REG regression.
        (Integer:myRestartGap 00010) 	;; The count of generations to allow, without measurable improvement, before initiating a new evolutionary process.
        (Integer:myRootINIT 00)  	 	;; The number of root WFFs the system will grow in each column island during the initialization step.
        (Integer:myRootGEN 00)       	;; The number of root WFFs the system will grow in each column island during each generation step. 
        (myRootRule REG:)            	;; The rule used for root WFFs grown in each column island (BGM, FRM, MVL, ENN, REG, or SVM). 
		(Integer:mySampleFactor 100)    ;; The reduction factor for determining the size of the sample training data (when mySampleON is true).
        (mySamplingON  true)   	     	;; Support initial scoring on a small sample set before scoring on the full training data set.
        (myScoreFocus full:)  	        ;; The focus of scoring on the training data (full, top25, top10, or top05).
        (mySeedDefault 2407987.0)    	;; The default seed number used to initialize the random number genenerator at the start of each training run.
        (Integer:mySurvivors  25)     	;; The count of surviving selector WFFs in each generation.
        (mySurvivorStyle equalOFF:)  	;; The option allowing surviving selector WFFs to have equal scores (equalON, or equalOFF).
        (mySvmKernelID cube:)       	;; The kernelID to use for all SVM learning (see svmRegress: all, binary, bipolar, composite, cosine, cube, exp, linear, log, poly, quart, sigmoid, sine, square, tan, tanh).
        (Integer:mySVMMaximum 1000)     ;; The maximum number of chromosomes to allowed in any one SVM regression.
        (mySvmMaxGen 1)             	;; The maximum number of training generations to use for all SVM learning (see svmRegress).
        (mySvmMaxLayers 1)             	;; The maximum number of training layers to use for all SVM learning (see svmRegress).
        (mySvmModelCount 1)             ;; The maximum number of training models to use for all SVM learning (see svmRegress).
        (Integer:myTournamentSize 05)	;; The size of the myBestSelectorChampions queue which initiates a tournament-of-champions when the next evolutionary process begins.
        (myTimeON false)  			 	;; Support separate sequence scoring of each individual time period.
		(Integer:myTrainingFactor 10)   ;; The reduction factor for determining the size of the final training data (when mySampleON is true).
        (myUseBGM false)             	;; Grow BGM selector candidates during the initialization step and during each generation step.
        (myUseENN false)             	;; Grow ENN selector candidates during the initialization step and during each generation step.
        (myUseFRM false)             	;; Grow FRM selector candidates during the initialization step and during each generation step.
        (myUseMVL  true)             	;; Grow MVL selector candidates during the initialization step and during each generation step.
        (myUseREG false)             	;; Grow REG selector candidates during the initialization step and during each generation step.
        (myUseSVM false)             	;; Grow SVM selector candidates during the initialization step and during each generation step.
        (myVerboseSW false)		     	;; The switch controlling verbose mode during testing, maintennance, and development.
        (myWFFSaveSW true)	            ;; The switch controlling the saving of WFFs already evaluated.
        (myWFFReinitSW true)            ;; The switch controlling the reinitialization of WFF cache at the start of every new run.
        ;; End User Defined Learning Options

		myBest            		     	;; The "best" selector found during the training process.
		myBestOfBreedIsland 	     	;; The index, into myPopulationIslands, of the best-of-breed island.
		myBestRegressor    		     	;; The "best" regressor found during the current training process.
		myBestRegressorChampions	 	;; The "best" regressor found during any evolutionary training process.
		myBestRegressorIsland 	     	;; The index, into myPopulationIslands, of the best-of-breed regressor island.
		myBestSelector    		     	;; The "best" selector found during the current training process.
		myBestSelectorChampions      	;; The "best" selector found during any evolutionary training process.
		myBestSelectorBestOfBest	    ;; The "best of the best" selectors found across all evolutionary training runs.
		myBestT     			     	;; The current best-of-breed Selector WFF for each time period.
        myCheckPoint                 	;; The checkpoint command (a) checkpoint: if checkpointing is desired, (b) restart: if checkpoint and restart is desired. 
        myColumnGenome               	;; The default genome containing references to all columns. 
		myEvolutionCount	    		;; The count of new evolutionary processes attempted since the start of checkpointing.
		Integer:myG			         	;; The maximum number of evolutionary generations to run.
		Integer:myGc			     	;; The current number of evolutionary generations which have run.
	    IntVector:myIslands          	;; The vector of root terms tried in each distinct island population of selector Lambdas.
		(Integer:myM 10)      	    	;; The number of columns in the training data (including xtime).
		Integer:myMaxRootWFFs        	;; The maximum number of root WFFs for each column.
		Integer:myN				     	;; The number of rows in the training data.
		Integer:myNextID		     	;; The next unique Selector Lambda identifier to be issued (uniquely identifies one and only one Selector Lambda).
		(Number:myNumberPct .50)     	;; The percent chance that a numeric constant will be generated during term generation.
		myParent				    	;; The top parent Lambda for this Lambda complex.
		myPopulation			    	;; The current population of selector Lambdas.
		myPopulationIslands	         	;; The vector of current distinct populations of selector Lambdas.
		myRepository                 	;; The repository for storing the knowledge gained from studying the training data.
		(myRepositoryDefaultName "esmRepository.db") ;; The repository default file name.
		myRowManager                 	;; The row manager for the entire training data set.
		myRowManagerSample           	;; The row manager for the sample training data set.
		myRowManagerTraining         	;; The row manager for the smaller training data set.
		Number:myS				     	;; The halting "score" which, if achieved, will halt the training run (as a percent of all elements 0% thru 100%).
		Integer:mySampleSize         	;; The size of the sampled training data for quick learning.
		Number:mySeed                	;; The seed number used to initialize the random number genenerator at the start of each training run.
        (mySvmModelCount 1)             ;; The maximum number of training models to use for all SVM learning (see svmRegress).    
        Integer:mySVMSampleSize         ;; The SVM suggested sample size for abbreviated quick learning.
		Integer:myT				     	;; The count of separate time periods in the training data (length of myTMgrs).
		myTMgrs		                 	;; The vector of sequential time period row managers organized from the training data.
		IntVector:myTN			     	;; The count of training examples in each time period.
		myTournamentCount	    		;; The count of new tournament-of-champion evolutionary processes attempted since the start of checkpointing.
		Integer:myTrainingSize       	;; The size of the training data for quick scoring.
		myWFFs			            	;; The Dictionary of all WFFs the system has ever seen and their scored Selector Lambda.
		myWFFLengthLimit             	;; The maximum length, after which, any Selector WFF will be rejected on the basis of size alone.
	    myWFFSaveLimit               	;; The maximum size of all WFFs saved by the system (after this size no more WFFs are saved).
        myWFFSaveON     	            ;; The switch controlling the saving of WFFs already evaluated.
		myX					         	;; The N by M vector array representing the training data.
		myXsample				     	;; The mySampleSize by M vector array representing the smaller sampled training data for quick learning.
		myXtrain				     	;; The myTrainingSize by M vector array representing the smaller training data for quick scoring.
		myY		              	     	;; The N Vector of score variables for each training example.
		myYsample              	     	;; The mySampleSize Vector of score variables for each of the smaller sampled training data for quick learning.
		myYtrain              	     	;; The myTrainingSize Vector of score variables for each of the smaller training data for quick scoring.
     	;; Public Child Lambdas
		bestRegressor       	     	;; Return the "fittest" regressor WFF from all of the distinct island populations.
		bestSelector       	         	;; Return the "fittest" selector WFF from all of the distinct island populations.
		clear    			         	;; Clear all persistent memory.
        compileSelector                 ;; Compile a selector wff and return a Selector Lambda.
		copyWff             	     	;; Copy a selector wff so that a completely different set of Pair objects is returned.
		createSelector       	     	;; Create and score a selector wff over each training time period.
		evalRule            	     	;; Evaluate an ESM grammar generator rule.
		findSelectorID       	     	;; Return the Selector Lambda, with the specified ID, by searching all of the distinct island populations.
        initTrainingData             	;; Initialize the training data into sequential time segments.
		listWff             	     	;; Convert a selector wff to a set of Pair objects (a list).
	    multipleRegression           	;; Performs a Gaussian multiple regression on the N x M+1 matrix
		report      		         	;; Displays a status report of the checkpoint repository.
		rowManager   		         	;; Manages a set of Number Vectors all having a single time stamp.
        saveUniqueNumericWffs        	;; Save the best unique numeric WFFs.
		selector			         	;; The private WFF language used for genetic programming of selector Lambdas Note: (ParseLib "Esm" "esm:selector").
		scoreWff   		             	;; Compute the score for a selector wff over each training time period.
		simplify   		            	 ;; Returns the normalized form of any basic Selector expression.
		stringWff             	     	;; Convert a selector wff to a string (from a list of Pair objects).
        stringSortedWff              	;; Convert a selector wff to a sorted string (from a list of Pair objects).
        trunc                    	    ;; Truncate a selector wff to the specified maximum display length.
		verboseOFF             	     	;; Turns the verbose testing mode off.
		verboseON             	     	;; Turns the verbose testing mode on.
     	;; Population Operators (these Lambdas promote the Selector WFF population to a "more fit" Selector population).
		makeCrossover                	;; Introduce a new Selector WFF (by splicing two parent WFFs) to the current population.
		makeMutation                 	;; Introduce a new Selector WFF (by mutating a candidate WFF) to the current population.
     	;; WFF Grammar Rules.
        ruleBgm      			     	;; The WFF grammar rules for all Selector basis grid machine Lambdas. 
        ruleExp      			     	;; The WFF grammar rules for all Selector numeric expression. 
        ruleFrm      			     	;; The WFF grammar rules for all Selector multivariate factor regression Lambdas. 
        ruleMvl      			     	;; The WFF grammar rules for all Selector multivariate linear regression Lambdas. 
        ruleEnn      			     	;; The WFF grammar rules for all Selector percentile grid machine Lambdas. 
        ruleReg      			     	;; The WFF grammar rules for all Selector linear regression Lambdas. 
        ruleSvm      			     	;; The WFF grammar rules for all Selector support vector machine Lambdas. 
     	;; WFF Operators (these Lambdas all link directly into the Selector numeric expression grammar).
		checkNumericWFF   		     	;; Checks a numeric Selector WFF for validity.
		cutoutNumericWFF   		     	;; Returns a numeric Selector WFF by randomly cutting out a sub-expression from a candidate Selector WFF {"(x3/sin(x4))" ==> "sin(x4)"}.
        extractNumericWFF		     	;; Return a numeric Selector WFF from any Selector WFF {"regress (x3/x13);" ==> "(x3/x13)"}.
        growWFF                      	;; Returns a randomly grown Selector WFF. 
        growCTermWFF                 	;; Returns a sequentially grown regression conditional term Selector WFF based upon a chosen column.
        growRootWFF                  	;; Returns a sequentially grown root Selector WFF xtime, thru xm, abs(xtime) thru abs(xm), cos(xtime) thru cos(xm), etc. 
        growTermWFF                  	;; Returns a sequentially grown regression basic term Selector WFF based upon a chosen column.
		lengthWFF   		         	;; Returns the left-depth-first length of any Selector WFF.
        marryNumericWFFs		     	;; Return a numeric Selector WFF by marrying two suitor Selector WFFs {"x3" , "x4" ==> "(x3/x4)"}.
		mutateNumericWFF   		     	;; Returns a numeric Selector WFF by randomly mutating a candidate Selector WFF {"(x3/x4)" ==> "(x3/sin(x4))"}.
		spliceNumericWFF    	     	;; Returns a numeric Selector WFF by randomly splicing one WFF into a candidate Selector WFF {"(x3/sin(x4))" , "log(x10)" ==> "(log(x10)/sin(x4))"}.
     	;; Machine Learning Child Lambdas
        bgmRegress                   	;; Manages the basis grid machine learning technology.
        ennRegress                   	;; Manages the evolutionary neural net regression technology.
        frmRegress                   	;; Manages the multivariate factor regression learning technology.
        mvlRegress                   	;; Manages the multiple linear regression learning technology.
        svmRegress                   	;; Manages the support vector machine learning technology.
		;; Maintenance Child Lambdas
        exportDocs                   	;; Export all embedded ontology objects from this Lambda Note: (esm.exportDocs).
		loadCheckPoint			     	;; Load the checkpoint information from a previous training run.
		loadRootIslands			     	;; Load the root island populations from the previous initialization step in this training run.
		makeSelector			     	;; Generate the private WFF language used for genetic programming of selector Lambdas Note: (esm.makeSelector).
		saveCheckPoint			     	;; Save the checkpoint information from the current training run.
		saveRootIslands			     	;; Save the root island populations from the current initialization step in this training run.
		setOptions   			     	;; Set predefined default option information, by name, for the current training run.
		random     			         	;; Pseudo random number routine.
		selfTest			         	;; Perform a series of diagnostic self tests of this learning machine.
        ) ; end persistant variables
   ;; *******************************************************************************
   ;; Define Public Child Lambdas 
   ;; *******************************************************************************
   ;; Return the "fittest" regressor WFF from all of the distinct island populations.
   (defun bestRegressor()
      regs:(m M n N)
      vars:(Lambda bestLambda)
      ;; Return the "fittest" regressor WFF.
      (if (<> myBestRegressor #void) (setq bestLambda myBestRegressor) (setq bestLambda myBest))
      (setq M (length myPopulationIslands)) 
      (loop for m from 0 until M do
         (setq N (length myPopulationIslands[m]))
         (loop for n from 0 until N do
           (setq Lambda myPopulationIslands[m][n])
           (cond
             ((> Lambda.ErrorPct bestLambda.ErrorPct) (setq bestLambda bestLambda))
             ((< Lambda.ErrorPct bestLambda.ErrorPct) (setq bestLambda Lambda))
             ((< Lambda.Score bestLambda.Score) (setq bestLambda Lambda))
             ((< (length Lambda.WFF) (length bestLambda.WFF)) (setq bestLambda Lambda))
             ) ; end cond
           ) ; end M loop
         ) ; end M loop
      (setq myBestRegressor bestLambda)
      bestLambda) ; end bestRegressor
   ;; Return the "fittest" selector WFF from all of the distinct island populations.
   (defun bestSelector()
      regs:(m M n N)
      vars:(Lambda bestLambda population score)
      ;; Return the "fittest" Selector WFF.
      (setq bestLambda myBest)
      (setq M (length myPopulationIslands)) 
      (loop for m from 0 until M do
         (setq N (length myPopulationIslands[m]))
         (loop for n from 0 until N do
           (setq Lambda myPopulationIslands[m][n])
           (cond
             ((> Lambda.Score bestLambda.Score) (setq bestLambda bestLambda))
             ((< Lambda.Score bestLambda.Score) (setq bestLambda Lambda))
             ((< Lambda.ErrorPct bestLambda.ErrorPct) (setq bestLambda Lambda))
             ((< Lambda.Error bestLambda.Error) (setq bestLambda Lambda))
             ((< (length Lambda.WFF) (length bestLambda.WFF)) (setq bestLambda Lambda))
             ) ; end cond
           ) ; end M loop
         ) ; end M loop
      ;; Insert the best Lambda into the Best-of-Breed island population
      (setq population myPopulationIslands[myBestOfBreedIsland])
      (setq myBest bestLambda)
      (setq myBestSelector bestLambda)
      (setq score bestLambda.Score)
      (setq N (length population))
      (loop for n from 0 until N do 
        (if (< score population[n].Score) (goto ReadyToInsert:))
        (if (or (= bestLambda population[0]) (and (= score population[0].Score) (= Lambda.WFF population[0].WFF))) (return bestLambda))
        ); end insertion loop
      ReadyToInsert::
      (insert population n bestLambda)
      ;; Kill off all except the most fit selector WFFs.
      (if (> (setq N (length population)) mySurvivors) then (resize population mySurvivors))
      bestLambda) ; end bestSelector
   ;; Checks a numeric Selector WFF for validity.
   (defun checkNumericWFF(wff) (ruleExp.checkNumericWFF wff))
   ;; Clear all persistent memory.
   (defun clear()
      ;; Redirect important public methods
	  (setq checkNumericWFF ruleExp.Pv.checkNumericWFF)
	  (setq cutoutNumericWFF ruleExp.Pv.cutoutNumericWFF)
	  (setq extractNumericWFF ruleExp.Pv.extractNumericWFF)
	  (setq growWFF ruleExp.Pv.growWFF)
	  (setq growCTermWFF ruleExp.Pv.growCTermWFF)
	  (setq growRootWFF ruleExp.Pv.growRootWFF)
	  (setq growTermWFF ruleExp.Pv.growTermWFF)
	  (setq lengthWFF ruleExp.Pv.lengthWFF)
	  (setq marryNumericWFFs ruleExp.Pv.marryNumericWFFs)
	  (setq mutateNumericWFF ruleExp.Pv.mutateNumericWFF)
	  (setq spliceNumericWFF ruleExp.Pv.spliceNumericWFF)
      ;; Clear important public variables
	  (setq	myG 0)
	  (setq	myM	0)
	  (setq	myN	0)
      (setq myEvolutionCount 0)
      (setq myTournamentCount 0)
      (setq myCheckPoint false)
	  (setq	myNumberPct	.50)
	  (setq	myParent #void)
	  (setq	myPopulation #void)
	  (setq	myPopulationIslands #void)
	  (setq	myS	0)
	  (setq	myWFFs #void)
	  (setq	myX	#void)
	  (setq	myY	#void)
      (setq myBestT #void)
      (setq myNextID 0)
      (setq myBestSelectorChampions #void)
      (setq myBestSelectorBestOfBest #void)
      (setq myBestSelector #void)
      (setq myBestRegressorChampions #void)
      (setq myBestRegressor #void)
      true) ; end clear
   ;; Compile a selector wff and return a Selector Lambda.
   (defun compileSelector(wffSource) (onError (lambda(err) #void)) (eval (compile (selector wffSource))))
   ;; Copy a selector wff so that a completely unique list of Pair objects is returned.
   ;; Note: Always returns a list of Pair objects.
   (defun copyWff(wff)
      vars:(newWff)
      (if (isString wff) 
          (setq newWff (lisp wff)[0]) 
          (setq newWff (lisp (string wff true))[0])
          ) ; end if
      newWff) ; end copyWff
   ;; Create and score a selector wff over each training time period.
   ;; Note: An optional second argument provides the previous genome (for use by particle swarm operators).
   (defun createSelector(wff ...)
      regs:(n N)
      vars:(Lambda source score errPct result prevGenome ID)
      vars:(regPopulation wffSource wffString wffPair stemp genome rule)
      (onError (lambda(msg) false))
      (if (>= (argCount) 2) (setq prevGenome (argFetch 1)))
      ;; Do not score if the WFF is too long or if we have seen this WFF before.
      ;; Note: Within the limitations of memory, we save each WFF so that 
      ;;       we will not waste time retesting WFFs we have already tested.
      (if (= wff #void) (return false))
      (if (isPair wff) 
          (begin (setq rule wff[0]) (setq genome wff[1])) 
          (begin (setq rule (setq wffPair (listWff wff))[0]) (setq genome wffPair[1]))
          ) ; end if
      (setq wffString (stringWff wff))
      (if (and (<> esm[rule].lengthExempt true) (> (length wffString) myWFFLengthLimit)) (return false))
      ;; Have we ever seen this WFF before?
      (if (<> (setq ID myWFFs[wffString]) #void)
          then
          ;; We have compiled and scored this WFF before.
          (begin
             (setq Lambda (findSelectorID ID))
             (if (= Lambda false) (return false))
          ) else
          ;; We have never compiled or scored this WFF before.
          (begin
             ;; Compile the selector WFF and save into the already-seen-list, until memory limitations are exceeded.
             (setq result (new Structure: ID:myNextID Score:999999999.0 WFF:wffString))
             (++ myNextID)
             (if (and (= myWFFSaveON true) (< (sizeof myWFFs) myWFFSaveLimit)) then (setq myWFFs[wffString] result.ID) else (setq myWFFSaveON false))         
             (setq wffSource (evalRule wff))
             (setq Lambda (compileSelector wffSource))
             (if (not (isLambda Lambda)) (begin (if myVerboseSW (writeln "esm.createSelector: compile failure warning on [" wffSource "]")) (return false)))
             ;; Score the WFF against the training data.
             (setq Lambda.Rule rule)
             (if (= (setq result (scoreWff Lambda result genome)) false) then (return false))
             (setq Lambda.pGenome prevGenome)
             (setq Lambda.Rule rule)
          )) ; end seen before if
      ;; Enter the selector WFF into the fittness regressor population.
      ;; Note1: We save these statistics:= Number:A Number:B Number:AvgYSq Number:Score Number:Error NumVector:EY Svm History WFF
      ;; Note2: We do NOT allow duplicates.
      ReadyToSaveRegressor::
      (setq regPopulation myPopulationIslands[myBestRegressorIsland])
      (setq errPct Lambda.ErrorPct)
      (setq N (length regPopulation))
      (loop for n from 0 until N do 
        (if (< errPct regPopulation[n].ErrorPct) (goto ReadyToInsertRegressor:))
        (if (and (= errPct regPopulation[n].ErrorPct) (= Lambda.WFF regPopulation[n].WFF)) (goto ReadyToSaveSelector:))
        (if (and (= mySurvivorStyle equalOFF:) (= errPct regPopulation[n].ErrorPct)) (goto ReadyToSaveSelector:))
        ); end insertion loop
      ReadyToInsertRegressor::
      (insert regPopulation n Lambda)
      ;; Kill off all except the most fit selector WFFs.
      (if (> (setq N (length regPopulation)) mySurvivors) then (resize regPopulation mySurvivors))
      ;; Enter the selector WFF into the fittness population.
      ;; Note1: We save these statistics:= Number:A Number:B Number:AvgYSq Number:Score Number:Error NumVector:EY Svm History WFF
      ;; Note2: We do NOT allow duplicates.
      ReadyToSaveSelector::
      (setq score Lambda.Score)
      (setq N (length myPopulation))
      (loop for n from 0 until N do 
        (if (< score myPopulation[n].Score) (goto ReadyToInsertSelector:))
        (if (and (= score myPopulation[n].Score) (= Lambda.WFF myPopulation[n].WFF)) (return false))
        (if (and (= mySurvivorStyle equalOFF:) (= score myPopulation[n].Score)) (return false))
        ); end insertion loop
      ReadyToInsertSelector::
      (insert myPopulation n Lambda)
      ;; Kill off all except the most fit selector WFFs.
      (if (> (setq N (length myPopulation)) mySurvivors) then (resize myPopulation mySurvivors))
      ;; Save the Best-Of-Breed Selector WFFs for each time period.
      (loop for n from 0 until myT do (if (< Lambda.History[n] myBestT[n].History[n]) (setq myBestT[n] Lambda))) 
      Lambda) ; end createSelector
   ;; Returns a numeric Selector WFF by randomly cutting out a sub-expression from a candidate Selector WFF {"(x3/sin(x4))" ==> "sin(x4)"}.
   (defun cutoutNumericWFF(wff ...) (if (= (argCount) 1) (ruleExp.cutoutNumericWFF wff) (ruleExp.cutoutNumericWFF wff (argFetch 1))))
   ;; Evaluate an ESM grammar production rule.
   (defun evalRule(rule)
      vars:(ruleHdr)
      (if (isString rule) (setq rule (listWff rule)))
      (if (isPair rule) (setq rule (apply (setq ruleHdr esm.ruleExp[(car rule)]) (cdr rule))) (setq rule (string rule true)))
      rule) ; end evalRule
   ;; Return a numeric Selector WFF from any Selector WFF {"regress (x3/x13);" ==> "(x3/x13)"}.
   (defun extractNumericWFF(wff ...) (if (= (argCount) 1) (ruleExp.extractNumericWFF wff) (ruleExp.extractNumericWFF wff (argFetch 1))))
   ;; Return the Selector Lambda, with the specified ID, by searching all of the distinct island populations.
   (defun findSelectorID(Integer:id)
      regs:(m M n N)
      vars:(Lambda)
      ;; Search independent island populations.
      (setq M (length myPopulationIslands)) 
      (loop for m from 0 until M do
         (setq N (length myPopulationIslands[m]))
         (loop for n from 0 until N do
           (setq Lambda myPopulationIslands[m][n])
           (if (= Lambda.ID id) then (return Lambda))
           ) ; end M loop
         ) ; end M loop
      ;; Search best regressor champions
      (setq N (length myBestRegressorChampions))
      (loop for n from 0 until N do
        (setq Lambda myBestRegressorChampions[n])
        (if (= Lambda.ID id) then (return Lambda))
        ) ; end M loop
      ;; Search best selector champions
      (setq N (length myBestSelectorChampions))
      (loop for n from 0 until N do
        (setq Lambda myBestSelectorChampions[n])
        (if (= Lambda.ID id) then (return Lambda))
        ) ; end M loop
      ;; Search best of best champions
      (setq N (length myBestSelectorBestOfBest))
      (loop for n from 0 until N do
        (setq Lambda myBestSelectorBestOfBest[n])
        (if (= Lambda.ID id) then (return Lambda))
        ) ; end M loop
      false) ; end findSelectorID
   ;; Returns a randomly grown Selector WFF. 
   (defun growWFF(Symbol:rule Integer:level) (ruleExp.growWFF rule level))
   ;; Returns a sequentially grown regression conditional term Selector WFF based upon a chosen column.
   (defun growCTermWFF(Integer:columnChoice) (ruleExp.growCTermWFF columnChoice))
   ;; Returns a sequentially grown root Selector WFF xtime, thru xm, abs(xtime) thru abs(xm), cos(xtime) thru cos(xm), etc. 
   (defun growRootWFF(Integer:rootChoice) (ruleExp.growRootWFF rootChoice))
   ;; Returns a sequentially grown regression basic term Selector WFF based upon a chosen column. 
   (defun growTermWFF(Integer:rootChoice Integer:columnChoice) (ruleExp.growTermWFF rootChoice columnChoice))
   ;; Initialize the training data into sequential time segments.
   (defun initTrainingData()
      regs:(n Number:timeStamp)
      regs:(k sn SN stepInc)
      vars:(IntVector:sortedY)
      vars:(NumVector:x timeVector mgrVector mgr (Integer:minTN 100))
      (setq myTMgrs (new Directory:))
      ;; Compute the optimal SVM suggested sample size for abbreviated quick learning.
      (if (> myN 300) (setq mySVMSampleSize (integer (min myN (max 125 (* 3 myM))))) (setq mySVMSampleSize myN))
      ;; Make sure we attached the appropriate y value to each matching x vector.
      (loop for n from 0 until myN do
         ;; Attach the score (y) to the cdr of each training example (x)
         (setq x myX[n])
         (setCdr x myY[n])
         ;; Organize the training examples sequentially by time stamp.
         (if (= myTimeON true)
             (begin
                (setq timeStamp x[0])
                (setq timeVector myTMgrs[timeStamp])
                (if (= timeVector #void) then (setq timeVector (new Vector: Object:)))
                (setq timeVector[(length timeVector)] x)
                (setq myTMgrs[timeStamp] timeVector)
             )) ; end if
         ) ; end row loop
      ;; Make sure we have enough training examples in each sequential time stamp.
      (if (= myTimeON true) (setq myT (length myTMgrs)) (setq myT 1))
      (loop for n from 0 until myT do
         ;; If ANY time period has too few training examples, then treat them all as one time period.
         (if (or (= myTimeON false) (< (length myTMgrs[n 1]) minTN))
             (begin
                (setq myTMgrs (new Directory:))
                (setq myTMgrs[0] myX) 
                (setq myT 1)               
             )) ; end 
         ) ; end time period loop    
      ;; Create row managers for the training examples in each sequential time stamp.
      (setq mgrVector (new Vector: Object: myT))
      (setq myTN (new Vector: Integer: myT))
      (loop for n from 0 until myT do
         (vmrefdirvalue n myTMgrs timeVector)
         (setq mgr (rowManager timeVector))      
         (setq mgrVector[n] mgr)
         (setq myTN[n] mgr.rowCount)      
         ) ; end row manager loop
      (setq myTMgrs mgrVector)
      (setq myRowManager (rowManager myX))
      ;; Create a small sample of the training data for abbreviated quick learning.
      ;; Note: Compute optimal small sample size for abbreviated quick learning.
      (if (>= myN 700) (setq mySampleSize (integer (max mySampleSize (min myN (max (divi myN mySampleFactor) (* 3 myM)))))) (setq mySampleSize myN))
      (setq SN mySampleSize)
      (setq sortedY (|Gv:sort| myY < true))
      (setq myXsample (new Vector: Object: SN))
      (setq myYsample (new Vector: Number: SN))
      (setq stepInc (integer (/ myN mySampleSize)))
      (setq n 0)
      (loop for sn from 0 until SN do
		(setq k sortedY[n])
        (setq myXsample[sn] myX[k])
        (setq myYsample[sn] myY[k])
        (+= n stepInc)
        ) ; end loop 
      (setq myRowManagerSample (rowManager myXsample))
      ;; Create a smaller training data set for abbreviated quick scoring.
      ;; Note: Compute optimal small training size for abbreviated quick learning.
      (if (>= myN 700) (setq myTrainingSize (integer (max mySampleSize (min myN (max (divi myN myTrainingFactor) (* 20 myM)))))) (setq myTrainingSize myN))
      (setq SN myTrainingSize)
      (setq sortedY (|Gv:sort| myX (lambda(xa xb) (or (< xa[0] xb[0]) (and (= xa[0] xb[0]) (< (cdr xa) (cdr xb)))) ) true))
      (setq myXtrain (new Vector: Object: SN))
      (setq myYtrain (new Vector: Number: SN))
      (setq stepInc (integer (/ myN myTrainingSize)))
      (setq n 0)
      (loop for sn from 0 until SN do
		(setq k sortedY[n])
        (setq myXtrain[sn] myX[k])
        (setq myYtrain[sn] myY[k])
        (+= n stepInc)
        ) ; end loop 
      (setq myRowManagerTraining (rowManager myXtrain))
      true) ; end initTrainingData
   ;; Returns the left-depth-first length of any Selector WFF. 
   (defun lengthWFF(wff) (ruleExp.lengthWFF wff))
   ;; Convert a selector wff to a set of Pair objects (a list).
   (defun listWff(wff)
      vars:(newWff wffSource)
      (if (isString wff) (setq wffSource wff) (setq wffSource (string wff true)))
      (setq newWff (lisp wffSource))
      newWff[0]) ; end listWff
   (defun makeCrossover(father mother)
   ;; *******************************************************************
   ;; summary:  Introduce a new Selector WFF (by splicing two parent WFFs) 
   ;;           to the current population (genetic crossover).
   ;;
   ;; args:     father     The father Selector Lambda (or WFF) to be mated.           
   ;;           mother     The mother Selector Lambda (or WFF) to be mated.           
   ;;           
   ;; Return:   none.
   ;;
   ;; *******************************************************************
      vars:(rule wff)
      (if (or (= father #void) (= mother #void)) (return false))
      (if (isLambda father) (setq wff (listWff father.WFF)) (setq wff father))
      (setq rule wff[0])
      (if (= rule #void) (return false))
      (esm[rule].crossOver father mother)
      true) ; end makeCrossover 
   (defun makeMutation(mutant)
   ;; *******************************************************************
   ;; summary:  Introduce a new Selector WFF (by mutating a candidate WFF) 
   ;;           to the current population.
   ;;
   ;; args:     mutant     The candidate Selector Lambda (or WFF) to be mutated.           
   ;;           
   ;; Return:   none.
   ;;
   ;; *******************************************************************      
      vars:(rule wff)
      (if (= mutant #void) (return false))
      (if (isLambda mutant) (setq wff (listWff mutant.WFF)) (setq wff mutant))
      (setq rule wff[0])
      (if (= rule #void) (return false))
      (esm[rule].mutate mutant)
      true) ; end makeMutation 
   ;; Return a numeric Selector WFF by marrying two suitor Selector WFFs {"x3" , "x4" ==> "(x3/x4)"}. 
   (defun marryNumericWFFs(wff1 wff2 ...) (if (= (argCount) 2) (ruleExp.marryNumericWFFs wff1 wff2) (ruleExp.marryNumericWFFs wff1 wff2 (argFetch 2))))
   ;; Returns a numeric Selector WFF by randomly mutating a candidate Selector WFF {"(x3/x4)" ==> "(x3/sin(x4))"}. 
   (defun mutateNumericWFF(wff ...) (if (= (argCount) 1) (ruleExp.mutateNumericWFF wff) (ruleExp.mutateNumericWFF wff (argFetch 1))))
   ;; summary:  Performs a Gaussian multiple regression on the N by M+1 matrix
   ;; Parms:    MXY:     The N by M+1 matrix representing the original observations
   ;;                    in the form of:    x x ... x y
   ;;                                       x x ... x y
   ;;                                           ... 
   ;;                                       x x ... x y
   ;; Return:   C:       The M coefficient vector for the regression.
   ;; Note1:    Average error statistics are computed as a percent of the target (dependent variable).
   ;; Note2:    See Sedgewick[2] chap 37.
   (defun multipleRegression(NumMatrix:MXY)
       vars:(NumMatrix:Xt NumVector:C)
       ;; Perform a least squares regression on all the factors.
       (setq Xt (|Gv:makeGaussianMatrix| MXY))
       (setq Xt (|Gv:matrixGaussianEliminate| Xt true))
       (setq C (|Gv:matrixGaussianSubstitute| Xt))
       ;; Return the coefficient vector for the regression.
       C) ; end multipleRegression
   ;; Displays a status report of the checkpoint repository.
   ;; Note: If argument is present and true, then provide a full report
   (defun report(...)
      regs:(m M n N)
      vars:(Lambda command)
      ;; Load the previous checkpoint repository.
      (setq command (if (>= (argCount) 1) (argFetch 0) false))  
      (setq myRepository (new ObjectRepository: myRepositoryDefaultName))
      (loadCheckPoint checkpoint:)
      ;; Display the status of the previous checkpoint repository.
      (writeln "") 
      (writeln _eol "Report: Final results of checkpoint training repository")
      (setq Lambda myBest)
      (writeln "Generations = [" myGc "], WFFs = [" (length myWFFs)  "], Score=[" myBest.Score "], ScoreHistory=[" (string myBest.History true) "]") 
      (writeln "myBest, Score=[" myBest.Score "], ErrorPct=[" myBest.ErrorPct "], lengthWFF=[" (length myBest.WFF) "], WFF = " (if (= command true) (evalRule myBest.WFF) (left (evalRule myBest.WFF) 300)))
      (writeln "myBestRegressor, Score=[" myBestRegressor.Score "], ErrorPct=[" myBestRegressor.ErrorPct "], lengthWFF=[" (length myBestRegressor.WFF) "], WFF = " (if (= command true) (evalRule myBestRegressor.WFF) (left (evalRule myBestRegressor.WFF) 300)))
      (writeln "myEvolutionCount = [" myEvolutionCount "], myTournamentCount = [" myTournamentCount "]")
      (if (= command true)
          ;; Full report
          (begin
            (writeln "myBestSelectorChampions")
            (loop for n from (subi (length myBestSelectorChampions) 1) to 0 by -1 do (writeln "myBestSelectorChampions[" n "],Score=[" myBestSelectorChampions[n 1].Score ",ErrorPct=[" myBestSelectorChampions[n 1].ErrorPct "],WFF= "(evalRule myBestSelectorChampions[n 1].WFF))) 
            (writeln "myBestRegressorChampions")
            (loop for n from 0 until (length myBestRegressorChampions) do (writeln "myBestRegressorChampions[" n "],ErrorPct=[" myBestRegressorChampions[n 1].ErrorPct ",Score=[" myBestRegressorChampions[n 1].Score "],WFF= "(evalRule myBestRegressorChampions[n 1].WFF))) 
            (writeln "myBestRegressorBestOfBest")
            (loop for n from 0 until (length myBestRegressorBestOfBest) do (writeln "myBestRegressorBestOfBest[" n "],ErrorPct=[" myBestRegressorBestOfBest[n 1].ErrorPct ",Score=[" myBestRegressorBestOfBest[n 1].Score "],WFF= "(evalRule myBestRegressorBestOfBest[n 1].WFF))) 
          ) else
          ;; Limited report
          (begin
            (writeln "myBestSelectorChampions")
            (loop for n from (subi (length myBestSelectorChampions) 1) to 0 by -1 do (writeln "myBestSelectorChampions[" n "],Score=[" myBestSelectorChampions[n 1].Score ",ErrorPct=[" myBestSelectorChampions[n 1].ErrorPct "],WFF= " (left (evalRule myBestSelectorChampions[n 1].WFF) 300))) 
            (writeln "myBestRegressorChampions")
            (loop for n from 0 until (length myBestRegressorChampions) do (writeln "myBestRegressorChampions[" n "],ErrorPct=[" myBestRegressorChampions[n 1].ErrorPct ",Score=[" myBestRegressorChampions[n 1].Score "],WFF= " (left (evalRule myBestRegressorChampions[n 1].WFF) 300))) 
            (writeln "myBestRegressorBestOfBest")
            (loop for n from 0 until (length myBestRegressorBestOfBest) do (writeln "myBestRegressorBestOfBest[" n "],ErrorPct=[" myBestRegressorBestOfBest[n 1].ErrorPct ",Score=[" myBestRegressorBestOfBest[n 1].Score "],WFF= " (left (evalRule myBestRegressorBestOfBest[n 1].WFF) 300))) 
          )) ; end full report if
      ;; Show the champions from each island population.
      (if (= command true)
          (begin
            (writeln "") 
            (writeln "Show the champions from each island population")
            (setq M (length myPopulationIslands))
            (loop for m from 0 until M do
              (writeln "Champions of Island [" m "], RootWFFCount=[" myIslands[m] "]...")
              (setq N (length myPopulationIslands[m])) 
              (loop for n from 0 until N do
                (setq Lambda myPopulationIslands[m][n])
                (if (<> Lambda #void) (writeln "myPopulationIslands[" m "][" n "], Score=[" Lambda.Score "," Lambda.ErrorPct "], lengthWFF=[" (length Lambda.WFF) "], WFF = " (evalRule Lambda.WFF))) 
                ) ; end population loop
              ) ; end island loop
          )) ; end full report if
      true) ; end report
   ;; Save the best unique numeric WFFs.
   (defun saveUniqueNumericWffs(Integer:maxWFFs)
      regs:(m n N)
      vars:(wffBest wffString1 wffString2)
      ;; Save the "best" five WFFs.
      (setq wffBest (new Vector: Object: 5))
      (setq wffString1 (string (extractNumericWFF myPopulation[0].WFF) true))
      (setq wffBest[0] (lisp wffString1)[0])
      (setq N (length myPopulation))
      (setq m 0)
      (loop for n from 1 until N do
         (setq wffString2 (string (extractNumericWFF myPopulation[n].WFF) true))
         (if (<> wffString1 wffString2) (setq wffBest[(++ m)] (lisp wffString2)[0]))
         (if (>= m 4) (setq n N))
         ) ; end N loop
      wffBest) ; end saveUniqueNumericWffs
   ;; Compute the score for a selector wff over each training time period.
   ;; Note: We return these statistics:= Number:A Number:B Number:AvgYSq Number:Score Number:Error NumVector:EY Svm History WFF
   (defun scoreWff(Lambda result genome)
      regs:(n N NN Number:ey Number:errPct Number:err)
      vars:(NumVector:history NumVector:EY mgr score worstBandScore)
      (onError (lambda(msg) false))

	  ;; Score the Lambda on the basis of the requested scoring style.
      (cond
       ;; Score the Lambda, first using the SAMPLE training data, then using the FULL training data set as an entire table, ignoring each time period as a separate table.
       ((= mySamplingON true)
        (begin
	       ;; Initialize the result structure and train the Lambda to be scored.
	       ;; Note: Some Selector Lambdas are trained on a smaller sample of the training data.
           (setq n (integer (max 0 (sub1 (setq NN (length myPopulation))))))
           (if (= NN mySurvivors)
               (setq worstBandScore (number myPopulation[n].BandScore))
               (setq worstBandScore 999999999.0)
               ) ; end if
	       (if (= (Lambda.train myXsample myYsample genome) false) (return false))
		   (setq Lambda.ID result.ID)
		   (setq Lambda.Score result.Score)
		   (setq Lambda.WFF result.WFF)
		   (setq Lambda.Genome genome)
		   (setq Lambda.Error (abs Lambda.Error))
		   (setq Lambda.AvgYSq (abs Lambda.AvgYSq))
		   (setq Lambda.EY #void)      
		   (setq Lambda.History (setq history (new Vector: Number: 1)))
		   (setq mgr myRowManagerTraining)
		   (setq score (mgr.score Lambda))
		   (if (= score false) (goto ReturnFailure:))
		   (setq score (VALIDATE score))
		   (setq Lambda.BandScore score)
		   (setq history[0] score)
		   (setq err Lambda.ErrorPct)
		   (setq err (VALIDATE err))
		   (setq errPct err)
		   (if (<= score worstBandScore) 
               (begin
                 ;; Continue with full scoring
                 (if (and (> myTrainingSize mySampleSize) (= (Lambda.train myXtrain myYtrain genome) false)) (return false))
               ) else
               ;; Discontinue any further scoring
               (return false)
               ) ; end if
           (if (= myTimeON true) (goto ScoreTimeON:) (goto ScoreTimeOFF:))
         )) ; end (mySamplingON) full training data, multiple time period case
       ;; Score the Lambda, using the FULL training data set as an entire table, ignoring each time period as a separate table.
       ((= myTimeON false)
        (begin
	       ;; Initialize the result structure and train the Lambda to be scored.
	       ;; Note: Some Selector Lambdas are trained on a smaller sample of the training data.
	       (if (= (Lambda.train myX myY genome) false) (return false))
           ScoreTimeOFF::
		   (setq Lambda.ID result.ID)
		   (setq Lambda.Score result.Score)
		   (setq Lambda.WFF result.WFF)
		   (setq Lambda.Genome genome)
		   (setq Lambda.Error (abs Lambda.Error))
		   (setq Lambda.AvgYSq (abs Lambda.AvgYSq))
		   (setq Lambda.EY #void)      
		   (setq Lambda.History (setq history (new Vector: Number: 1)))
		   (setq mgr myRowManager)
		   (setq score (mgr.score Lambda))
		   (if (= score false) (goto ReturnFailure:))
		   (setq score (VALIDATE score))
		   (setq history[0] score)
		   (setq err Lambda.ErrorPct)
		   (setq err (VALIDATE err))
		   (setq errPct err)
         )) ; end (timeOFF) full training data, single time period case
       ;; Score the Lambda on each time period table using the FULL training data set.
       (else ;; timeON
        (begin
	       ;; Initialize the result structure and train the Lambda to be scored.
	       ;; Note: Some Selector Lambdas are trained on a smaller sample of the training data.
	       (if (= (Lambda.train myX myY genome) false) (return false))
           ScoreTimeON::
		   (setq Lambda.ID result.ID)
		   (setq Lambda.Score result.Score)
		   (setq Lambda.WFF result.WFF)
		   (setq Lambda.Genome genome)
		   (setq Lambda.Error (abs Lambda.Error))
		   (setq Lambda.AvgYSq (abs Lambda.AvgYSq))
		   (setq Lambda.EY #void)      
		   (setq Lambda.History (setq history (new Vector: Number: myT)))
           (setq errPct 0.0)
		   (loop for n from 0 until myT do
		      (setq mgr myTMgrs[n])
		      (setq score (mgr.score Lambda))
		      (if (= score false) (goto ReturnFailure:))
		      (setq score (VALIDATE score))
		      (setq history[n] score)
		      (setq err Lambda.ErrorPct)
		      (setq err (VALIDATE err))
		      (+= errPct err)
		      ) ; end scoring loop
		   (/= errPct (number myT))
		   (setq errPct (VALIDATE errPct))
           (setq score (+ 1.0 history[0]))
           (loop for n from 1 until myT do (*= score (+ 1.0 history[n])))
		   (setq score (VALIDATE score))
		   (setq score (expt score (/ 1.0 (number myT))))
           (-= score 1.0)		
         )) ; end (timeON) full training data, multiple time period case
        ) ; end cond
      (setq Lambda.Score score)
      (setq Lambda.ErrorPct errPct)
      (if (and (> Lambda.Error 0.0) (= Lambda.ErrorPct 0.0)) 
          (begin
            ReturnFailure::
            (setq Lambda.BandScore result.Score)
            (setq Lambda.Score result.Score)
            (return false)
          )) ; end failure if
      (setq result.Score Lambda.Score)
      result) ; end scoreWff
   ;; Returns a numeric Selector WFF by randomly splicing one WFF into a candidate Selector WFF {"(x3/sin(x4))" , "log(x10)" ==> "(log(x10)/sin(x4))"}.
   (defun spliceNumericWFF(wff swff ...) (if (= (argCount) 2) (ruleExp.spliceNumericWFF wff swff) (ruleExp.spliceNumericWFF wff swff (argFetch 2))))
   ;; Convert a selector wff to a string (from a list of Pair objects).
   (defun stringWff(wff)
      vars:(wffSource)
      (if (isString wff) (setq wffSource wff) (setq wffSource (string wff true)))
      wffSource) ; end stringWff
   ;; Convert a selector wff to a sorted string (from a list of Pair objects).
   (defun stringSortedWff(wff)
      regs:(m M)
      vars:(wffSource rule genome)
      vars:(chromosome0 chromosome1)
      vars:(collisionSW (collision #{dic|| ruleBgm true ruleFrm true ruleMvl true ruleReg false ruleEnn true ruleSvm true }))
      (if (isString wff) (setq wff (lisp wff)[0]))
      (if (or (not (isPair wff)) (<> (length wff) 2)) (error "esm.stringSortedWff: invalid WFF"))
      (setq rule wff[0])
      (setq genome (sort (copy wff[1]) >))
      ;; Eliminate collisions for those rules which demand collision protection.
      (if (= (setq collisionSW collision[rule]) #void) (error (append "esm.stringSortedWff: unknown regression rule [" rule "]")))
      (if (= collisionSW true)
          (begin
             (setq M (length genome))
             (loop for m from 1 until M do
               (setq chromosome0 genome[(- m 1)])
               (setq chromosome1 genome[m])
               (if (and (<> chromosome1 #void) (= chromosome0 chromosome1))
                   (setq genome[(- m 1)] #void)
                   ) ; end if
               ) ; end loop   
             (setq genome (sort genome >))
          )) ; end collision if
      (setq wffSource (string (list rule genome) true))
      wffSource) ; end stringSortedWff
   ;; Truncate a selector wff to the specified maximum display length.
   (defun trunc(s len) (if (> (length s) len) then (setq s (append (left s len) " ... ;"))) s)
   ;; Turns the verbose testing mode off.
   (defun verboseOFF() (setq myVerboseSW false))
   ;; Turns the verbose testing mode on.
   (defun verboseON() (setq myVerboseSW true))
   ;; *******************************************************************************
   ;; Define Private Child Lambdas 
   ;; *******************************************************************************
   ;; Export all embedded ontology objects from this Lambda.
   (defun exportDocs(...)
      regs:(k K m M n N)
      vars:(embeddedOntologyNames embeddedOntologyPrefix tempNames currentFocus)
      vars:(ontologyObject (cabinetPrefix ";#text#") (cabinetPrefixLength 7))
      vars:((targetCabinetName "Esm") (targetLambdaName "esm") exportFolder)

	  ;; Convert the argument and initialize.
      (setq exportFolder (if (= (argCount) 0) "C:\\RefGuide\\Foundry\\" (argFetch 0)))
	  (setq exportFolder (string exportFolder))
	  (setq embeddedOntologyPrefix (append targetLambdaName ":%%"))
	
      ;; Load the names of all embedded Ontology objects.
      (setq currentFocus (browseLib.getFocus))
      (browseLib.setFocus targetCabinetName)
      (setq tempNames (browseLib.getChildNames))
      (browseLib.setFocus currentFocus)
      (setq K (length embeddedOntologyPrefix))
      (setq N (length tempNames))
      (setq m -1)(setq embeddedOntologyNames (new Vector:))
      (loop for n from 0 until N do (if (stringCiEQ (left tempNames[n] K) embeddedOntologyPrefix) (setq embeddedOntologyNames[(++ m)] (mid tempNames[n] K 100000))))  
      (setq N (length embeddedOntologyNames))

      ;; Export all embedded Ontology objects (only export no HTML generation).
      (setq N (length embeddedOntologyNames))
      (loop for n from 0 until N do
         (setq ontologyObject (browseLib.checkout targetCabinetName (append embeddedOntologyPrefix embeddedOntologyNames[n])))
         (setq ontologyObject (mid ontologyObject cabinetPrefixLength 10000000))
         (browseLib.writeSourceFile (append exportFolder embeddedOntologyNames[n] ".xml") ontologyObject)
         ) ; end loop
      true) ; end exportDocs
   ;; Load the checkpoint information from a previous training run.
   (defun loadCheckPoint(keyName)
      vars:(checkPointInfo)
      ;; Load the previous checkpoint information.
      (setq keyName (symbol keyName))
      (setq checkPointInfo myRepository[keyName])
      
      (if (isStructure checkPointInfo)
          (begin
             (setq myBest checkPointInfo.myBest)                                    ;; The current best-of-breed Selector WFF for all columns.
             (setq myBestOfBreedIsland checkPointInfo.myBestOfBreedIsland)          ;; The current best-of-breed island index.
             (setq myBestRegressor checkPointInfo.myBestRegressor)               	;; The current best-of-breed Regressor WFF for all columns.
             (setq myBestRegressorChampions checkPointInfo.myBestRegressorChampions);; The best-of-breed Regressor WFF, ever seen, for all columns.
             (setq myBestRegressorIsland checkPointInfo.myBestOfBreedIsland)      	;; The current best-of-breed regressor island index.
             (setq myBestSelector checkPointInfo.myBestSelector)                 	;; The current best-of-breed Selector WFF for all columns.
             (setq myBestSelectorChampions checkPointInfo.myBestSelectorChampions)  ;; The best-of-breed Regressor WFF, ever seen, for all columns.
             (setq myBestSelectorBestOfBest checkPointInfo.myBestSelectorBestOfBest);; The best-of-breed Regressor WFF, ever seen, for all columns.
             (setq myBestT checkPointInfo.myBestT)                               	;; The current best-of-breed Selector WFF for each column.
             (setq myEvolutionCount checkPointInfo.myEvolutionCount)                ;; The evolutionary processes count.
             (setq myGc checkPointInfo.myGc)                                     	;; The current generation counter at the time of this checkpoint.
             (setq myIslands checkPointInfo.myIslands)                           	;; The current term counters for each column.
             (setq myNextID checkPointInfo.myNextID)                             	;; The next unique Selector Lambda identifier to be issued.
             (setq myPopulation checkPointInfo.myPopulation)                     	;; The current population of selector Lambdas.
             (setq myPopulationIslands checkPointInfo.myPopulationIslands)       	;; The current population of selector Lambdas for each column.
             (setq myTournamentCount checkPointInfo.myTournamentCount)              ;; The tournament-of-champions count.
             (setq myWFFs checkPointInfo.myWFFs)                                 	;; The current Dictionary of all selector WFFs ever seen.
          )) ; end if

      checkPointInfo) ; end loadCheckPoint
   ;; Load the root island populations from the previous initialization step in this training run.
   (defun loadRootIslands(keyName)
      vars:(checkPointInfo)
      ;; Load the previous root island populations information.
      (setq keyName (symbol keyName))
      (setq checkPointInfo myRepository[keyName])
      
      (if (isStructure checkPointInfo)
          (begin
             (setq myBest checkPointInfo.myBest)                                    ;; The current best-of-breed Selector WFF for all columns.
             (setq myBestOfBreedIsland checkPointInfo.myBestOfBreedIsland)          ;; The current best-of-breed island index.
             (setq myBestRegressor checkPointInfo.myBestRegressor)               	;; The current best-of-breed Regressor WFF for all columns.
             (setq myBestRegressorIsland checkPointInfo.myBestOfBreedIsland)      	;; The current best-of-breed regressor island index.
             (setq myBestSelector checkPointInfo.myBestSelector)                 	;; The current best-of-breed Selector WFF for all columns.
             (setq myBestT checkPointInfo.myBestT)                               	;; The current best-of-breed Selector WFF for each column.
             (setq myIslands checkPointInfo.myIslands)                           	;; The current term counters for each column.
             (setq myNextID checkPointInfo.myNextID)                             	;; The next unique Selector Lambda identifier to be issued.
             (setq myPopulationIslands checkPointInfo.myPopulationIslands)       	;; The current population of selector Lambdas for each column.
             (setq myWFFs checkPointInfo.myWFFs)                                 	;; The current Dictionary of all selector WFFs ever seen.
          )) ; end if

      checkPointInfo) ; end loadRootIslands
   ;; Generate the private WFF language used for genetic programming of selector Lambdas Note: (esm.makeSelector)).
   (defun makeSelector()
      (ParseLib "Esm" "esm:selector")
      true) ; end makeSelector
   ;; Pseudo random n umber routine.
   (defun random(upperLimit)
      regs:(n N)
      vars:((elements #(0.5  0.25  0.125  0.0625  0.03125  0.015625  0.0078125  0.00390625  0.001953125  0.0009765625  0.00048828125  0.000244140625  0.0001220703125  6.103515625E-005  3.0517578125E-005  1.52587890625E-005  7.62939453125E-006  3.814697265625E-006  1.907348632813E-006  9.536743164063E-007  4.768371582031E-007  2.384185791016E-007  1.192092895508E-007  5.960464477539E-008  2.98023223877E-008  1.490116119385E-008  7.450580596924E-009  3.725290298462E-009  1.862645149231E-009  9.313225746155E-010  4.656612873077E-010  2.328306436539E-010  1.164153218269E-010  5.820766091347E-011  2.910383045673E-011  1.455191522837E-011  7.275957614183E-012  3.637978807092E-012  1.818989403546E-012  9.094947017729E-013  4.547473508865E-013  2.273736754432E-013  1.136868377216E-013  5.684341886081E-014  2.84217094304E-014  1.42108547152E-014  7.105427357601E-015)))
      vars:(result)
      ;; Support existing or argument persistent variables structure.
      (setq N (length elements))
      (loop for n from 0 until N do (if (>= (|Gv:srandom| 1.0) .5) (+= result elements[n])))
      (*= result upperLimit)
      result) ; end random
   ;; Save the checkpoint information from the current training run.
   (defun saveCheckPoint(keyName ...)
      vars:(data checkPointInfo)
      ;; Support existing or argument persistent variables structure.
      (if (= (argCount) 1) (setq data (myself)[Pv:]) (setq data (argFetch 1)))
      (setq keyName (symbol keyName))

      ;; Save the current checkpoint information.
      (setq checkPointInfo  (new Structure: myBest:                   data.myBest       		    	;; The current best-of-breed Selector WFF for all columns.
                                            myBestOfBreedIsland:      data.myBestOfBreedIsland      	;; The current best-of-breed island index.
                                            myBestRegressor:          data.myBestRegressor          	;; The current best-of-breed Regressor WFF for all columns.
                                            myBestRegressorChampions: data.myBestRegressorChampions     ;; The best-of-breed Regressor WFF, ever seen, for all columns.
                                            myBestRegressorIsland:    data.myBestRegressorIsland   	 	;; The current best-of-breed regressor island index.
                                            myBestSelector:           data.myBestSelector           	;; The current best-of-breed Selector WFF for all columns.
                                            myBestSelectorChampions:  data.myBestSelectorChampions      ;; The best-of-breed Selector WFF, ever seen, for all columns.
                                            myBestSelectorBestOfBest: data.myBestSelectorBestOfBest     ;; The best-of-breed Selector WFF, ever seen, for all columns.
                                            myBestT:     	          data.myBestT                  	;; The current best-of-breed Selector WFF for each column.
                                            myEvolutionCount:         data.myEvolutionCount             ;; The evolutionary processes count.
                                            myGc:   	          	  data.myGc    			        	;; The current generation counter at the time of this checkpoint.
                                            myIslands:                data.myIslands                	;; The current term counters for each column.
                                            myNextID:                 data.myNextID                 	;; The next unique Selector Lambda identifier to be issued.
                                            myPopulation:   	      data.myPopulation			    	;; The current population of selector Lambdas.
                                            myPopulationIslands:   	  data.myPopulationIslands			;; The current population of selector Lambdas for each column.
                                            myTournamentCount:        data.myTournamentCount            ;; The tournament-of-champions count.
                                            myWFFs:   	          	  data.myWFFs    			    	;; The current Dictionary of all selector WFFs ever seen.
                                          )) 

      (setq myRepository[keyName] checkPointInfo)
      true) ; end saveCheckPoint
   ;; Save the root island populations from the current initialization step in this training run.
   (defun saveRootIslands(keyName)
      vars:(data checkPointInfo)
      ;; Support existing or argument persistent variables structure.
      (if (= (argCount) 1) (setq data (myself)[Pv:]) (setq data (argFetch 1)))
      (setq keyName (symbol keyName))

      ;; Save the current checkpoint information.
      (setq checkPointInfo  (new Structure: myBest:                   data.myBest       		    	;; The current best-of-breed Selector WFF for all columns.
                                            myBestOfBreedIsland:      data.myBestOfBreedIsland      	;; The current best-of-breed island index.
                                            myBestRegressor:          data.myBestRegressor          	;; The current best-of-breed Regressor WFF for all columns.
                                            myBestRegressorIsland:    data.myBestRegressorIsland   	 	;; The current best-of-breed regressor island index.
                                            myBestSelector:           data.myBestSelector           	;; The current best-of-breed Selector WFF for all columns.
                                            myBestT:     	          data.myBestT                  	;; The current best-of-breed Selector WFF for each column.
                                            myIslands:                data.myIslands                	;; The current term counters for each column.
                                            myNextID:                 data.myNextID                 	;; The next unique Selector Lambda identifier to be issued.
                                            myPopulationIslands:   	  data.myPopulationIslands			;; The current population of selector Lambdas for each column.
                                            myWFFs:   	          	  data.myWFFs    			    	;; The current Dictionary of all selector WFFs ever seen.
                                          )) 

      (setq myRepository[keyName] checkPointInfo)
      true) ; end saveRootIslands
   ;; Set predefined default option information, by name, for the current training run.
   (defun setOptions(name randomError verboseSW)
      ;; Set the Default User Defined Learning Options
      (setq myBestAverage 1)  	    	;; The count of best selector champions to average in creating the final myBest selector Lambda.
      (setq myBestOfBest 10)            ;; The maximum number of best of the best selector champions to save.
      (setq myBGMMaximum 0003)     		;; The maximum number of chromosomes to allowed in any one BGM regression.
      (setq myChromosomeGEN 0000)       ;; The maximum random root-chromosome genomes from different columns to grow during each generation step.
      (setq myChromosomeINIT 0000)      ;; The maximum random root-chromosome genomes from different columns to grow during the initialization step.
	  (setq myChromosomeRule MVL:)      ;; The rule used for root-chromosome search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
      (setq myCrossChromosomeSW false)  ;; The use of chromosomes from different columns during cross over operations.
      (setq myCrossColPct 0.00)  		;; The probability of cross over in each column island during each generation step.
      (setq myCrossLinearSW false)  	;; The use of a linear grammar during cross over operations.
      (setq myCrossMarrySW true)  	    ;; The use of expression marrying during cross over operations.
      (setq	myCrossRegPct 0.00)  		;; The probability of cross over in the best-regressor island during each generation step.
      (setq myCrossSelPct 1.00)  		;; The probability of cross over in the best-of-breed island during each generation step.
      (setq myCrossSpliceSW true)  	    ;; The use of expression splicing during cross over operations.
      (setq myENNLayers 0003)     		;; The maximum number of hidden layers to grow in each ENN machine.
      (setq myFRMMaximum 1000)     		;; The maximum number of chromosomes to allowed in any one FRM regression.
      (setq myGreedyDepth 00)    		;; The depth of greedy search exploration of each chosen column island  during each generation step. 
      (setq myGreedyGEN  000)    		;; The number of greedy search WFFs the system will grow in the best-of-breed island  during each generation step. 
      (setq myGreedyINIT 0000)    	    ;; The number of greedy search WFFs the system will grow in the best-of-breed island  during the initialization step. 
      (setq myGreedyRule MVL:)          ;; The rule used for greedy search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
      (setq myGreedyWidth narrow:)    	;; The width of each greedy search WFF the system will grow in the best-of-breed island (base, full, narrow, random). 
      (setq myGrowColINIT 00)   		;; The number of random WFFs the system will grow in each column island during the initialization step.
      (setq myGrowColGEN 00)    		;; The number of random WFFs the system will grow in each column island during each generation step. 
      (setq myGrowColRule REG:)         ;; The rule used for random WFFs grown in each column island (BGM, FRM, MVL, ENN, REG, or SVM). 
      (setq myGrowSelINIT 0000)   		;; The number of random WFFs the system will grow in the best-of-breed island during the initialization step.
      (setq myGrowSelGEN 00)    		;; The number of random WFFs the system will grow in the best-of-breed island during each generation step. 
      (setq myGrowSelRule MVL:)         ;; The rule used for random WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
      (setq myGrowWFFStyle root:)       ;; The style used for random WFFs grown in the best-of-breed island (full, or root). 
      (setq myMaxColWFFLen 05)  		;; The maximum length of a single WFF for each column.
      (setq myMigratePct 0.00)   		;; The probability of Column island mutation during each generation step.
      (setq myMutateColPct 0.00) 		;; The probability of mutation in each column island during each generation step.
      (setq myMutateRegPct 0.00) 		;; The probability mutation in the best-regressor island during each generation step.
      (setq myMutateSelPct 1.00) 		;; The probability mutation in the best-of-breed island during each generation step.
      (setq myMutateSpliceSW true)  	;; The use of expression splicing during mutation operations.
      (setq myMVLMaximum 1000)     		;; The maximum number of chromosomes to allowed in any one MVL regression.
      (setq myParetoErrorSW false)      ;; The switch to support pareto front as an error fitness measure ( true or false).
      (setq myRandomError randomError)  ;; The amount of random error to add to each selfTest test case (0.00 thru .50).
      (setq myREGMultiple true)         ;; Support multiple columns when growing REG regression expressions.
      (setq myREGOperatorJoin "*")      ;; The operator used to join multiple columns when growing REG regression expressions ("+" or "*").
      (setq myREGMaximum 1000)     		;; The maximum number of chromosomes to allowed in any one REG regression.
      (setq myRestartGap 00010) 		;; The count of generations to allow, without measurable improvement, before initiating a new evolutionary process.
      (setq myRootINIT 00)  	 		;; The number of root WFFs the system will grow in each column island during the initialization step.
      (setq myRootGEN 00)       		;; The number of root WFFs the system will grow in each column island during each generation step. 
      (setq myRootRule REG:)            ;; The rule used for root WFFs grown in each column island (BGM, FRM, MVL, ENN, REG, or SVM). 
	  (setq mySampleFactor 100)         ;; The reduction factor for determining the size of the sample training data (when mySampleON is true).
      (setq mySamplingON  true)   	    ;; Support initial scoring on a small sample set before scoring on the full training data set.
      (setq myScoreFocus full:)  	    ;; The focus of scoring on the training data (full, top25, top10, or top05).
      (setq mySeedDefault 2407987.0)    ;; The default seed number used to initialize the random number genenerator at the start of each training run.
      (setq mySurvivors  25)     		;; The count of surviving selector WFFs in each generation.
      (setq mySurvivorStyle equalOFF:)  ;; The option allowing surviving selector WFFs to have equal scores (equalON, or equalOFF).
      (setq mySvmKernelID cube:)       	;; The kernelID to use for all SVM learning (see svmRegress: all, binary, bipolar, composite, cosine, cube, exp, linear, log, poly, quart, sigmoid, sine, square, tan, tanh).
      (setq mySVMMaximum 1000)     		;; The maximum number of chromosomes to allowed in any one SVM regression.
      (setq mySvmMaxGen 1)             	;; The maximum number of training generations to use for all SVM learning (see svmRegress).
      (setq mySvmMaxLayers 1)           ;; The maximum number of training layers to use for all SVM learning (see svmRegress).
      (setq mySvmModelCount 1)          ;; The maximum number of training models to use for all SVM learning (see svmRegress).
      (setq myTournamentSize 05)		;; The size of the myBestSelectorChampions queue which initiates a tournament-of-champions when the next evolutionary process begins.
      (setq myTimeON false)  			;; Support separate sequence scoring of each individual time period.
	  (setq myTrainingFactor 10)        ;; The reduction factor for determining the size of the final training data (when mySampleON is true).
      (setq myUseBGM false)             ;; Grow BGM selector candidates during the initialization step and during each generation step.
      (setq myUseENN false)             ;; Grow ENN selector candidates during the initialization step and during each generation step.
      (setq myUseFRM false)             ;; Grow FRM selector candidates during the initialization step and during each generation step.
      (setq myUseMVL false)             ;; Grow MVL selector candidates during the initialization step and during each generation step.
      (setq myUseREG false)             ;; Grow REG selector candidates during the initialization step and during each generation step.
      (setq myUseSVM false)             ;; Grow SVM selector candidates during the initialization step and during each generation step.
      (setq myVerboseSW verboseSW)	 	;; The switch controlling verbose mode during testing, maintennance, and development.
      (setq myWFFSaveSW true)	        ;; The switch controlling the saving of WFFs already evaluated.
      (setq myWFFReinitSW true)         ;; The switch controlling the reinitialization of WFF cache at the start of every new run.

      ;; Set the Named User Defined Learning Options
      (cond 
       ;; Set the defaultBGM option settings.
       ;; Note: These options work well for classification and sequencing of individuals in the the top 5%.
       ((= name defaultBGM:)
        (begin
          (setq myBGMMaximum 0003)     		;; The maximum number of chromosomes to allowed in any one BGM regression.
          (setq myGrowSelINIT 00010)   		;; The number of random WFFs the system will grow in the best-of-breed island during the initialization step.
          (setq myGrowSelGEN 00)    		;; The number of random WFFs the system will grow in the best-of-breed island during each generation step. 
          (setq myGrowSelRule BGM:)         ;; The rule used for random WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq mySamplingON false)   	    ;; Support initial scoring on a small sample set before scoring on the full training data set.
          (setq myScoreFocus top05:)  	    ;; The focus of scoring on the training data (full, top25, top10, or top05).
          (setq myTimeON  true)  			;; Support separate sequence scoring of each individual time period.
          (setq myUseBGM  true)             ;; Grow BGM selector candidates during the initialization step and during each generation step.
        )) ; end defaultFRM case
       ;; Set the defaultENN option settings.
       ;; Note: These options work well for classification and sequencing of individuals in the the top 5%.
       ((= name defaultENN:)
        (begin
          (setq myENNLayers 0003)     		;; The maximum number of hidden layers to grow in each ENN machine.
          (setq myGrowSelINIT 01000)   		;; The number of random WFFs the system will grow in the best-of-breed island during the initialization step.
          (setq myGrowSelGEN 00)    		;; The number of random WFFs the system will grow in the best-of-breed island during each generation step. 
          (setq myGrowSelRule ENN:)         ;; The rule used for random WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq mySamplingON true)   	    ;; Support initial scoring on a small sample set before scoring on the full training data set.
          (setq myScoreFocus top05:)  	    ;; The focus of scoring on the training data (full, top25, top10, or top05).
          (setq myTimeON  true)  			;; Support separate sequence scoring of each individual time period.
          (setq myUseENN  true)             ;; Grow ENN selector candidates during the initialization step and during each generation step.
        )) ; end defaultFRM case
       ;; Set the defaultFRM option settings.
       ;; Note: These options work well for time series, factored, multivariate regressions.
       ((= name defaultFRM:)
        (begin
          (setq myGrowSelINIT 5000)   		;; The number of random WFFs the system will grow in the best-of-breed island during the initialization step.
          (setq myGrowSelGEN 00)    		;; The number of random WFFs the system will grow in the best-of-breed island during each generation step. 
          (setq myGrowSelRule FRM:)         ;; The rule used for random WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myUseFRM  true)             ;; Grow FRM selector candidates during the initialization step and during each generation step.
        )) ; end defaultFRM case
       ;; Set the defaultMVL option settings.
       ;; Note: These options work well for general multivariate regressions.
       ((or (= name default:) (= name defaultMVL:))
        (begin
          (setq myCrossChromosomeSW false)  ;; The use of chromosomes from different columns during cross over operations.
	      (setq myCrossColPct 0.00)  		;; The probability of cross over in each column island during each generation step.
          (setq myCrossMarrySW true)  	    ;; The use of expression marrying during cross over operations.
      	  (setq myCrossSelPct 1.00)  		;; The probability of cross over in the best-of-breed island during each generation step.
          (setq myCrossSpliceSW true)  		;; The use of expression splicing during cross over operations.
	      (setq myGreedyDepth 25)    		;; The depth of greedy search exploration of each chosen column island  during each generation step. 
	      (setq myGreedyGEN 100)    		;; The number of greedy search WFFs the system will grow in the best-of-breed island  during each generation step. 
	      (setq myGreedyWidth random:)    	;; The width of each greedy search WFF the system will grow in the best-of-breed island (base, full, narrow, random). 
	      (setq myGreedyRule MVL:)          ;; The rule used for greedy search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
	      (setq myGrowColINIT 0000)   		;; The number of random WFFs the system will grow in each column island during the initialization step.
	      (setq myGrowColGEN 000)    		;; The number of random WFFs the system will grow in each column island during each generation step. 
	      (setq myGrowColRule REG:)         ;; The rule used for random WFFs grown in each column island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myGrowSelINIT 1000)   		;; The number of random WFFs the system will grow in the best-of-breed island during the initialization step.
          (setq myGrowSelGEN 00)    		;; The number of random WFFs the system will grow in the best-of-breed island during each generation step. 
          (setq myGrowSelRule MVL:)         ;; The rule used for random WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
	      (setq myMaxColWFFLen 100)  		;; The maximum length of a single WFF for each column.
	      (setq myMutateColPct 0.00) 		;; The probability of mutation in each column island during each generation step.
	      (setq myMutateSelPct 0.10) 		;; The probability mutation in the best-of-breed island during each generation step.
          (setq myMutateSpliceSW true)  	;; The use of expression splicing during mutation operations.
          (setq myREGMultiple false)        ;; Support multiple columns when growing REG regression expressions.
	      (setq myREGOperatorJoin "+")      ;; The operator used to join multiple columns when growing REG regression expressions ("+" or "*").
          (setq myRestartGap 0010)  		;; The count of generations to allow, without measurable improvement, before initiating a new evolutionary process.
          (setq myRootINIT 1000)  	 		;; The number of root WFFs the system will grow in each column island during the initialization step.
          (setq myRootGEN 100)       		;; The number of root WFFs the system will grow in each column island during each generation step. 
          (setq myRootRule REG:)            ;; The rule used for root WFFs grown in each column island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myTimeON false)  			;; Support separate sequence scoring of each individual time period.
          (setq myUseMVL  true)             ;; Grow MVL selector candidates during the initialization step and during each generation step.
          (setq myUseREG  true)             ;; Grow REG selector candidates during the initialization step and during each generation step.
        )) ; end defaultMVL case
       ;; Set the regressMVL option settings.
       ;; Note: These options work well for general multivariate regressions.
       ((= name regressMVL:)
        (begin
      	  (setq myCrossSelPct 1.00)  		;; The probability of cross over in the best-of-breed island during each generation step.
          (setq myGrowSelINIT 5000)   		;; The number of random WFFs the system will grow in the best-of-breed island during the initialization step.
          (setq myGrowSelINIT 0010)   		;; (MFK) The number of random WFFs the system will grow in the best-of-breed island during the initialization step.
          (setq myGrowSelGEN 00)    		;; The number of random WFFs the system will grow in the best-of-breed island during each generation step. 
          (setq myGrowSelRule MVL:)         ;; The rule used for random WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
	      (setq myMutateSelPct 0.10) 		;; The probability mutation in the best-of-breed island during each generation step.
          (setq mySurvivors 025)     		;; The count of surviving selector WFFs in each generation.
          (setq myTimeON false)  			;; Support separate sequence scoring of each individual time period.
          (setq myUseMVL  true)             ;; Grow MVL selector candidates during the initialization step and during each generation step.
        )) ; end regressMVL case
       ;; Set the regress2MVL option settings.
       ;; Note: These options work well for general multivariate regressions.
       ((= name regress2MVL:)
        (begin
      	  (setq myCrossSelPct 1.00)  		;; The probability of cross over in the best-of-breed island during each generation step.
          (setq myGrowSelINIT 5000)   		;; The number of random WFFs the system will grow in the best-of-breed island during the initialization step.
          (setq myGrowSelGEN 00)    		;; The number of random WFFs the system will grow in the best-of-breed island during each generation step. 
          (setq myGrowSelRule MVL:)         ;; The rule used for random WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myMutateSelPct 1.00) 		;; The probability mutation in the best-of-breed island during each generation step.
          (setq mySurvivors 500)     		;; The count of surviving selector WFFs in each generation.
          (setq myTimeON false)  			;; Support separate sequence scoring of each individual time period.
          (setq myUseMVL  true)             ;; Grow MVL selector candidates during the initialization step and during each generation step.
        )) ; end regress2MVL case
       ;; Set the regress3MVL option settings.
       ;; Note: These options work well for general multivariate regressions.
       ((= name regress3MVL:)
        (begin
      	  (setq myCrossSelPct 1.00)  		;; The probability of cross over in the best-of-breed island during each generation step.
          (setq myGrowSelINIT 5000)   		;; The number of random WFFs the system will grow in the best-of-breed island during the initialization step.
          (setq myGrowSelGEN 00)    		;; The number of random WFFs the system will grow in the best-of-breed island during each generation step. 
          (setq myGrowSelRule MVL:)         ;; The rule used for random WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myMutateSelPct 1.00) 		;; The probability mutation in the best-of-breed island during each generation step.
          (setq mySurvivors 010)     		;; The count of surviving selector WFFs in each generation.
          (setq myTimeON false)  			;; Support separate sequence scoring of each individual time period.
          (setq myUseMVL  true)             ;; Grow MVL selector candidates during the initialization step and during each generation step.
        )) ; end regress3MVL case
       ;; Set the regress4MVL option settings.
       ;; Note: These options work well for general multivariate regressions.
       ((= name regress4MVL:)
        (begin
      	  (setq myCrossSelPct 1.00)  		;; The probability of cross over in the best-of-breed island during each generation step.
          (setq myGrowSelINIT 0500)   		;; The number of random WFFs the system will grow in the best-of-breed island during the initialization step.
          (setq myGrowSelGEN 0100)    		;; The number of random WFFs the system will grow in the best-of-breed island during each generation step. 
          (setq myGrowSelRule MVL:)         ;; The rule used for random WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myMutateSelPct 1.00) 		;; The probability mutation in the best-of-breed island during each generation step.
          (setq mySurvivors 025)     		;; The count of surviving selector WFFs in each generation.
          (setq myTimeON false)  			;; Support separate sequence scoring of each individual time period.
          (setq myUseMVL  true)             ;; Grow MVL selector candidates during the initialization step and during each generation step.
        )) ; end regress4MVL case
       ;; Set the regress5MVL option settings.
       ;; Note: These options work well for general multivariate regressions.
       ((= name regress5MVL:)
        (begin
      	  (setq myCrossSelPct 1.00)  		;; The probability of cross over in the best-of-breed island during each generation step.
          (setq myGrowSelINIT 1000)   		;; The number of random WFFs the system will grow in the best-of-breed island during the initialization step.
          (setq myGrowSelGEN 00)    		;; The number of random WFFs the system will grow in the best-of-breed island during each generation step. 
          (setq myGrowSelRule MVL:)         ;; The rule used for random WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
	      (setq myMutateSelPct 0.10) 		;; The probability mutation in the best-of-breed island during each generation step.
          (setq mySurvivors 010)     		;; The count of surviving selector WFFs in each generation.
          (setq myTimeON false)  			;; Support separate sequence scoring of each individual time period.
          (setq myUseMVL  true)             ;; Grow MVL selector candidates during the initialization step and during each generation step.
        )) ; end regress5MVL case
       ;; Set the regress6MVL option settings.
       ;; Note: These options work well for general multivariate regressions.
       ((= name regress6MVL:)
        (begin
      	  (setq myCrossSelPct 1.00)  		;; The probability of cross over in the best-of-breed island during each generation step.
          (setq myGrowSelINIT 0500)   		;; The number of random WFFs the system will grow in the best-of-breed island during the initialization step.
          (setq myGrowSelGEN 00)    		;; The number of random WFFs the system will grow in the best-of-breed island during each generation step. 
          (setq myGrowSelRule MVL:)         ;; The rule used for random WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
	      (setq myMutateSelPct 0.10) 		;; The probability mutation in the best-of-breed island during each generation step.
          (setq mySurvivors 010)     		;; The count of surviving selector WFFs in each generation.
          (setq myTimeON false)  			;; Support separate sequence scoring of each individual time period.
          (setq myUseMVL  true)             ;; Grow MVL selector candidates during the initialization step and during each generation step.
        )) ; end regress6MVL case
       ;; Set the regress7MVL option settings.
       ;; Note: These options work well for general multivariate regressions.
       ((= name regress7MVL:)
        (begin
      	  (setq myCrossSelPct 1.00)  		;; The probability of cross over in the best-of-breed island during each generation step.
          (setq myGrowSelINIT 0200)   		;; The number of random WFFs the system will grow in the best-of-breed island during the initialization step.
          (setq myGrowSelGEN 00)    		;; The number of random WFFs the system will grow in the best-of-breed island during each generation step. 
          (setq myGrowSelRule MVL:)         ;; The rule used for random WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
	      (setq myMutateSelPct 0.10) 		;; The probability mutation in the best-of-breed island during each generation step.
          (setq mySurvivors 010)     		;; The count of surviving selector WFFs in each generation.
          (setq myTimeON false)  			;; Support separate sequence scoring of each individual time period.
          (setq myUseMVL  true)             ;; Grow MVL selector candidates during the initialization step and during each generation step.
        )) ; end regress7MVL case
       ;; Set the regress8MVL option settings.
       ;; Note: These options work well for general multivariate regressions.
       ((= name regress8MVL:)
        (begin
      	  (setq myCrossSelPct 1.00)  		;; The probability of cross over in the best-of-breed island during each generation step.
          (setq myGrowSelINIT 0100)   		;; The number of random WFFs the system will grow in the best-of-breed island during the initialization step.
          (setq myGrowSelGEN 00)    		;; The number of random WFFs the system will grow in the best-of-breed island during each generation step. 
          (setq myGrowSelRule MVL:)         ;; The rule used for random WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myGrowWFFStyle root:)       ;; The style used for random WFFs grown in the best-of-breed island (full, or root). 
	      (setq myMutateSelPct 0.10) 		;; The probability mutation in the best-of-breed island during each generation step.
          (setq mySurvivors 100)     		;; The count of surviving selector WFFs in each generation.
          (setq myTimeON false)  			;; Support separate sequence scoring of each individual time period.
          (setq myUseMVL  true)             ;; Grow MVL selector candidates during the initialization step and during each generation step.
        )) ; end regress8MVL case
       ;; Set the regress9MVL option settings.
       ;; Note: These options work well for general multivariate regressions.
       ((= name regress9MVL:)
        (begin
      	  (setq myCrossSelPct 1.00)  		;; The probability of cross over in the best-of-breed island during each generation step.
          (setq myGrowSelINIT 0100)   		;; The number of random WFFs the system will grow in the best-of-breed island during the initialization step.
          (setq myGrowSelGEN 00)    		;; The number of random WFFs the system will grow in the best-of-breed island during each generation step. 
          (setq myGrowSelRule MVL:)         ;; The rule used for random WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myGrowWFFStyle root:)       ;; The style used for random WFFs grown in the best-of-breed island (full, or root). 
	      (setq myMutateSelPct 0.10) 		;; The probability mutation in the best-of-breed island during each generation step.
          (setq mySurvivors 005)     		;; The count of surviving selector WFFs in each generation.
          (setq myTimeON false)  			;; Support separate sequence scoring of each individual time period.
          (setq myUseMVL  true)             ;; Grow MVL selector candidates during the initialization step and during each generation step.
        )) ; end regress9MVL case
       ;; Set the regress10MVL option settings.
       ;; Note: These options work well for general multivariate regressions.
       ((= name regress10MVL:)
        (begin
      	  (setq myCrossSelPct 1.00)  		;; The probability of cross over in the best-of-breed island during each generation step.
          (setq myGrowSelINIT 0200)   		;; The number of random WFFs the system will grow in the best-of-breed island during the initialization step.
          (setq myGrowSelGEN 00)    		;; The number of random WFFs the system will grow in the best-of-breed island during each generation step. 
          (setq myGrowSelRule MVL:)         ;; The rule used for random WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myCrossLinearSW  true)  	;; The use of a linear grammar during cross over operations.
	      (setq myMutateSelPct 0.10) 		;; The probability mutation in the best-of-breed island during each generation step.
          (setq mySurvivors 010)     		;; The count of surviving selector WFFs in each generation.
          (setq myTimeON false)  			;; Support separate sequence scoring of each individual time period.
          (setq myUseMVL  true)             ;; Grow MVL selector candidates during the initialization step and during each generation step.
        )) ; end regress10MVL case
       ;; Set the regressGreedyMVL option settings.
       ;; Note: These options work well for general multivariate regressions.
       ((= name regressGreedyMVL:)
        (begin
          (setq myChromosomeGEN 0000)       ;; The maximum random root-chromosome genomes from different columns to grow during each generation step.
          (setq myChromosomeINIT 0200)      ;; The maximum random root-chromosome genomes from different columns to grow during the initialization step.
	      (setq myChromosomeRule MVL:)      ;; The rule used for root-chromosome search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myCrossChromosomeSW false)  ;; The use of chromosomes from different columns during cross over operations.
	      (setq myCrossColPct 0.00)  		;; The probability of cross over in each column island during each generation step.
          (setq myCrossLinearSW  true)  	;; The use of a linear grammar during cross over operations.
          (setq myCrossMarrySW true)  	    ;; The use of expression marrying during cross over operations.
      	  (setq myCrossSelPct 1.00)  		;; The probability of cross over in the best-of-breed island during each generation step.
          (setq myCrossSpliceSW true)  		;; The use of expression splicing during cross over operations.
          (setq myGreedyDepth 10)    		;; The depth of greedy search exploration of each chosen column island during each generation step. 
          (setq myGreedyGEN  000)    		;; The number of greedy search WFFs the system will grow in the best-of-breed island  during each generation step. 
          (setq myGreedyINIT 0000)    	    ;; The number of greedy search WFFs the system will grow in the best-of-breed island  during the initialization step. 
          (setq myGreedyRule MVL:)          ;; The rule used for greedy search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myGreedyWidth narrow:)    	;; The width of each greedy search WFF the system will grow in the best-of-breed island (base, full, narrow, random). 
	      (setq myMaxColWFFLen 100)  		;; The maximum length of a single WFF for each column.
	      (setq myMutateColPct 0.00) 		;; The probability of mutation in each column island during each generation step.
	      (setq myMutateSelPct 0.10) 		;; The probability mutation in the best-of-breed island during each generation step.
          (setq myMutateSpliceSW true)  	;; The use of expression splicing during mutation operations.
          (setq myREGMultiple false)        ;; Support multiple columns when growing REG regression expressions.
	      (setq myREGOperatorJoin "+")      ;; The operator used to join multiple columns when growing REG regression expressions ("+" or "*").
          (setq myRestartGap 0010)  		;; The count of generations to allow, without measurable improvement, before initiating a new evolutionary process.
          (setq myRootINIT  5000)  	    	;; The number of root WFFs the system will grow in each column island during the initialization step.
          (setq myRootGEN 00)       		;; The number of root WFFs the system will grow in each column island during each generation step. 
          (setq myRootRule REG:)            ;; The rule used for root WFFs grown in each column island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq mySurvivors 010)     		;; The count of surviving selector WFFs in each generation.
          (setq myTimeON false)  			;; Support separate sequence scoring of each individual time period.
          (setq myUseMVL  true)             ;; Grow MVL selector candidates during the initialization step and during each generation step.
          (setq myUseREG  true)             ;; Grow REG selector candidates during the initialization step and during each generation step.
        )) ; end regressGreedyMVL case
       ;; Set the regressGreedySMVL option settings.
       ;; Note: These options work well for general multivariate regressions.
       ((= name regressGreedySMVL:)
        (begin
          (setq myChromosomeGEN 0000)       ;; The maximum random root-chromosome genomes from different columns to grow during each generation step.
          (setq myChromosomeINIT 0200)      ;; The maximum random root-chromosome genomes from different columns to grow during the initialization step.
	      (setq myChromosomeRule MVL:)      ;; The rule used for root-chromosome search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myCrossChromosomeSW false)  ;; The use of chromosomes from different columns during cross over operations.
	      (setq myCrossColPct 0.00)  		;; The probability of cross over in each column island during each generation step.
          (setq myCrossLinearSW  true)  	;; The use of a linear grammar during cross over operations.
          (setq myCrossMarrySW true)  	    ;; The use of expression marrying during cross over operations.
      	  (setq myCrossSelPct 1.00)  		;; The probability of cross over in the best-of-breed island during each generation step.
          (setq myCrossSpliceSW true)  		;; The use of expression splicing during cross over operations.
          (setq myGreedyDepth 10)    		;; The depth of greedy search exploration of each chosen column island during each generation step. 
          (setq myGreedyGEN  000)    		;; The number of greedy search WFFs the system will grow in the best-of-breed island  during each generation step. 
          (setq myGreedyINIT 0000)    	    ;; The number of greedy search WFFs the system will grow in the best-of-breed island  during the initialization step. 
          (setq myGreedyRule MVL:)          ;; The rule used for greedy search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myGreedyWidth narrow:)    	;; The width of each greedy search WFF the system will grow in the best-of-breed island (base, full, narrow, random). 
	      (setq myMaxColWFFLen 100)  		;; The maximum length of a single WFF for each column.
	      (setq myMutateColPct 0.00) 		;; The probability of mutation in each column island during each generation step.
	      (setq myMutateSelPct 0.10) 		;; The probability mutation in the best-of-breed island during each generation step.
          (setq myMutateSpliceSW true)  	;; The use of expression splicing during mutation operations.
          (setq myREGMultiple false)        ;; Support multiple columns when growing REG regression expressions.
	      (setq myREGOperatorJoin "+")      ;; The operator used to join multiple columns when growing REG regression expressions ("+" or "*").
          (setq myRestartGap 0010)  		;; The count of generations to allow, without measurable improvement, before initiating a new evolutionary process.
          (setq myRootINIT  5000)  	    	;; The number of root WFFs the system will grow in each column island during the initialization step.
          (setq myRootGEN 00)       		;; The number of root WFFs the system will grow in each column island during each generation step. 
          (setq myRootRule REG:)            ;; The rule used for root WFFs grown in each column island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq mySurvivors 010)     		;; The count of surviving selector WFFs in each generation.
          (setq myTimeON false)  			;; Support separate sequence scoring of each individual time period.
          (setq myUseMVL  true)             ;; Grow MVL selector candidates during the initialization step and during each generation step.
          (setq myUseREG  true)             ;; Grow REG selector candidates during the initialization step and during each generation step.
          (setq myWFFReinitSW false)        ;; The switch controlling the reinitialization of WFF cache at the start of every new run.
        )) ; end regressGreedySMVL case
       ;; Set the regressGreedy2MVL option settings.
       ;; Note: These options work well for general multivariate regressions.
       ((= name regressGreedy2MVL:)
        (begin
          (setq myChromosomeGEN 0000)       ;; The maximum random root-chromosome genomes from different columns to grow during each generation step.
          (setq myChromosomeINIT 1000)      ;; The maximum random root-chromosome genomes from different columns to grow during the initialization step.
	      (setq myChromosomeRule MVL:)      ;; The rule used for root-chromosome search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myCrossChromosomeSW false)  ;; The use of chromosomes from different columns during cross over operations.
	      (setq myCrossColPct 0.00)  		;; The probability of cross over in each column island during each generation step.
          (setq myCrossMarrySW true)  	    ;; The use of expression marrying during cross over operations.
      	  (setq myCrossSelPct 1.00)  		;; The probability of cross over in the best-of-breed island during each generation step.
          (setq myCrossSpliceSW true)  		;; The use of expression splicing during cross over operations.
          (setq myGreedyDepth 10)    		;; The depth of greedy search exploration of each chosen column island during each generation step. 
          (setq myGreedyGEN  000)    		;; The number of greedy search WFFs the system will grow in the best-of-breed island  during each generation step. 
          (setq myGreedyINIT 1000)    	    ;; The number of greedy search WFFs the system will grow in the best-of-breed island  during the initialization step. 
          (setq myGreedyRule MVL:)          ;; The rule used for greedy search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myGreedyWidth narrow:)    	;; The width of each greedy search WFF the system will grow in the best-of-breed island (base, full, narrow, random). 
	      (setq myMaxColWFFLen 100)  		;; The maximum length of a single WFF for each column.
	      (setq myMutateColPct 0.00) 		;; The probability of mutation in each column island during each generation step.
	      (setq myMutateSelPct 0.10) 		;; The probability mutation in the best-of-breed island during each generation step.
          (setq myMutateSpliceSW true)  	;; The use of expression splicing during mutation operations.
          (setq myREGMultiple false)        ;; Support multiple columns when growing REG regression expressions.
	      (setq myREGOperatorJoin "+")      ;; The operator used to join multiple columns when growing REG regression expressions ("+" or "*").
          (setq myRestartGap 0010)  		;; The count of generations to allow, without measurable improvement, before initiating a new evolutionary process.
          (setq myRootINIT  1000)  	    	;; The number of root WFFs the system will grow in each column island during the initialization step.
          (setq myRootGEN 00)       		;; The number of root WFFs the system will grow in each column island during each generation step. 
          (setq myRootRule REG:)            ;; The rule used for root WFFs grown in each column island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq mySurvivors 025)     		;; The count of surviving selector WFFs in each generation.
          (setq myTimeON false)  			;; Support separate sequence scoring of each individual time period.
          (setq myUseMVL  true)             ;; Grow MVL selector candidates during the initialization step and during each generation step.
          (setq myUseREG  true)             ;; Grow REG selector candidates during the initialization step and during each generation step.
        )) ; end regressGreedy2MVL case
       ;; Set the regressGreedy3MVL option settings.
       ;; Note: These options work well for general multivariate regressions.
       ((= name regressGreedy3MVL:)
        (begin
          (setq myChromosomeGEN 0000)       ;; The maximum random root-chromosome genomes from different columns to grow during each generation step.
          (setq myChromosomeINIT 1000)      ;; The maximum random root-chromosome genomes from different columns to grow during the initialization step.
	      (setq myChromosomeRule MVL:)      ;; The rule used for root-chromosome search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myCrossChromosomeSW false)  ;; The use of chromosomes from different columns during cross over operations.
	      (setq myCrossColPct 0.00)  		;; The probability of cross over in each column island during each generation step.
          (setq myCrossMarrySW true)  	    ;; The use of expression marrying during cross over operations.
      	  (setq myCrossSelPct 1.00)  		;; The probability of cross over in the best-of-breed island during each generation step.
          (setq myCrossSpliceSW true)  		;; The use of expression splicing during cross over operations.
          (setq myGreedyDepth 10)    		;; The depth of greedy search exploration of each chosen column island during each generation step. 
          (setq myGreedyGEN  000)    		;; The number of greedy search WFFs the system will grow in the best-of-breed island  during each generation step. 
          (setq myGreedyINIT 1000)    	    ;; The number of greedy search WFFs the system will grow in the best-of-breed island  during the initialization step. 
          (setq myGreedyRule MVL:)          ;; The rule used for greedy search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myGreedyWidth narrow:)    	;; The width of each greedy search WFF the system will grow in the best-of-breed island (base, full, narrow, random). 
	      (setq myMaxColWFFLen 100)  		;; The maximum length of a single WFF for each column.
	      (setq myMutateColPct 0.00) 		;; The probability of mutation in each column island during each generation step.
	      (setq myMutateSelPct 0.10) 		;; The probability mutation in the best-of-breed island during each generation step.
          (setq myMutateSpliceSW true)  	;; The use of expression splicing during mutation operations.
          (setq myREGMultiple false)        ;; Support multiple columns when growing REG regression expressions.
	      (setq myREGOperatorJoin "+")      ;; The operator used to join multiple columns when growing REG regression expressions ("+" or "*").
          (setq myRestartGap 0010)  		;; The count of generations to allow, without measurable improvement, before initiating a new evolutionary process.
          (setq myRootINIT  1000)  	    	;; The number of root WFFs the system will grow in each column island during the initialization step.
          (setq myRootGEN 00)       		;; The number of root WFFs the system will grow in each column island during each generation step. 
          (setq myRootRule REG:)            ;; The rule used for root WFFs grown in each column island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq mySurvivors 010)     		;; The count of surviving selector WFFs in each generation.
          (setq myTimeON false)  			;; Support separate sequence scoring of each individual time period.
          (setq myUseMVL  true)             ;; Grow MVL selector candidates during the initialization step and during each generation step.
          (setq myUseREG  true)             ;; Grow REG selector candidates during the initialization step and during each generation step.
        )) ; end regressGreedy3MVL case
       ;; Set the regressGreedy4MVL option settings.
       ;; Note: These options work well for general multivariate regressions.
       ((= name regressGreedy4MVL:)
        (begin
          (setq myChromosomeGEN 0000)       ;; The maximum random root-chromosome genomes from different columns to grow during each generation step.
          (setq myChromosomeINIT 1000)      ;; The maximum random root-chromosome genomes from different columns to grow during the initialization step.
	      (setq myChromosomeRule MVL:)      ;; The rule used for root-chromosome search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myCrossChromosomeSW false)  ;; The use of chromosomes from different columns during cross over operations.
	      (setq myCrossColPct 0.00)  		;; The probability of cross over in each column island during each generation step.
          (setq myCrossMarrySW true)  	    ;; The use of expression marrying during cross over operations.
      	  (setq myCrossSelPct 1.00)  		;; The probability of cross over in the best-of-breed island during each generation step.
          (setq myCrossSpliceSW true)  		;; The use of expression splicing during cross over operations.
          (setq myGreedyDepth 10)    		;; The depth of greedy search exploration of each chosen column island during each generation step. 
          (setq myGreedyGEN  000)    		;; The number of greedy search WFFs the system will grow in the best-of-breed island  during each generation step. 
          (setq myGreedyINIT 1000)    	    ;; The number of greedy search WFFs the system will grow in the best-of-breed island  during the initialization step. 
          (setq myGreedyRule MVL:)          ;; The rule used for greedy search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myGreedyWidth narrow:)    	;; The width of each greedy search WFF the system will grow in the best-of-breed island (base, full, narrow, random). 
          (setq myGrowColINIT 1000)   		;; The number of random WFFs the system will grow in each column island during the initialization step.
          (setq myGrowColGEN 00)    		;; The number of random WFFs the system will grow in each column island during each generation step. 
          (setq myGrowColRule REG:)         ;; The rule used for random WFFs grown in each column island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myGrowSelINIT 0000)   		;; The number of random WFFs the system will grow in the best-of-breed island during the initialization step.
          (setq myGrowSelGEN 00)    		;; The number of random WFFs the system will grow in the best-of-breed island during each generation step. 
          (setq myGrowSelRule MVL:)         ;; The rule used for random WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myGrowWFFStyle root:)       ;; The style used for random WFFs grown in the best-of-breed island (full, or root). 
	      (setq myMaxColWFFLen 100)  		;; The maximum length of a single WFF for each column.
	      (setq myMutateColPct 0.00) 		;; The probability of mutation in each column island during each generation step.
	      (setq myMutateSelPct 0.10) 		;; The probability mutation in the best-of-breed island during each generation step.
          (setq myMutateSpliceSW true)  	;; The use of expression splicing during mutation operations.
          (setq myREGMultiple false)        ;; Support multiple columns when growing REG regression expressions.
	      (setq myREGOperatorJoin "+")      ;; The operator used to join multiple columns when growing REG regression expressions ("+" or "*").
          (setq myRestartGap 0010)  		;; The count of generations to allow, without measurable improvement, before initiating a new evolutionary process.
          (setq myRootINIT  0000)  	    	;; The number of root WFFs the system will grow in each column island during the initialization step.
          (setq myRootGEN 00)       		;; The number of root WFFs the system will grow in each column island during each generation step. 
          (setq myRootRule REG:)            ;; The rule used for root WFFs grown in each column island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq mySurvivors 010)     		;; The count of surviving selector WFFs in each generation.
          (setq myTimeON false)  			;; Support separate sequence scoring of each individual time period.
          (setq myUseMVL  true)             ;; Grow MVL selector candidates during the initialization step and during each generation step.
          (setq myUseREG  true)             ;; Grow REG selector candidates during the initialization step and during each generation step.
        )) ; end regressGreedy4MVL case
       ;; Set the regressGreedy5MVL option settings.
       ;; Note: These options work well for general multivariate regressions.
       ((= name regressGreedy5MVL:)
        (begin
          (setq myChromosomeGEN 0000)       ;; The maximum random root-chromosome genomes from different columns to grow during each generation step.
          (setq myChromosomeINIT 0200)      ;; The maximum random root-chromosome genomes from different columns to grow during the initialization step.
	      (setq myChromosomeRule MVL:)      ;; The rule used for root-chromosome search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myCrossChromosomeSW false)  ;; The use of chromosomes from different columns during cross over operations.
	      (setq myCrossColPct 0.00)  		;; The probability of cross over in each column island during each generation step.
          (setq myCrossLinearSW  true)  	;; The use of a linear grammar during cross over operations.
          (setq myCrossMarrySW true)  	    ;; The use of expression marrying during cross over operations.
      	  (setq myCrossSelPct 1.00)  		;; The probability of cross over in the best-of-breed island during each generation step.
          (setq myCrossSpliceSW true)  		;; The use of expression splicing during cross over operations.
          (setq myGreedyDepth 10)    		;; The depth of greedy search exploration of each chosen column island during each generation step. 
          (setq myGreedyGEN  000)    		;; The number of greedy search WFFs the system will grow in the best-of-breed island  during each generation step. 
          (setq myGreedyINIT 1000)    	    ;; The number of greedy search WFFs the system will grow in the best-of-breed island  during the initialization step. 
          (setq myGreedyRule MVL:)          ;; The rule used for greedy search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myGreedyWidth narrow:)    	;; The width of each greedy search WFF the system will grow in the best-of-breed island (base, full, narrow, random). 
	      (setq myMaxColWFFLen 100)  		;; The maximum length of a single WFF for each column.
	      (setq myMutateColPct 0.00) 		;; The probability of mutation in each column island during each generation step.
	      (setq myMutateSelPct 1.00) 		;; The probability mutation in the best-of-breed island during each generation step.
          (setq myMutateSpliceSW true)  	;; The use of expression splicing during mutation operations.
          (setq myREGMultiple false)        ;; Support multiple columns when growing REG regression expressions.
	      (setq myREGOperatorJoin "+")      ;; The operator used to join multiple columns when growing REG regression expressions ("+" or "*").
          (setq myRestartGap 0010)  		;; The count of generations to allow, without measurable improvement, before initiating a new evolutionary process.
          (setq myRootINIT  1000)  	    	;; The number of root WFFs the system will grow in each column island during the initialization step.
          (setq myRootGEN 00)       		;; The number of root WFFs the system will grow in each column island during each generation step. 
          (setq myRootRule REG:)            ;; The rule used for root WFFs grown in each column island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq mySurvivors 010)     		;; The count of surviving selector WFFs in each generation.
          (setq myTimeON false)  			;; Support separate sequence scoring of each individual time period.
          (setq myUseMVL  true)             ;; Grow MVL selector candidates during the initialization step and during each generation step.
          (setq myUseREG  true)             ;; Grow REG selector candidates during the initialization step and during each generation step.
        )) ; end regressGreedy5MVL case
       ;; Set the regressGreedy6MVL option settings.
       ;; Note: These options work well for general multivariate regressions.
       ((= name regressGreedy6MVL:)
        (begin
          (setq myChromosomeGEN 0000)       ;; The maximum random root-chromosome genomes from different columns to grow during each generation step.
          (setq myChromosomeINIT 0200)      ;; The maximum random root-chromosome genomes from different columns to grow during the initialization step.
	      (setq myChromosomeRule MVL:)      ;; The rule used for root-chromosome search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myCrossChromosomeSW false)  ;; The use of chromosomes from different columns during cross over operations.
	      (setq myCrossColPct 0.00)  		;; The probability of cross over in each column island during each generation step.
          (setq myCrossLinearSW  true)  	;; The use of a linear grammar during cross over operations.
          (setq myCrossMarrySW true)  	    ;; The use of expression marrying during cross over operations.
      	  (setq myCrossSelPct 1.00)  		;; The probability of cross over in the best-of-breed island during each generation step.
          (setq myCrossSpliceSW true)  		;; The use of expression splicing during cross over operations.
          (setq myGreedyDepth 10)    		;; The depth of greedy search exploration of each chosen column island during each generation step. 
          (setq myGreedyGEN  000)    		;; The number of greedy search WFFs the system will grow in the best-of-breed island  during each generation step. 
          (setq myGreedyINIT 0000)    	    ;; The number of greedy search WFFs the system will grow in the best-of-breed island  during the initialization step. 
          (setq myGreedyRule MVL:)          ;; The rule used for greedy search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myGreedyWidth narrow:)    	;; The width of each greedy search WFF the system will grow in the best-of-breed island (base, full, narrow, random). 
	      (setq myMaxColWFFLen 100)  		;; The maximum length of a single WFF for each column.
	      (setq myMutateColPct 0.00) 		;; The probability of mutation in each column island during each generation step.
	      (setq myMutateSelPct 1.00) 		;; The probability mutation in the best-of-breed island during each generation step.
          (setq myMutateSpliceSW true)  	;; The use of expression splicing during mutation operations.
          (setq myREGMultiple false)        ;; Support multiple columns when growing REG regression expressions.
	      (setq myREGOperatorJoin "+")      ;; The operator used to join multiple columns when growing REG regression expressions ("+" or "*").
          (setq myRestartGap 0010)  		;; The count of generations to allow, without measurable improvement, before initiating a new evolutionary process.
          (setq myRootINIT  1000)  	    	;; The number of root WFFs the system will grow in each column island during the initialization step.
          (setq myRootGEN 00)       		;; The number of root WFFs the system will grow in each column island during each generation step. 
          (setq myRootRule REG:)            ;; The rule used for root WFFs grown in each column island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq mySurvivors 010)     		;; The count of surviving selector WFFs in each generation.
          (setq myTimeON false)  			;; Support separate sequence scoring of each individual time period.
          (setq myUseMVL  true)             ;; Grow MVL selector candidates during the initialization step and during each generation step.
          (setq myUseREG  true)             ;; Grow REG selector candidates during the initialization step and during each generation step.
        )) ; end regressGreedy6MVL case
       ;; Set the regressGreedy7MVL option settings.
       ;; Note: These options work well for general multivariate regressions.
       ((= name regressGreedy7MVL:)
        (begin
          (setq myChromosomeGEN 0000)       ;; The maximum random root-chromosome genomes from different columns to grow during each generation step.
          (setq myChromosomeINIT 0200)      ;; The maximum random root-chromosome genomes from different columns to grow during the initialization step.
	      (setq myChromosomeRule MVL:)      ;; The rule used for root-chromosome search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myCrossChromosomeSW false)  ;; The use of chromosomes from different columns during cross over operations.
	      (setq myCrossColPct 0.00)  		;; The probability of cross over in each column island during each generation step.
          (setq myCrossLinearSW  true)  	;; The use of a linear grammar during cross over operations.
          (setq myCrossMarrySW true)  	    ;; The use of expression marrying during cross over operations.
      	  (setq myCrossSelPct 1.00)  		;; The probability of cross over in the best-of-breed island during each generation step.
          (setq myCrossSpliceSW true)  		;; The use of expression splicing during cross over operations.
          (setq myGreedyDepth 10)    		;; The depth of greedy search exploration of each chosen column island during each generation step. 
          (setq myGreedyGEN  000)    		;; The number of greedy search WFFs the system will grow in the best-of-breed island  during each generation step. 
          (setq myGreedyINIT 0000)    	    ;; The number of greedy search WFFs the system will grow in the best-of-breed island  during the initialization step. 
          (setq myGreedyRule MVL:)          ;; The rule used for greedy search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myGreedyWidth narrow:)    	;; The width of each greedy search WFF the system will grow in the best-of-breed island (base, full, narrow, random). 
	      (setq myMaxColWFFLen 100)  		;; The maximum length of a single WFF for each column.
	      (setq myMutateColPct 0.00) 		;; The probability of mutation in each column island during each generation step.
	      (setq myMutateSelPct 0.10) 		;; The probability mutation in the best-of-breed island during each generation step.
          (setq myMutateSpliceSW true)  	;; The use of expression splicing during mutation operations.
          (setq myREGMultiple false)        ;; Support multiple columns when growing REG regression expressions.
	      (setq myREGOperatorJoin "+")      ;; The operator used to join multiple columns when growing REG regression expressions ("+" or "*").
          (setq myRestartGap 0010)  		;; The count of generations to allow, without measurable improvement, before initiating a new evolutionary process.
          (setq myRootINIT  1000)  	    	;; The number of root WFFs the system will grow in each column island during the initialization step.
          (setq myRootGEN 00)       		;; The number of root WFFs the system will grow in each column island during each generation step. 
          (setq myRootRule REG:)            ;; The rule used for root WFFs grown in each column island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq mySurvivors 010)     		;; The count of surviving selector WFFs in each generation.
          (setq myTimeON false)  			;; Support separate sequence scoring of each individual time period.
          (setq myUseMVL  true)             ;; Grow MVL selector candidates during the initialization step and during each generation step.
          (setq myUseREG  true)             ;; Grow REG selector candidates during the initialization step and during each generation step.
        )) ; end regressGreedy7MVL case
       ;; Set the regressGreedy8MVL option settings.
       ;; Note: These options work well for general multivariate regressions.
       ((= name regressGreedy8MVL:)
        (begin
          (setq myChromosomeGEN 0000)       ;; The maximum random root-chromosome genomes from different columns to grow during each generation step.
          (setq myChromosomeINIT 0200)      ;; The maximum random root-chromosome genomes from different columns to grow during the initialization step.
	      (setq myChromosomeRule MVL:)      ;; The rule used for root-chromosome search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myCrossChromosomeSW false)  ;; The use of chromosomes from different columns during cross over operations.
	      (setq myCrossColPct 0.00)  		;; The probability of cross over in each column island during each generation step.
          (setq myCrossLinearSW  true)  	;; The use of a linear grammar during cross over operations.
          (setq myCrossMarrySW true)  	    ;; The use of expression marrying during cross over operations.
      	  (setq myCrossSelPct 1.00)  		;; The probability of cross over in the best-of-breed island during each generation step.
          (setq myCrossSpliceSW true)  		;; The use of expression splicing during cross over operations.
          (setq myGreedyDepth 10)    		;; The depth of greedy search exploration of each chosen column island during each generation step. 
          (setq myGreedyGEN  000)    		;; The number of greedy search WFFs the system will grow in the best-of-breed island  during each generation step. 
          (setq myGreedyINIT 0000)    	    ;; The number of greedy search WFFs the system will grow in the best-of-breed island  during the initialization step. 
          (setq myGreedyRule MVL:)          ;; The rule used for greedy search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myGreedyWidth narrow:)    	;; The width of each greedy search WFF the system will grow in the best-of-breed island (base, full, narrow, random). 
	      (setq myMaxColWFFLen 100)  		;; The maximum length of a single WFF for each column.
	      (setq myMutateColPct 0.00) 		;; The probability of mutation in each column island during each generation step.
	      (setq myMutateSelPct 0.10) 		;; The probability mutation in the best-of-breed island during each generation step.
          (setq myMutateSpliceSW true)  	;; The use of expression splicing during mutation operations.
          (setq myREGMultiple false)        ;; Support multiple columns when growing REG regression expressions.
	      (setq myREGOperatorJoin "+")      ;; The operator used to join multiple columns when growing REG regression expressions ("+" or "*").
          (setq myRestartGap 0010)  		;; The count of generations to allow, without measurable improvement, before initiating a new evolutionary process.
          (setq myRootINIT  2000)  	    	;; The number of root WFFs the system will grow in each column island during the initialization step.
          (setq myRootGEN 00)       		;; The number of root WFFs the system will grow in each column island during each generation step. 
          (setq myRootRule REG:)            ;; The rule used for root WFFs grown in each column island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq mySurvivors 010)     		;; The count of surviving selector WFFs in each generation.
          (setq myTimeON false)  			;; Support separate sequence scoring of each individual time period.
          (setq myUseMVL  true)             ;; Grow MVL selector candidates during the initialization step and during each generation step.
          (setq myUseREG  true)             ;; Grow REG selector candidates during the initialization step and during each generation step.
        )) ; end regressGreedy8MVL case
       ;; Set the regressGreedy9MVL option settings.
       ;; Note: These options work well for general multivariate regressions.
       ((= name regressGreedy9MVL:)
        (begin
          (setq myChromosomeGEN 0000)       ;; The maximum random root-chromosome genomes from different columns to grow during each generation step.
          (setq myChromosomeINIT 0200)      ;; The maximum random root-chromosome genomes from different columns to grow during the initialization step.
	      (setq myChromosomeRule MVL:)      ;; The rule used for root-chromosome search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myCrossChromosomeSW false)  ;; The use of chromosomes from different columns during cross over operations.
	      (setq myCrossColPct 0.00)  		;; The probability of cross over in each column island during each generation step.
          (setq myCrossLinearSW  true)  	;; The use of a linear grammar during cross over operations.
          (setq myCrossMarrySW true)  	    ;; The use of expression marrying during cross over operations.
      	  (setq myCrossSelPct 1.00)  		;; The probability of cross over in the best-of-breed island during each generation step.
          (setq myCrossSpliceSW true)  		;; The use of expression splicing during cross over operations.
          (setq myGreedyDepth 10)    		;; The depth of greedy search exploration of each chosen column island during each generation step. 
          (setq myGreedyGEN  000)    		;; The number of greedy search WFFs the system will grow in the best-of-breed island  during each generation step. 
          (setq myGreedyINIT 0000)    	    ;; The number of greedy search WFFs the system will grow in the best-of-breed island  during the initialization step. 
          (setq myGreedyRule MVL:)          ;; The rule used for greedy search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myGreedyWidth narrow:)    	;; The width of each greedy search WFF the system will grow in the best-of-breed island (base, full, narrow, random). 
	      (setq myMaxColWFFLen 100)  		;; The maximum length of a single WFF for each column.
	      (setq myMutateColPct 0.00) 		;; The probability of mutation in each column island during each generation step.
	      (setq myMutateSelPct 0.10) 		;; The probability mutation in the best-of-breed island during each generation step.
          (setq myMutateSpliceSW true)  	;; The use of expression splicing during mutation operations.
          (setq myREGMultiple false)        ;; Support multiple columns when growing REG regression expressions.
	      (setq myREGOperatorJoin "+")      ;; The operator used to join multiple columns when growing REG regression expressions ("+" or "*").
          (setq myRestartGap 0010)  		;; The count of generations to allow, without measurable improvement, before initiating a new evolutionary process.
          (setq myRootINIT  5000)  	    	;; The number of root WFFs the system will grow in each column island during the initialization step.
          (setq myRootGEN 00)       		;; The number of root WFFs the system will grow in each column island during each generation step. 
          (setq myRootRule REG:)            ;; The rule used for root WFFs grown in each column island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq mySurvivors 010)     		;; The count of surviving selector WFFs in each generation.
          (setq myTimeON false)  			;; Support separate sequence scoring of each individual time period.
          (setq myUseMVL  true)             ;; Grow MVL selector candidates during the initialization step and during each generation step.
          (setq myUseREG  true)             ;; Grow REG selector candidates during the initialization step and during each generation step.
        )) ; end regressGreedy9MVL case
       ;; Set the regressGaMVL option settings.
       ;; Note: These options work well for general multivariate regressions.
       ((= name regressGaMVL:)
        (begin
      	  (setq myCrossSelPct 1.00)  		;; The probability of cross over in the best-of-breed island during each generation step.
          (setq myCrossMarrySW false)  	    ;; The use of expression marrying during cross over operations.
          (setq myCrossSpliceSW false)  	;; The use of expression splicing during cross over operations.
          (setq myGrowSelINIT 5000)   		;; The number of random WFFs the system will grow in the best-of-breed island during the initialization step.
          (setq myGrowSelGEN 00)    		;; The number of random WFFs the system will grow in the best-of-breed island during each generation step. 
          (setq myGrowSelRule MVL:)         ;; The rule used for random WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myMaxColWFFLen 100)  		;; The maximum length of a single WFF for each column.
	      (setq myMutateSelPct 0.10) 		;; The probability mutation in the best-of-breed island during each generation step.
          (setq myMutateSpliceSW false)  	;; The use of expression splicing during mutation operations.
	      (setq myREGOperatorJoin "+")      ;; The operator used to join multiple columns when growing REG regression expressions ("+" or "*").
          (setq myTimeON false)  			;; Support separate sequence scoring of each individual time period.
          (setq myUseMVL  true)             ;; Grow MVL selector candidates during the initialization step and during each generation step.
        )) ; end regressGaMVL case
       ;; Set the regressGaCMVL option settings.
       ;; Note: These options work well for general multivariate regressions.
       ((= name regressGaCMVL:)
        (begin
          (setq myChromosomeGEN 0000)       ;; The maximum random root-chromosome genomes from different columns to grow during each generation step.
          (setq myChromosomeINIT 5000)      ;; The maximum random root-chromosome genomes from different columns to grow during the initialization step.
	      (setq myChromosomeRule MVL:)      ;; The rule used for root-chromosome search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myCrossChromosomeSW false)  ;; The use of chromosomes from different columns during cross over operations.
	      (setq myCrossColPct 0.00)  		;; The probability of cross over in each column island during each generation step.
          (setq myCrossMarrySW true)  	    ;; The use of expression marrying during cross over operations.
      	  (setq myCrossSelPct 1.00)  		;; The probability of cross over in the best-of-breed island during each generation step.
          (setq myCrossSpliceSW true)  		;; The use of expression splicing during cross over operations.
	      (setq myMaxColWFFLen 100)  		;; The maximum length of a single WFF for each column.
	      (setq myMutateColPct 0.00) 		;; The probability of mutation in each column island during each generation step.
	      (setq myMutateSelPct 0.10) 		;; The probability mutation in the best-of-breed island during each generation step.
          (setq myMutateSpliceSW true)  	;; The use of expression splicing during mutation operations.
          (setq myREGMultiple false)        ;; Support multiple columns when growing REG regression expressions.
	      (setq myREGOperatorJoin "+")      ;; The operator used to join multiple columns when growing REG regression expressions ("+" or "*").
          (setq myRestartGap 0010)  		;; The count of generations to allow, without measurable improvement, before initiating a new evolutionary process.
          (setq myRootINIT  5000)  	    	;; (MFK) The number of root WFFs the system will grow in each column island during the initialization step.
          (setq myRootGEN 00)       		;; The number of root WFFs the system will grow in each column island during each generation step. 
          (setq myRootRule REG:)            ;; The rule used for root WFFs grown in each column island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myTimeON false)  			;; Support separate sequence scoring of each individual time period.
          (setq myUseMVL  true)             ;; Grow MVL selector candidates during the initialization step and during each generation step.
          (setq myUseREG  true)             ;; Grow REG selector candidates during the initialization step and during each generation step.
        )) ; end regressGaCMVL case
       ;; Set the regress2GaCMVL option settings.
       ;; Note: These options work well for general multivariate regressions.
       ((= name regress2GaCMVL:)
        (begin
          (setq myChromosomeGEN 0000)       ;; The maximum random root-chromosome genomes from different columns to grow during each generation step.
          (setq myChromosomeINIT 1000)      ;; (MFK) The maximum random root-chromosome genomes from different columns to grow during the initialization step.
	      (setq myChromosomeRule MVL:)      ;; The rule used for root-chromosome search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myCrossChromosomeSW false)  ;; The use of chromosomes from different columns during cross over operations.
	      (setq myCrossColPct 0.00)  		;; The probability of cross over in each column island during each generation step.
          (setq myCrossMarrySW true)  	    ;; The use of expression marrying during cross over operations.
      	  (setq myCrossSelPct 1.00)  		;; The probability of cross over in the best-of-breed island during each generation step.
          (setq myCrossSpliceSW true)  		;; The use of expression splicing during cross over operations.
	      (setq myMaxColWFFLen 100)  		;; The maximum length of a single WFF for each column.
	      (setq myMutateColPct 0.00) 		;; The probability of mutation in each column island during each generation step.
	      (setq myMutateSelPct 0.10) 		;; The probability mutation in the best-of-breed island during each generation step.
          (setq myMutateSpliceSW true)  	;; The use of expression splicing during mutation operations.
          (setq myREGMultiple false)        ;; Support multiple columns when growing REG regression expressions.
	      (setq myREGOperatorJoin "+")      ;; The operator used to join multiple columns when growing REG regression expressions ("+" or "*").
          (setq myRestartGap 0010)  		;; The count of generations to allow, without measurable improvement, before initiating a new evolutionary process.
          (setq myRootINIT  5000)  	    	;; (MFK) The number of root WFFs the system will grow in each column island during the initialization step.
          (setq myRootGEN 00)       		;; The number of root WFFs the system will grow in each column island during each generation step. 
          (setq myRootRule REG:)            ;; The rule used for root WFFs grown in each column island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myTimeON false)  			;; Support separate sequence scoring of each individual time period.
          (setq myUseMVL  true)             ;; Grow MVL selector candidates during the initialization step and during each generation step.
          (setq myUseREG  true)             ;; Grow REG selector candidates during the initialization step and during each generation step.
        )) ; end regress2GaCMVL case
       ;; Set the regress3GaCMVL option settings.
       ;; Note: These options work well for general multivariate regressions.
       ((= name regress3GaCMVL:)
        (begin
          (setq myChromosomeGEN 0000)       ;; The maximum random root-chromosome genomes from different columns to grow during each generation step.
          (setq myChromosomeINIT 1000)      ;; (MFK) The maximum random root-chromosome genomes from different columns to grow during the initialization step.
	      (setq myChromosomeRule MVL:)      ;; The rule used for root-chromosome search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myCrossChromosomeSW false)  ;; The use of chromosomes from different columns during cross over operations.
	      (setq myCrossColPct 0.00)  		;; The probability of cross over in each column island during each generation step.
          (setq myCrossMarrySW true)  	    ;; The use of expression marrying during cross over operations.
      	  (setq myCrossSelPct 1.00)  		;; The probability of cross over in the best-of-breed island during each generation step.
          (setq myCrossSpliceSW true)  		;; The use of expression splicing during cross over operations.
	      (setq myGrowColINIT 0000)   		;; The number of random WFFs the system will grow in each column island during the initialization step.
	      (setq myGrowColGEN 000)    		;; The number of random WFFs the system will grow in each column island during each generation step. 
	      (setq myGrowColRule REG:)         ;; The rule used for random WFFs grown in each column island (BGM, FRM, MVL, ENN, REG, or SVM). 
	      (setq myMaxColWFFLen 100)  		;; The maximum length of a single WFF for each column.
	      (setq myMutateColPct 0.00) 		;; The probability of mutation in each column island during each generation step.
	      (setq myMutateSelPct 0.10) 		;; The probability mutation in the best-of-breed island during each generation step.
          (setq myMutateSpliceSW true)  	;; The use of expression splicing during mutation operations.
          (setq myREGMultiple false)        ;; Support multiple columns when growing REG regression expressions.
	      (setq myREGOperatorJoin "+")      ;; The operator used to join multiple columns when growing REG regression expressions ("+" or "*").
          (setq myRestartGap 0010)  		;; The count of generations to allow, without measurable improvement, before initiating a new evolutionary process.
          (setq myRootINIT  1000)  	    	;; (MFK) The number of root WFFs the system will grow in each column island during the initialization step.
          (setq myRootGEN 00)       		;; The number of root WFFs the system will grow in each column island during each generation step. 
          (setq myRootRule REG:)            ;; The rule used for root WFFs grown in each column island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myTimeON false)  			;; Support separate sequence scoring of each individual time period.
          (setq myUseMVL  true)             ;; Grow MVL selector candidates during the initialization step and during each generation step.
          (setq myUseREG  true)             ;; Grow REG selector candidates during the initialization step and during each generation step.
        )) ; end regress3GaCMVL case
       ;; Set the regress4GaCMVL option settings.
       ;; Note: These options work well for general multivariate regressions.
       ((= name regress4GaCMVL:)
        (begin
          (setq myChromosomeGEN 0000)       ;; The maximum random root-chromosome genomes from different columns to grow during each generation step.
          (setq myChromosomeINIT 1000)      ;; (MFK) The maximum random root-chromosome genomes from different columns to grow during the initialization step.
	      (setq myChromosomeRule MVL:)      ;; The rule used for root-chromosome search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myCrossChromosomeSW false)  ;; The use of chromosomes from different columns during cross over operations.
	      (setq myCrossColPct 0.00)  		;; The probability of cross over in each column island during each generation step.
          (setq myCrossMarrySW true)  	    ;; The use of expression marrying during cross over operations.
      	  (setq myCrossSelPct 1.00)  		;; The probability of cross over in the best-of-breed island during each generation step.
          (setq myCrossSpliceSW true)  		;; The use of expression splicing during cross over operations.
	      (setq myGrowColINIT 1000)   		;; (MFK) The number of random WFFs the system will grow in each column island during the initialization step.
	      (setq myGrowColGEN 000)    		;; The number of random WFFs the system will grow in each column island during each generation step. 
	      (setq myGrowColRule REG:)         ;; The rule used for random WFFs grown in each column island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myGrowWFFStyle root:)       ;; The style used for random WFFs grown in the best-of-breed island (full, or root). 
	      (setq myMaxColWFFLen 100)  		;; The maximum length of a single WFF for each column.
	      (setq myMutateColPct 0.00) 		;; The probability of mutation in each column island during each generation step.
	      (setq myMutateSelPct 0.10) 		;; The probability mutation in the best-of-breed island during each generation step.
          (setq myMutateSpliceSW true)  	;; The use of expression splicing during mutation operations.
          (setq myREGMultiple false)        ;; Support multiple columns when growing REG regression expressions.
	      (setq myREGOperatorJoin "+")      ;; The operator used to join multiple columns when growing REG regression expressions ("+" or "*").
          (setq myRestartGap 0010)  		;; The count of generations to allow, without measurable improvement, before initiating a new evolutionary process.
          (setq myRootINIT  0000)  	    	;; (MFK) The number of root WFFs the system will grow in each column island during the initialization step.
          (setq myRootGEN 00)       		;; The number of root WFFs the system will grow in each column island during each generation step. 
          (setq myRootRule REG:)            ;; The rule used for root WFFs grown in each column island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myTimeON false)  			;; Support separate sequence scoring of each individual time period.
          (setq myUseMVL  true)             ;; Grow MVL selector candidates during the initialization step and during each generation step.
          (setq myUseREG  true)             ;; Grow REG selector candidates during the initialization step and during each generation step.
        )) ; end regress4GaCMVL case
       ;; Set the regress5GaCMVL option settings.
       ;; Note: These options work well for general multivariate regressions.
       ((= name regress5GaCMVL:)
        (begin
          (setq myChromosomeGEN 0000)       ;; The maximum random root-chromosome genomes from different columns to grow during each generation step.
          (setq myChromosomeINIT 5000)      ;; The maximum random root-chromosome genomes from different columns to grow during the initialization step.
	      (setq myChromosomeRule MVL:)      ;; The rule used for root-chromosome search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myCrossChromosomeSW false)  ;; The use of chromosomes from different columns during cross over operations.
	      (setq myCrossColPct 0.00)  		;; The probability of cross over in each column island during each generation step.
          (setq myCrossMarrySW true)  	    ;; The use of expression marrying during cross over operations.
      	  (setq myCrossSelPct 1.00)  		;; The probability of cross over in the best-of-breed island during each generation step.
          (setq myCrossSpliceSW true)  		;; The use of expression splicing during cross over operations.
	      (setq myMaxColWFFLen 100)  		;; The maximum length of a single WFF for each column.
	      (setq myMutateColPct 0.00) 		;; The probability of mutation in each column island during each generation step.
	      (setq myMutateSelPct 0.10) 		;; The probability mutation in the best-of-breed island during each generation step.
          (setq myMutateSpliceSW true)  	;; The use of expression splicing during mutation operations.
          (setq myREGMultiple false)        ;; Support multiple columns when growing REG regression expressions.
	      (setq myREGOperatorJoin "+")      ;; The operator used to join multiple columns when growing REG regression expressions ("+" or "*").
          (setq myRestartGap 0010)  		;; The count of generations to allow, without measurable improvement, before initiating a new evolutionary process.
          (setq myRootINIT 999999)  	    ;; The number of root WFFs the system will grow in each column island during the initialization step.
          (setq myRootGEN 00)       		;; The number of root WFFs the system will grow in each column island during each generation step. 
          (setq myRootRule REG:)            ;; The rule used for root WFFs grown in each column island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myTimeON false)  			;; Support separate sequence scoring of each individual time period.
          (setq myUseMVL  true)             ;; Grow MVL selector candidates during the initialization step and during each generation step.
          (setq myUseREG  true)             ;; Grow REG selector candidates during the initialization step and during each generation step.
        )) ; end regress5GaCMVL case
       ;; Set the regress6GaCMVL option settings.
       ;; Note: These options work well for general multivariate regressions.
       ((= name regress6GaCMVL:)
        (begin
          (setq myChromosomeGEN 0000)       ;; The maximum random root-chromosome genomes from different columns to grow during each generation step.
          (setq myChromosomeINIT 1000)      ;; The maximum random root-chromosome genomes from different columns to grow during the initialization step.
	      (setq myChromosomeRule MVL:)      ;; The rule used for root-chromosome search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myCrossChromosomeSW false)  ;; The use of chromosomes from different columns during cross over operations.
	      (setq myCrossColPct 0.00)  		;; The probability of cross over in each column island during each generation step.
          (setq myCrossMarrySW true)  	    ;; The use of expression marrying during cross over operations.
      	  (setq myCrossSelPct 1.00)  		;; The probability of cross over in the best-of-breed island during each generation step.
          (setq myCrossSpliceSW true)  		;; The use of expression splicing during cross over operations.
	      (setq myGrowColINIT 1000)   		;; The number of random WFFs the system will grow in each column island during the initialization step.
	      (setq myGrowColGEN 000)    		;; The number of random WFFs the system will grow in each column island during each generation step. 
	      (setq myGrowColRule REG:)         ;; The rule used for random WFFs grown in each column island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myGrowWFFStyle root:)       ;; The style used for random WFFs grown in the best-of-breed island (full, or root). 
	      (setq myMaxColWFFLen 100)  		;; The maximum length of a single WFF for each column.
	      (setq myMutateColPct 0.00) 		;; The probability of mutation in each column island during each generation step.
	      (setq myMutateSelPct 0.10) 		;; The probability mutation in the best-of-breed island during each generation step.
          (setq myMutateSpliceSW true)  	;; The use of expression splicing during mutation operations.
          (setq myREGMultiple false)        ;; Support multiple columns when growing REG regression expressions.
	      (setq myREGOperatorJoin "+")      ;; The operator used to join multiple columns when growing REG regression expressions ("+" or "*").
          (setq myRestartGap 0010)  		;; The count of generations to allow, without measurable improvement, before initiating a new evolutionary process.
          (setq myRootINIT  0000)  	    	;; The number of root WFFs the system will grow in each column island during the initialization step.
          (setq myRootGEN 00)       		;; The number of root WFFs the system will grow in each column island during each generation step. 
          (setq myRootRule REG:)            ;; The rule used for root WFFs grown in each column island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myTimeON false)  			;; Support separate sequence scoring of each individual time period.
          (setq myUseMVL  true)             ;; Grow MVL selector candidates during the initialization step and during each generation step.
          (setq myUseREG  true)             ;; Grow REG selector candidates during the initialization step and during each generation step.
        )) ; end regress6GaCMVL case
       ;; Set the regress7GaCMVL option settings.
       ;; Note: These options work well for general multivariate regressions.
       ((= name regress7GaCMVL:)
        (begin
          (setq myChromosomeGEN 0000)       ;; The maximum random root-chromosome genomes from different columns to grow during each generation step.
          (setq myChromosomeINIT 1000)      ;; The maximum random root-chromosome genomes from different columns to grow during the initialization step.
	      (setq myChromosomeRule MVL:)      ;; The rule used for root-chromosome search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myCrossChromosomeSW true)   ;; The use of chromosomes from different columns during cross over operations.
	      (setq myCrossColPct 0.00)  		;; The probability of cross over in each column island during each generation step.
          (setq myCrossMarrySW true)  	    ;; The use of expression marrying during cross over operations.
      	  (setq myCrossSelPct 1.00)  		;; The probability of cross over in the best-of-breed island during each generation step.
          (setq myCrossSpliceSW true)  		;; The use of expression splicing during cross over operations.
	      (setq myGrowColINIT 1000)   		;; The number of random WFFs the system will grow in each column island during the initialization step.
	      (setq myGrowColGEN 000)    		;; The number of random WFFs the system will grow in each column island during each generation step. 
	      (setq myGrowColRule REG:)         ;; The rule used for random WFFs grown in each column island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myGrowWFFStyle root:)       ;; The style used for random WFFs grown in the best-of-breed island (full, or root). 
	      (setq myMaxColWFFLen 100)  		;; The maximum length of a single WFF for each column.
	      (setq myMutateColPct 0.00) 		;; The probability of mutation in each column island during each generation step.
	      (setq myMutateSelPct 0.10) 		;; The probability mutation in the best-of-breed island during each generation step.
          (setq myMutateSpliceSW true)  	;; The use of expression splicing during mutation operations.
          (setq myREGMultiple false)        ;; Support multiple columns when growing REG regression expressions.
	      (setq myREGOperatorJoin "+")      ;; The operator used to join multiple columns when growing REG regression expressions ("+" or "*").
          (setq myRestartGap 0010)  		;; The count of generations to allow, without measurable improvement, before initiating a new evolutionary process.
          (setq myRootINIT  0000)  	    	;; The number of root WFFs the system will grow in each column island during the initialization step.
          (setq myRootGEN 00)       		;; The number of root WFFs the system will grow in each column island during each generation step. 
          (setq myRootRule REG:)            ;; The rule used for root WFFs grown in each column island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myTimeON false)  			;; Support separate sequence scoring of each individual time period.
          (setq myUseMVL  true)             ;; Grow MVL selector candidates during the initialization step and during each generation step.
          (setq myUseREG  true)             ;; Grow REG selector candidates during the initialization step and during each generation step.
        )) ; end regress7GaCMVL case
       ;; Set the regress8GaCMVL option settings.
       ;; Note: These options work well for general multivariate regressions.
       ((= name regress8GaCMVL:)
        (begin
          (setq myChromosomeGEN 0000)       ;; The maximum random root-chromosome genomes from different columns to grow during each generation step.
          (setq myChromosomeINIT 0200)      ;; The maximum random root-chromosome genomes from different columns to grow during the initialization step.
	      (setq myChromosomeRule MVL:)      ;; The rule used for root-chromosome search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myCrossChromosomeSW false)  ;; The use of chromosomes from different columns during cross over operations.
	      (setq myCrossColPct 0.00)  		;; The probability of cross over in each column island during each generation step.
          (setq myCrossMarrySW true)  	    ;; The use of expression marrying during cross over operations.
      	  (setq myCrossSelPct 1.00)  		;; The probability of cross over in the best-of-breed island during each generation step.
          (setq myCrossSpliceSW true)  		;; The use of expression splicing during cross over operations.
	      (setq myGrowColINIT 0200)   		;; The number of random WFFs the system will grow in each column island during the initialization step.
	      (setq myGrowColGEN 000)    		;; The number of random WFFs the system will grow in each column island during each generation step. 
	      (setq myGrowColRule REG:)         ;; The rule used for random WFFs grown in each column island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myGrowWFFStyle root:)       ;; The style used for random WFFs grown in the best-of-breed island (full, or root). 
	      (setq myMaxColWFFLen 100)  		;; The maximum length of a single WFF for each column.
	      (setq myMutateColPct 0.00) 		;; The probability of mutation in each column island during each generation step.
	      (setq myMutateSelPct 0.10) 		;; The probability mutation in the best-of-breed island during each generation step.
          (setq myMutateSpliceSW true)  	;; The use of expression splicing during mutation operations.
          (setq myREGMultiple false)        ;; Support multiple columns when growing REG regression expressions.
	      (setq myREGOperatorJoin "+")      ;; The operator used to join multiple columns when growing REG regression expressions ("+" or "*").
          (setq myRestartGap 0010)  		;; The count of generations to allow, without measurable improvement, before initiating a new evolutionary process.
          (setq myRootINIT  0000)  	    	;; The number of root WFFs the system will grow in each column island during the initialization step.
          (setq myRootGEN 00)       		;; The number of root WFFs the system will grow in each column island during each generation step. 
          (setq myRootRule REG:)            ;; The rule used for root WFFs grown in each column island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq mySurvivors 010)     		;; The count of surviving selector WFFs in each generation.
          (setq myTimeON false)  			;; Support separate sequence scoring of each individual time period.
          (setq myUseMVL  true)             ;; Grow MVL selector candidates during the initialization step and during each generation step.
          (setq myUseREG  true)             ;; Grow REG selector candidates during the initialization step and during each generation step.
        )) ; end regress8GaCMVL case
       ;; Set the regressBestGaCMVL option settings.
       ;; Note: These options work well for general multivariate regressions.
       ((= name regressBestGaCMVL:)
        (begin
          (setq myChromosomeGEN 0000)       ;; The maximum random root-chromosome genomes from different columns to grow during each generation step.
          (setq myChromosomeINIT 1000)      ;; The maximum random root-chromosome genomes from different columns to grow during the initialization step.
	      (setq myChromosomeRule MVL:)      ;; The rule used for root-chromosome search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myCrossChromosomeSW false)  ;; The use of chromosomes from different columns during cross over operations.
	      (setq myCrossColPct 0.00)  		;; The probability of cross over in each column island during each generation step.
          (setq myCrossMarrySW true)  	    ;; The use of expression marrying during cross over operations.
      	  (setq myCrossSelPct 1.00)  		;; The probability of cross over in the best-of-breed island during each generation step.
          (setq myCrossSpliceSW true)  		;; The use of expression splicing during cross over operations.
	      (setq myGrowColINIT 1000)   		;; The number of random WFFs the system will grow in each column island during the initialization step.
	      (setq myGrowColGEN 000)    		;; The number of random WFFs the system will grow in each column island during each generation step. 
	      (setq myGrowColRule REG:)         ;; The rule used for random WFFs grown in each column island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myGrowWFFStyle root:)       ;; The style used for random WFFs grown in the best-of-breed island (full, or root). 
	      (setq myMaxColWFFLen 100)  		;; The maximum length of a single WFF for each column.
	      (setq myMutateColPct 0.00) 		;; The probability of mutation in each column island during each generation step.
	      (setq myMutateSelPct 0.10) 		;; The probability mutation in the best-of-breed island during each generation step.
          (setq myMutateSpliceSW true)  	;; The use of expression splicing during mutation operations.
          (setq myREGMultiple false)        ;; Support multiple columns when growing REG regression expressions.
	      (setq myREGOperatorJoin "+")      ;; The operator used to join multiple columns when growing REG regression expressions ("+" or "*").
          (setq myRestartGap 0010)  		;; The count of generations to allow, without measurable improvement, before initiating a new evolutionary process.
          (setq myRootINIT  0000)  	    	;; The number of root WFFs the system will grow in each column island during the initialization step.
          (setq myRootGEN 00)       		;; The number of root WFFs the system will grow in each column island during each generation step. 
          (setq myRootRule REG:)            ;; The rule used for root WFFs grown in each column island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myTimeON false)  			;; Support separate sequence scoring of each individual time period.
          (setq myUseMVL  true)             ;; Grow MVL selector candidates during the initialization step and during each generation step.
          (setq myUseREG  true)             ;; Grow REG selector candidates during the initialization step and during each generation step.
        )) ; end regressBestGaCMVL case
       ;; Set the regressSvmMVL option settings.
       ;; Note: These options work well for general multivariate regressions.
       ((= name regressSvmMVL:)
        (begin
	      (setq myCrossColPct 1.00)  		;; The probability of cross over in each column island during each generation step.
      	  (setq myCrossSelPct 1.00)  		;; The probability of cross over in the best-of-breed island during each generation step.
          (setq myGrowSelINIT 5000)   		;; The number of random WFFs the system will grow in the best-of-breed island during the initialization step.
          (setq myGrowSelGEN 00)    		;; The number of random WFFs the system will grow in the best-of-breed island during each generation step. 
          (setq myGrowSelRule MVL:)         ;; The rule used for random WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
	      (setq myMutateColPct 0.10) 		;; The probability of mutation in each column island during each generation step.
	      (setq myMutateSelPct 0.10) 		;; The probability mutation in the best-of-breed island during each generation step.
          (setq myTimeON false)  			;; Support separate sequence scoring of each individual time period.
          (setq myUseMVL  true)             ;; Grow MVL selector candidates during the initialization step and during each generation step.
          (setq myUseSVM  true)             ;; Grow SVM selector candidates during the initialization step.
        )) ; end regressSvmMVL case
       ;; Set the regressCMVL option settings.
       ;; Note: These options work well for general multivariate regressions.
       ((= name regressCMVL:)
        (begin
	      (setq myCrossColPct 1.00)  		;; The probability of cross over in each column island during each generation step.
      	  (setq myCrossSelPct 1.00)  		;; The probability of cross over in the best-of-breed island during each generation step.
	      (setq myGreedyDepth 10)    		;; The depth of greedy search exploration of each chosen column island  during each generation step. 
	      (setq myGreedyGEN 025)    		;; The number of greedy search WFFs the system will grow in the best-of-breed island  during each generation step. 
	      (setq myGreedyWidth narrow:)    	;; The width of each greedy search WFF the system will grow in the best-of-breed island (base, full, narrow, random). 
	      (setq myGreedyRule MVL:)          ;; The rule used for greedy search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
	      (setq myGrowColINIT 1000)   		;; The number of random WFFs the system will grow in each column island during the initialization step.
	      (setq myGrowColGEN 000)    		;; The number of random WFFs the system will grow in each column island during each generation step. 
	      (setq myGrowColRule REG:)         ;; The rule used for random WFFs grown in each column island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myGrowSelINIT 1000)   		;; The number of random WFFs the system will grow in the best-of-breed island during the initialization step.
          (setq myGrowSelGEN 00)    		;; The number of random WFFs the system will grow in the best-of-breed island during each generation step. 
          (setq myGrowSelRule MVL:)         ;; The rule used for random WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
	      (setq myMaxColWFFLen 100)  		;; The maximum length of a single WFF for each column.
	      (setq myMutateColPct 0.10) 		;; The probability of mutation in each column island during each generation step.
	      (setq myMutateSelPct 0.10) 		;; The probability mutation in the best-of-breed island during each generation step.
          (setq myREGMultiple false)        ;; Support multiple columns when growing REG regression expressions.
	      (setq myREGOperatorJoin "+")      ;; The operator used to join multiple columns when growing REG regression expressions ("+" or "*").
          (setq myTimeON false)  			;; Support separate sequence scoring of each individual time period.
          (setq myUseMVL  true)             ;; Grow MVL selector candidates during the initialization step and during each generation step.
          (setq myUseREG  true)             ;; Grow REG selector candidates during the initialization step and during each generation step.
        )) ; end regressCMVL case
       ;; Set the regress2CMVL option settings.
       ;; Note: These options work well for general multivariate regressions.
       ((= name regress2CMVL:)
        (begin
	      (setq myCrossColPct 1.00)  		;; The probability of cross over in each column island during each generation step.
      	  (setq myCrossSelPct 1.00)  		;; The probability of cross over in the best-of-breed island during each generation step.
	      (setq myGreedyDepth 25)    		;; The depth of greedy search exploration of each chosen column island  during each generation step. 
	      (setq myGreedyGEN 100)    		;; The number of greedy search WFFs the system will grow in the best-of-breed island  during each generation step. 
	      (setq myGreedyWidth narrow:)    	;; The width of each greedy search WFF the system will grow in the best-of-breed island (base, full, narrow, random). 
	      (setq myGreedyRule MVL:)          ;; The rule used for greedy search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
	      (setq myGrowColINIT 1000)   		;; The number of random WFFs the system will grow in each column island during the initialization step.
	      (setq myGrowColGEN 000)    		;; The number of random WFFs the system will grow in each column island during each generation step. 
	      (setq myGrowColRule REG:)         ;; The rule used for random WFFs grown in each column island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myGrowSelINIT 0010)   		;; The number of random WFFs the system will grow in the best-of-breed island during the initialization step.
          (setq myGrowSelGEN 00)    		;; The number of random WFFs the system will grow in the best-of-breed island during each generation step. 
          (setq myGrowSelRule MVL:)         ;; The rule used for random WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
	      (setq myMaxColWFFLen 100)  		;; The maximum length of a single WFF for each column.
	      (setq myMutateColPct 0.10) 		;; The probability of mutation in each column island during each generation step.
	      (setq myMutateSelPct 0.10) 		;; The probability mutation in the best-of-breed island during each generation step.
          (setq myREGMultiple false)        ;; Support multiple columns when growing REG regression expressions.
	      (setq myREGOperatorJoin "+")      ;; The operator used to join multiple columns when growing REG regression expressions ("+" or "*").
          (setq myTimeON false)  			;; Support separate sequence scoring of each individual time period.
          (setq myUseMVL  true)             ;; Grow MVL selector candidates during the initialization step and during each generation step.
          (setq myUseREG  true)             ;; Grow REG selector candidates during the initialization step and during each generation step.
        )) ; end regress2CMVL case
       ;; Set the regressSMVL option settings.
       ;; Note: These options work well for general multivariate regressions.
       ((= name regressSMVL:)
        (begin
          (setq myGrowSelINIT 5000)   		;; The number of random WFFs the system will grow in the best-of-breed island during the initialization step.
          (setq myGrowSelGEN 00)    		;; The number of random WFFs the system will grow in the best-of-breed island during each generation step. 
          (setq myGrowSelRule MVL:)         ;; The rule used for random WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myTimeON false)  			;; Support separate sequence scoring of each individual time period.
          (setq myUseMVL  true)             ;; Grow MVL selector candidates during the initialization step and during each generation step.
        )) ; end regressSMVL case
       ;; Set the regress2SMVL option settings.
       ;; Note: These options work well for general multivariate regressions.
       ((= name regress2SMVL:)
        (begin
      	  (setq myCrossSelPct 1.00)  		;; The probability of cross over in the best-of-breed island during each generation step.
          (setq myGrowSelINIT 5000)   		;; The number of random WFFs the system will grow in the best-of-breed island during the initialization step.
          (setq myGrowSelGEN 00)    		;; The number of random WFFs the system will grow in the best-of-breed island during each generation step. 
          (setq myGrowSelRule MVL:)         ;; The rule used for random WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myMutateSelPct 1.00) 		;; The probability mutation in the best-of-breed island during each generation step.
          (setq mySurvivors 500)     		;; The count of surviving selector WFFs in each generation.
          (setq myTimeON false)  			;; Support separate sequence scoring of each individual time period.
          (setq myUseMVL  true)             ;; Grow MVL selector candidates during the initialization step and during each generation step.
        )) ; end regress2SMVL case
       ;; Set the regressGPMVL option settings.
       ;; Note: These options work well for general multivariate regressions.
       ((= name regressGPMVL:)
        (begin
      	  (setq myCrossSelPct 0.05)  		;; The probability of cross over in the best-of-breed island during each generation step.
          (setq myGrowSelINIT 5000)   		;; The number of random WFFs the system will grow in the best-of-breed island during the initialization step.
          (setq myGrowSelGEN 00)    		;; The number of random WFFs the system will grow in the best-of-breed island during each generation step. 
          (setq myGrowSelRule MVL:)         ;; The rule used for random WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myMutateSelPct 0.05) 		;; The probability mutation in the best-of-breed island during each generation step.
          (setq mySurvivors 500)     		;; The count of surviving selector WFFs in each generation.
          (setq myTimeON false)  			;; Support separate sequence scoring of each individual time period.
          (setq myUseMVL  true)             ;; Grow MVL selector candidates during the initialization step and during each generation step.
        )) ; end regressGPMVL case
       ;; Set the defaultREG option settings.
       ;; Note: These options work well for classification and sequencing of individuals in the the top 5%.
       ((= name defaultREG:)
        (begin
          (setq myGrowSelINIT 5000)   		;; The number of random WFFs the system will grow in the best-of-breed island during the initialization step.
          (setq myGrowSelGEN 00)    		;; The number of random WFFs the system will grow in the best-of-breed island during each generation step. 
          (setq myGrowSelRule REG:)         ;; The rule used for random WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
	      (setq mySampleFactor 10)          ;; The reduction factor for determining the size of the sample training data (when mySampleON is true).
          (setq myScoreFocus top05:)  	    ;; The focus of scoring on the training data (full, top25, top10, or top05).
          (setq myTimeON  true)  			;; Support separate sequence scoring of each individual time period.
	      (setq myTrainingFactor 01)        ;; The reduction factor for determining the size of the final training data (when mySampleON is true).
          (setq myUseREG  true)             ;; Grow REG selector candidates during the initialization step and during each generation step.
        )) ; end defaultREG case
       ;; Set the selectMVL option settings.
       ;; Note: These options work well for classification and sequencing of individuals in the the top 5%.
       ((= name selectMVL:)
        (begin
          (setq myGrowSelINIT 2000)   		;; The number of random WFFs the system will grow in the best-of-breed island during the initialization step.
          (setq myGrowSelGEN 00)    		;; The number of random WFFs the system will grow in the best-of-breed island during each generation step. 
          (setq myGrowSelRule MVL:)         ;; The rule used for random WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myMaxColWFFLen 100)  		;; The maximum length of a single WFF for each column.
          (setq myREGMultiple false)        ;; Support multiple columns when growing REG regression expressions.
	      (setq mySampleFactor 10)          ;; The reduction factor for determining the size of the sample training data (when mySampleON is true).
          (setq myScoreFocus top05:)  	    ;; The focus of scoring on the training data (full, top25, top10, or top05).
          (setq myTimeON  true)  			;; Support separate sequence scoring of each individual time period.
	      (setq myTrainingFactor 01)        ;; The reduction factor for determining the size of the final training data (when mySampleON is true).
          (setq myUseMVL true)              ;; Grow MVL selector candidates during the initialization step and during each generation step.
        )) ; end selectMVL case
       ;; Set the selectGreedyMVL option settings.
       ;; Note: These options work well for general multivariate regressions.
       ((= name selectGreedyMVL:)
        (begin
          (setq myChromosomeGEN 0000)       ;; The maximum random root-chromosome genomes from different columns to grow during each generation step.
          (setq myChromosomeINIT 0200)      ;; The maximum random root-chromosome genomes from different columns to grow during the initialization step.
	      (setq myChromosomeRule MVL:)      ;; The rule used for root-chromosome search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myCrossChromosomeSW false)  ;; The use of chromosomes from different columns during cross over operations.
	      (setq myCrossColPct 0.00)  		;; The probability of cross over in each column island during each generation step.
          (setq myCrossLinearSW  true)  	;; The use of a linear grammar during cross over operations.
          (setq myCrossMarrySW true)  	    ;; The use of expression marrying during cross over operations.
      	  (setq myCrossSelPct 1.00)  		;; The probability of cross over in the best-of-breed island during each generation step.
          (setq myCrossSpliceSW true)  		;; The use of expression splicing during cross over operations.
          (setq myGreedyDepth 10)    		;; The depth of greedy search exploration of each chosen column island during each generation step. 
          (setq myGreedyGEN  000)    		;; The number of greedy search WFFs the system will grow in the best-of-breed island  during each generation step. 
          (setq myGreedyINIT 0000)    	    ;; The number of greedy search WFFs the system will grow in the best-of-breed island  during the initialization step. 
          (setq myGreedyRule MVL:)          ;; The rule used for greedy search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myGreedyWidth narrow:)    	;; The width of each greedy search WFF the system will grow in the best-of-breed island (base, full, narrow, random). 
	      (setq myMaxColWFFLen 100)  		;; The maximum length of a single WFF for each column.
	      (setq myMutateColPct 0.00) 		;; The probability of mutation in each column island during each generation step.
	      (setq myMutateSelPct 0.10) 		;; The probability mutation in the best-of-breed island during each generation step.
          (setq myMutateSpliceSW true)  	;; The use of expression splicing during mutation operations.
          (setq myREGMultiple false)        ;; Support multiple columns when growing REG regression expressions.
	      (setq myREGOperatorJoin "+")      ;; The operator used to join multiple columns when growing REG regression expressions ("+" or "*").
          (setq myRestartGap 0010)  		;; The count of generations to allow, without measurable improvement, before initiating a new evolutionary process.
          (setq myRootINIT  5000)  	    	;; The number of root WFFs the system will grow in each column island during the initialization step.
          (setq myRootGEN 00)       		;; The number of root WFFs the system will grow in each column island during each generation step. 
          (setq myRootRule REG:)            ;; The rule used for root WFFs grown in each column island (BGM, FRM, MVL, ENN, REG, or SVM). 
	      (setq mySampleFactor 10)          ;; The reduction factor for determining the size of the sample training data (when mySampleON is true).
          (setq myScoreFocus top05:)  	    ;; The focus of scoring on the training data (full, top25, top10, or top05).
          (setq mySurvivors 010)     		;; The count of surviving selector WFFs in each generation.
          (setq myTimeON  true)  			;; Support separate sequence scoring of each individual time period.
	      (setq myTrainingFactor 10)        ;; The reduction factor for determining the size of the final training data (when mySampleON is true).
          (setq myUseMVL  true)             ;; Grow MVL selector candidates during the initialization step and during each generation step.
          (setq myUseREG  true)             ;; Grow REG selector candidates during the initialization step and during each generation step.
        )) ; end selectGreedyMVL case
       ;; Set the selectREG option settings.
       ;; Note: These options work well for classification and sequencing of individuals in the the top 5%.
       ((= name selectREG:)
        (begin
          (setq myGrowSelINIT 5000)   		;; The number of random WFFs the system will grow in the best-of-breed island during the initialization step.
          (setq myGrowSelGEN 00)    		;; The number of random WFFs the system will grow in the best-of-breed island during each generation step. 
          (setq myGrowSelRule REG:)         ;; The rule used for random WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myGrowWFFStyle root:)       ;; The style used for random WFFs grown in the best-of-breed island (full, or root). 
          (setq myMaxColWFFLen 100)  		;; The maximum length of a single WFF for each column.
          (setq myREGMultiple false)        ;; Support multiple columns when growing REG regression expressions.
	      (setq mySampleFactor 10)          ;; The reduction factor for determining the size of the sample training data (when mySampleON is true).
          (setq myScoreFocus top05:)  	    ;; The focus of scoring on the training data (full, top25, top10, or top05).
          (setq myTimeON  true)  			;; Support separate sequence scoring of each individual time period.
	      (setq myTrainingFactor 01)        ;; The reduction factor for determining the size of the final training data (when mySampleON is true).
          (setq myUseREG  true)             ;; Grow REG selector candidates during the initialization step and during each generation step.
        )) ; end selectREG case
       ;; Set the select2REG option settings.
       ;; Note: These options work well for classification and sequencing of individuals in the the top 5%.
       ((= name select2REG:)
        (begin
          (setq myGrowSelINIT 5000)   		;; The number of random WFFs the system will grow in the best-of-breed island during the initialization step.
          (setq myGrowSelGEN 00)    		;; The number of random WFFs the system will grow in the best-of-breed island during each generation step. 
          (setq myGrowSelRule REG:)         ;; The rule used for random WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myGrowWFFStyle root:)       ;; The style used for random WFFs grown in the best-of-breed island (full, or root). 
          (setq myMaxColWFFLen 005)  		;; The maximum length of a single WFF for each column.
          (setq myREGMultiple false)        ;; Support multiple columns when growing REG regression expressions.
	      (setq mySampleFactor 10)          ;; The reduction factor for determining the size of the sample training data (when mySampleON is true).
          (setq myScoreFocus top05:)  	    ;; The focus of scoring on the training data (full, top25, top10, or top05).
          (setq myTimeON  true)  			;; Support separate sequence scoring of each individual time period.
	      (setq myTrainingFactor 01)        ;; The reduction factor for determining the size of the final training data (when mySampleON is true).
          (setq myUseREG  true)             ;; Grow REG selector candidates during the initialization step and during each generation step.
        )) ; end select2REG case
       ;; Set the select3REG option settings.
       ;; Note: These options work well for classification and sequencing of individuals in the the top 5%.
       ((= name select3REG:)
        (begin
          (setq myChromosomeGEN 0000)       ;; The maximum random root-chromosome genomes from different columns to grow during each generation step.
          (setq myChromosomeINIT 1000)      ;; The maximum random root-chromosome genomes from different columns to grow during the initialization step.
	      (setq myChromosomeRule REG:)      ;; The rule used for root-chromosome search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myGreedyDepth 25)    		;; The depth of greedy search exploration of each chosen column island  during each generation step. 
          (setq myGreedyGEN  100)    		;; The number of greedy search WFFs the system will grow in the best-of-breed island  during each generation step. 
          (setq myGreedyWidth narrow:)    	;; The width of each greedy search WFF the system will grow in the best-of-breed island (base, full, narrow, random). 
          (setq myGreedyRule REG:)          ;; The rule used for greedy search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myGrowSelINIT 0000)   		;; The number of random WFFs the system will grow in the best-of-breed island during the initialization step.
          (setq myGrowSelGEN 00)    		;; The number of random WFFs the system will grow in the best-of-breed island during each generation step. 
          (setq myGrowSelRule REG:)         ;; The rule used for random WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myGrowWFFStyle root:)       ;; The style used for random WFFs grown in the best-of-breed island (full, or root). 
          (setq myMaxColWFFLen 100)  		;; The maximum length of a single WFF for each column.
          (setq myREGMultiple false)        ;; Support multiple columns when growing REG regression expressions.
          (setq myRestartGap 00010) 		;; The count of generations to allow, without measurable improvement, before initiating a new evolutionary process.
          (setq myRootINIT 1000)  	 		;; The number of root WFFs the system will grow in each column island during the initialization step.
          (setq myRootGEN 00)       		;; The number of root WFFs the system will grow in each column island during each generation step. 
          (setq myRootRule REG:)            ;; The rule used for root WFFs grown in each column island (BGM, FRM, MVL, ENN, REG, or SVM). 
	      (setq mySampleFactor 10)          ;; The reduction factor for determining the size of the sample training data (when mySampleON is true).
          (setq myScoreFocus top05:)  	    ;; The focus of scoring on the training data (full, top25, top10, or top05).
          (setq myTimeON  true)  			;; Support separate sequence scoring of each individual time period.
	      (setq myTrainingFactor 01)        ;; The reduction factor for determining the size of the final training data (when mySampleON is true).
          (setq myUseREG  true)             ;; Grow REG selector candidates during the initialization step and during each generation step.
        )) ; end select3REG case
       ;; Set the select4REG option settings.
       ;; Note: These options work well for classification and sequencing of individuals in the the top 5%.
       ((= name select4REG:)
        (begin
          (setq myCrossColPct 1.00)  		;; The probability of cross over in each column island during each generation step.
          (setq myCrossSelPct 1.00)  		;; The probability of cross over in the best-of-breed island during each generation step.
          (setq myChromosomeGEN 0000)       ;; The maximum random root-chromosome genomes from different columns to grow during each generation step.
          (setq myChromosomeINIT 1000)      ;; The maximum random root-chromosome genomes from different columns to grow during the initialization step.
	      (setq myChromosomeRule REG:)      ;; The rule used for root-chromosome search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myGreedyDepth 25)    		;; The depth of greedy search exploration of each chosen column island  during each generation step. 
          (setq myGreedyGEN  100)    		;; The number of greedy search WFFs the system will grow in the best-of-breed island  during each generation step. 
          (setq myGreedyWidth narrow:)    	;; The width of each greedy search WFF the system will grow in the best-of-breed island (base, full, narrow, random). 
          (setq myGreedyRule REG:)          ;; The rule used for greedy search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myGrowSelINIT 0000)   		;; The number of random WFFs the system will grow in the best-of-breed island during the initialization step.
          (setq myGrowSelGEN 00)    		;; The number of random WFFs the system will grow in the best-of-breed island during each generation step. 
          (setq myGrowSelRule REG:)         ;; The rule used for random WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myGrowWFFStyle root:)       ;; The style used for random WFFs grown in the best-of-breed island (full, or root). 
          (setq myMaxColWFFLen 100)  		;; The maximum length of a single WFF for each column.
          (setq myMutateColPct 1.00) 		;; The probability of mutation in each column island during each generation step.
          (setq myMutateSelPct 1.00) 		;; The probability mutation in the best-of-breed island during each generation step.
          (setq myREGMultiple false)        ;; Support multiple columns when growing REG regression expressions.
          (setq myRestartGap 00010) 		;; The count of generations to allow, without measurable improvement, before initiating a new evolutionary process.
          (setq myRootINIT 1000)  	 		;; The number of root WFFs the system will grow in each column island during the initialization step.
          (setq myRootGEN 00)       		;; The number of root WFFs the system will grow in each column island during each generation step. 
          (setq myRootRule REG:)            ;; The rule used for root WFFs grown in each column island (BGM, FRM, MVL, ENN, REG, or SVM). 
	      (setq mySampleFactor 10)          ;; The reduction factor for determining the size of the sample training data (when mySampleON is true).
          (setq myScoreFocus top05:)  	    ;; The focus of scoring on the training data (full, top25, top10, or top05).
          (setq myTimeON  true)  			;; Support separate sequence scoring of each individual time period.
	      (setq myTrainingFactor 01)        ;; The reduction factor for determining the size of the final training data (when mySampleON is true).
          (setq myUseREG  true)             ;; Grow REG selector candidates during the initialization step and during each generation step.
        )) ; end select4REG case
       ;; Set the select5REG option settings.
       ;; Note: These options work well for classification and sequencing of individuals in the the top 5%.
       ((= name select5REG:)
        (begin
          (setq myCrossColPct 0.00)  		;; The probability of cross over in each column island during each generation step.
          (setq myCrossSelPct 1.00)  		;; The probability of cross over in the best-of-breed island during each generation step.
          (setq myChromosomeGEN 0000)       ;; The maximum random root-chromosome genomes from different columns to grow during each generation step.
          (setq myChromosomeINIT 0000)      ;; The maximum random root-chromosome genomes from different columns to grow during the initialization step.
	      (setq myChromosomeRule REG:)      ;; The rule used for root-chromosome search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myGreedyDepth 25)    		;; The depth of greedy search exploration of each chosen column island  during each generation step. 
          (setq myGreedyGEN 1000)    		;; The number of greedy search WFFs the system will grow in the best-of-breed island  during each generation step. 
          (setq myGreedyWidth narrow:)    	;; The width of each greedy search WFF the system will grow in the best-of-breed island (base, full, narrow, random). 
          (setq myGreedyRule REG:)          ;; The rule used for greedy search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myGrowSelINIT 0000)   		;; The number of random WFFs the system will grow in the best-of-breed island during the initialization step.
          (setq myGrowSelGEN 00)    		;; The number of random WFFs the system will grow in the best-of-breed island during each generation step. 
          (setq myGrowSelRule REG:)         ;; The rule used for random WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myGrowWFFStyle root:)       ;; The style used for random WFFs grown in the best-of-breed island (full, or root). 
          (setq myMaxColWFFLen 100)  		;; The maximum length of a single WFF for each column.
          (setq myMutateColPct 0.00) 		;; The probability of mutation in each column island during each generation step.
          (setq myMutateSelPct 0.10) 		;; The probability mutation in the best-of-breed island during each generation step.
          (setq myREGMultiple false)        ;; Support multiple columns when growing REG regression expressions.
          (setq myRestartGap 00010) 		;; The count of generations to allow, without measurable improvement, before initiating a new evolutionary process.
          (setq myRootINIT 1000)  	 		;; The number of root WFFs the system will grow in each column island during the initialization step.
          (setq myRootGEN 0000)       		;; The number of root WFFs the system will grow in each column island during each generation step. 
          (setq myRootRule REG:)            ;; The rule used for root WFFs grown in each column island (BGM, FRM, MVL, ENN, REG, or SVM). 
	      (setq mySampleFactor 10)          ;; The reduction factor for determining the size of the sample training data (when mySampleON is true).
          (setq myScoreFocus top05:)  	    ;; The focus of scoring on the training data (full, top25, top10, or top05).
          (setq myTimeON  true)  			;; Support separate sequence scoring of each individual time period.
	      (setq myTrainingFactor 01)        ;; The reduction factor for determining the size of the final training data (when mySampleON is true).
          (setq myUseREG  true)             ;; Grow REG selector candidates during the initialization step and during each generation step.
        )) ; end select5REG case
       ;; Set the regress5REG option settings.
       ;; Note: These options work well for classification and sequencing of individuals in the the top 5%.
       ((= name regress5REG:)
        (begin
          (setq myCrossColPct 0.00)  		;; The probability of cross over in each column island during each generation step.
          (setq myCrossSelPct 1.00)  		;; The probability of cross over in the best-of-breed island during each generation step.
          (setq myChromosomeGEN 0000)       ;; The maximum random root-chromosome genomes from different columns to grow during each generation step.
          (setq myChromosomeINIT 0000)      ;; The maximum random root-chromosome genomes from different columns to grow during the initialization step.
	      (setq myChromosomeRule REG:)      ;; The rule used for root-chromosome search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myGreedyDepth 25)    		;; The depth of greedy search exploration of each chosen column island  during each generation step. 
          (setq myGreedyGEN 1000)    		;; The number of greedy search WFFs the system will grow in the best-of-breed island  during each generation step. 
          (setq myGreedyWidth narrow:)    	;; The width of each greedy search WFF the system will grow in the best-of-breed island (base, full, narrow, random). 
          (setq myGreedyRule REG:)          ;; The rule used for greedy search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myGrowSelINIT 0000)   		;; The number of random WFFs the system will grow in the best-of-breed island during the initialization step.
          (setq myGrowSelGEN 00)    		;; The number of random WFFs the system will grow in the best-of-breed island during each generation step. 
          (setq myGrowSelRule REG:)         ;; The rule used for random WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myGrowWFFStyle root:)       ;; The style used for random WFFs grown in the best-of-breed island (full, or root). 
          (setq myMaxColWFFLen 100)  		;; The maximum length of a single WFF for each column.
          (setq myMutateColPct 0.00) 		;; The probability of mutation in each column island during each generation step.
          (setq myMutateSelPct 0.10) 		;; The probability mutation in the best-of-breed island during each generation step.
          (setq myREGMultiple false)        ;; Support multiple columns when growing REG regression expressions.
          (setq myRestartGap 00010) 		;; The count of generations to allow, without measurable improvement, before initiating a new evolutionary process.
          (setq myRootINIT 1000)  	 		;; The number of root WFFs the system will grow in each column island during the initialization step.
          (setq myRootGEN 0000)       		;; The number of root WFFs the system will grow in each column island during each generation step. 
          (setq myRootRule REG:)            ;; The rule used for root WFFs grown in each column island (BGM, FRM, MVL, ENN, REG, or SVM). 
	      (setq mySampleFactor 10)          ;; The reduction factor for determining the size of the sample training data (when mySampleON is true).
          (setq myScoreFocus full:)  	    ;; The focus of scoring on the training data (full, top25, top10, or top05).
          (setq myTimeON  false)  			;; Support separate sequence scoring of each individual time period.
	      (setq myTrainingFactor 01)        ;; The reduction factor for determining the size of the final training data (when mySampleON is true).
          (setq myUseREG  true)             ;; Grow REG selector candidates during the initialization step and during each generation step.
        )) ; end regress5REG case
       ;; Set the regress5MVL option settings.
       ;; Note: These options work well for classification and sequencing of individuals in the the top 5%.
       ((= name regress5MVL:)
        (begin
          (setq myCrossColPct 0.00)  		;; The probability of cross over in each column island during each generation step.
          (setq myCrossSelPct 1.00)  		;; The probability of cross over in the best-of-breed island during each generation step.
          (setq myChromosomeGEN 0000)       ;; The maximum random root-chromosome genomes from different columns to grow during each generation step.
          (setq myChromosomeINIT 0000)      ;; The maximum random root-chromosome genomes from different columns to grow during the initialization step.
	      (setq myChromosomeRule REG:)      ;; The rule used for root-chromosome search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myGreedyDepth 25)    		;; The depth of greedy search exploration of each chosen column island  during each generation step. 
          (setq myGreedyGEN 1000)    		;; The number of greedy search WFFs the system will grow in the best-of-breed island  during each generation step. 
          (setq myGreedyWidth narrow:)    	;; The width of each greedy search WFF the system will grow in the best-of-breed island (base, full, narrow, random). 
          (setq myGreedyRule MVL:)          ;; The rule used for greedy search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myGrowSelINIT 0000)   		;; The number of random WFFs the system will grow in the best-of-breed island during the initialization step.
          (setq myGrowSelGEN 00)    		;; The number of random WFFs the system will grow in the best-of-breed island during each generation step. 
          (setq myGrowSelRule REG:)         ;; The rule used for random WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myGrowWFFStyle root:)       ;; The style used for random WFFs grown in the best-of-breed island (full, or root). 
          (setq myMaxColWFFLen 100)  		;; The maximum length of a single WFF for each column.
          (setq myMutateColPct 0.00) 		;; The probability of mutation in each column island during each generation step.
          (setq myMutateSelPct 0.10) 		;; The probability mutation in the best-of-breed island during each generation step.
          (setq myREGMultiple false)        ;; Support multiple columns when growing REG regression expressions.
          (setq myRestartGap 00010) 		;; The count of generations to allow, without measurable improvement, before initiating a new evolutionary process.
          (setq myRootINIT 1000)  	 		;; The number of root WFFs the system will grow in each column island during the initialization step.
          (setq myRootGEN 0000)       		;; The number of root WFFs the system will grow in each column island during each generation step. 
          (setq myRootRule REG:)            ;; The rule used for root WFFs grown in each column island (BGM, FRM, MVL, ENN, REG, or SVM). 
	      (setq mySampleFactor 10)          ;; The reduction factor for determining the size of the sample training data (when mySampleON is true).
          (setq myScoreFocus full:)  	    ;; The focus of scoring on the training data (full, top25, top10, or top05).
          (setq myTimeON  false)  			;; Support separate sequence scoring of each individual time period.
	      (setq myTrainingFactor 01)        ;; The reduction factor for determining the size of the final training data (when mySampleON is true).
          (setq myUseMVL  true)             ;; Grow MVL selector candidates during the initialization step and during each generation step.
          (setq myUseREG  true)             ;; Grow REG selector candidates during the initialization step and during each generation step.
        )) ; end regress5MVL case
       ;; Set the defaultSVM option settings.
       ;; Note: These options work well for general multivariate regressions.
       ((= name defaultSVM:)
        (begin
	      (setq myCrossColPct 1.00)  		;; The probability of cross over in each column island during each generation step.
      	  (setq myCrossSelPct 1.00)  		;; The probability of cross over in the best-of-breed island during each generation step.
          (setq myGrowSelINIT 1000)   		;; The number of random WFFs the system will grow in the best-of-breed island during the initialization step.
          (setq myGrowSelGEN 00)    		;; The number of random WFFs the system will grow in the best-of-breed island during each generation step. 
          (setq myGrowSelRule SVM:)         ;; The rule used for random WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
	      (setq myMutateColPct 0.10) 		;; The probability of mutation in each column island during each generation step.
	      (setq myMutateSelPct 0.10) 		;; The probability mutation in the best-of-breed island during each generation step.
          (setq mySamplingON  true)   	    ;; Support initial scoring on a small sample set before scoring on the full training data set.
          (setq mySvmKernelID cube:)       	;; The kernelID to use for all SVM learning (see svmRegress: all, binary, bipolar, composite, cosine, cube, exp, linear, log, poly, quart, sigmoid, sine, square, tan, tanh).
          (setq mySVMMaximum 1000)     		;; The maximum number of chromosomes to allowed in any one SVM regression.
          (setq mySvmMaxGen 1)             	;; The maximum number of training generations to use for all SVM learning (see svmRegress).
          (setq mySvmMaxLayers 1)           ;; The maximum number of training layers to use for all SVM learning (see svmRegress).
          (setq mySvmModelCount 1)          ;; The maximum number of training models to use for all SVM learning (see svmRegress).
          (setq myUseSVM  true)             ;; Grow SVM selector candidates during the initialization step and during each generation step.
        )) ; end defaultFRM case
       ;; Set the defaultGPSR option settings.
       ;; Note: These options work well for regression and sequencing of all individuals using classic genetic programming symbolic regression.
       ((= name defaultGPSR:)
        (begin
          (setq myGrowSelINIT 5000)   		;; The number of random WFFs the system will grow in the best-of-breed island during the initialization step.
          (setq myGrowSelGEN 00)    		;; The number of random WFFs the system will grow in the best-of-breed island during each generation step. 
          (setq myGrowSelRule REG:)         ;; The rule used for random WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myGrowWFFStyle full:)      	;; The style used for random WFFs grown in the best-of-breed island (full, or root). 
          (setq myMaxColWFFLen 100)  		;; The maximum length of a single WFF for each column.
          (setq myREGMultiple false)        ;; Support multiple columns when growing REG regression expressions.
          (setq myREGOperatorJoin "+")      ;; The operator used to join multiple columns when growing REG regression expressions ("+" or "*").
          (setq mySamplingON  false)   	    ;; Support initial scoring on a small sample set before scoring on the full training data set.
          (setq myTimeON false)  			;; Support no time sequence scoring of each individual time period.
          (setq myUseREG  true)             ;; Grow REG selector candidates during the initialization step and during each generation step.
        )) ; end defaultGPSR case
       ;; Set the regress2GPSR option settings.
       ;; Note: These options work well for regression and sequencing of all individuals using classic genetic programming symbolic regression.
       ((= name regressGPSR:)
        (begin
          (setq myGrowSelINIT 5000)   		;; The number of random WFFs the system will grow in the best-of-breed island during the initialization step.
          (setq myGrowSelGEN 00)    		;; The number of random WFFs the system will grow in the best-of-breed island during each generation step. 
          (setq myGrowSelRule REG:)         ;; The rule used for random WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myGrowWFFStyle full:)      	;; The style used for random WFFs grown in the best-of-breed island (full, or root). 
          (setq myMaxColWFFLen 100)  		;; The maximum length of a single WFF for each column.
          (setq myREGMultiple false)        ;; Support multiple columns when growing REG regression expressions.
          (setq myREGOperatorJoin "+")      ;; The operator used to join multiple columns when growing REG regression expressions ("+" or "*").
          (setq myTimeON false)  			;; Support no time sequence scoring of each individual time period.
          (setq myUseREG  true)             ;; Grow REG selector candidates during the initialization step and during each generation step.
        )) ; end regressGPSR case
       ;; Set the regress2GPSR option settings.
       ;; Note: These options work well for regression and sequencing of all individuals using classic genetic programming symbolic regression.
       ((= name regressDGPSR:)
        (begin
          (setq myCrossLinearSW  true)  	;; The use of a linear grammar during cross over operations.
          (setq myCrossMarrySW true)  	    ;; The use of expression marrying during cross over operations.
      	  (setq myCrossSelPct 1.00)  		;; The probability of cross over in the best-of-breed island during each generation step.
          (setq myCrossSpliceSW true)  		;; The use of expression splicing during cross over operations.
          (setq myGrowSelINIT 5000)   		;; The number of random WFFs the system will grow in the best-of-breed island during the initialization step.
          (setq myGrowSelGEN 00)    		;; The number of random WFFs the system will grow in the best-of-breed island during each generation step. 
          (setq myGrowSelRule REG:)         ;; The rule used for random WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myGrowWFFStyle full:)      	;; The style used for random WFFs grown in the best-of-breed island (full, or root). 
	      (setq myMaxColWFFLen 10000)  		;; The maximum length of a single WFF for each column.
	      (setq myMutateSelPct 0.10) 		;; The probability mutation in the best-of-breed island during each generation step.
          (setq myMutateSpliceSW true)  	;; The use of expression splicing during mutation operations.
          (setq myREGMultiple false)        ;; Support multiple columns when growing REG regression expressions.
	      (setq myREGOperatorJoin "+")      ;; The operator used to join multiple columns when growing REG regression expressions ("+" or "*").
          (setq myRestartGap 0010)  		;; The count of generations to allow, without measurable improvement, before initiating a new evolutionary process.
	      (setq mySampleFactor 100)         ;; The reduction factor for determining the size of the sample training data (when mySampleON is true).
          (setq mySamplingON  true)   	    ;; Support initial scoring on a small sample set before scoring on the full training data set.
          (setq mySurvivors 025)     		;; The count of surviving selector WFFs in each generation.
          (setq myTimeON false)  			;; Support separate sequence scoring of each individual time period.
	      (setq myTrainingFactor 10)        ;; The reduction factor for determining the size of the final training data (when mySampleON is true).
          (setq myUseMVL  true)             ;; Grow MVL selector candidates during the initialization step and during each generation step.
          (setq myUseREG  true)             ;; Grow REG selector candidates during the initialization step and during each generation step.
          (setq myWFFReinitSW false)        ;; The swtich controlling the reinitialization of WFF cache at the start of every new run.
        )) ; end regress2GPSR case
       ;; Set the regress3GPSR option settings.
       ;; Note: These options work well for regression and sequencing of all individuals using classic genetic programming symbolic regression.
       ((= name regress3GPSR:)
        (begin
      	  (setq myCrossSelPct 1.00)  		;; The probability of cross over in the best-of-breed island during each generation step.
          (setq myGrowSelINIT 5000)   		;; The number of random WFFs the system will grow in the best-of-breed island during the initialization step.
          (setq myGrowSelGEN 00)    		;; The number of random WFFs the system will grow in the best-of-breed island during each generation step. 
          (setq myGrowSelRule REG:)         ;; The rule used for random WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
          (setq myGrowWFFStyle full:)      	;; The style used for random WFFs grown in the best-of-breed island (full, or root). 
          (setq myMaxColWFFLen 1000)  		;; The maximum length of a single WFF for each column.
          (setq myMutateSelPct 1.00) 		;; The probability mutation in the best-of-breed island during each generation step.
          (setq myREGMultiple false)        ;; Support multiple columns when growing REG regression expressions.
          (setq myREGOperatorJoin "+")      ;; The operator used to join multiple columns when growing REG regression expressions ("+" or "*").
          (setq myTimeON false)  			;; Support no time sequence scoring of each individual time period.
          (setq myUseREG  true)             ;; Grow REG selector candidates during the initialization step and during each generation step.
        )) ; end regress3GPSR case
       (else (error (append "esm.setOptions: invalid default option name [" name "]")))
       ) ; end option cond
      true) ; end setOptions
   ;; Perform a series of diagnostic self tests of this learning machine.
   (defun selfTest(Symbol:Test Integer:Ts Integer:Ns Integer:Ms Integer:Gs Number:Ss ...)
       regs:(B k m M mstart Mp1 mm1 mm MM n N nn NX t T)
       regs:(Number:y Number:ey Number:score Number:sortScore Number:oldY Number:errPct Number:err  Number:avgY Number:avgDevY Number:avgFactor)
       vars:(Lambda XT Rf Ck Seed modelString aHistory eHistory dHistory yHistory (Integer:minTN 100)
             NumVector:C NumVector:x X NumVector:Y NumVector:EY IntVector:sortedY IntVector:sortedEY
             Number:startTime Number:endTime Number:startTimeT Number:endTimeT
             (maxFormulaDisplayLen 800)
             ) ; end temporary variables
       (clear)
       (if (>= (argCount) 7) then (setq Ck (argFetch 6)) else (setq Ck false))
       (if (>= (argCount) 8) then (setq Seed (number (argFetch 7))) else (setq Seed mySeedDefault))
       ;; Always make sure the test data is created using the same nil seed.
       ;; Note: We may want to vary the random seeds for finding solutions,
       ;;       but we always want to be finding solutions to the same problem.
       ;;       Therefore we always generate the training data with a fixed seed.    
       (setq mySeed 0.0)     
       (setq srandom.seed mySeed)      
       ;; Select the requested test case
       ;; Test Case crossCorrelation 
       (if (or (= Test all:) (= Test crossCorrelation:))
           (begin
		       (writeln _eol "Starting test case: crossCorrelation")
               (setq mySeed 0.0)     
               (setq srandom.seed mySeed)      
		       (setq M Ms)
               (if (= myTimeON true) (setq mstart 1) (setq mstart 0)) 
               (setq Mp1 (addi M mstart))
		       (setq N Ns)
		       (setq T Ts)
		       (setq X (new Vector: Object: (muli N T)))
		       (setq Y (new Vector: Number: (muli N T)))
		       (setq C (new Vector: Number: Mp1))
		       (loop for m from 0 until Mp1 do
		          (setq C[m] (- (random 100.0) 50.0))
		          ) ; end C loop
               (setq nn 0)
			   (loop for t from 0 until T do
	              (setq k (modi t 4)) 
			      (loop for n from 0 until N do
			         (setq X[nn] (setq x (new Vector: Number: Mp1)))
			         (if (= myTimeON true) (setq x[0] (number t)))
			         (setq y C[0])
			         (loop for m from mstart until Mp1 do
			          	(setq x[m] (- (random 100.0) 50.0))
	                    ;; Cross Correlation model
                        (cond 
	                     ((= m 0) (setq y (+ y (* x[m] x[m] x[m] C[m]))))
	                     ((= m 1) (setq y (+ y (* x[(- m 1)] x[m] x[m] C[m]))))
	                     (else (setq y (+ y (* x[(- m 2)] x[(- m 1)] x[m] C[m]))))
                         ) ; end cond
			            ) ; end M loop
			         (setq Y[nn] (+ (* y (- 1.0 (* .50 myRandomError))) (* y (random myRandomError))))
                     (setCdr x y)
                     (++ nn)
			         ) ; end N loop
			       ) ; end T loop

		       ;; Train on the test case.
		       (setq modelString (append "Building test data as: y = " C[0]))
		       (loop for m from 0 until Mp1 do
                  (cond 
	                ((= m 0) (setq modelString (append modelString (if (< C[m] 0.0) " - (" " + (") (abs C[m]) "*" (esm.ruleExp.ruleName m) "*" (esm.ruleExp.ruleName m) "*" (esm.ruleExp.ruleName m) ")")))
	                ((= m 1) (setq modelString (append modelString (if (< C[m] 0.0) " - (" " + (") (abs C[m]) "*" (esm.ruleExp.ruleName (- m 1)) "*" (esm.ruleExp.ruleName m) "*" (esm.ruleExp.ruleName m) ")")))
	                (else (setq modelString (append modelString (if (< C[m] 0.0) " - (" " + (") (abs C[m]) "*" (esm.ruleExp.ruleName (- m 2)) "*" (esm.ruleExp.ruleName (- m 1)) "*" (esm.ruleExp.ruleName m) ")")))
                    ) ; end cond
		          ) ; end C loop
		       (writeln modelString ";")
               (if (<> myRandomError 0.0) (writeln "Additionally, we add random error as: y = (y * " (- 1.0 (* .50 myRandomError)) ") + (random " myRandomError ");")) 
               (setq startTimeT (getTickCount 0))
		       (setq Lambda (esm X Y Gs Ss Ck Seed))
               (setq endTimeT (getTickCount startTimeT))
		       (writeln "") 
		       (writeln "Final results of training")
		       (writeln "esm: N = [" Ns "], M = [" Mp1 "], Generations = [" myGc "], WFFs = [" (length myWFFs)  "], Score=[" Lambda.Score "], ScoreHistory=[" (string Lambda.History true) "]") 
               (writeln "esm.myBest, Score=[" Lambda.Score "], ErrorPct=[" Lambda.ErrorPct "], lengthWFF=[" (length Lambda.WFF) "], WFF = " (trunc (esm.evalRule Lambda.WFF) maxFormulaDisplayLen))
               (writeln "esm.myBestRegressor, Score=[" myBestRegressor.Score "], ErrorPct=[" myBestRegressor.ErrorPct "], lengthWFF=[" (length myBestRegressor.WFF) "], WFF = " (trunc (esm.evalRule myBestRegressor.WFF) maxFormulaDisplayLen))
               (writeln "myBestSelectorChampions")
               (loop for n from 0 until (length esm.myBestSelectorChampions) do (writeln "myBestSelectorChampions[" n "],Score=[" esm.myBestSelectorChampions[n 1].Score ",ErrorPct=[" esm.myBestSelectorChampions[n 1].ErrorPct "], WFF = " (trunc (esm.evalRule esm.myBestSelectorChampions[n 1].WFF) maxFormulaDisplayLen))) 
               (writeln "myBestRegressorChampions")
               (loop for n from 0 until (length esm.myBestRegressorChampions) do (writeln "myBestRegressorChampions[" n "],ErrorPct=[" esm.myBestRegressorChampions[n 1].ErrorPct ",Score=[" esm.myBestRegressorChampions[n 1].Score "], WFF = " (trunc (esm.evalRule esm.myBestRegressorChampions[n 1].WFF) maxFormulaDisplayLen))) 

               (if (= myVerboseSW true)
                   (begin
		             
                     ;; Show the champions from each island population.
		             (writeln "") 
		             (writeln "Show the champions from each island population")
                     (setq M (length esm.myPopulationIslands))
                     (loop for m from 0 until M do
                       (writeln "Champions of Island [" m "], RootWFFCount=[" myIslands[m] "]...")
                       (setq N (length esm.myPopulationIslands[m])) 
                       (loop for n from 0 until N do
                          (setq Lambda esm.myPopulationIslands[m][n])
		                  (if (<> Lambda #void) (writeln "esm.myPopulationIslands[" m "][" n "], Score=[" Lambda.Score "," Lambda.ErrorPct "], lengthWFF=[" (length Lambda.WFF) "], WFF = " (trunc (esm.evalRule Lambda.WFF) maxFormulaDisplayLen))) 
                          ) ; end population loop
                       ) ; end island loop

		             ;; Run on the training data.
		             (writeln "")
                     (setq Lambda esm.myBest) 
		             (writeln "Show final results on training data, Score=[" myBest.Score "], ErrorPct=[" myBest.ErrorPct "]")
                     (setq NX (length Y))
                     (setq avgY (avg Y))
                     (setq avgDevY 0.0)
                     (loop for n from 0 until NX (setq avgDevY (+ avgDevY (abs (- Y[n] avgY)))))
                     (/= avgDevY (number NX))
                     (setq sortedY (|Gv:sort| Y < true))
                     (setq nn (integer (max 1 (/ NX 20))))
                     (setq errPct 0.0)
                     (setq EY (Lambda.run X))
                     (setq avgFactor (abs (+ avgDevY .000000000001))) 
                     (loop for n from 0 until NX do
                        (setq k sortedY[n])
                        (setq ey EY[k])
                        (setq y Y[k])
	                    (setq err (- ey y))
	                    (setq err (abs err))
                        (/= err avgFactor) 
                        (if (> err 2.0) (setq err (+ 1.0 (expt err .001)))) 
	                    (+= errPct err)
		                (if (= (modi n nn) 0) (writeln "X[" n "], ey=[" ey "], y=[" y "], ErrorPct=[" err "]"))
                     ) ; end N loop
                     (/= errPct (number NX)) 
                     (setq yHistory (new Vector: Number: 10))
                     (setq eHistory (new Vector: Number: 10))
                     (setq aHistory (new Vector: Number: 10))
                     (setq dHistory (new Vector: Number: 5))
                     (setq MM (integer (max 1 (/ NX 10))))
                     (setq sortedEY (|Gv:sort| EY < true))
                     (loop for n from 0 until NX do
                        (setq k sortedEY[n])
                        (setq ey EY[k])
                        (setq y Y[k])
                        (setq mm (/ n MM))
                        (setq eHistory[mm] (+ eHistory[mm] (/ y MM)))
                        (setq k sortedY[n])
                        (setq y Y[k])
                        (setq yHistory[mm] (+ yHistory[mm] (/ y MM)))
                        ) ; end N loop
                     (setq aHistory[4] (avg eHistory[4] eHistory[3] eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[5] (avg eHistory[5] eHistory[6] eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[3] (avg eHistory[3] eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[6] (avg eHistory[6] eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[2] (avg eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[7] (avg eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[1] (avg eHistory[1] eHistory[0]))
                     (setq aHistory[8] (avg eHistory[8] eHistory[9]))
                     (setq aHistory[0] eHistory[0])
                     (setq aHistory[9] eHistory[9])
                     (setq dHistory[4] (- aHistory[9] aHistory[0]))
                     (setq dHistory[3] (- aHistory[8] aHistory[1]))
                     (setq dHistory[2] (- aHistory[7] aHistory[2]))
                     (setq dHistory[1] (- aHistory[6] aHistory[3]))
                     (setq dHistory[0] (- aHistory[5] aHistory[4]))
		             (writeln "Actual computed error on training data is ErrorPct=[" errPct "] versus reported ErrorPct=[" Lambda.ErrorPct "] while average Y is AvgY=[" (avg Y) "]")
		             (writeln "yHistory=[" (string yHistory true) "]")
		             (writeln "eHistory=[" (string eHistory true) "]")
		             (writeln "aHistory=[" (string aHistory true) "]")
		             (writeln "dHistory=[" (string dHistory true) "]")
		             (writeln "") 

                   )) ; end if 

		       ;; Build the test data.
               (setq srandom.seed (* mySeed (pi)))
		       (setq M Ms)
		       (setq N Ns)
		       (setq T Ts)
		       (setq X (new Vector: Object: (muli N)))
		       (setq Y (new Vector: Number: (muli N)))
               (setq nn 0)
			   (loop for t from T to T do
	              (setq k (modi t 4)) 
			      (loop for n from 0 until N do
			         (setq X[nn] (setq x (new Vector: Number: Mp1)))
			         (if (= myTimeON true) (setq x[0] (number t)))
			         (setq y C[0])
			         (loop for m from mstart until Mp1 do
			          	(setq x[m] (- (random 100.0) 50.0))
	                    ;; Cross Correlation model
                        (cond 
	                     ((= m 0) (setq y (+ y (* x[m] x[m] x[m] C[m]))))
	                     ((= m 1) (setq y (+ y (* x[(- m 1)] x[m] x[m] C[m]))))
	                     (else (setq y (+ y (* x[(- m 2)] x[(- m 1)] x[m] C[m]))))
                         ) ; end cond
			            ) ; end M loop
			         (setq Y[nn] (+ (* y (- 1.0 (* .50 myRandomError))) (* y (random myRandomError))))
                     (setCdr x y)
                     (++ nn)
			         ) ; end N loop
			       ) ; end T loop

		       ;; Run on the test data.
               (setq Lambda esm.myBest)
               (if (>= (length X) minTN)
                      (begin (setq XT (rowManager X)) (setq score (XT.score Lambda)))
                      (setq score Lambda.Score)
                      ) ; end if
		       (writeln "") 
		       (writeln "Final testing on test data returns Score=[" score "], ErrorPct=[" Lambda.ErrorPct "]")
               (if true
                   (begin
                     (setq NX (length Y))
                     (setq avgY (avg Y))
                     (setq avgDevY 0.0)
                     (loop for n from 0 until NX (setq avgDevY (+ avgDevY (abs (- Y[n] avgY)))))
                     (/= avgDevY (number NX))
                     (setq sortedY (|Gv:sort| Y < true))
                     (setq nn (integer (max 1 (/ NX 20))))
                     (setq errPct 0.0)
                     (setq EY (Lambda.run X))
                     (setq avgFactor (abs (+ avgDevY .000000000001))) 
                     (loop for t from T to T do
                        (loop for n from 0 until NX do
                              (setq k sortedY[n])
                              (setq ey EY[k])
                              (setq ey (Lambda X[k]))
                              (setq y Y[k])
	                          (setq err (- ey y))
	                          (setq err (abs err))
                              (/= err avgFactor) 
                              (if (> err 2.0) (setq err (+ 1.0 (expt err .001)))) 
	                          (+= errPct err)
		                      (if (= (modi n nn) 0) (writeln "X[" n "], ey=[" ey "], y=[" y "], ErrorPct=[" err "]"))
                           ) ; end N loop
                         ) ; end T loop
                     (/= errPct (number NX)) 
                     (setq yHistory (new Vector: Number: 10))
                     (setq eHistory (new Vector: Number: 10))
                     (setq aHistory (new Vector: Number: 10))
                     (setq dHistory (new Vector: Number: 5))
                     (setq MM (integer (max 1 (/ NX 10))))
                     (setq sortedEY (|Gv:sort| EY < true))
                     (setq sortScore 0.0)
                     (setq oldY Y[sortedEY[0]])
                     (loop for t from T to T do
                        (loop for n from 0 until NX do
                              (setq k sortedEY[n])
                              (setq ey EY[k])
                              (setq y Y[k])
                              (if (> oldY y) (++ sortScore))(setq oldY y)
                              (setq mm (/ n MM))
                              (setq eHistory[mm] (+ eHistory[mm] (/ y MM)))
                              (setq k sortedY[n])
                              (setq y Y[k])
                              (setq yHistory[mm] (+ yHistory[mm] (/ y MM)))
                           ) ; end N loop
                         ) ; end T loop
                     (/= sortScore (number NX))
                     (setq aHistory[4] (avg eHistory[4] eHistory[3] eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[5] (avg eHistory[5] eHistory[6] eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[3] (avg eHistory[3] eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[6] (avg eHistory[6] eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[2] (avg eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[7] (avg eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[1] (avg eHistory[1] eHistory[0]))
                     (setq aHistory[8] (avg eHistory[8] eHistory[9]))
                     (setq aHistory[0] eHistory[0])
                     (setq aHistory[9] eHistory[9])
                     (setq dHistory[4] (- aHistory[9] aHistory[0]))
                     (setq dHistory[3] (- aHistory[8] aHistory[1]))
                     (setq dHistory[2] (- aHistory[7] aHistory[2]))
                     (setq dHistory[1] (- aHistory[6] aHistory[3]))
                     (setq dHistory[0] (- aHistory[5] aHistory[4]))
		             (writeln "Actual computed error on testing data is ErrorPct=[" errPct "], Sort=[" sortScore "], Avg Y=[" (avg Y) "], AvgDev Y=[" avgDevY "]")
		             (writeln "yHistory=[" (string yHistory true) "]")
		             (writeln "eHistory=[" (string eHistory true) "]")
		             (writeln "aHistory=[" (string aHistory true) "]")
		             (writeln "dHistory=[" (string dHistory true) "]")
		             (writeln "") 
                   )) ; end if 
               (writeln "esm.selfTest[crossCorrelation]: completed in [" (/ endTimeT 60.0) "] minutes.")       
         )) ; end Test Case crossCorrelation
       ;; Test Case cubicRegression 
       (if (or (= Test all:) (= Test cubicRegression:))
           (begin
		       (writeln _eol "Starting test case: cubicRegression")
               (setq mySeed 0.0)     
               (setq srandom.seed mySeed)      
		       (setq M Ms)
               (if (= myTimeON true) (setq mstart 1) (setq mstart 0)) 
               (setq Mp1 (addi M mstart))
		       (setq N Ns)
		       (setq T Ts)
		       (setq X (new Vector: Object: (muli N T)))
		       (setq Y (new Vector: Number: (muli N T)))
		       (setq C (new Vector: Number: Mp1))
		       (loop for m from 0 until Mp1 do
		          (setq C[m] (- (random 100.0) 50.0))
		          ) ; end C loop
               (setq nn 0)
			   (loop for t from 0 until T do
	              (setq k (modi t 4)) 
			      (loop for n from 0 until N do
			         (setq X[nn] (setq x (new Vector: Number: Mp1)))
			         (if (= myTimeON true) (setq x[0] (number t)))
			         (setq y C[0])
			         (loop for m from mstart until Mp1 do
			          	(setq x[m] (- (random 100.0) 50.0))
	                    ;; Cubic model
	                    (setq y (+ y (* x[m] x[m] x[m] C[m])))
			            ) ; end M loop
			         (setq Y[nn] (+ (* y (- 1.0 (* .50 myRandomError))) (* y (random myRandomError))))
                     (setCdr x y)
                     (++ nn)
			         ) ; end N loop
			       ) ; end T loop

		       ;; Train on the test case.
		       (setq modelString (append "Building test data as: y = " C[0]))
		       (loop for m from mstart until Mp1 do
                  (setq modelString (append modelString (if (< C[m] 0.0) " - (" " + (") (abs C[m]) "*" (esm.ruleExp.ruleName m) "*" (esm.ruleExp.ruleName m) "*" (esm.ruleExp.ruleName m) ")"))
		          ) ; end C loop
		       (writeln modelString ";")
               (if (<> myRandomError 0.0) (writeln "Additionally, we add random error as: y = (y * " (- 1.0 (* .50 myRandomError)) ") + (random " myRandomError ");")) 
               (setq startTimeT (getTickCount 0))
		       (setq Lambda (esm X Y Gs Ss Ck Seed))
               (setq endTimeT (getTickCount startTimeT))
		       (writeln "") 
		       (writeln "Final results of training")
		       (writeln "esm: N = [" Ns "], M = [" Mp1 "], Generations = [" myGc "], WFFs = [" (length myWFFs)  "], Score=[" Lambda.Score "], ScoreHistory=[" (string Lambda.History true) "]") 
               (writeln "esm.myBest, Score=[" Lambda.Score "], ErrorPct=[" Lambda.ErrorPct "], lengthWFF=[" (length Lambda.WFF) "], WFF = " (trunc (esm.evalRule Lambda.WFF) maxFormulaDisplayLen))
               (writeln "esm.myBestRegressor, Score=[" myBestRegressor.Score "], ErrorPct=[" myBestRegressor.ErrorPct "], lengthWFF=[" (length myBestRegressor.WFF) "], WFF = " (trunc (esm.evalRule myBestRegressor.WFF) maxFormulaDisplayLen))
               (writeln "myBestSelectorChampions")
               (loop for n from 0 until (length esm.myBestSelectorChampions) do (writeln "myBestSelectorChampions[" n "],Score=[" esm.myBestSelectorChampions[n 1].Score ",ErrorPct=[" esm.myBestSelectorChampions[n 1].ErrorPct "], WFF = " (trunc (esm.evalRule esm.myBestSelectorChampions[n 1].WFF) maxFormulaDisplayLen))) 
               (writeln "myBestRegressorChampions")
               (loop for n from 0 until (length esm.myBestRegressorChampions) do (writeln "myBestRegressorChampions[" n "],ErrorPct=[" esm.myBestRegressorChampions[n 1].ErrorPct ",Score=[" esm.myBestRegressorChampions[n 1].Score "], WFF = " (trunc (esm.evalRule esm.myBestRegressorChampions[n 1].WFF) maxFormulaDisplayLen))) 

               (if (= myVerboseSW true)
                   (begin
		             
                     ;; Show the champions from each island population.
		             (writeln "") 
		             (writeln "Show the champions from each island population")
                     (setq M (length esm.myPopulationIslands))
                     (loop for m from 0 until M do
                       (writeln "Champions of Island [" m "], RootWFFCount=[" myIslands[m] "]...")
                       (setq N (length esm.myPopulationIslands[m])) 
                       (loop for n from 0 until N do
                          (setq Lambda esm.myPopulationIslands[m][n])
		                  (if (<> Lambda #void) (writeln "esm.myPopulationIslands[" m "][" n "], Score=[" Lambda.Score "," Lambda.ErrorPct "], lengthWFF=[" (length Lambda.WFF) "], WFF = " (trunc (esm.evalRule Lambda.WFF) maxFormulaDisplayLen))) 
                          ) ; end population loop
                       ) ; end island loop

		             ;; Run on the training data.
		             (writeln "")
                     (setq Lambda esm.myBest) 
		             (writeln "Show final results on training data, Score=[" myBest.Score "], ErrorPct=[" myBest.ErrorPct "]")
                     (setq NX (length Y))
                     (setq avgY (avg Y))
                     (setq avgDevY 0.0)
                     (loop for n from 0 until NX (setq avgDevY (+ avgDevY (abs (- Y[n] avgY)))))
                     (/= avgDevY (number NX))
                     (setq sortedY (|Gv:sort| Y < true))
                     (setq nn (integer (max 1 (/ NX 20))))
                     (setq errPct 0.0)
                     (setq EY (Lambda.run X))
                     (setq avgFactor (abs (+ avgDevY .000000000001))) 
                     (loop for n from 0 until NX do
                        (setq k sortedY[n])
                        (setq ey EY[k])
                        (setq y Y[k])
	                    (setq err (- ey y))
	                    (setq err (abs err))
                        (/= err avgFactor) 
                        (if (> err 2.0) (setq err (+ 1.0 (expt err .001)))) 
	                    (+= errPct err)
		                (if (= (modi n nn) 0) (writeln "X[" n "], ey=[" ey "], y=[" y "], ErrorPct=[" err "]"))
                     ) ; end N loop
                     (/= errPct (number NX)) 
                     (setq yHistory (new Vector: Number: 10))
                     (setq eHistory (new Vector: Number: 10))
                     (setq aHistory (new Vector: Number: 10))
                     (setq dHistory (new Vector: Number: 5))
                     (setq MM (integer (max 1 (/ NX 10))))
                     (setq sortedEY (|Gv:sort| EY < true))
                     (loop for n from 0 until NX do
                        (setq k sortedEY[n])
                        (setq ey EY[k])
                        (setq y Y[k])
                        (setq mm (/ n MM))
                        (setq eHistory[mm] (+ eHistory[mm] (/ y MM)))
                        (setq k sortedY[n])
                        (setq y Y[k])
                        (setq yHistory[mm] (+ yHistory[mm] (/ y MM)))
                        ) ; end N loop
                     (setq aHistory[4] (avg eHistory[4] eHistory[3] eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[5] (avg eHistory[5] eHistory[6] eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[3] (avg eHistory[3] eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[6] (avg eHistory[6] eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[2] (avg eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[7] (avg eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[1] (avg eHistory[1] eHistory[0]))
                     (setq aHistory[8] (avg eHistory[8] eHistory[9]))
                     (setq aHistory[0] eHistory[0])
                     (setq aHistory[9] eHistory[9])
                     (setq dHistory[4] (- aHistory[9] aHistory[0]))
                     (setq dHistory[3] (- aHistory[8] aHistory[1]))
                     (setq dHistory[2] (- aHistory[7] aHistory[2]))
                     (setq dHistory[1] (- aHistory[6] aHistory[3]))
                     (setq dHistory[0] (- aHistory[5] aHistory[4]))
		             (writeln "Actual computed error on training data is ErrorPct=[" errPct "] versus reported ErrorPct=[" Lambda.ErrorPct "] while average Y is AvgY=[" (avg Y) "]")
		             (writeln "yHistory=[" (string yHistory true) "]")
		             (writeln "eHistory=[" (string eHistory true) "]")
		             (writeln "aHistory=[" (string aHistory true) "]")
		             (writeln "dHistory=[" (string dHistory true) "]")
		             (writeln "") 

                   )) ; end if 

		       ;; Build the test data.
               (setq srandom.seed (* mySeed (pi)))
		       (setq M Ms)
               (if (= myTimeON true) (setq mstart 1) (setq mstart 0)) 
               (setq Mp1 (addi M mstart))
		       (setq N Ns)
		       (setq T Ts)
		       (setq X (new Vector: Object: (muli N)))
		       (setq Y (new Vector: Number: (muli N)))
               (setq nn 0)
			   (loop for t from T to T do
	              (setq k (modi t 4)) 
			      (loop for n from 0 until N do
			         (setq X[nn] (setq x (new Vector: Number: Mp1)))
			         (if (= myTimeON true) (setq x[0] (number t)))
			         (setq y C[0])
			         (loop for m from mstart until Mp1 do
			          	(setq x[m] (- (random 100.0) 50.0))
	                    ;; Cubic model
	                    (setq y (+ y (* x[m] x[m] x[m] C[m])))
			            ) ; end M loop
			         (setq Y[nn] (+ (* y (- 1.0 (* .50 myRandomError))) (* y (random myRandomError))))
                     (setCdr x y)
                     (++ nn)
			         ) ; end N loop
			       ) ; end T loop

		       ;; Run on the test data.
               (setq Lambda esm.myBest)
               (if (>= (length X) minTN)
                      (begin (setq XT (rowManager X)) (setq score (XT.score Lambda)))
                      (setq score Lambda.Score)
                      ) ; end if
		       (writeln "") 
		       (writeln "Final testing on test data returns Score=[" score "], ErrorPct=[" Lambda.ErrorPct "]")
               (if true
                   (begin
                     (setq NX (length Y))
                     (setq avgY (avg Y))
                     (setq avgDevY 0.0)
                     (loop for n from 0 until NX (setq avgDevY (+ avgDevY (abs (- Y[n] avgY)))))
                     (/= avgDevY (number NX))
                     (setq sortedY (|Gv:sort| Y < true))
                     (setq nn (integer (max 1 (/ NX 20))))
                     (setq errPct 0.0)
                     (setq EY (Lambda.run X))
                     (setq avgFactor (abs (+ avgDevY .000000000001))) 
                     (loop for t from T to T do
                        (loop for n from 0 until NX do
                              (setq k sortedY[n])
                              (setq ey EY[k])
                              (setq ey (Lambda X[k]))
                              (setq y Y[k])
	                          (setq err (- ey y))
	                          (setq err (abs err))
                              (/= err avgFactor) 
                              (if (> err 2.0) (setq err (+ 1.0 (expt err .001)))) 
	                          (+= errPct err)
		                      (if (= (modi n nn) 0) (writeln "X[" n "], ey=[" ey "], y=[" y "], ErrorPct=[" err "]"))
                           ) ; end N loop
                         ) ; end T loop
                     (/= errPct (number NX)) 
                     (setq yHistory (new Vector: Number: 10))
                     (setq eHistory (new Vector: Number: 10))
                     (setq aHistory (new Vector: Number: 10))
                     (setq dHistory (new Vector: Number: 5))
                     (setq MM (integer (max 1 (/ NX 10))))
                     (setq sortedEY (|Gv:sort| EY < true))
                     (setq sortScore 0.0)
                     (setq oldY Y[sortedEY[0]])
                     (loop for t from T to T do
                        (loop for n from 0 until NX do
                              (setq k sortedEY[n])
                              (setq ey EY[k])
                              (setq y Y[k])
                              (if (> oldY y) (++ sortScore))(setq oldY y)
                              (setq mm (/ n MM))
                              (setq eHistory[mm] (+ eHistory[mm] (/ y MM)))
                              (setq k sortedY[n])
                              (setq y Y[k])
                              (setq yHistory[mm] (+ yHistory[mm] (/ y MM)))
                           ) ; end N loop
                         ) ; end T loop
                     (/= sortScore (number NX))
                     (setq aHistory[4] (avg eHistory[4] eHistory[3] eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[5] (avg eHistory[5] eHistory[6] eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[3] (avg eHistory[3] eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[6] (avg eHistory[6] eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[2] (avg eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[7] (avg eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[1] (avg eHistory[1] eHistory[0]))
                     (setq aHistory[8] (avg eHistory[8] eHistory[9]))
                     (setq aHistory[0] eHistory[0])
                     (setq aHistory[9] eHistory[9])
                     (setq dHistory[4] (- aHistory[9] aHistory[0]))
                     (setq dHistory[3] (- aHistory[8] aHistory[1]))
                     (setq dHistory[2] (- aHistory[7] aHistory[2]))
                     (setq dHistory[1] (- aHistory[6] aHistory[3]))
                     (setq dHistory[0] (- aHistory[5] aHistory[4]))
		             (writeln "Actual computed error on testing data is ErrorPct=[" errPct "], Sort=[" sortScore "], Avg Y=[" (avg Y) "], AvgDev Y=[" avgDevY "]")
		             (writeln "yHistory=[" (string yHistory true) "]")
		             (writeln "eHistory=[" (string eHistory true) "]")
		             (writeln "aHistory=[" (string aHistory true) "]")
		             (writeln "dHistory=[" (string dHistory true) "]")
		             (writeln "") 
                   )) ; end if 
               (writeln "esm.selfTest[cubicRegression]: completed in [" (/ endTimeT 60.0) "] minutes.")       
         )) ; end Test Case cubicRegression
       ;; Test Case hyperTangent 
       (if (or (= Test all:) (= Test hyperTangent:))
           (begin
		       (writeln _eol "Starting test case: hyperTangent")
               (setq mySeed 0.0)     
               (setq srandom.seed mySeed)      
		       (setq M Ms)
               (if (= myTimeON true) (setq mstart 1) (setq mstart 0)) 
               (setq Mp1 (addi M mstart))
		       (setq N Ns)
		       (setq T Ts)
		       (setq X (new Vector: Object: (muli N T)))
		       (setq Y (new Vector: Number: (muli N T)))
		       (setq C (new Vector: Number: Mp1))
		       (loop for m from 0 until Mp1 do
		          (setq C[m] (- (random 100.0) 50.0))
		          ) ; end C loop
               (setq nn 0)
			   (loop for t from 0 until T do
	              (setq k (modi t 4)) 
			      (loop for n from 0 until N do
			         (setq X[nn] (setq x (new Vector: Number: Mp1)))
			         (if (= myTimeON true) (setq x[0] (number t)))
			         (setq y C[0])
			         (loop for m from mstart until Mp1 do
			          	(setq x[m] (- (random 100.0) 50.0))
	                    ;; HyperTangent model
	                    (setq y (+ y (tanh (* x[m] x[m] x[m] C[m]))))
			            ) ; end M loop
			         (setq Y[nn] (+ (* y (- 1.0 (* .50 myRandomError))) (* y (random myRandomError))))
                     (setCdr x y)
                     (++ nn)
			         ) ; end N loop
			       ) ; end T loop

		       ;; Train on the test case.
		       (setq modelString (append "Building test data as: y = " C[0]))
		       (loop for m from mstart until Mp1 do
                  (setq modelString (append modelString (if (< C[m] 0.0) " - (" " + (") (abs C[m]) "*tanh(" (esm.ruleExp.ruleName m) "*" (esm.ruleExp.ruleName m) "*" (esm.ruleExp.ruleName m) "))"))
		          ) ; end C loop
		       (writeln modelString ";")
               (if (<> myRandomError 0.0) (writeln "Additionally, we add random error as: y = (y * " (- 1.0 (* .50 myRandomError)) ") + (random " myRandomError ");")) 
               (setq startTimeT (getTickCount 0))
		       (setq Lambda (esm X Y Gs Ss Ck Seed))
               (setq endTimeT (getTickCount startTimeT))
		       (writeln "") 
		       (writeln "Final results of training")
		       (writeln "esm: N = [" Ns "], M = [" Mp1 "], Generations = [" myGc "], WFFs = [" (length myWFFs)  "], Score=[" Lambda.Score "], ScoreHistory=[" (string Lambda.History true) "]") 
               (writeln "esm.myBest, Score=[" Lambda.Score "], ErrorPct=[" Lambda.ErrorPct "], lengthWFF=[" (length Lambda.WFF) "], WFF = " (trunc (esm.evalRule Lambda.WFF) maxFormulaDisplayLen))
               (writeln "esm.myBestRegressor, Score=[" myBestRegressor.Score "], ErrorPct=[" myBestRegressor.ErrorPct "], lengthWFF=[" (length myBestRegressor.WFF) "], WFF = " (trunc (esm.evalRule myBestRegressor.WFF) maxFormulaDisplayLen))
               (writeln "myBestSelectorChampions")
               (loop for n from 0 until (length esm.myBestSelectorChampions) do (writeln "myBestSelectorChampions[" n "],Score=[" esm.myBestSelectorChampions[n 1].Score ",ErrorPct=[" esm.myBestSelectorChampions[n 1].ErrorPct "], WFF = " (trunc (esm.evalRule esm.myBestSelectorChampions[n 1].WFF) maxFormulaDisplayLen))) 
               (writeln "myBestRegressorChampions")
               (loop for n from 0 until (length esm.myBestRegressorChampions) do (writeln "myBestRegressorChampions[" n "],ErrorPct=[" esm.myBestRegressorChampions[n 1].ErrorPct ",Score=[" esm.myBestRegressorChampions[n 1].Score "], WFF = " (trunc (esm.evalRule esm.myBestRegressorChampions[n 1].WFF) maxFormulaDisplayLen))) 

               (if (= myVerboseSW true)
                   (begin
		             
                     ;; Show the champions from each island population.
		             (writeln "") 
		             (writeln "Show the champions from each island population")
                     (setq M (length esm.myPopulationIslands))
                     (loop for m from 0 until M do
                       (writeln "Champions of Island [" m "], RootWFFCount=[" myIslands[m] "]...")
                       (setq N (length esm.myPopulationIslands[m])) 
                       (loop for n from 0 until N do
                          (setq Lambda esm.myPopulationIslands[m][n])
		                  (if (<> Lambda #void) (writeln "esm.myPopulationIslands[" m "][" n "], Score=[" Lambda.Score "," Lambda.ErrorPct "], lengthWFF=[" (length Lambda.WFF) "], WFF = " (trunc (esm.evalRule Lambda.WFF) maxFormulaDisplayLen))) 
                          ) ; end population loop
                       ) ; end island loop

		             ;; Run on the training data.
		             (writeln "")
                     (setq Lambda esm.myBest) 
		             (writeln "Show final results on training data, Score=[" myBest.Score "], ErrorPct=[" myBest.ErrorPct "]")
                     (setq NX (length Y))
                     (setq avgY (avg Y))
                     (setq avgDevY 0.0)
                     (loop for n from 0 until NX (setq avgDevY (+ avgDevY (abs (- Y[n] avgY)))))
                     (/= avgDevY (number NX))
                     (setq sortedY (|Gv:sort| Y < true))
                     (setq nn (integer (max 1 (/ NX 20))))
                     (setq errPct 0.0)
                     (setq EY (Lambda.run X))
                     (setq avgFactor (abs (+ avgDevY .000000000001))) 
                     (loop for n from 0 until NX do
                        (setq k sortedY[n])
                        (setq ey EY[k])
                        (setq y Y[k])
	                    (setq err (- ey y))
	                    (setq err (abs err))
                        (/= err avgFactor) 
                        (if (> err 2.0) (setq err (+ 1.0 (expt err .001)))) 
	                    (+= errPct err)
		                (if (= (modi n nn) 0) (writeln "X[" n "], ey=[" ey "], y=[" y "], ErrorPct=[" err "]"))
                     ) ; end N loop
                     (/= errPct (number NX)) 
                     (setq yHistory (new Vector: Number: 10))
                     (setq eHistory (new Vector: Number: 10))
                     (setq aHistory (new Vector: Number: 10))
                     (setq dHistory (new Vector: Number: 5))
                     (setq MM (integer (max 1 (/ NX 10))))
                     (setq sortedEY (|Gv:sort| EY < true))
                     (loop for n from 0 until NX do
                        (setq k sortedEY[n])
                        (setq ey EY[k])
                        (setq y Y[k])
                        (setq mm (/ n MM))
                        (setq eHistory[mm] (+ eHistory[mm] (/ y MM)))
                        (setq k sortedY[n])
                        (setq y Y[k])
                        (setq yHistory[mm] (+ yHistory[mm] (/ y MM)))
                        ) ; end N loop
                     (setq aHistory[4] (avg eHistory[4] eHistory[3] eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[5] (avg eHistory[5] eHistory[6] eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[3] (avg eHistory[3] eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[6] (avg eHistory[6] eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[2] (avg eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[7] (avg eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[1] (avg eHistory[1] eHistory[0]))
                     (setq aHistory[8] (avg eHistory[8] eHistory[9]))
                     (setq aHistory[0] eHistory[0])
                     (setq aHistory[9] eHistory[9])
                     (setq dHistory[4] (- aHistory[9] aHistory[0]))
                     (setq dHistory[3] (- aHistory[8] aHistory[1]))
                     (setq dHistory[2] (- aHistory[7] aHistory[2]))
                     (setq dHistory[1] (- aHistory[6] aHistory[3]))
                     (setq dHistory[0] (- aHistory[5] aHistory[4]))
		             (writeln "Actual computed error on training data is ErrorPct=[" errPct "] versus reported ErrorPct=[" Lambda.ErrorPct "] while average Y is AvgY=[" (avg Y) "]")
		             (writeln "yHistory=[" (string yHistory true) "]")
		             (writeln "eHistory=[" (string eHistory true) "]")
		             (writeln "aHistory=[" (string aHistory true) "]")
		             (writeln "dHistory=[" (string dHistory true) "]")
		             (writeln "") 

                   )) ; end if 

		       ;; Build the test data.
               (setq srandom.seed (* mySeed (pi)))
		       (setq M Ms)
               (if (= myTimeON true) (setq mstart 1) (setq mstart 0)) 
               (setq Mp1 (addi M mstart))
		       (setq N Ns)
		       (setq T Ts)
		       (setq X (new Vector: Object: (muli N)))
		       (setq Y (new Vector: Number: (muli N)))
               (setq nn 0)
			   (loop for t from T to T do
	              (setq k (modi t 4)) 
			      (loop for n from 0 until N do
			         (setq X[nn] (setq x (new Vector: Number: Mp1)))
			         (if (= myTimeON true) (setq x[0] (number t)))
			         (setq y C[0])
			         (loop for m from mstart until Mp1 do
			          	(setq x[m] (- (random 100.0) 50.0))
	                    ;; HyperTangent model
                        (setq y (+ y (tanh (* x[m] x[m] x[m] C[m]))))
			            ) ; end M loop
			         (setq Y[nn] (+ (* y (- 1.0 (* .50 myRandomError))) (* y (random myRandomError))))
                     (setCdr x y)
                     (++ nn)
			         ) ; end N loop
			       ) ; end T loop

		       ;; Run on the test data.
               (setq Lambda esm.myBest)
               (if (>= (length X) minTN)
                      (begin (setq XT (rowManager X)) (setq score (XT.score Lambda)))
                      (setq score Lambda.Score)
                      ) ; end if
		       (writeln "") 
		       (writeln "Final testing on test data returns Score=[" score "], ErrorPct=[" Lambda.ErrorPct "]")
               (if true
                   (begin
                     (setq NX (length Y))
                     (setq avgY (avg Y))
                     (setq avgDevY 0.0)
                     (loop for n from 0 until NX (setq avgDevY (+ avgDevY (abs (- Y[n] avgY)))))
                     (/= avgDevY (number NX))
                     (setq sortedY (|Gv:sort| Y < true))
                     (setq nn (integer (max 1 (/ NX 20))))
                     (setq errPct 0.0)
                     (setq EY (Lambda.run X))
                     (setq avgFactor (abs (+ avgDevY .000000000001))) 
                     (loop for t from T to T do
                        (loop for n from 0 until NX do
                              (setq k sortedY[n])
                              (setq ey EY[k])
                              (setq ey (Lambda X[k]))
                              (setq y Y[k])
	                          (setq err (- ey y))
	                          (setq err (abs err))
                              (/= err avgFactor) 
                              (if (> err 2.0) (setq err (+ 1.0 (expt err .001)))) 
	                          (+= errPct err)
		                      (if (= (modi n nn) 0) (writeln "X[" n "], ey=[" ey "], y=[" y "], ErrorPct=[" err "]"))
                           ) ; end N loop
                         ) ; end T loop
                     (/= errPct (number NX)) 
                     (setq yHistory (new Vector: Number: 10))
                     (setq eHistory (new Vector: Number: 10))
                     (setq aHistory (new Vector: Number: 10))
                     (setq dHistory (new Vector: Number: 5))
                     (setq MM (integer (max 1 (/ NX 10))))
                     (setq sortedEY (|Gv:sort| EY < true))
                     (setq sortScore 0.0)
                     (setq oldY Y[sortedEY[0]])
                     (loop for t from T to T do
                        (loop for n from 0 until NX do
                              (setq k sortedEY[n])
                              (setq ey EY[k])
                              (setq y Y[k])
                              (if (> oldY y) (++ sortScore))(setq oldY y)
                              (setq mm (/ n MM))
                              (setq eHistory[mm] (+ eHistory[mm] (/ y MM)))
                              (setq k sortedY[n])
                              (setq y Y[k])
                              (setq yHistory[mm] (+ yHistory[mm] (/ y MM)))
                           ) ; end N loop
                         ) ; end T loop
                     (/= sortScore (number NX))
                     (setq aHistory[4] (avg eHistory[4] eHistory[3] eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[5] (avg eHistory[5] eHistory[6] eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[3] (avg eHistory[3] eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[6] (avg eHistory[6] eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[2] (avg eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[7] (avg eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[1] (avg eHistory[1] eHistory[0]))
                     (setq aHistory[8] (avg eHistory[8] eHistory[9]))
                     (setq aHistory[0] eHistory[0])
                     (setq aHistory[9] eHistory[9])
                     (setq dHistory[4] (- aHistory[9] aHistory[0]))
                     (setq dHistory[3] (- aHistory[8] aHistory[1]))
                     (setq dHistory[2] (- aHistory[7] aHistory[2]))
                     (setq dHistory[1] (- aHistory[6] aHistory[3]))
                     (setq dHistory[0] (- aHistory[5] aHistory[4]))
		             (writeln "Actual computed error on testing data is ErrorPct=[" errPct "], Sort=[" sortScore "], Avg Y=[" (avg Y) "], AvgDev Y=[" avgDevY "]")
		             (writeln "yHistory=[" (string yHistory true) "]")
		             (writeln "eHistory=[" (string eHistory true) "]")
		             (writeln "aHistory=[" (string aHistory true) "]")
		             (writeln "dHistory=[" (string dHistory true) "]")
		             (writeln "") 
                   )) ; end if 
               (writeln "esm.selfTest[hyperTangent]: completed in [" (/ endTimeT 60.0) "] minutes.")       
         )) ; end Test Case hyperTangent
       ;; Test Case elipsoid 
       (if (or (= Test all:) (= Test elipsoid:))
           (begin
		       (writeln _eol "Starting test case: elipsoid")
               (setq mySeed 0.0)     
               (setq srandom.seed mySeed)      
		       (setq M Ms)
               (if (= myTimeON true) (setq mstart 1) (setq mstart 0)) 
               (setq Mp1 (addi M mstart))
		       (setq N Ns)
		       (setq T Ts)
		       (setq X (new Vector: Object: (muli N T)))
		       (setq Y (new Vector: Number: (muli N T)))
		       (setq C (new Vector: Number: Mp1))
		       (loop for m from 0 until Mp1 do
		          (if (= myTimeON true) (setq C[m] m) (setq C[m] (+ m 1)))
		          ) ; end C loop
               (setq nn 0)
			   (loop for t from 0 until T do
	              (setq k (modi t 4)) 
			      (loop for n from 0 until N do
			         (setq X[nn] (setq x (new Vector: Number: Mp1)))
			         (if (= myTimeON true) (setq x[0] (number t)))
			         (setq y 0.0)
			         (loop for m from mstart until Mp1 do
			          	(setq x[m] (- (random 100.0) 50.0))
	                    ;; elipsoid model
	                    (setq y (+ y (* x[m] x[m] C[m])))
			            ) ; end M loop
			         (setq Y[nn] (+ (* y (- 1.0 (* .50 myRandomError))) (* y (random myRandomError))))
                     (setCdr x y)
                     (++ nn)
			         ) ; end N loop
			       ) ; end T loop

		       ;; Train on the test case.
		       (setq modelString "Building test data as: y = 0.0")
		       (loop for m from mstart until Mp1 do
                  (setq modelString (append modelString " + (" (abs C[m]) "*" (esm.ruleExp.ruleName m) "*" (esm.ruleExp.ruleName m) ")"))
		          ) ; end C loop
		       (writeln modelString ";")
               (if (<> myRandomError 0.0) (writeln "Additionally, we add random error as: y = (y * " (- 1.0 (* .50 myRandomError)) ") + (random " myRandomError ");")) 
               (setq startTimeT (getTickCount 0))
		       (setq Lambda (esm X Y Gs Ss Ck Seed))
               (setq endTimeT (getTickCount startTimeT))
		       (writeln "") 
		       (writeln "Final results of training")
		       (writeln "esm: N = [" Ns "], M = [" Mp1 "], Generations = [" myGc "], WFFs = [" (length myWFFs)  "], Score=[" Lambda.Score "], ScoreHistory=[" (string Lambda.History true) "]") 
               (writeln "esm.myBest, Score=[" Lambda.Score "], ErrorPct=[" Lambda.ErrorPct "], lengthWFF=[" (length Lambda.WFF) "], WFF = " (trunc (esm.evalRule Lambda.WFF) maxFormulaDisplayLen))
               (writeln "esm.myBestRegressor, Score=[" myBestRegressor.Score "], ErrorPct=[" myBestRegressor.ErrorPct "], lengthWFF=[" (length myBestRegressor.WFF) "], WFF = " (trunc (esm.evalRule myBestRegressor.WFF) maxFormulaDisplayLen))
               (writeln "myBestSelectorChampions")
               (loop for n from 0 until (length esm.myBestSelectorChampions) do (writeln "myBestSelectorChampions[" n "],Score=[" esm.myBestSelectorChampions[n 1].Score ",ErrorPct=[" esm.myBestSelectorChampions[n 1].ErrorPct "], WFF = " (trunc (esm.evalRule esm.myBestSelectorChampions[n 1].WFF) maxFormulaDisplayLen))) 
               (writeln "myBestRegressorChampions")
               (loop for n from 0 until (length esm.myBestRegressorChampions) do (writeln "myBestRegressorChampions[" n "],ErrorPct=[" esm.myBestRegressorChampions[n 1].ErrorPct ",Score=[" esm.myBestRegressorChampions[n 1].Score "], WFF = " (trunc (esm.evalRule esm.myBestRegressorChampions[n 1].WFF) maxFormulaDisplayLen))) 

               (if (= myVerboseSW true)
                   (begin
		             
                     ;; Show the champions from each island population.
		             (writeln "") 
		             (writeln "Show the champions from each island population")
                     (setq M (length esm.myPopulationIslands))
                     (loop for m from 0 until M do
                       (writeln "Champions of Island [" m "], RootWFFCount=[" myIslands[m] "]...")
                       (setq N (length esm.myPopulationIslands[m])) 
                       (loop for n from 0 until N do
                          (setq Lambda esm.myPopulationIslands[m][n])
		                  (if (<> Lambda #void) (writeln "esm.myPopulationIslands[" m "][" n "], Score=[" Lambda.Score "," Lambda.ErrorPct "], lengthWFF=[" (length Lambda.WFF) "], WFF = " (trunc (esm.evalRule Lambda.WFF) maxFormulaDisplayLen))) 
                          ) ; end population loop
                       ) ; end island loop

		             ;; Run on the training data.
		             (writeln "")
                     (setq Lambda esm.myBest) 
		             (writeln "Show final results on training data, Score=[" myBest.Score "], ErrorPct=[" myBest.ErrorPct "]")
                     (setq NX (length Y))
                     (setq avgY (avg Y))
                     (setq avgDevY 0.0)
                     (loop for n from 0 until NX (setq avgDevY (+ avgDevY (abs (- Y[n] avgY)))))
                     (/= avgDevY (number NX))
                     (setq sortedY (|Gv:sort| Y < true))
                     (setq nn (integer (max 1 (/ NX 20))))
                     (setq errPct 0.0)
                     (setq EY (Lambda.run X))
                     (setq avgFactor (abs (+ avgDevY .000000000001))) 
                     (loop for n from 0 until NX do
                        (setq k sortedY[n])
                        (setq ey EY[k])
                        (setq y Y[k])
	                    (setq err (- ey y))
	                    (setq err (abs err))
                        (/= err avgFactor) 
                        (if (> err 2.0) (setq err (+ 1.0 (expt err .001)))) 
	                    (+= errPct err)
		                (if (= (modi n nn) 0) (writeln "X[" n "], ey=[" ey "], y=[" y "], ErrorPct=[" err "]"))
                     ) ; end N loop
                     (/= errPct (number NX)) 
                     (setq yHistory (new Vector: Number: 10))
                     (setq eHistory (new Vector: Number: 10))
                     (setq aHistory (new Vector: Number: 10))
                     (setq dHistory (new Vector: Number: 5))
                     (setq MM (integer (max 1 (/ NX 10))))
                     (setq sortedEY (|Gv:sort| EY < true))
                     (loop for n from 0 until NX do
                        (setq k sortedEY[n])
                        (setq ey EY[k])
                        (setq y Y[k])
                        (setq mm (/ n MM))
                        (setq eHistory[mm] (+ eHistory[mm] (/ y MM)))
                        (setq k sortedY[n])
                        (setq y Y[k])
                        (setq yHistory[mm] (+ yHistory[mm] (/ y MM)))
                        ) ; end N loop
                     (setq aHistory[4] (avg eHistory[4] eHistory[3] eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[5] (avg eHistory[5] eHistory[6] eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[3] (avg eHistory[3] eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[6] (avg eHistory[6] eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[2] (avg eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[7] (avg eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[1] (avg eHistory[1] eHistory[0]))
                     (setq aHistory[8] (avg eHistory[8] eHistory[9]))
                     (setq aHistory[0] eHistory[0])
                     (setq aHistory[9] eHistory[9])
                     (setq dHistory[4] (- aHistory[9] aHistory[0]))
                     (setq dHistory[3] (- aHistory[8] aHistory[1]))
                     (setq dHistory[2] (- aHistory[7] aHistory[2]))
                     (setq dHistory[1] (- aHistory[6] aHistory[3]))
                     (setq dHistory[0] (- aHistory[5] aHistory[4]))
		             (writeln "Actual computed error on training data is ErrorPct=[" errPct "] versus reported ErrorPct=[" Lambda.ErrorPct "] while average Y is AvgY=[" (avg Y) "]")
		             (writeln "yHistory=[" (string yHistory true) "]")
		             (writeln "eHistory=[" (string eHistory true) "]")
		             (writeln "aHistory=[" (string aHistory true) "]")
		             (writeln "dHistory=[" (string dHistory true) "]")
		             (writeln "") 

                   )) ; end if 

		       ;; Build the test data.
               (setq srandom.seed (* mySeed (pi)))
		       (setq M Ms)
               (if (= myTimeON true) (setq mstart 1) (setq mstart 0)) 
               (setq Mp1 (addi M mstart))
		       (setq N Ns)
		       (setq T Ts)
		       (setq X (new Vector: Object: (muli N)))
		       (setq Y (new Vector: Number: (muli N)))
               (setq nn 0)
			   (loop for t from T to T do
	              (setq k (modi t 4)) 
			      (loop for n from 0 until N do
			         (setq X[nn] (setq x (new Vector: Number: Mp1)))
			         (if (= myTimeON true) (setq x[0] (number t)))
			         (setq y 0.0)
			         (loop for m from mstart until Mp1 do
			          	(setq x[m] (- (random 100.0) 50.0))
	                    ;; Cubic model
	                    (setq y (+ y (* x[m] x[m] C[m])))
			            ) ; end M loop
			         (setq Y[nn] (+ (* y (- 1.0 (* .50 myRandomError))) (* y (random myRandomError))))
                     (setCdr x y)
                     (++ nn)
			         ) ; end N loop
			       ) ; end T loop

		       ;; Run on the test data.
               (setq Lambda esm.myBest)
               (if (>= (length X) minTN)
                      (begin (setq XT (rowManager X)) (setq score (XT.score Lambda)))
                      (setq score Lambda.Score)
                      ) ; end if
		       (writeln "") 
		       (writeln "Final testing on test data returns Score=[" score "], ErrorPct=[" Lambda.ErrorPct "]")
               (if true
                   (begin
                     (setq NX (length Y))
                     (setq avgY (avg Y))
                     (setq avgDevY 0.0)
                     (loop for n from 0 until NX (setq avgDevY (+ avgDevY (abs (- Y[n] avgY)))))
                     (/= avgDevY (number NX))
                     (setq sortedY (|Gv:sort| Y < true))
                     (setq nn (integer (max 1 (/ NX 20))))
                     (setq errPct 0.0)
                     (setq EY (Lambda.run X))
                     (setq avgFactor (abs (+ avgDevY .000000000001))) 
                     (loop for t from T to T do
                        (loop for n from 0 until NX do
                              (setq k sortedY[n])
                              (setq ey EY[k])
                              (setq ey (Lambda X[k]))
                              (setq y Y[k])
	                          (setq err (- ey y))
	                          (setq err (abs err))
                              (/= err avgFactor) 
                              (if (> err 2.0) (setq err (+ 1.0 (expt err .001)))) 
	                          (+= errPct err)
		                      (if (= (modi n nn) 0) (writeln "X[" n "], ey=[" ey "], y=[" y "], ErrorPct=[" err "]"))
                           ) ; end N loop
                         ) ; end T loop
                     (/= errPct (number NX)) 
                     (setq yHistory (new Vector: Number: 10))
                     (setq eHistory (new Vector: Number: 10))
                     (setq aHistory (new Vector: Number: 10))
                     (setq dHistory (new Vector: Number: 5))
                     (setq MM (integer (max 1 (/ NX 10))))
                     (setq sortedEY (|Gv:sort| EY < true))
                     (setq sortScore 0.0)
                     (setq oldY Y[sortedEY[0]])
                     (loop for t from T to T do
                        (loop for n from 0 until NX do
                              (setq k sortedEY[n])
                              (setq ey EY[k])
                              (setq y Y[k])
                              (if (> oldY y) (++ sortScore))(setq oldY y)
                              (setq mm (/ n MM))
                              (setq eHistory[mm] (+ eHistory[mm] (/ y MM)))
                              (setq k sortedY[n])
                              (setq y Y[k])
                              (setq yHistory[mm] (+ yHistory[mm] (/ y MM)))
                           ) ; end N loop
                         ) ; end T loop
                     (/= sortScore (number NX))
                     (setq aHistory[4] (avg eHistory[4] eHistory[3] eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[5] (avg eHistory[5] eHistory[6] eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[3] (avg eHistory[3] eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[6] (avg eHistory[6] eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[2] (avg eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[7] (avg eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[1] (avg eHistory[1] eHistory[0]))
                     (setq aHistory[8] (avg eHistory[8] eHistory[9]))
                     (setq aHistory[0] eHistory[0])
                     (setq aHistory[9] eHistory[9])
                     (setq dHistory[4] (- aHistory[9] aHistory[0]))
                     (setq dHistory[3] (- aHistory[8] aHistory[1]))
                     (setq dHistory[2] (- aHistory[7] aHistory[2]))
                     (setq dHistory[1] (- aHistory[6] aHistory[3]))
                     (setq dHistory[0] (- aHistory[5] aHistory[4]))
		             (writeln "Actual computed error on testing data is ErrorPct=[" errPct "], Sort=[" sortScore "], Avg Y=[" (avg Y) "], AvgDev Y=[" avgDevY "]")
		             (writeln "yHistory=[" (string yHistory true) "]")
		             (writeln "eHistory=[" (string eHistory true) "]")
		             (writeln "aHistory=[" (string aHistory true) "]")
		             (writeln "dHistory=[" (string dHistory true) "]")
		             (writeln "") 
                   )) ; end if 
               (writeln "esm.selfTest[elipsoid]: completed in [" (/ endTimeT 60.0) "] minutes.")       
         )) ; end Test Case elipsoid
       ;; Test Case hiddenModel 
       (if (or (= Test all:) (= Test hiddenModel:))
           (begin
		       (writeln _eol "Starting test case: hiddenModel")
               (setq mySeed 0.0)     
               (setq srandom.seed mySeed)      
		       (setq M Ms)
               (if (= myTimeON true) (setq mstart 1) (setq mstart 0)) 
               (setq Mp1 (addi M mstart))
		       (setq N Ns)
		       (setq T Ts)
		       (setq X (new Vector: Object: (muli N T)))
		       (setq Y (new Vector: Number: (muli N T)))
		       (setq C (new Vector: Number: Mp1))
		       (loop for m from 0 until Mp1 do
		          (setq C[m] (- (random 100.0) 50.0))
		          ) ; end C loop
               (setq nn 0)
			   (loop for t from 0 until T do
	              (setq k (modi t 4)) 
			      (loop for n from 0 until N do
			         (setq X[nn] (setq x (new Vector: Number: Mp1)))
			         (if (= myTimeON true) (setq x[0] (number t)))  ;; Set training example time stamp
			         (setq y C[0])
			         (loop for m from mstart until Mp1 do
			          	(setq x[m] (- (random 100.0) 50.0))
	                    ;; Hidden models cause only one variable to be relevant.
	                    (if (= (divi Mp1 2) m) (setq y (+ y (* (sin x[m]) C[m]))))
			            ) ; end M loop
			         (setq Y[nn] (+ (* y (- 1.0 (* .50 myRandomError))) (* y (random myRandomError))))
                     (setCdr x y)
                     (++ nn)
			         ) ; end N loop
			       ) ; end T loop

		       ;; Train on the test case.
		       (setq modelString (append "Building test data as: y = " C[0]))
		       (loop for m from mstart until Mp1 do
                  (if (= (divi Mp1 2) m) (setq modelString (append modelString (if (< C[m] 0.0) " - (" " + (") (abs C[m]) "*sin(" (esm.ruleExp.ruleName m) "))")))
		          ) ; end C loop
		       (writeln modelString ";")
               (if (<> myRandomError 0.0) (writeln "Additionally, we add random error as: y = (y * " (- 1.0 (* .50 myRandomError)) ") + (random " myRandomError ");")) 
               (setq startTimeT (getTickCount 0))
		       (setq Lambda (esm X Y Gs Ss Ck Seed))
               (setq endTimeT (getTickCount startTimeT))
		       (writeln "") 
		       (writeln "Final results of training")
		       (writeln "esm: N = [" Ns "], M = [" Mp1 "], Generations = [" myGc "], WFFs = [" (length myWFFs)  "], Score=[" Lambda.Score "], ScoreHistory=[" (string Lambda.History true) "]") 
               (writeln "esm.myBest, Score=[" Lambda.Score "], ErrorPct=[" Lambda.ErrorPct "], lengthWFF=[" (length Lambda.WFF) "], WFF = " (trunc (esm.evalRule Lambda.WFF) maxFormulaDisplayLen))
               (writeln "esm.myBestRegressor, Score=[" myBestRegressor.Score "], ErrorPct=[" myBestRegressor.ErrorPct "], lengthWFF=[" (length myBestRegressor.WFF) "], WFF = " (trunc (esm.evalRule myBestRegressor.WFF) maxFormulaDisplayLen))
               (writeln "myBestSelectorChampions")
               (loop for n from 0 until (length esm.myBestSelectorChampions) do (writeln "myBestSelectorChampions[" n "],Score=[" esm.myBestSelectorChampions[n 1].Score ",ErrorPct=[" esm.myBestSelectorChampions[n 1].ErrorPct "], WFF = " (trunc (esm.evalRule esm.myBestSelectorChampions[n 1].WFF) maxFormulaDisplayLen))) 
               (writeln "myBestRegressorChampions")
               (loop for n from 0 until (length esm.myBestRegressorChampions) do (writeln "myBestRegressorChampions[" n "],ErrorPct=[" esm.myBestRegressorChampions[n 1].ErrorPct ",Score=[" esm.myBestRegressorChampions[n 1].Score "], WFF = " (trunc (esm.evalRule esm.myBestRegressorChampions[n 1].WFF) maxFormulaDisplayLen))) 

               (if (= myVerboseSW true)
                   (begin
		             
                     ;; Show the champions from each island population.
		             (writeln "") 
		             (writeln "Show the champions from each island population")
                     (setq M (length esm.myPopulationIslands))
                     (loop for m from 0 until M do
                       (writeln "Champions of Island [" m "], RootWFFCount=[" myIslands[m] "]...")
                       (setq N (length esm.myPopulationIslands[m])) 
                       (loop for n from 0 until N do
                          (setq Lambda esm.myPopulationIslands[m][n])
		                  (if (<> Lambda #void) (writeln "esm.myPopulationIslands[" m "][" n "], Score=[" Lambda.Score "," Lambda.ErrorPct "], lengthWFF=[" (length Lambda.WFF) "], WFF = " (trunc (esm.evalRule Lambda.WFF) maxFormulaDisplayLen))) 
                          ) ; end population loop
                       ) ; end island loop

		             ;; Run on the training data.
		             (writeln "")
                     (setq Lambda esm.myBest) 
		             (writeln "Show final results on training data, Score=[" myBest.Score "], ErrorPct=[" myBest.ErrorPct "]")
                     (setq NX (length Y))
                     (setq avgY (avg Y))
                     (setq avgDevY 0.0)
                     (loop for n from 0 until NX (setq avgDevY (+ avgDevY (abs (- Y[n] avgY)))))
                     (/= avgDevY (number NX))
                     (setq sortedY (|Gv:sort| Y < true))
                     (setq nn (integer (max 1 (/ NX 20))))
                     (setq errPct 0.0)
                     (setq EY (Lambda.run X))
                     (setq avgFactor (abs (+ avgDevY .000000000001))) 
                     (loop for n from 0 until NX do
                        (setq k sortedY[n])
                        (setq ey EY[k])
                        (setq y Y[k])
	                    (setq err (- ey y))
	                    (setq err (abs err))
                        (/= err avgFactor) 
                        (if (> err 2.0) (setq err (+ 1.0 (expt err .001)))) 
	                    (+= errPct err)
		                (if (= (modi n nn) 0) (writeln "X[" n "], ey=[" ey "], y=[" y "], ErrorPct=[" err "]"))
                     ) ; end N loop
                     (/= errPct (number NX)) 
                     (setq yHistory (new Vector: Number: 10))
                     (setq eHistory (new Vector: Number: 10))
                     (setq aHistory (new Vector: Number: 10))
                     (setq dHistory (new Vector: Number: 5))
                     (setq MM (integer (max 1 (/ NX 10))))
                     (setq sortedEY (|Gv:sort| EY < true))
                     (loop for n from 0 until NX do
                        (setq k sortedEY[n])
                        (setq ey EY[k])
                        (setq y Y[k])
                        (setq mm (/ n MM))
                        (setq eHistory[mm] (+ eHistory[mm] (/ y MM)))
                        (setq k sortedY[n])
                        (setq y Y[k])
                        (setq yHistory[mm] (+ yHistory[mm] (/ y MM)))
                        ) ; end N loop
                     (setq aHistory[4] (avg eHistory[4] eHistory[3] eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[5] (avg eHistory[5] eHistory[6] eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[3] (avg eHistory[3] eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[6] (avg eHistory[6] eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[2] (avg eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[7] (avg eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[1] (avg eHistory[1] eHistory[0]))
                     (setq aHistory[8] (avg eHistory[8] eHistory[9]))
                     (setq aHistory[0] eHistory[0])
                     (setq aHistory[9] eHistory[9])
                     (setq dHistory[4] (- aHistory[9] aHistory[0]))
                     (setq dHistory[3] (- aHistory[8] aHistory[1]))
                     (setq dHistory[2] (- aHistory[7] aHistory[2]))
                     (setq dHistory[1] (- aHistory[6] aHistory[3]))
                     (setq dHistory[0] (- aHistory[5] aHistory[4]))
		             (writeln "Actual computed error on training data is ErrorPct=[" errPct "] versus reported ErrorPct=[" Lambda.ErrorPct "] while average Y is AvgY=[" (avg Y) "]")
		             (writeln "yHistory=[" (string yHistory true) "]")
		             (writeln "eHistory=[" (string eHistory true) "]")
		             (writeln "aHistory=[" (string aHistory true) "]")
		             (writeln "dHistory=[" (string dHistory true) "]")
		             (writeln "") 

                   )) ; end if 

		       ;; Build the test data.
               (setq srandom.seed (* mySeed (pi)))
		       (setq M Ms)
               (if (= myTimeON true) (setq mstart 1) (setq mstart 0)) 
               (setq Mp1 (addi M mstart))
		       (setq N Ns)
		       (setq T Ts)
		       (setq X (new Vector: Object: (muli N)))
		       (setq Y (new Vector: Number: (muli N)))
               (setq nn 0)
			   (loop for t from T to T do
	              (setq k (modi t 4)) 
			      (loop for n from 0 until N do
			         (setq X[nn] (setq x (new Vector: Number: Mp1)))
			         (if (= myTimeON true) (setq x[0] (number t)))
			         (setq y C[0])
			         (loop for m from mstart until Mp1 do
			          	(setq x[m] (- (random 100.0) 50.0))
	                    ;; Hidden models cause only one variable to be relevant.
	                    (if (= (divi Mp1 2) m) (setq y (+ y (* (sin x[m]) C[m]))))
			            ) ; end M loop
			         (setq Y[nn] (+ (* y (- 1.0 (* .50 myRandomError))) (* y (random myRandomError))))
                     (setCdr x y)
                     (++ nn)
			         ) ; end N loop
			       ) ; end T loop

		       ;; Run on the test data.
               (setq Lambda esm.myBest)
               (if (>= (length X) minTN)
                      (begin (setq XT (rowManager X)) (setq score (XT.score Lambda)))
                      (setq score Lambda.Score)
                      ) ; end if
		       (writeln "") 
		       (writeln "Final testing on test data returns Score=[" score "], ErrorPct=[" Lambda.ErrorPct "]")
               (if true
                   (begin
                     (setq NX (length Y))
                     (setq avgY (avg Y))
                     (setq avgDevY 0.0)
                     (loop for n from 0 until NX (setq avgDevY (+ avgDevY (abs (- Y[n] avgY)))))
                     (/= avgDevY (number NX))
                     (setq sortedY (|Gv:sort| Y < true))
                     (setq nn (integer (max 1 (/ NX 20))))
                     (setq errPct 0.0)
                     (setq EY (Lambda.run X))
                     (setq avgFactor (abs (+ avgDevY .000000000001))) 
                     (loop for t from T to T do
                        (loop for n from 0 until NX do
                              (setq k sortedY[n])
                              (setq ey EY[k])
                              (setq ey (Lambda X[k]))
                              (setq y Y[k])
	                          (setq err (- ey y))
	                          (setq err (abs err))
                              (/= err avgFactor) 
                              (if (> err 2.0) (setq err (+ 1.0 (expt err .001)))) 
	                          (+= errPct err)
		                      (if (= (modi n nn) 0) (writeln "X[" n "], ey=[" ey "], y=[" y "], ErrorPct=[" err "]"))
                           ) ; end N loop
                         ) ; end T loop
                     (/= errPct (number NX)) 
                     (setq yHistory (new Vector: Number: 10))
                     (setq eHistory (new Vector: Number: 10))
                     (setq aHistory (new Vector: Number: 10))
                     (setq dHistory (new Vector: Number: 5))
                     (setq MM (integer (max 1 (/ NX 10))))
                     (setq sortedEY (|Gv:sort| EY < true))
                     (setq sortScore 0.0)
                     (setq oldY Y[sortedEY[0]])
                     (loop for t from T to T do
                        (loop for n from 0 until NX do
                              (setq k sortedEY[n])
                              (setq ey EY[k])
                              (setq y Y[k])
                              (if (> oldY y) (++ sortScore))(setq oldY y)
                              (setq mm (/ n MM))
                              (setq eHistory[mm] (+ eHistory[mm] (/ y MM)))
                              (setq k sortedY[n])
                              (setq y Y[k])
                              (setq yHistory[mm] (+ yHistory[mm] (/ y MM)))
                           ) ; end N loop
                         ) ; end T loop
                     (/= sortScore (number NX))
                     (setq aHistory[4] (avg eHistory[4] eHistory[3] eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[5] (avg eHistory[5] eHistory[6] eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[3] (avg eHistory[3] eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[6] (avg eHistory[6] eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[2] (avg eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[7] (avg eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[1] (avg eHistory[1] eHistory[0]))
                     (setq aHistory[8] (avg eHistory[8] eHistory[9]))
                     (setq aHistory[0] eHistory[0])
                     (setq aHistory[9] eHistory[9])
                     (setq dHistory[4] (- aHistory[9] aHistory[0]))
                     (setq dHistory[3] (- aHistory[8] aHistory[1]))
                     (setq dHistory[2] (- aHistory[7] aHistory[2]))
                     (setq dHistory[1] (- aHistory[6] aHistory[3]))
                     (setq dHistory[0] (- aHistory[5] aHistory[4]))
		             (writeln "Actual computed error on testing data is ErrorPct=[" errPct "], Sort=[" sortScore "], Avg Y=[" (avg Y) "], AvgDev Y=[" avgDevY "]")
		             (writeln "yHistory=[" (string yHistory true) "]")
		             (writeln "eHistory=[" (string eHistory true) "]")
		             (writeln "aHistory=[" (string aHistory true) "]")
		             (writeln "dHistory=[" (string dHistory true) "]")
		             (writeln "") 
                   )) ; end if 
               (writeln "esm.selfTest[hiddenModel]: completed in [" (/ endTimeT 60.0) "] minutes.")       
         )) ; end Test Case hiddenModel
       ;; Test Case linearRegression 
       (if (or (= Test all:) (= Test linearRegression:))
           (begin
		       (writeln _eol "Starting test case: linearRegression")
               (setq mySeed 0.0)     
               (setq srandom.seed mySeed)      
		       (setq M Ms)
               (if (= myTimeON true) (setq mstart 1) (setq mstart 0)) 
               (setq Mp1 (addi M mstart))
		       (setq N Ns)
		       (setq T Ts)
		       (setq X (new Vector: Object: (muli N T)))
		       (setq Y (new Vector: Number: (muli N T)))
		       (setq C (new Vector: Number: Mp1))
		       (loop for m from 0 until Mp1 do
		          (setq C[m] (- (random 100.0) 50.0))
		          ) ; end C loop
               (setq nn 0)
			   (loop for t from 0 until T do
	              (setq k (modi t 4)) 
			      (loop for n from 0 until N do
			         (setq X[nn] (setq x (new Vector: Number: Mp1)))
			         (if (= myTimeON true) (setq x[0] (number t)))
			         (setq y C[0])
			         (loop for m from mstart until Mp1 do
			          	(setq x[m] (- (random 100.0) 50.0))
	                    ;; Linear model
	                    (setq y (+ y (* x[m] C[m])))
			            ) ; end M loop
			         (setq Y[nn] (+ (* y (- 1.0 (* .50 myRandomError))) (* y (random myRandomError))))
                     (setCdr x y)
                     (++ nn)
			         ) ; end N loop
			       ) ; end T loop

		       ;; Train on the test case.
		       (setq modelString (append "Building test data as: y = " C[0]))
		       (loop for m from mstart until Mp1 do
                  (setq modelString (append modelString (if (< C[m] 0.0) " - (" " + (") (abs C[m]) "*" (esm.ruleExp.ruleName m) ")"))
		          ) ; end C loop
		       (writeln modelString ";")
               (if (<> myRandomError 0.0) (writeln "Additionally, we add random error as: y = (y * " (- 1.0 (* .50 myRandomError)) ") + (random " myRandomError ");")) 
               (setq startTimeT (getTickCount 0))
		       (setq Lambda (esm X Y Gs Ss Ck Seed))
               (setq endTimeT (getTickCount startTimeT))
		       (writeln "") 
		       (writeln "Final results of training")
		       (writeln "esm: N = [" Ns "], M = [" Mp1 "], Generations = [" myGc "], WFFs = [" (length myWFFs)  "], Score=[" Lambda.Score "], ScoreHistory=[" (string Lambda.History true) "]") 
               (writeln "esm.myBest, Score=[" Lambda.Score "], ErrorPct=[" Lambda.ErrorPct "], lengthWFF=[" (length Lambda.WFF) "], WFF = " (trunc (esm.evalRule Lambda.WFF) maxFormulaDisplayLen))
               (writeln "esm.myBestRegressor, Score=[" myBestRegressor.Score "], ErrorPct=[" myBestRegressor.ErrorPct "], lengthWFF=[" (length myBestRegressor.WFF) "], WFF = " (trunc (esm.evalRule myBestRegressor.WFF) maxFormulaDisplayLen))
               (writeln "myBestSelectorChampions")
               (loop for n from 0 until (length esm.myBestSelectorChampions) do (writeln "myBestSelectorChampions[" n "],Score=[" esm.myBestSelectorChampions[n 1].Score ",ErrorPct=[" esm.myBestSelectorChampions[n 1].ErrorPct "], WFF = " (trunc (esm.evalRule esm.myBestSelectorChampions[n 1].WFF) maxFormulaDisplayLen))) 
               (writeln "myBestRegressorChampions")
               (loop for n from 0 until (length esm.myBestRegressorChampions) do (writeln "myBestRegressorChampions[" n "],ErrorPct=[" esm.myBestRegressorChampions[n 1].ErrorPct ",Score=[" esm.myBestRegressorChampions[n 1].Score "], WFF = " (trunc (esm.evalRule esm.myBestRegressorChampions[n 1].WFF) maxFormulaDisplayLen))) 

               (if (= myVerboseSW true)
                   (begin
		             
                     ;; Show the champions from each island population.
		             (writeln "") 
		             (writeln "Show the champions from each island population")
                     (setq M (length esm.myPopulationIslands))
                     (loop for m from 0 until M do
                       (writeln "Champions of Island [" m "], RootWFFCount=[" myIslands[m] "]...")
                       (setq N (length esm.myPopulationIslands[m])) 
                       (loop for n from 0 until N do
                          (setq Lambda esm.myPopulationIslands[m][n])
		                  (if (<> Lambda #void) (writeln "esm.myPopulationIslands[" m "][" n "], Score=[" Lambda.Score "," Lambda.ErrorPct "], lengthWFF=[" (length Lambda.WFF) "], WFF = " (trunc (esm.evalRule Lambda.WFF) maxFormulaDisplayLen))) 
                          ) ; end population loop
                       ) ; end island loop

		             ;; Run on the training data.
		             (writeln "")
                     (setq Lambda esm.myBest) 
		             (writeln "Show final results on training data, Score=[" myBest.Score "], ErrorPct=[" myBest.ErrorPct "]")
                     (setq NX (length Y))
                     (setq avgY (avg Y))
                     (setq avgDevY 0.0)
                     (loop for n from 0 until NX (setq avgDevY (+ avgDevY (abs (- Y[n] avgY)))))
                     (/= avgDevY (number NX))
                     (setq sortedY (|Gv:sort| Y < true))
                     (setq nn (integer (max 1 (/ NX 20))))
                     (setq errPct 0.0)
                     (setq EY (Lambda.run X))
                     (setq avgFactor (abs (+ avgDevY .000000000001))) 
                     (loop for n from 0 until NX do
                        (setq k sortedY[n])
                        (setq ey EY[k])
                        (setq y Y[k])
	                    (setq err (- ey y))
	                    (setq err (abs err))
                        (/= err avgFactor) 
                        (if (> err 2.0) (setq err (+ 1.0 (expt err .001)))) 
	                    (+= errPct err)
		                (if (= (modi n nn) 0) (writeln "X[" n "], ey=[" ey "], y=[" y "], ErrorPct=[" err "]"))
                     ) ; end N loop
                     (/= errPct (number NX)) 
                     (setq yHistory (new Vector: Number: 10))
                     (setq eHistory (new Vector: Number: 10))
                     (setq aHistory (new Vector: Number: 10))
                     (setq dHistory (new Vector: Number: 5))
                     (setq MM (integer (max 1 (/ NX 10))))
                     (setq sortedEY (|Gv:sort| EY < true))
                     (loop for n from 0 until NX do
                        (setq k sortedEY[n])
                        (setq ey EY[k])
                        (setq y Y[k])
                        (setq mm (/ n MM))
                        (setq eHistory[mm] (+ eHistory[mm] (/ y MM)))
                        (setq k sortedY[n])
                        (setq y Y[k])
                        (setq yHistory[mm] (+ yHistory[mm] (/ y MM)))
                        ) ; end N loop
                     (setq aHistory[4] (avg eHistory[4] eHistory[3] eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[5] (avg eHistory[5] eHistory[6] eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[3] (avg eHistory[3] eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[6] (avg eHistory[6] eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[2] (avg eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[7] (avg eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[1] (avg eHistory[1] eHistory[0]))
                     (setq aHistory[8] (avg eHistory[8] eHistory[9]))
                     (setq aHistory[0] eHistory[0])
                     (setq aHistory[9] eHistory[9])
                     (setq dHistory[4] (- aHistory[9] aHistory[0]))
                     (setq dHistory[3] (- aHistory[8] aHistory[1]))
                     (setq dHistory[2] (- aHistory[7] aHistory[2]))
                     (setq dHistory[1] (- aHistory[6] aHistory[3]))
                     (setq dHistory[0] (- aHistory[5] aHistory[4]))
		             (writeln "Actual computed error on training data is ErrorPct=[" errPct "] versus reported ErrorPct=[" Lambda.ErrorPct "] while average Y is AvgY=[" (avg Y) "]")
		             (writeln "yHistory=[" (string yHistory true) "]")
		             (writeln "eHistory=[" (string eHistory true) "]")
		             (writeln "aHistory=[" (string aHistory true) "]")
		             (writeln "dHistory=[" (string dHistory true) "]")
		             (writeln "") 

                   )) ; end if 

		       ;; Build the test data.
               (setq srandom.seed (* mySeed (pi)))
		       (setq M Ms)
               (if (= myTimeON true) (setq mstart 1) (setq mstart 0)) 
               (setq Mp1 (addi M mstart))
		       (setq N Ns)
		       (setq T Ts)
		       (setq X (new Vector: Object: (muli N)))
		       (setq Y (new Vector: Number: (muli N)))
               (setq nn 0)
			   (loop for t from T to T do
	              (setq k (modi t 4)) 
			      (loop for n from 0 until N do
			         (setq X[nn] (setq x (new Vector: Number: Mp1)))
			         (if (= myTimeON true) (setq x[0] (number t)))
			         (setq y C[0])
			         (loop for m from mstart until Mp1 do
			          	(setq x[m] (- (random 100.0) 50.0))
	                    ;; Linear model
	                    (setq y (+ y (* x[m] C[m])))
			            ) ; end M loop
			         (setq Y[nn] (+ (* y (- 1.0 (* .50 myRandomError))) (* y (random myRandomError))))
                     (setCdr x y)
                     (++ nn)
			         ) ; end N loop
			       ) ; end T loop

		       ;; Run on the test data.
               (setq Lambda esm.myBest)
               (if (>= (length X) minTN)
                      (begin (setq XT (rowManager X)) (setq score (XT.score Lambda)))
                      (setq score Lambda.Score)
                      ) ; end if
		       (writeln "") 
		       (writeln "Final testing on test data returns Score=[" score "], ErrorPct=[" Lambda.ErrorPct "]")
               (if true
                   (begin
                     (setq NX (length Y))
                     (setq avgY (avg Y))
                     (setq avgDevY 0.0)
                     (loop for n from 0 until NX (setq avgDevY (+ avgDevY (abs (- Y[n] avgY)))))
                     (/= avgDevY (number NX))
                     (setq sortedY (|Gv:sort| Y < true))
                     (setq nn (integer (max 1 (/ NX 20))))
                     (setq errPct 0.0)
                     (setq EY (Lambda.run X))
                     (setq avgFactor (abs (+ avgDevY .000000000001))) 
                     (loop for t from T to T do
                        (loop for n from 0 until NX do
                              (setq k sortedY[n])
                              (setq ey EY[k])
                              (setq ey (Lambda X[k]))
                              (setq y Y[k])
	                          (setq err (- ey y))
	                          (setq err (abs err))
                              (/= err avgFactor) 
                              (if (> err 2.0) (setq err (+ 1.0 (expt err .001)))) 
	                          (+= errPct err)
		                      (if (= (modi n nn) 0) (writeln "X[" n "], ey=[" ey "], y=[" y "], ErrorPct=[" err "]"))
                           ) ; end N loop
                         ) ; end T loop
                     (/= errPct (number NX)) 
                     (setq yHistory (new Vector: Number: 10))
                     (setq eHistory (new Vector: Number: 10))
                     (setq aHistory (new Vector: Number: 10))
                     (setq dHistory (new Vector: Number: 5))
                     (setq MM (integer (max 1 (/ NX 10))))
                     (setq sortedEY (|Gv:sort| EY < true))
                     (setq sortScore 0.0)
                     (setq oldY Y[sortedEY[0]])
                     (loop for t from T to T do
                        (loop for n from 0 until NX do
                              (setq k sortedEY[n])
                              (setq ey EY[k])
                              (setq y Y[k])
                              (if (> oldY y) (++ sortScore))(setq oldY y)
                              (setq mm (/ n MM))
                              (setq eHistory[mm] (+ eHistory[mm] (/ y MM)))
                              (setq k sortedY[n])
                              (setq y Y[k])
                              (setq yHistory[mm] (+ yHistory[mm] (/ y MM)))
                           ) ; end N loop
                         ) ; end T loop
                     (/= sortScore (number NX))
                     (setq aHistory[4] (avg eHistory[4] eHistory[3] eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[5] (avg eHistory[5] eHistory[6] eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[3] (avg eHistory[3] eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[6] (avg eHistory[6] eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[2] (avg eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[7] (avg eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[1] (avg eHistory[1] eHistory[0]))
                     (setq aHistory[8] (avg eHistory[8] eHistory[9]))
                     (setq aHistory[0] eHistory[0])
                     (setq aHistory[9] eHistory[9])
                     (setq dHistory[4] (- aHistory[9] aHistory[0]))
                     (setq dHistory[3] (- aHistory[8] aHistory[1]))
                     (setq dHistory[2] (- aHistory[7] aHistory[2]))
                     (setq dHistory[1] (- aHistory[6] aHistory[3]))
                     (setq dHistory[0] (- aHistory[5] aHistory[4]))
		             (writeln "Actual computed error on testing data is ErrorPct=[" errPct "], Sort=[" sortScore "], Avg Y=[" (avg Y) "], AvgDev Y=[" avgDevY "]")
		             (writeln "yHistory=[" (string yHistory true) "]")
		             (writeln "eHistory=[" (string eHistory true) "]")
		             (writeln "aHistory=[" (string aHistory true) "]")
		             (writeln "dHistory=[" (string dHistory true) "]")
		             (writeln "") 
                   )) ; end if 
               (writeln "esm.selfTest[linearRegression]: completed in [" (/ endTimeT 60.0) "] minutes.")       
         )) ; end Test Case linearRegression
       ;; Test Case mixedModels
       (if (or (= Test all:) (= Test mixedModels:))
           (begin
		       (writeln _eol "Starting test case: mixedModels")
               (setq mySeed 0.0)     
               (setq srandom.seed mySeed)      
		       (setq M Ms)
               (if (= myTimeON true) (setq mstart 1) (setq mstart 0)) 
               (setq Mp1 (addi M mstart))
		       (setq N Ns)
		       (setq T Ts)
		       (setq X (new Vector: Object: (muli N T)))
		       (setq Y (new Vector: Number: (muli N T)))
		       (setq C (new Vector: Number: Mp1))
		       (loop for m from 0 until Mp1 do
		          (setq C[m] (- (random 100.0) 50.0))
		          ) ; end C loop
               (setq nn 0)
			   (loop for t from 0 until T do
	              (setq k (modi t 4)) 
			      (loop for n from 0 until N do
			         (setq X[nn] (setq x (new Vector: Number: Mp1)))
			         (if (= myTimeON true) (setq x[0] (number t))) 
			         (setq y 0.0)
			         (loop for m from mstart until Mp1 do
			          	(setq x[m] (- (random 100.0) 50.0))
	                    ;; Mix the four models together
	                    (cond
	                       ;; Linear model
	                       ((= k 0) (setq y (+ y (* (|Gv:log| (+ .000001 (|Gv:abs| x[m]))) C[m]))))
	                       ;; Square model
	                       ((= k 1) (setq y (+ y (* x[m] x[m] C[m]))))
	                       ;; Sine model
	                       ((= k 2) (setq y (+ y (* (|Gv:sin| x[m]) C[m]))))
	                       ;; Log model
	                       (else (setq y (+ y (* x[m] C[m]))))
	                       ) ; end cond
			            ) ; end M loop
			         (setq Y[nn] (+ (* y (- 1.0 (* .50 myRandomError))) (* y (random myRandomError))))
                     (setCdr x y)
                     (++ nn)
			         ) ; end N loop
			       ) ; end T loop

		       ;; Train on the test case.
		       (setq modelString "Building test data as:")
		       (writeln modelString)
		       (setq modelString (append "if ((" (esm.ruleExp.ruleName 0) " % 4) == 0) y ="))
		       (loop for m from mstart until Mp1 do
                  (setq modelString (append modelString (if (< C[m] 0.0) " - (" " + (") (abs C[m]) "*log(.000001+abs(" (esm.ruleExp.ruleName m) ")))"))
		          ) ; end C loop
		       (writeln modelString ";")
		       (setq modelString (append "if ((" (esm.ruleExp.ruleName 0) " % 4) == 1) y ="))
		       (loop for m from mstart until Mp1 do
                  (setq modelString (append modelString (if (< C[m] 0.0) " - (" " + (") (abs C[m]) "*" (esm.ruleExp.ruleName m) "*" (esm.ruleExp.ruleName m)")"))
		          ) ; end C loop
		       (writeln modelString ";")
		       (setq modelString (append "if ((" (esm.ruleExp.ruleName 0) " % 4) == 2) y ="))
		       (loop for m from mstart until Mp1 do
                  (setq modelString (append modelString (if (< C[m] 0.0) " - (" " + (") (abs C[m]) "*sin(" (esm.ruleExp.ruleName m) "))"))
		          ) ; end C loop
		       (writeln modelString ";")
		       (setq modelString (append "if ((" (esm.ruleExp.ruleName 0) " % 4) == 3) y ="))
		       (loop for m from mstart until Mp1 do
                  (setq modelString (append modelString (if (< C[m] 0.0) " - (" " + (") (abs C[m]) "*" (esm.ruleExp.ruleName m) ")"))
		          ) ; end C loop
		       (writeln modelString ";")
               (if (<> myRandomError 0.0) (writeln "Additionally, we add random error as: y = (y * " (- 1.0 (* .50 myRandomError)) ") + (random " myRandomError ");")) 
               (setq startTimeT (getTickCount 0))
		       (setq Lambda (esm X Y Gs Ss Ck Seed))
               (setq endTimeT (getTickCount startTimeT))
		       (writeln "") 
		       (writeln "Final results of training")
		       (writeln "esm: N = [" Ns "], M = [" Mp1 "], Generations = [" myGc "], WFFs = [" (length myWFFs)  "], Score=[" Lambda.Score "], ScoreHistory=[" (string Lambda.History true) "]") 
               (writeln "esm.myBest, Score=[" Lambda.Score "], ErrorPct=[" Lambda.ErrorPct "], lengthWFF=[" (length Lambda.WFF) "], WFF = " (trunc (esm.evalRule Lambda.WFF) maxFormulaDisplayLen))
               (writeln "esm.myBestRegressor, Score=[" myBestRegressor.Score "], ErrorPct=[" myBestRegressor.ErrorPct "], lengthWFF=[" (length myBestRegressor.WFF) "], WFF = " (trunc (esm.evalRule myBestRegressor.WFF) maxFormulaDisplayLen))
               (writeln "myBestSelectorChampions")
               (loop for n from 0 until (length esm.myBestSelectorChampions) do (writeln "myBestSelectorChampions[" n "],Score=[" esm.myBestSelectorChampions[n 1].Score ",ErrorPct=[" esm.myBestSelectorChampions[n 1].ErrorPct "], WFF = " (trunc (esm.evalRule esm.myBestSelectorChampions[n 1].WFF) maxFormulaDisplayLen))) 
               (writeln "myBestRegressorChampions")
               (loop for n from 0 until (length esm.myBestRegressorChampions) do (writeln "myBestRegressorChampions[" n "],ErrorPct=[" esm.myBestRegressorChampions[n 1].ErrorPct ",Score=[" esm.myBestRegressorChampions[n 1].Score "], WFF = " (trunc (esm.evalRule esm.myBestRegressorChampions[n 1].WFF) maxFormulaDisplayLen))) 

               (if (= myVerboseSW true)
                   (begin
		             
                     ;; Show the champions from each island population.
		             (writeln "") 
		             (writeln "Show the champions from each island population")
                     (setq M (length esm.myPopulationIslands))
                     (loop for m from 0 until M do
                       (writeln "Champions of Island [" m "], RootWFFCount=[" myIslands[m] "]...")
                       (setq N (length esm.myPopulationIslands[m])) 
                       (loop for n from 0 until N do
                          (setq Lambda esm.myPopulationIslands[m][n])
		                  (if (<> Lambda #void) (writeln "esm.myPopulationIslands[" m "][" n "], Score=[" Lambda.Score "," Lambda.ErrorPct "], lengthWFF=[" (length Lambda.WFF) "], WFF = " (trunc (esm.evalRule Lambda.WFF) maxFormulaDisplayLen))) 
                          ) ; end population loop
                       ) ; end island loop

		             ;; Run on the training data.
		             (writeln "")
                     (setq Lambda esm.myBest) 
		             (writeln "Show final results on training data, Score=[" myBest.Score "], ErrorPct=[" myBest.ErrorPct "]")
                     (setq NX (length Y))
                     (setq avgY (avg Y))
                     (setq avgDevY 0.0)
                     (loop for n from 0 until NX (setq avgDevY (+ avgDevY (abs (- Y[n] avgY)))))
                     (/= avgDevY (number NX))
                     (setq sortedY (|Gv:sort| Y < true))
                     (setq nn (integer (max 1 (/ NX 20))))
                     (setq errPct 0.0)
                     (setq EY (Lambda.run X))
                     (setq avgFactor (abs (+ avgDevY .000000000001))) 
                     (loop for n from 0 until NX do
                        (setq k sortedY[n])
                        (setq ey EY[k])
                        (setq y Y[k])
	                    (setq err (- ey y))
	                    (setq err (abs err))
                        (/= err avgFactor) 
                        (if (> err 2.0) (setq err (+ 1.0 (expt err .001)))) 
	                    (+= errPct err)
		                (if (= (modi n nn) 0) (writeln "X[" n "], ey=[" ey "], y=[" y "], ErrorPct=[" err "]"))
                     ) ; end N loop
                     (/= errPct (number NX)) 
                     (setq yHistory (new Vector: Number: 10))
                     (setq eHistory (new Vector: Number: 10))
                     (setq aHistory (new Vector: Number: 10))
                     (setq dHistory (new Vector: Number: 5))
                     (setq MM (integer (max 1 (/ NX 10))))
                     (setq sortedEY (|Gv:sort| EY < true))
                     (loop for n from 0 until NX do
                        (setq k sortedEY[n])
                        (setq ey EY[k])
                        (setq y Y[k])
                        (setq mm (/ n MM))
                        (setq eHistory[mm] (+ eHistory[mm] (/ y MM)))
                        (setq k sortedY[n])
                        (setq y Y[k])
                        (setq yHistory[mm] (+ yHistory[mm] (/ y MM)))
                        ) ; end N loop
                     (setq aHistory[4] (avg eHistory[4] eHistory[3] eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[5] (avg eHistory[5] eHistory[6] eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[3] (avg eHistory[3] eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[6] (avg eHistory[6] eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[2] (avg eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[7] (avg eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[1] (avg eHistory[1] eHistory[0]))
                     (setq aHistory[8] (avg eHistory[8] eHistory[9]))
                     (setq aHistory[0] eHistory[0])
                     (setq aHistory[9] eHistory[9])
                     (setq dHistory[4] (- aHistory[9] aHistory[0]))
                     (setq dHistory[3] (- aHistory[8] aHistory[1]))
                     (setq dHistory[2] (- aHistory[7] aHistory[2]))
                     (setq dHistory[1] (- aHistory[6] aHistory[3]))
                     (setq dHistory[0] (- aHistory[5] aHistory[4]))
		             (writeln "Actual computed error on training data is ErrorPct=[" errPct "] versus reported ErrorPct=[" Lambda.ErrorPct "] while average Y is AvgY=[" (avg Y) "]")
		             (writeln "yHistory=[" (string yHistory true) "]")
		             (writeln "eHistory=[" (string eHistory true) "]")
		             (writeln "aHistory=[" (string aHistory true) "]")
		             (writeln "dHistory=[" (string dHistory true) "]")
		             (writeln "") 

                   )) ; end if 

		       ;; Build the test data.
               (setq srandom.seed (* mySeed (pi)))
		       (setq M Ms)
               (if (= myTimeON true) (setq mstart 1) (setq mstart 0)) 
               (setq Mp1 (addi M mstart))
		       (setq N Ns)
		       (setq T Ts)
		       (setq X (new Vector: Object: (muli N)))
		       (setq Y (new Vector: Number: (muli N)))
               (setq nn 0)
			   (loop for t from T to T do
	              (setq k (modi t 4)) 
			      (loop for n from 0 until N do
			         (setq X[nn] (setq x (new Vector: Number: Mp1)))
			         (if (= myTimeON true) (setq x[0] (number t)))
			         (setq y 0.0)
			         (loop for m from mstart until Mp1 do
			          	(setq x[m] (- (random 100.0) 50.0))
	                    ;; Mix the four models together
	                    (cond
	                       ;; Linear model
	                       ((= k 0) (setq y (+ y (* (|Gv:log| (+ .000001 (|Gv:abs| x[m]))) C[m]))))
	                       ;; Square model
	                       ((= k 1) (setq y (+ y (* x[m] x[m] C[m]))))
	                       ;; Sine model
	                       ((= k 2) (setq y (+ y (* (|Gv:sin| x[m]) C[m]))))
	                       ;; Log model
	                       (else (setq y (+ y (* x[m] C[m]))))
	                       ) ; end cond
			            ) ; end M loop
			         (setq Y[nn] (+ (* y (- 1.0 (* .50 myRandomError))) (* y (random myRandomError))))
                     (setCdr x y)
                     (++ nn)
			         ) ; end N loop
			       ) ; end T loop

		       ;; Run on the test data.
               (setq Lambda esm.myBest)
               (if (>= (length X) minTN)
                      (begin (setq XT (rowManager X)) (setq score (XT.score Lambda)))
                      (setq score Lambda.Score)
                      ) ; end if
		       (writeln "") 
		       (writeln "Final testing on test data returns Score=[" score "], ErrorPct=[" Lambda.ErrorPct "]")
               (if true
                   (begin
                     (setq NX (length Y))
                     (setq avgY (avg Y))
                     (setq avgDevY 0.0)
                     (loop for n from 0 until NX (setq avgDevY (+ avgDevY (abs (- Y[n] avgY)))))
                     (/= avgDevY (number NX))
                     (setq sortedY (|Gv:sort| Y < true))
                     (setq nn (integer (max 1 (/ NX 20))))
                     (setq errPct 0.0)
                     (setq EY (Lambda.run X))
                     (setq avgFactor (abs (+ avgDevY .000000000001))) 
                     (loop for t from T to T do
                        (loop for n from 0 until NX do
                              (setq k sortedY[n])
                              (setq ey EY[k])
                              (setq ey (Lambda X[k]))
                              (setq y Y[k])
	                          (setq err (- ey y))
	                          (setq err (abs err))
                              (/= err avgFactor) 
                              (if (> err 2.0) (setq err (+ 1.0 (expt err .001)))) 
	                          (+= errPct err)
		                      (if (= (modi n nn) 0) (writeln "X[" n "], ey=[" ey "], y=[" y "], ErrorPct=[" err "]"))
                           ) ; end N loop
                         ) ; end T loop
                     (/= errPct (number NX)) 
                     (setq yHistory (new Vector: Number: 10))
                     (setq eHistory (new Vector: Number: 10))
                     (setq aHistory (new Vector: Number: 10))
                     (setq dHistory (new Vector: Number: 5))
                     (setq MM (integer (max 1 (/ NX 10))))
                     (setq sortedEY (|Gv:sort| EY < true))
                     (setq sortScore 0.0)
                     (setq oldY Y[sortedEY[0]])
                     (loop for t from T to T do
                        (loop for n from 0 until NX do
                              (setq k sortedEY[n])
                              (setq ey EY[k])
                              (setq y Y[k])
                              (if (> oldY y) (++ sortScore))(setq oldY y)
                              (setq mm (/ n MM))
                              (setq eHistory[mm] (+ eHistory[mm] (/ y MM)))
                              (setq k sortedY[n])
                              (setq y Y[k])
                              (setq yHistory[mm] (+ yHistory[mm] (/ y MM)))
                           ) ; end N loop
                         ) ; end T loop
                     (/= sortScore (number NX))
                     (setq aHistory[4] (avg eHistory[4] eHistory[3] eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[5] (avg eHistory[5] eHistory[6] eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[3] (avg eHistory[3] eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[6] (avg eHistory[6] eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[2] (avg eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[7] (avg eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[1] (avg eHistory[1] eHistory[0]))
                     (setq aHistory[8] (avg eHistory[8] eHistory[9]))
                     (setq aHistory[0] eHistory[0])
                     (setq aHistory[9] eHistory[9])
                     (setq dHistory[4] (- aHistory[9] aHistory[0]))
                     (setq dHistory[3] (- aHistory[8] aHistory[1]))
                     (setq dHistory[2] (- aHistory[7] aHistory[2]))
                     (setq dHistory[1] (- aHistory[6] aHistory[3]))
                     (setq dHistory[0] (- aHistory[5] aHistory[4]))
		             (writeln "Actual computed error on testing data is ErrorPct=[" errPct "], Sort=[" sortScore "], Avg Y=[" (avg Y) "], AvgDev Y=[" avgDevY "]")
		             (writeln "yHistory=[" (string yHistory true) "]")
		             (writeln "eHistory=[" (string eHistory true) "]")
		             (writeln "aHistory=[" (string aHistory true) "]")
		             (writeln "dHistory=[" (string dHistory true) "]")
		             (writeln "") 
                   )) ; end if 
               (writeln "esm.selfTest[mixedModel]: completed in [" (/ endTimeT 60.0) "] minutes.")       
         )) ; end Test Case mixedModel
       ;; Test Case ratioRegression 
       (if (or (= Test all:) (= Test ratioRegression:))
           (begin
		       (writeln _eol "Starting test case: ratioRegression")
               (setq mySeed 0.0)     
               (setq srandom.seed mySeed)      
		       (setq M Ms)
               (if (= myTimeON true) (setq mstart 1) (setq mstart 0)) 
               (setq Mp1 (addi M mstart))
		       (setq N Ns)
		       (setq T Ts)
		       (setq X (new Vector: Object: (muli N T)))
		       (setq Y (new Vector: Number: (muli N T)))
		       (setq C (new Vector: Number: Mp1))
		       (loop for m from 0 until Mp1 do
		          (setq C[m] (- (random 100.0) 50.0))
		          ) ; end C loop
               (setq nn 0)
			   (loop for t from 0 until T do
	              (setq k (modi t 4)) 
			      (loop for n from 0 until N do
			         (setq X[nn] (setq x (new Vector: Number: Mp1)))
			         (if (= myTimeON true) (setq x[0] (number t)))
			         (setq y 0.0)
			         (loop for m from (addi mstart 1) until Mp1 do
                        (setq mm1 (- m 1))
			          	(setq x[m] (- (random 100.0) 50.0))
	                    ;; Mix the four models together
	                    (cond
	                       ;; Linear model
	                       ((= k 0) (setq y (+ y (pdiv (* x[mm1] C[mm1]) (* x[m] C[m])))))
	                       ;; Square model
	                       ((= k 1) (setq y (+ y (mod (* x[mm1] C[mm1]) (* x[m] C[m])))))
	                       ;; Sine model
	                       ((= k 2) (setq y (+ y (pdiv (* (|Gv:sin| x[mm1]) C[mm1]) (* (|Gv:tan| x[m]) C[m])))))
	                       ;; Log model
	                       (else (setq y (+ y (* (|Gv:log| (+ .000001 (|Gv:abs| x[m]))) C[m]))))
	                       ) ; end cond
			            ) ; end M loop
			         (setq Y[nn] (+ (* y (- 1.0 (* .50 myRandomError))) (* y (random myRandomError))))
                     (setCdr x y)
                     (++ nn)
			         ) ; end N loop
			       ) ; end T loop

		       ;; Train on the test case.
		       (setq modelString "Building test data as:")
		       (writeln modelString)
		       (setq modelString (append "if ((" (esm.ruleExp.ruleName 0) " % 4) == 0) y ="))
		       (loop for m from (addi mstart 1) until Mp1 do
                  (setq modelString (append modelString " + ((" (abs C[(- m 1)]) "*" (esm.ruleExp.ruleName (- m 1)) ")/(" (abs C[m]) "*" (esm.ruleExp.ruleName m) "))"))
		          ) ; end C loop
		       (writeln modelString ";")
		       (setq modelString (append "if ((" (esm.ruleExp.ruleName 0) " % 4) == 1) y ="))
		       (loop for m from (addi mstart 1) until Mp1 do
                  (setq modelString (append modelString " + ((" (abs C[(- m 1)]) "*" (esm.ruleExp.ruleName (- m 1)) ")%(" (abs C[m]) "*" (esm.ruleExp.ruleName m) "))"))
		          ) ; end C loop
		       (writeln modelString ";")
		       (setq modelString (append "if ((" (esm.ruleExp.ruleName 0) " % 4) == 2) y ="))
		       (loop for m from (addi mstart 1) until Mp1 do
                  (setq modelString (append modelString " + ((" (abs C[(- m 1)]) "*sin(" (esm.ruleExp.ruleName (- m 1)) "))/(" (abs C[m]) "*tan(" (esm.ruleExp.ruleName m) ")))"))
		          ) ; end C loop
		       (writeln modelString ";")
		       (setq modelString (append "if ((" (esm.ruleExp.ruleName 0) " % 4) == 3) y ="))
		       (loop for m from (addi mstart 1) until Mp1 do
                  (setq modelString (append modelString (if (< C[m] 0.0) " - (" " + (") (abs C[m]) "* log(.000001+abs(" (esm.ruleExp.ruleName m) ")))"))
		          ) ; end C loop
		       (writeln modelString ";")
               (if (<> myRandomError 0.0) (writeln "Additionally, we add random error as: y = (y * " (- 1.0 (* .50 myRandomError)) ") + (random " myRandomError ");")) 
               (setq startTimeT (getTickCount 0))
		       (setq Lambda (esm X Y Gs Ss Ck Seed))
               (setq endTimeT (getTickCount startTimeT))
		       (writeln "") 
		       (writeln "Final results of training")
		       (writeln "esm: N = [" Ns "], M = [" Mp1 "], Generations = [" myGc "], WFFs = [" (length myWFFs)  "], Score=[" Lambda.Score "], ScoreHistory=[" (string Lambda.History true) "]") 
               (writeln "esm.myBest, Score=[" Lambda.Score "], ErrorPct=[" Lambda.ErrorPct "], lengthWFF=[" (length Lambda.WFF) "], WFF = " (trunc (esm.evalRule Lambda.WFF) maxFormulaDisplayLen))
               (writeln "esm.myBestRegressor, Score=[" myBestRegressor.Score "], ErrorPct=[" myBestRegressor.ErrorPct "], lengthWFF=[" (length myBestRegressor.WFF) "], WFF = " (trunc (esm.evalRule myBestRegressor.WFF) maxFormulaDisplayLen))
               (writeln "myBestSelectorChampions")
               (loop for n from 0 until (length esm.myBestSelectorChampions) do (writeln "myBestSelectorChampions[" n "],Score=[" esm.myBestSelectorChampions[n 1].Score ",ErrorPct=[" esm.myBestSelectorChampions[n 1].ErrorPct "], WFF = " (trunc (esm.evalRule esm.myBestSelectorChampions[n 1].WFF) maxFormulaDisplayLen))) 
               (writeln "myBestRegressorChampions")
               (loop for n from 0 until (length esm.myBestRegressorChampions) do (writeln "myBestRegressorChampions[" n "],ErrorPct=[" esm.myBestRegressorChampions[n 1].ErrorPct ",Score=[" esm.myBestRegressorChampions[n 1].Score "], WFF = " (trunc (esm.evalRule esm.myBestRegressorChampions[n 1].WFF) maxFormulaDisplayLen))) 

               (if (= myVerboseSW true)
                   (begin
		             
                     ;; Show the champions from each island population.
		             (writeln "") 
		             (writeln "Show the champions from each island population")
                     (setq M (length esm.myPopulationIslands))
                     (loop for m from 0 until M do
                       (writeln "Champions of Island [" m "], RootWFFCount=[" myIslands[m] "]...")
                       (setq N (length esm.myPopulationIslands[m])) 
                       (loop for n from 0 until N do
                          (setq Lambda esm.myPopulationIslands[m][n])
		                  (if (<> Lambda #void) (writeln "esm.myPopulationIslands[" m "][" n "], Score=[" Lambda.Score "," Lambda.ErrorPct "], lengthWFF=[" (length Lambda.WFF) "], WFF = " (trunc (esm.evalRule Lambda.WFF) maxFormulaDisplayLen))) 
                          ) ; end population loop
                       ) ; end island loop

		             ;; Run on the training data.
		             (writeln "")
                     (setq Lambda esm.myBest) 
		             (writeln "Show final results on training data, Score=[" myBest.Score "], ErrorPct=[" myBest.ErrorPct "]")
                     (setq NX (length Y))
                     (setq avgY (avg Y))
                     (setq avgDevY 0.0)
                     (loop for n from 0 until NX (setq avgDevY (+ avgDevY (abs (- Y[n] avgY)))))
                     (/= avgDevY (number NX))
                     (setq sortedY (|Gv:sort| Y < true))
                     (setq nn (integer (max 1 (/ NX 20))))
                     (setq errPct 0.0)
                     (setq EY (Lambda.run X))
                     (setq avgFactor (abs (+ avgDevY .000000000001))) 
                     (loop for n from 0 until NX do
                        (setq k sortedY[n])
                        (setq ey EY[k])
                        (setq y Y[k])
	                    (setq err (- ey y))
	                    (setq err (abs err))
                        (/= err avgFactor) 
                        (if (> err 2.0) (setq err (+ 1.0 (expt err .001)))) 
	                    (+= errPct err)
		                (if (= (modi n nn) 0) (writeln "X[" n "], ey=[" ey "], y=[" y "], ErrorPct=[" err "]"))
                     ) ; end N loop
                     (/= errPct (number NX)) 
                     (setq yHistory (new Vector: Number: 10))
                     (setq eHistory (new Vector: Number: 10))
                     (setq aHistory (new Vector: Number: 10))
                     (setq dHistory (new Vector: Number: 5))
                     (setq MM (integer (max 1 (/ NX 10))))
                     (setq sortedEY (|Gv:sort| EY < true))
                     (loop for n from 0 until NX do
                        (setq k sortedEY[n])
                        (setq ey EY[k])
                        (setq y Y[k])
                        (setq mm (/ n MM))
                        (setq eHistory[mm] (+ eHistory[mm] (/ y MM)))
                        (setq k sortedY[n])
                        (setq y Y[k])
                        (setq yHistory[mm] (+ yHistory[mm] (/ y MM)))
                        ) ; end N loop
                     (setq aHistory[4] (avg eHistory[4] eHistory[3] eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[5] (avg eHistory[5] eHistory[6] eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[3] (avg eHistory[3] eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[6] (avg eHistory[6] eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[2] (avg eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[7] (avg eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[1] (avg eHistory[1] eHistory[0]))
                     (setq aHistory[8] (avg eHistory[8] eHistory[9]))
                     (setq aHistory[0] eHistory[0])
                     (setq aHistory[9] eHistory[9])
                     (setq dHistory[4] (- aHistory[9] aHistory[0]))
                     (setq dHistory[3] (- aHistory[8] aHistory[1]))
                     (setq dHistory[2] (- aHistory[7] aHistory[2]))
                     (setq dHistory[1] (- aHistory[6] aHistory[3]))
                     (setq dHistory[0] (- aHistory[5] aHistory[4]))
		             (writeln "Actual computed error on training data is ErrorPct=[" errPct "] versus reported ErrorPct=[" Lambda.ErrorPct "] while average Y is AvgY=[" (avg Y) "]")
		             (writeln "yHistory=[" (string yHistory true) "]")
		             (writeln "eHistory=[" (string eHistory true) "]")
		             (writeln "aHistory=[" (string aHistory true) "]")
		             (writeln "dHistory=[" (string dHistory true) "]")
		             (writeln "") 

                   )) ; end if 

		       ;; Build the test data.
               (setq srandom.seed (* mySeed (pi)))
		       (setq M Ms)
               (if (= myTimeON true) (setq mstart 1) (setq mstart 0)) 
               (setq Mp1 (addi M mstart))
		       (setq N Ns)
		       (setq T Ts)
		       (setq X (new Vector: Object: (muli N)))
		       (setq Y (new Vector: Number: (muli N)))
               (setq nn 0)
			   (loop for t from T to T do
	              (setq k (modi t 4)) 
			      (loop for n from 0 until N do
			         (setq X[nn] (setq x (new Vector: Number: Mp1)))
			         (if (= myTimeON true) (setq x[0] (number t)))
			         (setq y 0.0)
			         (loop for m from (addi mstart 1) until Mp1 do
                        (setq mm1 (- m 1))
			          	(setq x[m] (- (random 100.0) 50.0))
	                    ;; Mix the four models together
	                    (cond
	                       ;; Linear model
	                       ((= k 0) (setq y (+ y (pdiv (* x[mm1] C[mm1]) (* x[m] C[m])))))
	                       ;; Square model
	                       ((= k 1) (setq y (+ y (mod (* x[mm1] C[mm1]) (* x[m] C[m])))))
	                       ;; Sine model
	                       ((= k 2) (setq y (+ y (pdiv (* (|Gv:sin| x[mm1]) C[mm1]) (* (|Gv:tan| x[m]) C[m])))))
	                       ;; Log model
	                       (else (setq y (+ y (* (|Gv:log| (+ .000001 (|Gv:abs| x[m]))) C[m]))))
	                       ) ; end cond
			            ) ; end M loop
			         (setq Y[nn] (+ (* y (- 1.0 (* .50 myRandomError))) (* y (random myRandomError))))
                     (setCdr x y)
                     (++ nn)
			         ) ; end N loop
			       ) ; end T loop

		       ;; Run on the test data.
               (setq Lambda esm.myBest)
               (if (>= (length X) minTN)
                      (begin (setq XT (rowManager X)) (setq score (XT.score Lambda)))
                      (setq score Lambda.Score)
                      ) ; end if
		       (writeln "") 
		       (writeln "Final testing on test data returns Score=[" score "], ErrorPct=[" Lambda.ErrorPct "]")
               (if true
                   (begin
                     (setq NX (length Y))
                     (setq avgY (avg Y))
                     (setq avgDevY 0.0)
                     (loop for n from 0 until NX (setq avgDevY (+ avgDevY (abs (- Y[n] avgY)))))
                     (/= avgDevY (number NX))
                     (setq sortedY (|Gv:sort| Y < true))
                     (setq nn (integer (max 1 (/ NX 20))))
                     (setq errPct 0.0)
                     (setq EY (Lambda.run X))
                     (setq avgFactor (abs (+ avgDevY .000000000001))) 
                     (loop for t from T to T do
                        (loop for n from 0 until NX do
                              (setq k sortedY[n])
                              (setq ey EY[k])
                              (setq ey (Lambda X[k]))
                              (setq y Y[k])
	                          (setq err (- ey y))
	                          (setq err (abs err))
                              (/= err avgFactor) 
                              (if (> err 2.0) (setq err (+ 1.0 (expt err .001)))) 
	                          (+= errPct err)
		                      (if (= (modi n nn) 0) (writeln "X[" n "], ey=[" ey "], y=[" y "], ErrorPct=[" err "]"))
                           ) ; end N loop
                         ) ; end T loop
                     (/= errPct (number NX)) 
                     (setq yHistory (new Vector: Number: 10))
                     (setq eHistory (new Vector: Number: 10))
                     (setq aHistory (new Vector: Number: 10))
                     (setq dHistory (new Vector: Number: 5))
                     (setq MM (integer (max 1 (/ NX 10))))
                     (setq sortedEY (|Gv:sort| EY < true))
                     (setq sortScore 0.0)
                     (setq oldY Y[sortedEY[0]])
                     (loop for t from T to T do
                        (loop for n from 0 until NX do
                              (setq k sortedEY[n])
                              (setq ey EY[k])
                              (setq y Y[k])
                              (if (> oldY y) (++ sortScore))(setq oldY y)
                              (setq mm (/ n MM))
                              (setq eHistory[mm] (+ eHistory[mm] (/ y MM)))
                              (setq k sortedY[n])
                              (setq y Y[k])
                              (setq yHistory[mm] (+ yHistory[mm] (/ y MM)))
                           ) ; end N loop
                         ) ; end T loop
                     (/= sortScore (number NX))
                     (setq aHistory[4] (avg eHistory[4] eHistory[3] eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[5] (avg eHistory[5] eHistory[6] eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[3] (avg eHistory[3] eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[6] (avg eHistory[6] eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[2] (avg eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[7] (avg eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[1] (avg eHistory[1] eHistory[0]))
                     (setq aHistory[8] (avg eHistory[8] eHistory[9]))
                     (setq aHistory[0] eHistory[0])
                     (setq aHistory[9] eHistory[9])
                     (setq dHistory[4] (- aHistory[9] aHistory[0]))
                     (setq dHistory[3] (- aHistory[8] aHistory[1]))
                     (setq dHistory[2] (- aHistory[7] aHistory[2]))
                     (setq dHistory[1] (- aHistory[6] aHistory[3]))
                     (setq dHistory[0] (- aHistory[5] aHistory[4]))
		             (writeln "Actual computed error on testing data is ErrorPct=[" errPct "], Sort=[" sortScore "], Avg Y=[" (avg Y) "], AvgDev Y=[" avgDevY "]")
		             (writeln "yHistory=[" (string yHistory true) "]")
		             (writeln "eHistory=[" (string eHistory true) "]")
		             (writeln "aHistory=[" (string aHistory true) "]")
		             (writeln "dHistory=[" (string dHistory true) "]")
		             (writeln "") 
                   )) ; end if 
               (writeln "esm.selfTest[ratioRegression]: completed in [" (/ endTimeT 60.0) "] minutes.")       
         )) ; end Test Case ratioRegression
       ;; Test Case cyclicSeries 
       (if (or (= Test all:) (= Test cyclicSeries:))
           (begin
		       (writeln _eol "Starting test case: cyclicSeries")
               (setq mySeed 0.0)     
               (setq srandom.seed mySeed)      
		       (setq M Ms)
               (if (= myTimeON true) (setq mstart 1) (setq mstart 0)) 
               (setq Mp1 (addi M mstart))
		       (setq N Ns)
		       (setq T Ts)
		       (setq X (new Vector: Object: (muli N T)))
		       (setq Y (new Vector: Number: (muli N T)))
		       (setq C (new Vector: Number: Mp1))
		       (loop for m from 0 until Mp1 do
		          (setq C[m] (- (random 100.0) 50.0))
		          ) ; end C loop
               (setq nn 0)
			   (loop for t from 0 until T do
	              (setq k (modi t 4)) 
			      (loop for n from 0 until N do
			         (setq X[nn] (setq x (new Vector: Number: Mp1)))
			         (if (= myTimeON true) (setq x[0] (number t)))
			         (setq y C[0])
			         (loop for m from mstart until Mp1 do
			          	(setq x[m] (- (random 100.0) 50.0))
	                    ;; time varying model
	                    (cond
	                       ;; sine model
	                       ((= (modi m 3) 0) (setq y (+ y (* (sin x[0]) x[m] C[m]))))
	                       ;; cosine model
	                       ((= (modi m 3) 1) (setq y (+ y (* (cos x[0]) x[m] C[m]))))
	                       ;; tangent model
	                       (else (setq y (+ y (* (tan x[0]) x[m] C[m]))))
	                       ) ; end cond
			            ) ; end M loop
			         (setq Y[nn] (+ (* y (- 1.0 (* .50 myRandomError))) (* y (random myRandomError))))
                     (setCdr x y)
                     (++ nn)
			         ) ; end N loop
			       ) ; end T loop

		       ;; Train on the test case.
		       (setq modelString (append "Building test data as: y = " C[0]))
		       (loop for m from mstart until Mp1 do
                  (cond 
	                ((= (modi m 3) 0) (setq modelString (append modelString (if (< C[m] 0.0) " - (" " + (") (abs C[m]) "*" (esm.ruleExp.ruleName m) "*sin(" (esm.ruleExp.ruleName 0) "))")))
	                ((= (modi m 3) 1) (setq modelString (append modelString (if (< C[m] 0.0) " - (" " + (") (abs C[m]) "*" (esm.ruleExp.ruleName m) "*cos(" (esm.ruleExp.ruleName 0) "))")))
	                (else (setq modelString (append modelString (if (< C[m] 0.0) " - (" " + (") (abs C[m]) "*" (esm.ruleExp.ruleName m) "*tan(" (esm.ruleExp.ruleName 0) "))")))
                    ) ; end cond
		          ) ; end C loop
		       (writeln modelString ";")
               (if (<> myRandomError 0.0) (writeln "Additionally, we add random error as: y = (y * " (- 1.0 (* .50 myRandomError)) ") + (random " myRandomError ");")) 
               (setq startTimeT (getTickCount 0))
		       (setq Lambda (esm X Y Gs Ss Ck Seed))
               (setq endTimeT (getTickCount startTimeT))
		       (writeln "") 
		       (writeln "Final results of training")
		       (writeln "esm: N = [" Ns "], M = [" Mp1 "], Generations = [" myGc "], WFFs = [" (length myWFFs)  "], Score=[" Lambda.Score "], ScoreHistory=[" (string Lambda.History true) "]") 
               (writeln "esm.myBest, Score=[" Lambda.Score "], ErrorPct=[" Lambda.ErrorPct "], lengthWFF=[" (length Lambda.WFF) "], WFF = " (trunc (esm.evalRule Lambda.WFF) maxFormulaDisplayLen))
               (writeln "esm.myBestRegressor, Score=[" myBestRegressor.Score "], ErrorPct=[" myBestRegressor.ErrorPct "], lengthWFF=[" (length myBestRegressor.WFF) "], WFF = " (trunc (esm.evalRule myBestRegressor.WFF) maxFormulaDisplayLen))
               (writeln "myBestSelectorChampions")
               (loop for n from 0 until (length esm.myBestSelectorChampions) do (writeln "myBestSelectorChampions[" n "],Score=[" esm.myBestSelectorChampions[n 1].Score ",ErrorPct=[" esm.myBestSelectorChampions[n 1].ErrorPct "], WFF = " (trunc (esm.evalRule esm.myBestSelectorChampions[n 1].WFF) maxFormulaDisplayLen))) 
               (writeln "myBestRegressorChampions")
               (loop for n from 0 until (length esm.myBestRegressorChampions) do (writeln "myBestRegressorChampions[" n "],ErrorPct=[" esm.myBestRegressorChampions[n 1].ErrorPct ",Score=[" esm.myBestRegressorChampions[n 1].Score "], WFF = " (trunc (esm.evalRule esm.myBestRegressorChampions[n 1].WFF) maxFormulaDisplayLen))) 

               (if (= myVerboseSW true)
                   (begin
		             
                     ;; Show the champions from each island population.
		             (writeln "") 
		             (writeln "Show the champions from each island population")
                     (setq M (length esm.myPopulationIslands))
                     (loop for m from 0 until M do
                       (writeln "Champions of Island [" m "], RootWFFCount=[" myIslands[m] "]...")
                       (setq N (length esm.myPopulationIslands[m])) 
                       (loop for n from 0 until N do
                          (setq Lambda esm.myPopulationIslands[m][n])
		                  (if (<> Lambda #void) (writeln "esm.myPopulationIslands[" m "][" n "], Score=[" Lambda.Score "," Lambda.ErrorPct "], lengthWFF=[" (length Lambda.WFF) "], WFF = " (trunc (esm.evalRule Lambda.WFF) maxFormulaDisplayLen))) 
                          ) ; end population loop
                       ) ; end island loop

		             ;; Run on the training data.
		             (writeln "")
                     (setq Lambda esm.myBest)
		             (writeln "Show final results on training data, Score=[" myBest.Score "], ErrorPct=[" myBest.ErrorPct "]")
                     (setq NX (length Y))
                     (setq avgY (avg Y))
                     (setq avgDevY 0.0)
                     (loop for n from 0 until NX (setq avgDevY (+ avgDevY (abs (- Y[n] avgY)))))
                     (/= avgDevY (number NX))
                     (setq sortedY (|Gv:sort| Y < true))
                     (setq nn (integer (max 1 (/ NX 20))))
                     (setq errPct 0.0)
                     (setq EY (Lambda.run X))
                     (setq avgFactor (abs (+ avgDevY .000000000001))) 
                     (loop for n from 0 until NX do
                        (setq k sortedY[n])
                        (setq ey EY[k])
                        (setq y Y[k])
	                    (setq err (- ey y))
	                    (setq err (abs err))
                        (/= err avgFactor) 
                        (if (> err 2.0) (setq err (+ 1.0 (expt err .001)))) 
	                    (+= errPct err)
		                (if (= (modi n nn) 0) (writeln "X[" n "], ey=[" ey "], y=[" y "], ErrorPct=[" err "]"))
                     ) ; end N loop
                     (/= errPct (number NX)) 
                     (setq yHistory (new Vector: Number: 10))
                     (setq eHistory (new Vector: Number: 10))
                     (setq aHistory (new Vector: Number: 10))
                     (setq dHistory (new Vector: Number: 5))
                     (setq MM (integer (max 1 (/ NX 10))))
                     (setq sortedEY (|Gv:sort| EY < true))
                     (loop for n from 0 until NX do
                        (setq k sortedEY[n])
                        (setq ey EY[k])
                        (setq y Y[k])
                        (setq mm (/ n MM))
                        (setq eHistory[mm] (+ eHistory[mm] (/ y MM)))
                        (setq k sortedY[n])
                        (setq y Y[k])
                        (setq yHistory[mm] (+ yHistory[mm] (/ y MM)))
                        ) ; end N loop
                     (setq aHistory[4] (avg eHistory[4] eHistory[3] eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[5] (avg eHistory[5] eHistory[6] eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[3] (avg eHistory[3] eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[6] (avg eHistory[6] eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[2] (avg eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[7] (avg eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[1] (avg eHistory[1] eHistory[0]))
                     (setq aHistory[8] (avg eHistory[8] eHistory[9]))
                     (setq aHistory[0] eHistory[0])
                     (setq aHistory[9] eHistory[9])
                     (setq dHistory[4] (- aHistory[9] aHistory[0]))
                     (setq dHistory[3] (- aHistory[8] aHistory[1]))
                     (setq dHistory[2] (- aHistory[7] aHistory[2]))
                     (setq dHistory[1] (- aHistory[6] aHistory[3]))
                     (setq dHistory[0] (- aHistory[5] aHistory[4]))
		             (writeln "Actual computed error on training data is ErrorPct=[" errPct "] versus reported ErrorPct=[" Lambda.ErrorPct "] while average Y is AvgY=[" (avg Y) "]")
		             (writeln "yHistory=[" (string yHistory true) "]")
		             (writeln "eHistory=[" (string eHistory true) "]")
		             (writeln "aHistory=[" (string aHistory true) "]")
		             (writeln "dHistory=[" (string dHistory true) "]")
		             (writeln "") 

                   )) ; end if 

		       ;; Build the test data.
               (setq srandom.seed (* mySeed (pi)))
		       (setq M Ms)
               (if (= myTimeON true) (setq mstart 1) (setq mstart 0)) 
               (setq Mp1 (addi M mstart))
		       (setq N Ns)
		       (setq T Ts)
		       (setq X (new Vector: Object: (muli N)))
		       (setq Y (new Vector: Number: (muli N)))
               (setq nn 0)
			   (loop for t from T to T do
	              (setq k (modi t 4)) 
			      (loop for n from 0 until N do
			         (setq X[nn] (setq x (new Vector: Number: Mp1)))
			         (if (= myTimeON true) (setq x[0] (number t)))
			         (setq y C[0])
			         (loop for m from mstart until Mp1 do
			          	(setq x[m] (- (random 100.0) 50.0))
	                    ;; time varying model
	                    (cond
	                       ;; sine model
	                       ((= (modi m 3) 0) (setq y (+ y (* (sin x[0]) x[m] C[m]))))
	                       ;; cosine model
	                       ((= (modi m 3) 1) (setq y (+ y (* (cos x[0]) x[m] C[m]))))
	                       ;; tangent model
	                       (else (setq y (+ y (* (tan x[0]) x[m] C[m]))))
	                       ) ; end cond
			            ) ; end M loop
			         (setq Y[nn] (+ (* y (- 1.0 (* .50 myRandomError))) (* y (random myRandomError))))
                     (setCdr x y)
                     (++ nn)
			         ) ; end N loop
			       ) ; end T loop

		       ;; Run on the test data.
               (setq Lambda esm.myBest) 
               (if (>= (length X) minTN)
                   (begin (setq XT (rowManager X)) (setq score (XT.score Lambda)))
                   (setq score Lambda.Score)
                   ) ; end if
		       (writeln "") 
		       (writeln "Final testing on test data returns Score=[" score "], ErrorPct=[" Lambda.ErrorPct "]")
               (if true
                   (begin
                     (setq NX (length Y))
                     (setq avgY (avg Y))
                     (setq avgDevY 0.0)
                     (loop for n from 0 until NX (setq avgDevY (+ avgDevY (abs (- Y[n] avgY)))))
                     (/= avgDevY (number NX))
                     (setq sortedY (|Gv:sort| Y < true))
                     (setq nn (integer (max 1 (/ NX 20))))
                     (setq errPct 0.0)
                     (setq EY (Lambda.run X))
                     (setq avgFactor (abs (+ avgDevY .000000000001))) 
                     (loop for t from T to T do
                        (loop for n from 0 until NX do
                              (setq k sortedY[n])
                              (setq ey EY[k])
                              (setq ey (Lambda X[k]))
                              (setq y Y[k])
	                          (setq err (- ey y))
	                          (setq err (abs err))
                              (/= err avgFactor) 
                              (if (> err 2.0) (setq err (+ 1.0 (expt err .001)))) 
	                          (+= errPct err)
		                      (if (= (modi n nn) 0) (writeln "X[" n "], ey=[" ey "], y=[" y "], ErrorPct=[" err "]"))
                           ) ; end N loop
                         ) ; end T loop
                     (/= errPct (number NX)) 
                     (setq yHistory (new Vector: Number: 10))
                     (setq eHistory (new Vector: Number: 10))
                     (setq aHistory (new Vector: Number: 10))
                     (setq dHistory (new Vector: Number: 5))
                     (setq MM (integer (max 1 (/ NX 10))))
                     (setq sortedEY (|Gv:sort| EY < true))
                     (setq sortScore 0.0)
                     (setq oldY Y[sortedEY[0]])
                     (loop for t from T to T do
                        (loop for n from 0 until NX do
                              (setq k sortedEY[n])
                              (setq ey EY[k])
                              (setq y Y[k])
                              (if (> oldY y) (++ sortScore))(setq oldY y)
                              (setq mm (/ n MM))
                              (setq eHistory[mm] (+ eHistory[mm] (/ y MM)))
                              (setq k sortedY[n])
                              (setq y Y[k])
                              (setq yHistory[mm] (+ yHistory[mm] (/ y MM)))
                           ) ; end N loop
                         ) ; end T loop
                     (/= sortScore (number NX))
                     (setq aHistory[4] (avg eHistory[4] eHistory[3] eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[5] (avg eHistory[5] eHistory[6] eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[3] (avg eHistory[3] eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[6] (avg eHistory[6] eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[2] (avg eHistory[2] eHistory[1] eHistory[0]))
                     (setq aHistory[7] (avg eHistory[7] eHistory[8] eHistory[9]))
                     (setq aHistory[1] (avg eHistory[1] eHistory[0]))
                     (setq aHistory[8] (avg eHistory[8] eHistory[9]))
                     (setq aHistory[0] eHistory[0])
                     (setq aHistory[9] eHistory[9])
                     (setq dHistory[4] (- aHistory[9] aHistory[0]))
                     (setq dHistory[3] (- aHistory[8] aHistory[1]))
                     (setq dHistory[2] (- aHistory[7] aHistory[2]))
                     (setq dHistory[1] (- aHistory[6] aHistory[3]))
                     (setq dHistory[0] (- aHistory[5] aHistory[4]))
		             (writeln "Actual computed error on testing data is ErrorPct=[" errPct "], Sort=[" sortScore "], Avg Y=[" (avg Y) "], AvgDev Y=[" avgDevY "]")
		             (writeln "yHistory=[" (string yHistory true) "]")
		             (writeln "eHistory=[" (string eHistory true) "]")
		             (writeln "aHistory=[" (string aHistory true) "]")
		             (writeln "dHistory=[" (string dHistory true) "]")
		             (writeln "") 
                   )) ; end if 
               (writeln "esm.selfTest[cyclicSeries]: completed in [" (/ endTimeT 60.0) "] minutes.")       
         )) ; end Test Case cyclicSeries
       true) ; end selfTest

  	;; *******************************************************************************
  	;; Begin Main Logic 
  	;; *******************************************************************************
  	regs:(m M n N mm MM nn NN maxIslands)
  	regs:(k1 I1 K1 k2 I2 K2 k3 I3 K3 greedyCounter)
    vars:((Integer:migrationIsland -1)  (Integer:VerboseSample 1))
  	vars:(result Lambda wff newWff wffString rule prevGenome)
  	vars:(bestIslands greedyRuleCnt genome migPopulation oldMyWFFs)
  	vars:(Integer:generationGap Number:oldBestScore)

    ;; ----------------------------
    ;; Initialize persistent memory
    ;; ----------------------------
    (if (debug getcompileon:) (error "esm: cannot train with compiler set to generate debug information"))
    (clear)
    (gc compact:)
	(setq myWFFSaveLimit (integer (/ (inspect) 10)))
    (if (>= (argCount) 5) then (setq myCheckPoint (argFetch 4)) else (setq myCheckPoint false))
    (if (>= (argCount) 6) then (setq mySeed (number (argFetch 5))) else (setq mySeed mySeedDefault))
	(setq myG G)
    (setq myGc 0)
	(setq myX X)
	(setq myY Y)
	(setq myM (length X[0]))
	(setq myN (length Y))
	(setq myS S)
    (if (< myN 150) (error "esm: too few training examples"))

    ;; Initialize training data and subdivide into time periods (if requested)
    (setq srandom.seed mySeed)
    (setq myRepository (new ObjectRepository: myRepositoryDefaultName))
    (setq myRepository[roots:] #void)
    (setq myMaxRootWFFs (growTermWFF -1 0))
    (setq maxIslands (+ 2 myM))
    (setq myBestOfBreedIsland myM)
    (setq myBestRegressorIsland (+ 1 myM))
	(setq myWFFLengthLimit (integer (* myM 400)))
    (setq myBestRegressorChampions (new Directory:))
    (setq myBestSelectorChampions (new Directory:))
    (setq myBestSelectorBestOfBest (new Directory:))
    (initTrainingData)
    (setq myWFFSaveON myWFFSaveSW)
  	(setq myWFFs (new Directory:))

    ;; Generate default genome containing each column name.
    (setq myColumnGenome (new Vector: myM))
    (setq M myM)
    (loop for m from 0 until M do (setq myColumnGenome[m] (list ruleName: m)))

    ;; ************************************************************************
    ;; Reload the previous checkpoint before resuming the evolutionary process.
    ;; ************************************************************************

    ;; Has a restart from a previous checkpoint been requested?
    (if (and (= myCheckPoint restart:) (<> (loadCheckPoint checkpoint:) #void)) then (goto StartMainGenerations:)) 

    ;; Has reuse initial state and checkpointing been requested?
    (if (and (= myCheckPoint reuse:) (<> (loadCheckPoint initial:) #void)) then (goto StartMainGenerations:))

    ;; ************************************************************************
    ;; Run the initial evolutionary process before the first generation.
    ;; ************************************************************************
    StartNewEvolutionaryProcess::
    
    ;; Initialize all of the island communities once again here.
    ;; Note: Here we allocate one island community per data column,
	(if myVerboseSW (writeln "...starting new evolutionary process."))
    (++ myEvolutionCount)
    (if (<> myBestRegressor #void) (setq myBestRegressorChampions[myBestRegressor.ErrorPct] myBestRegressor))
    (if (<> myBestSelector #void) (setq myBestSelectorChampions[myBestSelector.Score] myBestSelector))
    (if (<> myBestSelector #void) (setq myBestSelectorBestOfBest[myBestSelector.Score] myBestSelector))
    (if (> (length myBestSelectorBestOfBest) myBestOfBest) (resize myBestSelectorBestOfBest myBestOfBest))
    (setq myIslands (new Vector: Integer: myM))
    (setq myPopulationIslands (new Vector: Object: maxIslands))
    (loop for n from 0 until maxIslands do (setq myPopulationIslands[n] (new Vector: Object:))) 	
  	(if (= myWFFReinitSW true) (begin (setq myWFFSaveON myWFFSaveSW) (setq myWFFs (new Directory:))))
    (setq myBestT (new Vector: Object: myT))
    (setq generationGap 0)
  	(setq oldBestScore BIGPOSNUM)

    ;; Generate default selector Lambda.
    (setq myBest (eval (compile (selector "regress 0.0;"))))
    (setq myBest.Error 999999999.0)      
    (setq myBest.ErrorPct 999999999.0)
    (setq myBest.Score 999999999.0)
    (setq myBest.WFF "0.0")   
    (setq myBestRegressor myBest) 
    (setq myBestSelector myBestRegressor) 

    ;; Initialize the best-of-breed population with a constant WFF.
    ;; Note1: This must be done for the case where Y is all the same.
    ;; Note2: WFF = "regress 1.0;"
    (if (or (> myRootINIT 0) (> myRootGEN 0))
        (begin
           (if myVerboseSW (writeln "...starting generation of constant selector WFF."))
           (setq wff 1.0)
           (setq myPopulation myPopulationIslands[myBestOfBreedIsland])
       
           (setq genome (new Vector: myM))
           (setq genome[0] wff)
           (ruleReg.growWFF genome)
           (bestRegressor)
           (if (<= (bestSelector)[Score:] myS) (goto Last:))

        )) ; end if

    ;; Generate basic regress selectors for each column name.
    ;; Note1: This must be done for the case where Y is a simple linear regression on one or more columns.
    ;; Note2: WFF = "regress x0;" (regress on each column name in the respective column island)
    (if (or (> myRootINIT 0) (> myRootGEN 0))
        (begin
           (if myVerboseSW (writeln "...starting regress selector for each column name."))
           (setq M myM)
           (loop for m from 0 to M do
              ;; Generate a basic regress selector for this column in the training data.
              (setq myPopulation myPopulationIslands[m])
              (setq wff (list ruleName: m))
              (setq genome (new Vector: myM))
              (setq genome[m] wff)
              (cond
               ((and (= myUseBGM true) (= myRootRule BGM:)) (ruleBgm.growWFF genome))
               ((and (= myUseFRM true) (= myRootRule FRM:)) (ruleFrm.growWFF genome))
               ((and (= myUseMVL true) (= myRootRule MVL:)) (ruleMvl.growWFF genome))
               ((and (= myUseENN true) (= myRootRule ENN:)) (ruleEnn.growWFF ))
               ((and (= myUseREG true) (= myRootRule REG:)) (ruleReg.growWFF genome))
               ((and (= myUseSVM true) (= myRootRule SVM:)) (ruleSvm.growWFF genome))
               (else (error "esm: root WFF growth specified with invalid regression rule"))
               ) ; end cond
              ) ; end generate basic colum selectors
           (bestRegressor)
           (if (<= (bestSelector)[Score:] myS) (goto Last:))
             
        )) ; end if

    ;; Create a simple BGM selector WFF.
    (if (= myUseBGM true)
        (begin
          (if myVerboseSW (writeln "...starting a best-of-breed BGM selector WFF."))
          (setq myPopulation myPopulationIslands[myBestOfBreedIsland])
          (ruleBgm.growWFF myColumnGenome)
          (bestRegressor)
          (if (<= (bestSelector)[Score:] myS) (goto Last:))
        )) ; end if

    ;; Create a simple FRM selector WFF.
    (if (= myUseFRM true)
        (begin
          (if myVerboseSW (writeln "...starting a best-of-breed FRM selector WFF."))
          (setq myPopulation myPopulationIslands[myBestOfBreedIsland])
          (ruleFrm.growWFF myColumnGenome)
          (bestRegressor)
          (if (<= (bestSelector)[Score:] myS) (goto Last:))
        )) ; end if

    ;; Create a simple MVL selector WFF.
    (if (= myUseMVL true)
        (begin
          (if myVerboseSW (writeln "...starting a best-of-breed MVL selector WFF."))
          (setq myPopulation myPopulationIslands[myBestOfBreedIsland])
          (ruleMvl.growWFF myColumnGenome)
          (bestRegressor)
          (if (<= (bestSelector)[Score:] myS) (goto Last:))
        )) ; end if

    ;; Create a simple REG selector WFF.
    (if (= myUseREG true)
        (begin
          (if myVerboseSW (writeln "...starting a best-of-breed REG selector WFF."))
          (setq myPopulation myPopulationIslands[myBestOfBreedIsland])
          (ruleReg.growWFF myColumnGenome)
          (bestRegressor)
          (if (<= (bestSelector)[Score:] myS) (goto Last:))
        )) ; end if

    ;; Create a simple ENN selector WFF.
    (if (= myUseENN true)
        (begin
          (if myVerboseSW (writeln "...starting a best-of-breed ENN selector WFF."))
          (setq myPopulation myPopulationIslands[myBestOfBreedIsland])
          (ruleEnn.growWFF )
          (bestRegressor)
          (if (<= (bestSelector)[Score:] myS) (goto Last:))
        )) ; end if

    ;; Create a simple cubic SVM selector WFF.
    (if (= myUseSVM true)
        (begin
          (if myVerboseSW (writeln "...starting a best-of-breed SVM selector WFF on all columns."))
          (setq myPopulation myPopulationIslands[myBestOfBreedIsland])
          (ruleSvm.growWFF myColumnGenome)
          (bestRegressor)
          (if (<= (bestSelector)[Score:] myS) (goto Last:))
        )) ; end if

    ;; Generate the initial population of root Selector WFFs in each column island.
    ;; Note1: We grow sequential basic root WFF selectors.
    ;; Note2: This must be done for each distinct island population in a race for fitness.
    (if (> myRootINIT 0)
        (begin
           (if myVerboseSW (writeln "...starting generation of sequential root selector WFFs in each column island."))
           (if (= myWFFReinitSW false) (setq oldmyWFFs myWFFs))
           (if (= (loadRootIslands roots:) #void)
               (begin                        
                 (loop for mm from 0 until myM do
                    (setq myPopulation myPopulationIslands[mm])
                    (loop for n from 0 until myRootINIT do 
                       (++ myIslands[mm])
                       (if (< myIslands[mm] myMaxRootWFFs)
                           (begin 
                              (setq wff (growTermWFF myIslands[mm] mm))
                              (setq genome (new Vector: myM))
                              (setq genome[mm] wff)
                              (cond
                               ((and (= myUseBGM true) (= myRootRule BGM:)) (ruleBgm.growWFF genome))
                               ((and (= myUseFRM true) (= myRootRule FRM:)) (ruleFrm.growWFF genome))
                               ((and (= myUseMVL true) (= myRootRule MVL:)) (ruleMvl.growWFF genome))
                               ((and (= myUseENN true) (= myRootRule ENN:)) (ruleEnn.growWFF ))
                               ((and (= myUseREG true) (= myRootRule REG:)) (ruleReg.growWFF genome))
                               ((and (= myUseSVM true) (= myRootRule SVM:)) (ruleSvm.growWFF genome))
                               (else (error "esm: root WFF growth specified with invalid regression rule"))
                               ) ; end cond      
                           )) ; end if 
                       ) ; end loop
                    ) ; end grow root WFF loop
                 (saveRootIslands roots:)                     
               )) ; end if
           (if (= myWFFReinitSW false) (setq myWFFs oldmyWFFs)) 
           (bestRegressor)
           (if (<= (bestSelector)[Score:] myS) (goto Last:))       
        )) ; end if

    ;; Generate the initial population of random Selector WFFs in each column island.
    ;; Note1: We grow new random selector WFFs.
    ;; Note2: This must be done for each distinct island population in a race for fitness.
    (if (> myGrowColINIT 0)
        (begin
           (if myVerboseSW (writeln "...starting generation of random selector WFFs in each column island."))
           (loop for mm from 0 until myM do
              (setq myPopulation myPopulationIslands[mm])
              (loop for n from 0 until myGrowColINIT do 
                 (cond
                  ((and (= myUseBGM true) (= myGrowColRule BGM:)) (ruleBgm.growWFF myGrowWFFStyle))
                  ((and (= myUseFRM true) (= myGrowColRule FRM:)) (ruleFrm.growWFF myGrowWFFStyle))
                  ((and (= myUseMVL true) (= myGrowColRule MVL:)) (ruleMvl.growWFF myGrowWFFStyle))
                  ((and (= myUseENN true) (= myGrowColRule ENN:)) (ruleEnn.growWFF ))
                  ((and (= myUseREG true) (= myGrowColRule REG:)) (ruleReg.growWFF myGrowWFFStyle mm))
                  ((and (= myUseSVM true) (= myGrowColRule SVM:)) (ruleSvm.growWFF myGrowWFFStyle))
                  (else (error "esm: random WFF column growth specified with invalid regression rule"))
                  ) ; end cond
                 ) ; end loop
              ) ; end grow root WFF loop
           (bestRegressor)
           (if (<= (bestSelector)[Score:] myS) (goto Last:))       
        )) ; end if

    ;; Generate the initial random root-chromosome selector WFFs in the best-of-breed island.
    ;; Note: This must be done in the best-of-breed island population in a race for fitness.
    (if (> myChromosomeINIT 0)
        (begin
           (if myVerboseSW (writeln "...starting generation of random root-chromosome selector WFFs in the main island."))
           (setq myPopulation myPopulationIslands[myBestOfBreedIsland])
           (loop for n from 0 until myChromosomeINIT do
              (setq genome (new Vector: myM)) 
              (loop for mm from 0 until myM do (setq genome[mm] myPopulationIslands[mm][(integer (random (length myPopulationIslands[mm])))].Genome[0]))
              ;; Use each column island population to feed chromosomes into candiate WFFs for the best-of-breed island.
              (cond
               ((and (= myUseBGM true) (= myChromosomeRule BGM:)) (ruleBgm.growWFF genome))
               ((and (= myUseFRM true) (= myChromosomeRule FRM:)) (ruleFrm.growWFF genome))
               ((and (= myUseMVL true) (= myChromosomeRule MVL:)) (ruleMvl.growWFF genome))
               ((and (= myUseENN true) (= myChromosomeRule ENN:)) (ruleEnn.growWFF ))
               ((and (= myUseREG true) (= myChromosomeRule REG:)) (ruleReg.growWFF genome))
               ((and (= myUseSVM true) (= myChromosomeRule SVM:)) (ruleSvm.growWFF genome))
               (else (error "esm: root-chromosome WFF init specified with invalid regression rule"))
               ) ; end cond
              (bestRegressor)
              (if (<= (bestSelector)[Score:] myS) (goto Last:))
              ) ; end loop
        )) ; end if

    ;; Generate initial greedy search WFFs in the best-of-breed island.
    ;; Note: This must be done in the best-of-breed island population in a race for fitness.
    (if (and (> myGreedyINIT 0) (>= myM 3))
        (begin
           (setq myPopulation myPopulationIslands[myBestOfBreedIsland])
           ;; Order the island populations by fitness.
           (setq bestIslands (new Vector: Number: myM))
           (loop for mm from 0 until myM (setq bestIslands[mm] myPopulationIslands[mm][0].Score))
           (setq bestIslands (|Gv:sort| bestIslands < true))
           (setq greedyCounter 0)
           (setq I1 bestIslands[0])
           (setq K1 (min myGreedyDepth (length myPopulationIslands[I1])))
           (setq I2 bestIslands[1])
           (setq K2 (min myGreedyDepth (length myPopulationIslands[I2])))
           (setq I3 bestIslands[2])
           (setq K3 (min myGreedyDepth (length myPopulationIslands[I3])))
           (cond
            ((= myGreedyWidth base:) (setq genome (copy myColumnGenome)))
            ((= myGreedyWidth full:) (begin (setq genome (new Vector: myM)) (loop for mm from 0 until myM do (setq genome[mm] myPopulationIslands[mm][(integer (random (length myPopulationIslands[mm])))].Genome[0]))))
            ((= myGreedyWidth narrow:)  (setq genome (new Vector: myM)))
            ((= myGreedyWidth random:) (setq genome genome))
            (else (error "esm: invalid myGreedyWidth user specified option"))
            ) ; end greedy width cond
           (loop for k1 from 0 until K1 do
              (loop for k2 from 0 until K2 do
                 (loop for k3 from 0 until K3 do
                   (cond
                    ((or (= myGreedyWidth base:) (= myGreedyWidth full:) (= myGreedyWidth narrow:))
                     (begin
                       (setq genome[I1] myPopulationIslands[I1][k1].Genome[0])  
                       (setq genome[I2] myPopulationIslands[I2][k2].Genome[0])  
                       (setq genome[I3] myPopulationIslands[I3][k3].Genome[0])  
                     )) ; end base full narrow case
                    ((= myGreedyWidth random:) 
                     (begin 
                       (setq genome (new Vector: myM)) 
                       (loop for mm from 0 until myM do 
                         (setq genome[mm] myPopulationIslands[mm][(integer (random (length myPopulationIslands[mm])))].Genome[0])
                         )
                     )) ; end random case
                    (else (error "esm: invalid myGreedyWidth user specified option"))
                    ) ; end greedy width cond
                   (setq greedyRuleCnt (length myWFFs))
                   ;; Use greedy search to combine island population genomes into WFFs in the best-of-breed island.
                   (cond
                     ((and (= myUseBGM true) (= myGreedyRule BGM:)) (ruleBgm.growWFF genome))
                     ((and (= myUseFRM true) (= myGreedyRule FRM:)) (ruleFrm.growWFF genome))
                     ((and (= myUseMVL true) (= myGreedyRule MVL:)) (ruleMvl.growWFF genome))
                     ((and (= myUseENN true) (= myGreedyRule ENN:)) (ruleEnn.growWFF ))
                     ((and (= myUseREG true) (= myGreedyRule REG:)) (ruleReg.growWFF genome))
                     ((and (= myUseSVM true) (= myGreedyRule SVM:)) (ruleSvm.growWFF genome))
                     (else (error "esm: greedy WFF search specified with invalid regression rule"))
                     ) ; end cond
                   (bestRegressor)
                   (if (<= (bestSelector)[Score:] myS) (goto Last:))
                   (if (or (< greedyRuleCnt (length myWFFs)) (= myWFFSaveON false)) (++ greedyCounter))
                   (if (>= greedyCounter myGreedyINIT) (goto LastInitGreedySearch:))        
                   ) ; end I3 island loop  
                 ) ; end I2 island loop  
              ) ; end I1 island loop  

           LastInitGreedySearch::
        )) ; end if

    ;; Generate the initial population of random WFFs in the best-of-breed island.
    ;; Note1: We grow new random selector WFFs.
    ;; Note2: This must be done in the best-of-breed island population in a race for fitness.
    (if (> myGrowSelINIT 0)
        (begin
           (if myVerboseSW (writeln "...starting generation of random selector WFFs."))
           (setq myPopulation myPopulationIslands[myBestOfBreedIsland])
           (loop for n from 0 until myGrowSelINIT do
              (cond
               ((and (= myUseBGM true) (= myGrowSelRule BGM:)) (ruleBgm.growWFF myGrowWFFStyle))
               ((and (= myUseFRM true) (= myGrowSelRule FRM:)) (ruleFrm.growWFF myGrowWFFStyle))
               ((and (= myUseMVL true) (= myGrowSelRule MVL:)) (ruleMvl.growWFF myGrowWFFStyle))
               ((and (= myUseENN true) (= myGrowSelRule ENN:)) (ruleEnn.growWFF ))
               ((and (= myUseREG true) (= myGrowSelRule REG:)) (ruleReg.growWFF myGrowWFFStyle))
               ((and (= myUseSVM true) (= myGrowSelRule SVM:)) (ruleSvm.growWFF myGrowWFFStyle))
               (else (error "esm: random WFF best-of-breed growth specified with invalid regression rule"))
               ) ; end cond
              ) ; end grow random WFF loop
           (bestRegressor)
           (if (<= (bestSelector)[Score:] myS) (goto Last:))       
        )) ; end if


    ;; Initialize a tournament-of-champions (if appropriate).
    ;; Note: This must be done once for every time myBestSelectorChampions grows to myTournamentSize.
    (if (>= (setq NN (length myBestSelectorChampions)) myTournamentSize)
        (begin
           (if myVerboseSW (writeln "...starting tournament of champions."))
           (setq myPopulation myPopulationIslands[myBestOfBreedIsland])
           (++ myTournamentCount)
           (loop for nn from 0 until NN do
             (setq wff myBestSelectorChampions[nn 1].WFF)
             (setq prevGenome myBestSelectorChampions[nn 1].pGenome)
             (createSelector wff prevGenome)
             ) ; end myBestSelectorChampions if
           (setq NN (length myBestRegressorChampions))

           (loop for nn from 0 until NN do
             (setq wff myBestRegressorChampions[nn 1].WFF)
             (setq prevGenome myBestRegressorChampions[nn 1].pGenome)
             (createSelector wff prevGenome)
             ) ; end myBestRegressorChampions if
           (bestRegressor)
           (if (<= (bestSelector)[Score:] myS) (goto Last:))

           ;; Keep only the best of the champions before starting the next tournament.
           (resize myBestSelectorChampions 1)
           (resize myBestRegressorChampions 1)

           ;; Compact memory before starting the next tournament.
           (gc compact:)
        )) ; end if

    ;; Has checkpointing been requested?
    (if (or (= myCheckPoint restart:) (= myCheckPoint reuse:) (= myCheckPoint checkpoint:)) (saveCheckPoint initial:)) 

    ;; ************************************************************************
    ;; Run the main evolutionary process for the maximum number of generations.
    ;; ************************************************************************
    StartMainGenerations:: 	
    (if (<= myBest.Score myS) (goto Last:))
    (loop for myGc from myGc until myG do

       ;; Re-initialize the pseudo random number generator Lambda.
       (setq srandom.seed (* 1000000.0 (fraction (log (+ (+ myGc (pi)) srandom.seed)))))
       (bestRegressor)
       (bestSelector)
        
       ;; Display current progress (if we are in verbose mode).
       (if (and myVerboseSW (= (modi myGc VerboseSample) 0)) (writeln "Starting generation [" myGc " of " myG "],WFFs=[" (length myWFFs) "],Score=[" myBest.Score "],ErrorPct=[" myBest.ErrorPct "," myBestRegressor.ErrorPct "],length(BestWFF)=[" (length myBest.WFF) "],BestWFF= " (left (evalRule myBest.WFF) 100) "..."))
     
       ;; ------------------------------------------------------------------
       ;; Start a new evolutionary process if too many generations have passed without improvement.
       ;; ------------------------------------------------------------------
       (if (>= generationGap myRestartGap) (begin (++ myGc) (goto StartNewEvolutionaryProcess:)))
       (if (> oldBestScore myBest.Score) (setq generationGap 0) (++ generationGap))
       (setq oldBestScore myBest.Score) 
        
       ;; ------------------------------------------------------------------
       ;; Quit if the best available selector has reached the halting score.
       ;; ------------------------------------------------------------------
       (if (<= myBest.Score myS) (goto Last:))

       ;; ------------------------------------------------------------------
       ;; Perform checkpointing (if requested).
       ;; ------------------------------------------------------------------
       (if (or (= myCheckPoint restart:) (= myCheckPoint reuse:) (= myCheckPoint checkpoint:)) (saveCheckPoint checkpoint:)) 

       ;; --------------------------------------------------------------------------
       ;; Run through the current selector WFF population.
       ;; Note: Attempt to perform the population operators for each Selector WFF in
       ;;       the survivor population.
       ;; Note: The percent control, for each population operator, determines the
       ;;       frequency with which the operation is performed.  
       ;; --------------------------------------------------------------------------

       ;; Grow create new selector WFFs from generation of basic root WFFs.
       ;; Note: This must be done in each column island.
       (loop for mm from 0 until myM do
                 
          ;; Grow new selectors from sequential basic root WFF terms.
          ;; Note: This must be done for each distinct island population.
          (if (<= myIslands[mm] myMaxRootWFFs)
              (begin
                 (setq myPopulation myPopulationIslands[mm])
                 (loop for n from 0 until myRootGEN do
                    (++ myIslands[mm]) 
                    (setq wff (growTermWFF myIslands[mm] mm))
                    (setq genome (new Vector: myM))
                    (setq genome[mm] wff)
                    (cond
                     ((and (= myUseBGM true) (= myRootRule BGM:)) (ruleBgm.growWFF genome))
                     ((and (= myUseFRM true) (= myRootRule FRM:)) (ruleFrm.growWFF genome))
                     ((and (= myUseMVL true) (= myRootRule MVL:)) (ruleMvl.growWFF genome))
                     ((and (= myUseENN true) (= myRootRule ENN:)) (ruleEnn.growWFF ))
                     ((and (= myUseREG true) (= myRootRule REG:)) (ruleReg.growWFF genome))
                     ((and (= myUseSVM true) (= myRootRule SVM:)) (ruleSvm.growWFF genome))
                     (else (error "esm: root WFF growth specified with invalid regression rule"))
                     ) ; end cond
                    ) ; end grow Selectors       
              )) ; end if

          ;; Grow new selectors from random WFF terms.
          ;; Note: This must be done for each distinct island population.
          (if (> myGrowColGEN 0)
              (begin
                 (setq myPopulation myPopulationIslands[mm])
                 (loop for n from 0 until myGrowColGEN do
                    (cond
                     ((and (= myUseBGM true) (= myGrowColRule BGM:)) (ruleBgm.growWFF myGrowWFFStyle))
                     ((and (= myUseFRM true) (= myGrowColRule FRM:)) (ruleFrm.growWFF myGrowWFFStyle))
                     ((and (= myUseMVL true) (= myGrowColRule MVL:)) (ruleMvl.growWFF myGrowWFFStyle))
                     ((and (= myUseENN true) (= myGrowColRule ENN:)) (ruleEnn.growWFF ))
                     ((and (= myUseREG true) (= myGrowColRule REG:)) (ruleReg.growWFF myGrowWFFStyle))
                     ((and (= myUseSVM true) (= myGrowColRule SVM:)) (ruleSvm.growWFF myGrowWFFStyle))
                     (else (error "esm: random WFF column growth specified with invalid regression rule"))
                     ) ; end cond
                    ) ; end grow Selectors       
              )) ; end if

          ;; Apply mutation population operator to create new column island selectors.
          (if (<= (random 1.0) myMutateColPct) 
              (begin
                (setq myPopulation myPopulationIslands[mm])
                (setq N (length myPopulation))
                (setq NN (integer (* myMutateColPct (number N))))
                (makeMutation myPopulation[0])
                (makeMutation myPopulation[1])
                (loop for nn from 0 until NN do (makeMutation myPopulation[(integer (random N))]))
             )) ; end mutation
  
          ;; Apply children population operator to create new column island selectors.
          (if (<= (random 1.0) myCrossColPct) 
              (begin
                (setq myPopulation myPopulationIslands[mm])
                (setq N (length myPopulation))
                (setq NN (integer (* myCrossColPct (number N))))
                (makeCrossover myPopulation[0] myPopulation[1])
                (makeCrossover myPopulation[1] myPopulation[0])
                (loop for nn from 0 until NN do (makeCrossover myPopulation[(setq n (integer (random (- N 1))))] myPopulation[(+ n 1 (integer (random (- N n 1))))]))
              )) ; end children		

          ) ; end main island loop      

       ;; Generate new random root-chromosome selector WFFs in the best-of-breed island.
       ;; Note: This must be done in the best-of-breed island population in a race for fitness.
       (if (> myChromosomeGEN 0)
           (begin
              (setq myPopulation myPopulationIslands[myBestOfBreedIsland])
              (loop for n from 0 until myChromosomeGEN do
                 (setq genome (new Vector: myM)) 
                 (loop for mm from 0 until myM do (setq genome[mm] myPopulationIslands[mm][(integer (random (length myPopulationIslands[mm])))].Genome[0]))
                 ;; Use each column island population to feed chromosomes into candiate WFFs for the best-of-breed island.
                 (cond
                  ((and (= myUseBGM true) (= myChromosomeRule BGM:)) (ruleBgm.growWFF genome))
                  ((and (= myUseFRM true) (= myChromosomeRule FRM:)) (ruleFrm.growWFF genome))
                  ((and (= myUseMVL true) (= myChromosomeRule MVL:)) (ruleMvl.growWFF genome))
                  ((and (= myUseENN true) (= myChromosomeRule ENN:)) (ruleEnn.growWFF ))
                  ((and (= myUseREG true) (= myChromosomeRule REG:)) (ruleReg.growWFF genome))
                  ((and (= myUseSVM true) (= myChromosomeRule SVM:)) (ruleSvm.growWFF genome))
                  (else (error "esm: root-chromosome WFF init specified with invalid regression rule"))
                  ) ; end cond
                 (bestRegressor)
                 (if (<= (bestSelector)[Score:] myS) (goto Last:))
                 ) ; end loop
           )) ; end if
   
       ;; Generate new random WFFs in the best-of-breed island.
       ;; Note: This must be done in the best-of-breed island population in a race for fitness.
       (if (> myGrowSelGEN 0)
           (begin
              (setq myPopulation myPopulationIslands[myBestOfBreedIsland])
              (loop for n from 0 until myGrowSelGEN do 
                 (cond
                  ((and (= myUseBGM true) (= myGrowSelRule BGM:)) (ruleBgm.growWFF myGrowWFFStyle))
                  ((and (= myUseFRM true) (= myGrowSelRule FRM:)) (ruleFrm.growWFF myGrowWFFStyle))
                  ((and (= myUseMVL true) (= myGrowSelRule MVL:)) (ruleMvl.growWFF myGrowWFFStyle))
                  ((and (= myUseENN true) (= myGrowSelRule ENN:)) (ruleEnn.growWFF ))
                  ((and (= myUseREG true) (= myGrowSelRule REG:)) (ruleReg.growWFF myGrowWFFStyle))
                  ((and (= myUseSVM true) (= myGrowSelRule SVM:)) (ruleSvm.growWFF myGrowWFFStyle))
                  (else (error "esm: random WFF best-of-breed growth specified with invalid regression rule"))
                  ) ; end cond
                 ) ; end grow root WFF loop
              (bestRegressor)
              (if (<= (bestSelector)[Score:] myS) (goto Last:))       
           )) ; end if

       ;; Generate new greedy search WFFs in the best-of-breed island.
       ;; Note: This must be done in the best-of-breed island population in a race for fitness.
       (if (and (> myGreedyGEN 0) (>= myM 3))
           (begin
              (setq myPopulation myPopulationIslands[myBestOfBreedIsland])
              ;; Order the island populations by fitness.
              (setq bestIslands (new Vector: Number: myM))
              (loop for mm from 0 until myM (setq bestIslands[mm] myPopulationIslands[mm][0].Score))
              (setq bestIslands (|Gv:sort| bestIslands < true))
              (setq greedyCounter 0)
              (setq I1 bestIslands[0])
              (setq K1 (min myGreedyDepth (length myPopulationIslands[I1])))
              (setq I2 bestIslands[1])
              (setq K2 (min myGreedyDepth (length myPopulationIslands[I2])))
              (setq I3 bestIslands[2])
              (setq K3 (min myGreedyDepth (length myPopulationIslands[I3])))
              (cond
               ((= myGreedyWidth base:) (setq genome (copy myColumnGenome)))
               ((= myGreedyWidth full:) (begin (setq genome (new Vector: myM)) (loop for mm from 0 until myM do (setq genome[mm] myPopulationIslands[mm][(integer (random (length myPopulationIslands[mm])))].Genome[0]))))
               ((= myGreedyWidth narrow:)  (setq genome (new Vector: myM)))
               ((= myGreedyWidth random:) (setq genome genome))
               (else (error "esm: invalid myGreedyWidth user specified option"))
               ) ; end greedy width cond
              (loop for k1 from 0 until K1 do
                 (loop for k2 from 0 until K2 do
                    (loop for k3 from 0 until K3 do
		               (cond
		                ((or (= myGreedyWidth base:) (= myGreedyWidth full:) (= myGreedyWidth narrow:))
                         (begin
	                       (setq genome[I1] myPopulationIslands[I1][k1].Genome[0])  
	                       (setq genome[I2] myPopulationIslands[I2][k2].Genome[0])  
	                       (setq genome[I3] myPopulationIslands[I3][k3].Genome[0])  
                         )) ; end base full narrow case
		                ((= myGreedyWidth random:) 
                         (begin 
                           (setq genome (new Vector: myM)) 
                           (loop for mm from 0 until myM do 
                              (setq genome[mm] myPopulationIslands[mm][(integer (random (length myPopulationIslands[mm])))].Genome[0])
                              )
                         )) ; end random case
		                (else (error "esm: invalid myGreedyWidth user specified option"))
		                ) ; end greedy width cond
	                   (setq greedyRuleCnt (length myWFFs))
                       ;; Use greedy search to combine island population genomes into WFFs in the best-of-breed island.
                       (cond
                        ((and (= myUseBGM true) (= myGreedyRule BGM:)) (ruleBgm.growWFF genome))
                        ((and (= myUseFRM true) (= myGreedyRule FRM:)) (ruleFrm.growWFF genome))
                        ((and (= myUseMVL true) (= myGreedyRule MVL:)) (ruleMvl.growWFF genome))
                        ((and (= myUseENN true) (= myGreedyRule ENN:)) (ruleEnn.growWFF ))
                        ((and (= myUseREG true) (= myGreedyRule REG:)) (ruleReg.growWFF genome))
                        ((and (= myUseSVM true) (= myGreedyRule SVM:)) (ruleSvm.growWFF genome))
                        (else (error "esm: greedy WFF search specified with invalid regression rule"))
                        ) ; end cond
                       (bestRegressor)
                       (if (<= (bestSelector)[Score:] myS) (goto Last:))
                       (if (or (< greedyRuleCnt (length myWFFs)) (= myWFFSaveON false)) (++ greedyCounter))
                       (if (>= greedyCounter myGreedyGEN) (goto LastGreedySearch:))        
                       ) ; end I3 island loop  
                    ) ; end I2 island loop  
                 ) ; end I1 island loop  

              LastGreedySearch::
           )) ; end if

       ;; Apply mutation population operator to create new best-of-breed selectors.
       (if (<= (random 1.0) myMutateSelPct) 
           (begin
             (setq myPopulation myPopulationIslands[myBestOfBreedIsland])
             (setq N (length myPopulation))
             (setq NN (integer (* myMutateSelPct (number N))))
             (makeMutation myPopulation[0])
             (makeMutation myPopulation[1])
             (loop for nn from 0 until NN do (makeMutation myPopulation[(integer (random N))]))
          )) ; end mutation

       ;; Apply crossover population operator to create new best-of-breed selectors.
       (if (<= (random 1.0) myCrossSelPct) 
           (begin
             (setq myPopulation myPopulationIslands[myBestOfBreedIsland])
             (setq N (length myPopulation))
             (setq NN (integer (* myCrossSelPct (number N))))
             (makeCrossover myPopulation[0] myPopulation[1])
             (makeCrossover myPopulation[1] myPopulation[0])
             (loop for nn from 0 until NN do (makeCrossover myPopulation[(integer (random N))] myPopulation[(integer (random N))]))
           )) ; end children
   
       ;; Apply mutation population operator to create new best-of-breed regressors.
       (if (<= (random 1.0) myMutateRegPct) 
           (begin
             (setq myPopulation myPopulationIslands[myBestRegressorIsland])
             (setq N (length myPopulation))
             (setq NN (integer (* myMutateRegPct (number N))))
             (makeMutation myPopulation[0])
             (makeMutation myPopulation[1])
             (loop for nn from 0 until NN do (makeMutation myPopulation[(integer (random N))]))
          )) ; end mutation

       ;; Apply crossover population operator to create new best-of-breed regressors.
       (if (<= (random 1.0) myCrossRegPct) 
           (begin
             (setq myPopulation myPopulationIslands[myBestRegressorIsland])
             (setq N (length myPopulation))
             (setq NN (integer (* myCrossRegPct (number N))))
             (makeCrossover myPopulation[0] myPopulation[1])
             (makeCrossover myPopulation[1] myPopulation[0])
             (loop for nn from 0 until NN do (makeCrossover myPopulation[(integer (random N))] myPopulation[(integer (random N))]))
           )) ; end children
   
       ;; Apply incremental population migration to create new selectors.
       ;; Note: all inhabitants of the chosen island are mated with all inhabitants of the best-of-breed island.
       (if (<= (random 1.0) myMigratePct) 
           (begin
             (setq myPopulation myPopulationIslands[myBestRegressorIsland])
             (++ migrationIsland)(setq migrationIsland (modi migrationIsland myM))
             (setq migPopulation myPopulationIslands[migrationIsland])
             (setq N (length myPopulation))
             (if (<= N (length migPopulation))
                 (begin
                   (setq NN (integer (* myMigratePct (number N))))
                   (makeCrossover myPopulation[0] migPopulation[1])
                   (makeCrossover myPopulation[1] migPopulation[0])
                   (loop for nn from 0 until NN do (makeCrossover myPopulation[(integer (random N))] migPopulation[(integer (random N))]))
                 )) ; end if
           )) ; end migration if

       RetryMainGenerations::

      ) ; end main generation loop

    ;; ------------------------------------------------------------------
    ;; Return the best selector Lambda available.
    ;; ------------------------------------------------------------------
    Last::

    ;; Locate the best selector and the best regressor from this training run. 
    (if myVerboseSW (writeln "...locating the best selector and the best regressor from this training run."))
    (bestSelector)
    (bestRegressor)
    (setq myBestRegressorChampions[myBestRegressor.ErrorPct] myBestRegressor)
    (setq myBestSelectorChampions[myBestSelector.Score] myBestSelector)
    (setq myBestSelectorBestOfBest[myBestSelector.Score] myBestSelector)
    (setq myBestSelector myBestSelectorBestOfBest[0 1])
    (setq myBestRegressor myBestRegressorChampions[0 1])

    ;; ------------------------------------------------------------------
    ;; Perform checkpointing (if requested).
    ;; ------------------------------------------------------------------
    (if (or (= myCheckPoint restart:) (= myCheckPoint reuse:) (= myCheckPoint checkpoint:)) (saveCheckPoint checkpoint:)) 

    ;; ------------------------------------------------------------------
    ;; Build the average of the best champion selectors from this training run.
    ;; ------------------------------------------------------------------
    (if (or (<= myBestAverage 1) (<= (length myBestSelectorChampions) 1))
        ;; Select the best Lambda from this training run. 
        (setq myBest myBestSelector)
        else
        ;; Select the average of the best Lambdas from this training run. 
        (begin        
          (setq myBest (eval
                       {(lambda(xv) pvars:(Selectors Error ErrorPct ErrScore Score History WFF Genome pGenome) 
                           (defun run(X)
                              regs:(nn NN)
                              vars:(EY ey)
                              (setq NN (length Selectors))
                              (setq EY (Selectors[0].run X))
                              (loop for nn from 1 until NN do
                                 (setq ey (Selectors[nn].run X))
                                 (setq EY (math.vectorAdd EY ey)) 
                                 )  
                              (math.vectorDivide EY NN)
                              EY)
                           regs:(nn NN)
                           vars:(x) 
                           (setq NN (length Selectors))
                           (setq x (Selectors[0] xv))
                           (loop for nn from 1 until NN do
                              (setq x (+ x (Selectors[nn] xv)))
                              )
                           (setq x (/ x NN))  
                           x)}))
          (setq NN (min myBestAverage (length myBestSelectorChampions)))
          (setq myBest.Selectors (new Vector: NN))
          (loop for nn from 0 until NN do   
              (setq Lambda myBestSelectorChampions[nn 1])
              (setq myBest.Selectors[nn] Lambda)
              (setq myBest.Score (+ myBest.Score Lambda.Score))
              (setq myBest.Error (+ myBest.Error Lambda.Error))
              (setq myBest.ErrorPct (+ myBest.ErrorPct Lambda.ErrorPct))
              (setq myBest.ErrScore (+ myBest.ErrorPct Lambda.ErrScore))
              (if (= myBest.WFF #void) 
                  (setq myBest.WFF (append "(ruleAvg " Lambda.WFF))
                  (setq myBest.WFF (append myBest.WFF " " Lambda.WFF))
                  ) ; end if
              ) ; end loop
          (setq myBest.WFF (append myBest.WFF ")"))
          (setq myBest.Score (/ myBest.Score NN))
          (setq myBest.Error (/ myBest.Error NN))
          (setq myBest.ErrorPct (/ myBest.ErrorPct NN))
          (setq myBest.ErrScore (/ myBest.ErrScore NN))
       )); end if 

    ;; Return the best WFF seen so far.
    myBest) ; end esm























;;**EXPORTKEY**:esm:%%Data_Performance_Tests
;#text#
(writeln "********** regressGaCMVL:")(esm.setOptions regressGaCMVL: 00% true)(esm.selfTest all: 1 5 1000 200 .99% checkpoint:)
********** regressGaCMVL:

Starting test case: crossCorrelation
Building test data as: y = -9.165146942478 - (9.165146942478*x0*x0*x0) - (19.5666514757*x0*x1*x1) + (21.87460482304*x0*x1*x2) - (17.48124453288*x1*x2*x3) + (38.81839452492*x2*x3*x4);
...starting new evolutionary process.
...starting generation of constant selector WFF.
...starting regress selector for each column name.
...starting a best-of-breed MVL selector WFF.
...starting a best-of-breed REG selector WFF.
...starting generation of root selector WFFs in each column island.
...starting generation of random selector WFFs.

Final results of training
esm: N = [1000], M = [5], Generations = [200], WFFs = [12210], Score=[0.05533372884228], ScoreHistory=[#(num| 0.05533372884228 )]
esm.myBest, Score=[0.05533372884228], ErrorPct=[0.4643613922236], lengthWFF=[1889], WFF = mvlregress(((avg(x0 , ((x3 * x4) * (x2*x2*x2))) + ((x1*x1) * x0)) - max(((1.0 / x0) / (1.0 / x0)) , avg(tanh(((x3 * (sqrt(abs(x1)) * x0)) * tan(x4))) , x0))),min(max((x3 * x4) , sign((x3 * x4))) , ((x3 * x4) * (tanh(x2)*tanh(x2)*tanh(x2)))),avg(avg(avg((cos(x0) + tan(x3)) , avg((x0*x0*x0) , x3)) , (avg(x0 , ((x3 * x4) * (x2*x2*x2))) + ((x1*x1) * x0))) , avg(avg((cos(x0) + tan(x3)) , avg((x0*x0*x0) , x3)) , x3)),((avg((cos(x0) + tan(x3)) , ((x1*x1) * x0)) - cos(max((avg((cos(x0) + tan(x3)) , ((x1*x1) * x0)) - cos(max((abs(x1) / (1.0 / x0)) , avg(tanh(x2) , x0)))) , avg(tanh(x2) , x0)))) + max((1.0 / tan(x4)) , sign(x2))),(max(avg((cos(x0) + tan(x3)) , ((x3 * x4) % (sqrt(abs(x1)) * x0))) , avg((cos(x0) + tan(x3)) , avg((x0*x0*x0) , x3))) + max(avg((cos(((((x3 * x4) * tanh(x2)) - min(exp(x4) , x0)) + ((avg(x0 , ((x3 * x4) * (x2*x2*x2))) + avg(cos(avg(min(tan(x2) , x0) , (abs(x1) * (abs(x1) * avg((1.0 / x3) , avg(sin(x3) , x0)))))) , (min(exp(x4) , x0)*min(exp(x4) , x0)*min(exp(x4) , x0)))) - max((abs(((sqrt(abs(avg(tan(x4) , x0))) * (x3 % x4)) % tanh(avg(tanh(x2) , x0)))) / (1.0 / x0)) , avg(tanh(x2) , x0))))) + tan(x3)) , ((x3 * x4) % (sqrt(abs(x1)) * x0))) , avg((cos(x0) + tan(x3)) , avg((x0*x0*x0) , x3)))));
esm.myBestRegressor, Score=[0.05621922329569], ErrorPct=[0.4631593038439], lengthWFF=[1598], WFF = mvlregress(min(((tan(max(((x1*x1) * x0) , sign(x2))) - tan((avg(cos(avg(min(tan(x2) , x0) , (abs(x1) * x0))) , (min(exp(x4) , x0)*min(exp(x4) , x0)*min(exp(x4) , x0))) % avg(cos(x4) , (min(exp(x4) , x0)*min(exp(x4) , x0)*min(exp(x4) , x0)))))) - max((abs(((sqrt(abs(avg(tan(x4) , x0))) * (x3 % x4)) % tanh(x2))) / (1.0 / x0)) , avg(tanh(x2) , x0))) , (tanh(x2) - ((((x3 * x4) * (x2*x2*x2)) * x4) * (x2*x2*x2)))),min(max(((x1*x1) * (avg(x0 , ((x3 * x4) * (x2*x2*x2))) + (1.0 / (if (x2 < -4.427016694615) {x3} else {2.414783726122})))) , sign(max((abs(((sqrt(abs(avg(tan(x4) , x0))) * (x3 % x4)) % tanh(avg(tanh(x2) , x0)))) / (1.0 / x0)) , avg(tanh(x2) , x0)))) , ((x3 * x4) * (tanh(x2)*tanh(x2)*tanh(x2)))),avg(avg(((x3 * x4) * (x2*x2*x2)) , (avg(cos(avg(min(tan(x2) , x0) , (abs(x1) * x0))) , (min(exp(x4) , x0)*min(exp(x4) , x0)*min(exp(x4) , x0))) % avg(cos(x4) , (min(exp(x4) , x0)*min(exp(x4) , x0)*min(exp(x4) , x0))))) , avg(avg((cos(x0) + tan(x3)) , avg((x0*x0*x0) , x3)) , x3)),((min(exp(x4) , x0)*min(exp(x4) , x0)*min(exp(x4) , x0)) + max((1.0 / tan(max((1.0 / x0) , sign(x2)))) , sign(x2))),(avg((x0*x0*x0) , x3) + avg((cos(x0) + tan(avg(x0 , ((x3 * x4) * (x2*x2*x2))))) , ((x1*x1) * x0))));
myBestSelectorChampions
myBestSelectorChampions[0],Score=[0.05533372884228,ErrorPct=[0.4643613922236],WFF= mvlregress(((avg(x0 , ((x3 * x4) * (x2*x2*x2))) + ((x1*x1) * x0)) - max(((1.0 / x0) / (1.0 / x0)) , avg(tanh(((x3 * (sqrt(abs(x1)) * x0)) * tan(x4))) , x0))),min(max((x3 * x4) , sign((x3 * x4))) , ((x3 * x4) * (tanh(x2)*tanh(x2)*tanh(x2)))),avg(avg(avg((cos(x0) + tan(x3)) , avg((x0*x0*x0) , x3)) , (avg(x0 , ((x3 * x4) * (x2*x2*x2))) + ((x1*x1) * x0))) , avg(avg((cos(x0) + tan(x3)) , avg((x0*x0*x0) , x3)) , x3)),((avg((cos(x0) + tan(x3)) , ((x1*x1) * x0)) - cos(max((avg((cos(x0) + tan(x3)) , ((x1*x1) * x0)) - cos(max((abs(x1) / (1.0 / x0)) , avg(tanh(x2) , x0)))) , avg(tanh(x2) , x0)))) + max((1.0 / tan(x4)) , sign(x2))),(max(avg((cos(x0) + tan(x3)) , ((x3 * x4) % (sqrt(abs(x1)) * x0))) , avg((cos(x0) + tan(x3)) , avg((x0*x0*x0) , x3))) + max(avg((cos(((((x3 * x4) * tanh(x2)) - min(exp(x4) , x0)) + ((avg(x0 , ((x3 * x4) * (x2*x2*x2))) + avg(cos(avg(min(tan(x2) , x0) , (abs(x1) * (abs(x1) * avg((1.0 / x3) , avg(sin(x3) , x0)))))) , (min(exp(x4) , x0)*min(exp(x4) , x0)*min(exp(x4) , x0)))) - max((abs(((sqrt(abs(avg(tan(x4) , x0))) * (x3 % x4)) % tanh(avg(tanh(x2) , x0)))) / (1.0 / x0)) , avg(tanh(x2) , x0))))) + tan(x3)) , ((x3 * x4) % (sqrt(abs(x1)) * x0))) , avg((cos(x0) + tan(x3)) , avg((x0*x0*x0) , x3)))));
myBestSelectorChampions[1],Score=[0.09915078803321,ErrorPct=[0.5923574083452],WFF= mvlregress(((((((((sin(sin(x2)) - x0) - avg(log(abs(x3)) , x1)) - (x4 - (x0*x0*x0))) + avg((x0*x0*x0) , avg(log(abs(x4)) , (sqrt(abs(x0)) * x0)))) - x0) - x4) - ((((((((sin(sin(x2)) - x0) - avg(log(abs(x3)) , x1)) - (x4 - (x0*x0*x0))) + avg((x0*x0*x0) , avg(log(abs(x4)) , (sqrt(abs(x0)) * x0)))) - x0) - x4) - ((sin(sin(x2)) - x0) - (x0*x0*x0))) - (x0*x0*x0))) - max((cos(x2) + (avg(cos(x2) , x0) + cos(x4))) , (x3 % (tanh(x2) * avg(sqrt(abs(exp(x4))) , x0))))),avg((x3 % x4) , avg((x0*x0*x0) , x1)),avg(min(avg(exp(x2) , (-x0)) , (expt(abs(x3) ,tanh(x2)) * (x3 % x4))) , avg(((-abs(sign(x3))) + sign(avg(avg(log(abs(sign(x3))) , avg(((-abs(sign(x3))) + sign(x3)) , (sqrt(abs(x2)) * (x3 % x4)))) , (x3 % x4)))) , (sqrt(abs(x2)) * (x3 % x4)))),avg(((((sqrt(abs(cos(((sqrt(abs(x4)) % x0) - ((x3 % x4) / expt(abs(x3) ,tanh(x2))))))) % x0) - ((x3 % x4) / expt(abs(x3) ,tanh(x2)))) + (sqrt(abs(x4)) % x0)) + avg(sign(x2) , cos(log(abs(x1))))) , avg(avg(abs((-x0)) , x0) , avg(((-abs(sign(x3))) + sign(x3)) , (sqrt(abs(x2)) * (x3 % x4))))),((((x1*x1) * x0) + avg((x0*x0*x0) , x4)) + ((abs(x1) * min(avg((-x3) , x0) , x0)) + avg((abs(avg(ninteger(x0) , x0)) * x0) , sqrt(abs((x3 % x4)))))));
myBestSelectorChampions[2],Score=[0.1035856917746,ErrorPct=[0.608000830611],WFF= mvlregress((((((((sin(sin(x2)) - x0) - avg(log(abs(x3)) , x1)) - (x4 - (x0*x0*x0))) + avg((x0*x0*x0) , avg(log(abs(x4)) , (sqrt(abs(x0)) * x0)))) - x0) - x4) - ((sin(sin(x2)) - x0) - (x0*x0*x0))),min(avg(exp(x2) , (log(abs(avg((abs(x1) * log(abs(x4))) , ((log(abs((((log(abs(x2)) + x0) / avg(tan(x4) , ((log(abs(x2)) + x0) / avg(tan(x4) , sqrt(abs(x1)))))) * x0))) * avg((abs(x1) * (-x0)) , (log(abs(x1)) * x0))) * abs(x1))))) % (-x0))) , (expt(abs(x3) ,tanh(x2)) * (exp(x2) % x4))),avg(min(avg(exp(x2) , (-x0)) , (expt(abs(x3) ,tanh(x2)) * (x3 % x4))) , avg(((-abs(sign(x3))) + sign(x3)) , (sqrt(abs(x2)) * (x3 % x4)))),avg(avg(log(abs(sign(x3))) , avg(((-abs(sign(x3))) + sign(x3)) , (sqrt(abs(x2)) * (x3 % x4)))) , (x3 % x4)),(avg((abs(x1) * (-x0)) , (log(abs(x1)) * avg(tanh(x2) , (log(abs(((x2*x2) * x0))) * x0)))) + min(avg(exp(min(avg(exp(x2) , (log(abs(x1)) % (-x0))) , (expt(abs(x3) ,tanh(x2)) * (x3 % x4)))) , (log(abs(x1)) % (-x0))) , min(avg(exp(x2) , (log(abs(x1)) % (-x0))) , (expt(abs(x3) ,tanh(x2)) * (log(abs(x1)) % x4))))));
myBestRegressorChampions
myBestRegressorChampions[0],ErrorPct=[0.4631593038439,Score=[0.05621922329569],WFF= mvlregress(min(((tan(max(((x1*x1) * x0) , sign(x2))) - tan((avg(cos(avg(min(tan(x2) , x0) , (abs(x1) * x0))) , (min(exp(x4) , x0)*min(exp(x4) , x0)*min(exp(x4) , x0))) % avg(cos(x4) , (min(exp(x4) , x0)*min(exp(x4) , x0)*min(exp(x4) , x0)))))) - max((abs(((sqrt(abs(avg(tan(x4) , x0))) * (x3 % x4)) % tanh(x2))) / (1.0 / x0)) , avg(tanh(x2) , x0))) , (tanh(x2) - ((((x3 * x4) * (x2*x2*x2)) * x4) * (x2*x2*x2)))),min(max(((x1*x1) * (avg(x0 , ((x3 * x4) * (x2*x2*x2))) + (1.0 / (if (x2 < -4.427016694615) {x3} else {2.414783726122})))) , sign(max((abs(((sqrt(abs(avg(tan(x4) , x0))) * (x3 % x4)) % tanh(avg(tanh(x2) , x0)))) / (1.0 / x0)) , avg(tanh(x2) , x0)))) , ((x3 * x4) * (tanh(x2)*tanh(x2)*tanh(x2)))),avg(avg(((x3 * x4) * (x2*x2*x2)) , (avg(cos(avg(min(tan(x2) , x0) , (abs(x1) * x0))) , (min(exp(x4) , x0)*min(exp(x4) , x0)*min(exp(x4) , x0))) % avg(cos(x4) , (min(exp(x4) , x0)*min(exp(x4) , x0)*min(exp(x4) , x0))))) , avg(avg((cos(x0) + tan(x3)) , avg((x0*x0*x0) , x3)) , x3)),((min(exp(x4) , x0)*min(exp(x4) , x0)*min(exp(x4) , x0)) + max((1.0 / tan(max((1.0 / x0) , sign(x2)))) , sign(x2))),(avg((x0*x0*x0) , x3) + avg((cos(x0) + tan(avg(x0 , ((x3 * x4) * (x2*x2*x2))))) , ((x1*x1) * x0))));
myBestRegressorChampions[1],ErrorPct=[0.5889202578483,Score=[0.1002346386468],WFF= mvlregress(min(avg(ninteger(x3) , ((cos(x4) + ((x1*x1) * x0)) + (cos(x4) + ((x3 % x4) * x0)))) , sign(x3)),avg(avg(avg((x3 % x4) , (sqrt(abs((sqrt(abs(x4)) % x0))) % ((abs(x2) - x0) + ((cos(x4) - max(tan(x1) , x0)) + (cos(x4) - ((x1*x1) * x0)))))) , cos((cos(x4) + (cos(x4) + (x3 % x4))))) , avg(min(avg(exp(x2) , (-x0)) , (expt(abs(x3) ,tanh(x2)) * (x3 % x4))) , avg(((-abs(sign(x3))) + sign(x3)) , (sqrt(abs(x2)) * (x3 % x4))))),avg((avg((x3 % x4) , cos((cos(x4) + x0))) + (avg((x0 * (x1*x1)) , (x0*x0*x0)) + max(tan(x1) , ((x3 % x4) / expt(abs(x2) ,tanh(x2)))))) , (avg(log(abs(sign(x3))) , avg(log(abs(sign(abs(x1)))) , avg(((-abs(sign(x3))) + sign(x3)) , (sqrt(abs(x2)) * (x3 % x4))))) + min(avg(exp(min(avg(exp(x2) , (log(abs(x1)) % (-x0))) , (expt(abs(x3) ,tanh(x2)) * (x3 % x4)))) , (log(abs(x1)) % (-x0))) , min(avg(exp(x2) , (log(abs(x1)) % (-x0))) , (expt(abs(x3) ,tanh(x2)) * (log(abs(x1)) % x4)))))),((((sqrt(abs(cos(((sqrt(abs(x4)) % x0) - ((x3 % x4) / expt(abs(x3) ,tanh(x2))))))) % x0) - ((x3 % x4) / expt(abs(x3) ,tanh(x2)))) + (sqrt(abs(x4)) % x0)) + avg(((-abs(sign(x3))) + sign(x3)) , (sqrt(abs(x2)) * (x3 % x4)))),(((cos(x4) + ((x1*x1) * x0)) + (cos(x4) + ((x3 % x4) * x0))) + cos(log(abs(sign(x3))))));
myBestRegressorChampions[2],ErrorPct=[0.5981922083995,Score=[0.1043133228454],WFF= mvlregress(((((((sin(sin(x2)) - x0) - avg(log(abs(x3)) , x1)) - (x4 - (x0*x0*x0))) + avg((x0*x0*x0) , avg(log(abs(x4)) , (sqrt(abs(x0)) * x0)))) - x0) - sign(x3)),(min((avg(exp(x2) , (log(abs(x1)) % (-x0))) * (((-abs(sign(x3))) + sign(x3)) * (sign((avg(sqrt(abs(x2)) , x0)*avg(sqrt(abs(x2)) , x0)*avg(sqrt(abs(x2)) , x0))) * ninteger(x4)))) , avg(avg(log(abs(x4)) , (sqrt(abs(avg(log(abs((expt(abs(x3) ,tanh(x2)) * (x3 % x4)))) , (expt(abs(x3) ,tanh(x2)) * (x3 % x4))))) * x0)) , ((log(abs((((log(abs(x2)) + x0) / avg(tan(x4) , ((log(abs(x2)) + x0) / avg(tan(x4) , sqrt(abs(x1)))))) * x0))) * avg((abs(x1) * (-x0)) , (log(abs(x1)) * x0))) * abs(x1)))) * (avg(exp(x2) , (log(abs(x1)) % (-x0))) * (((-abs(sign(x3))) + sign(x3)) * (sign((avg(sqrt(abs(x2)) , x0)*avg(sqrt(abs(x2)) , x0)*avg(sqrt(abs(x2)) , x0))) * ninteger(x4))))),min(min(min(avg(exp(x2) , (-x0)) , (expt(abs(x3) ,tanh(x2)) * (x3 % x4))) , (expt(abs(x3) ,tanh(x2)) * (x3 % x4))) , min(avg(exp(x2) , (-x0)) , (expt(abs(x3) ,tanh(x2)) * (x3 % x4)))),avg(log(abs(sign(x3))) , avg(log(abs(sign(abs(x1)))) , avg(((-abs(sign(x3))) + sign(x3)) , (sqrt(abs(x2)) * (x3 % x4))))),avg(avg(((-abs(sign(x3))) + sign(x3)) , (sqrt(abs(x2)) * (x3 % x4))) , ((log(abs((((log(abs(x2)) + x0) / avg(tan(x4) , ((log(abs(x2)) + x0) / avg(tan(x4) , sqrt(abs(x1)))))) * x0))) * avg((abs(x1) * (-x0)) , (log(abs(x1)) * x0))) * abs(x1))));


Show final results on training data, Score=[0.05533372884228], ErrorPct=[0.4643613922236]
X[0], ey=[-3382635.6709], y=[-5935562.722379], ErrorPct=[2.362280303226]
X[50], ey=[-2830168.995894], y=[-2484984.660629], ErrorPct=[0.3194067592745]
X[100], ey=[-1577560.739446], y=[-1852662.911699], ErrorPct=[0.2545581717712]
X[150], ey=[-1426809.387599], y=[-1326205.529618], ErrorPct=[0.09309099216087]
X[200], ey=[1282178.940141], y=[-1009931.065108], ErrorPct=[2.120940476967]
X[250], ey=[-714181.6139456], y=[-815532.4079131], ErrorPct=[0.09378214867795]
X[300], ey=[-380825.4184214], y=[-578673.9615271], ErrorPct=[0.1830736668053]
X[350], ey=[-424651.3791178], y=[-405470.1919013], ErrorPct=[0.01774878006318]
X[400], ey=[-301501.988447], y=[-298898.8645721], ErrorPct=[0.002408728542834]
X[450], ey=[-658984.8682764], y=[-157954.6202984], ErrorPct=[0.4636144559764]
X[500], ey=[-40156.4816461], y=[-26493.53248762], ErrorPct=[0.01264263139143]
X[550], ey=[-328955.0378036], y=[69636.25232623], ErrorPct=[0.3688254050055]
X[600], ey=[360442.3868347], y=[168319.7432961], ErrorPct=[0.1777753643106]
X[650], ey=[-161728.2790871], y=[341312.903806], ErrorPct=[0.4654752188753]
X[700], ey=[-260680.3777003], y=[538465.6578771], ErrorPct=[0.73946763898]
X[750], ey=[432895.9884724], y=[755287.672819], ErrorPct=[0.2983162113522]
X[800], ey=[-533545.4017671], y=[1010018.825617], ErrorPct=[1.428294384284]
X[850], ey=[4718411.425731], y=[1289552.967002], ErrorPct=[3.172799158093]
X[900], ey=[1424045.114315], y=[1754908.058265], ErrorPct=[0.3061548566798]
X[950], ey=[1643323.077474], y=[2688668.736147], ErrorPct=[0.9672816378029]
Actual computed error on training data is ErrorPct=[0.4643613922236] versus reported ErrorPct=[0.4643613922236] while average Y is AvgY=[-6300.443084283]
yHistory=[#(num| -2706693.290524 -1353128.763653 -800545.8677153 -419874.3313429 -153414.2291583 66198.1378281 349608.6246592 771241.1160152 1299900.015197 2883704.157851 )]
eHistory=[#(num| -2342951.127492 -1280272.924027 -708879.6455921 -525525.3823186 -152030.4137783 70411.04709836 407603.6801115 791142.3638679 1210047.205188 2467450.7661 )]
aHistory=[#(num| -2342951.127492 -1811612.02576 -1444034.565704 -1214407.269858 -1001931.898642 989331.0124731 1219061.003817 1489546.778385 1838748.985644 2467450.7661 )]
dHistory=[#(num| 1991262.911115 2433468.273674 2933581.344089 3650361.011404 4810401.893593 )]


Final testing on test data returns Score=[0.05248749688186], ErrorPct=[0.4355619130171]
X[0], ey=[-3483998.650135], y=[-6167537.518464], ErrorPct=[2.458011084835]
X[50], ey=[-1784758.140341], y=[-2369636.680035], ErrorPct=[0.5357246547895]
X[100], ey=[-1006370.15647], y=[-1656766.439932], ErrorPct=[0.5957362098055]
X[150], ey=[-1616781.641149], y=[-1251081.570521], ErrorPct=[0.3349662037452]
X[200], ey=[-1635075.140946], y=[-994608.5457237], ErrorPct=[0.5866410243211]
X[250], ey=[-603745.8252964], y=[-728073.9769428], ErrorPct=[0.113879466592]
X[300], ey=[-354458.4745381], y=[-508204.5166234], ErrorPct=[0.140825042691]
X[350], ey=[-313482.1088138], y=[-317719.4222018], ErrorPct=[0.00388120455435]
X[400], ey=[94725.30600557], y=[-191729.8181024], ErrorPct=[0.2623810963482]
X[450], ey=[13803.6438376], y=[-86268.04319267], ErrorPct=[0.09166154397896]
X[500], ey=[67319.96426118], y=[32254.80509062], ErrorPct=[0.03211824168078]
X[550], ey=[-266112.6801799], y=[156101.6645394], ErrorPct=[0.3867309513362]
X[600], ey=[7169.041233451], y=[243820.5789268], ErrorPct=[0.2167630622976]
X[650], ey=[1496285.923643], y=[412331.5780664], ErrorPct=[0.9928575390981]
X[700], ey=[59340.53616753], y=[612046.185868], ErrorPct=[0.5062556125602]
X[750], ey=[1300982.171339], y=[827643.0566656], ErrorPct=[0.4335591350979]
X[800], ey=[1014341.589765], y=[1112614.916968], ErrorPct=[0.09001432043998]
X[850], ey=[2258128.951052], y=[1605433.069666], ErrorPct=[0.5978425467976]
X[900], ey=[835600.7146885], y=[2012387.880649], ErrorPct=[1.077888579354]
X[950], ey=[2549227.537538], y=[2695446.213125], ErrorPct=[0.1339302849848]
Actual computed error on testing data is ErrorPct=[0.4355619130171], Avg Y=[87576.3006039], AvgDev Y=[1091752.14257]
yHistory=[#(num| -2574542.033674 -1283274.478391 -743114.1781336 -326037.5711094 -85358.47325735 140547.7454164 411206.7548932 830897.7010798 1578724.998891 2926712.540324 )]
eHistory=[#(num| -2208339.357249 -1165651.451187 -793139.3795838 -291926.8108488 -106526.3931938 64367.41520984 462244.2268267 806657.0865819 1565954.049927 2542123.619556 )]
aHistory=[#(num| -2208339.357249 -1686995.404218 -1389043.396007 -1114764.249717 -913116.6784124 1088269.27962 1344244.745723 1638244.918688 2054038.834741 2542123.619556 )]
dHistory=[#(num| 2001385.958033 2459008.99544 3027288.314695 3741034.238959 4750462.976805 )]

esm.selfTest[crossCorrelation]: completed in [52.1164] minutes.

Starting test case: cubicRegression
Building test data as: y = 36.28472509997 + (36.28472509997*x0*x0*x0) + (30.58638330984*x1*x1*x1) + (13.02686266162*x2*x2*x2) - (29.03445701301*x3*x3*x3) + (35.00133469577*x4*x4*x4);
...starting new evolutionary process.
...starting generation of constant selector WFF.
...starting regress selector for each column name.
...starting a best-of-breed MVL selector WFF.
...starting a best-of-breed REG selector WFF.
...starting generation of root selector WFFs in each column island.
...starting generation of random selector WFFs.

Final results of training
esm: N = [1000], M = [5], Generations = [23], WFFs = [17679], Score=[0.003384085730641], ScoreHistory=[#(num| 0.003384085730641 )]
esm.myBest, Score=[0.003384085730641], ErrorPct=[0.1205479528489], lengthWFF=[1512], WFF = mvlregress(((avg(x2 , (x0*x0*x0)) - log(abs(x3))) - avg(avg(avg((avg(avg(cos(x3) , (x0*x0*x0)) , ninteger(x0)) - (avg(avg(cos(x3) , (x0*x0*x0)) , ninteger(x0)) - x3)) , (x0*x0*x0)) , cos(x3)) , (x0*x0*x0))),avg((x0 - (((-x1) - log(abs(min(x1 , x4))))*((-x1) - log(abs(min(x1 , x4))))*((-x1) - log(abs(min(x1 , x4)))))) , (((-x1) - (((-x1) - log(abs(min(x1 , x4))))*((-x1) - log(abs(min(x1 , x4))))*((-x1) - log(abs(min(x1 , x4)))))) - ((((x4*x4) * x4) - avg(cos(x3) , (x0*x0*x0))) - (x4*x4)))),avg((sign((if (x3 > log(abs(x1))) {sign(x1)} else {(2.177323394589 / -1.026959215617)})) - avg(avg(avg((avg(avg(cos(x3) , (x0*x0*x0)) , ninteger(x0)) - (avg(avg(cos(x3) , (x0*x0*x0)) , ninteger(x0)) - x3)) , (x0*x0*x0)) , cos(x3)) , avg((avg(avg(cos(x3) , (x0*x0*x0)) , ninteger(x0)) - (avg(avg(cos(x3) , (x0*x0*x0)) , ninteger(x0)) - x3)) , (((-x1) - log(abs(min(x1 , x4))))*((-x1) - log(abs(min(x1 , x4))))*((-x1) - log(abs(min(x1 , x4)))))))) , ((((x4*x4) * x4) - log(abs(x0))) - (sign((((x4*x4) * x4) - log(abs(x0)))) - (sign(x2) - (((-x1) - log(abs(min(x1 , x4))))*((-x1) - log(abs(min(x1 , x4))))*((-x1) - log(abs(min(x1 , x4))))))))),avg(ninteger(x3) , avg((avg(avg(cos(x3) , (x0*x0*x0)) , ninteger(x0)) - avg(cos(x3) , (x0*x0*x0))) , (ninteger(x3)*ninteger(x3)*ninteger(x3)))),avg((x0*x0*x0) , (x0*x0*x0)));
esm.myBestRegressor, Score=[0.003384085730641], ErrorPct=[0.1205479528489], lengthWFF=[1512], WFF = mvlregress(((avg(x2 , (x0*x0*x0)) - log(abs(x3))) - avg(avg(avg((avg(avg(cos(x3) , (x0*x0*x0)) , ninteger(x0)) - (avg(avg(cos(x3) , (x0*x0*x0)) , ninteger(x0)) - x3)) , (x0*x0*x0)) , cos(x3)) , (x0*x0*x0))),avg((x0 - (((-x1) - log(abs(min(x1 , x4))))*((-x1) - log(abs(min(x1 , x4))))*((-x1) - log(abs(min(x1 , x4)))))) , (((-x1) - (((-x1) - log(abs(min(x1 , x4))))*((-x1) - log(abs(min(x1 , x4))))*((-x1) - log(abs(min(x1 , x4)))))) - ((((x4*x4) * x4) - avg(cos(x3) , (x0*x0*x0))) - (x4*x4)))),avg((sign((if (x3 > log(abs(x1))) {sign(x1)} else {(2.177323394589 / -1.026959215617)})) - avg(avg(avg((avg(avg(cos(x3) , (x0*x0*x0)) , ninteger(x0)) - (avg(avg(cos(x3) , (x0*x0*x0)) , ninteger(x0)) - x3)) , (x0*x0*x0)) , cos(x3)) , avg((avg(avg(cos(x3) , (x0*x0*x0)) , ninteger(x0)) - (avg(avg(cos(x3) , (x0*x0*x0)) , ninteger(x0)) - x3)) , (((-x1) - log(abs(min(x1 , x4))))*((-x1) - log(abs(min(x1 , x4))))*((-x1) - log(abs(min(x1 , x4)))))))) , ((((x4*x4) * x4) - log(abs(x0))) - (sign((((x4*x4) * x4) - log(abs(x0)))) - (sign(x2) - (((-x1) - log(abs(min(x1 , x4))))*((-x1) - log(abs(min(x1 , x4))))*((-x1) - log(abs(min(x1 , x4))))))))),avg(ninteger(x3) , avg((avg(avg(cos(x3) , (x0*x0*x0)) , ninteger(x0)) - avg(cos(x3) , (x0*x0*x0))) , (ninteger(x3)*ninteger(x3)*ninteger(x3)))),avg((x0*x0*x0) , (x0*x0*x0)));
myBestSelectorChampions
myBestSelectorChampions[0],Score=[0.003384085730641,ErrorPct=[0.1205479528489],WFF= mvlregress(((avg(x2 , (x0*x0*x0)) - log(abs(x3))) - avg(avg(avg((avg(avg(cos(x3) , (x0*x0*x0)) , ninteger(x0)) - (avg(avg(cos(x3) , (x0*x0*x0)) , ninteger(x0)) - x3)) , (x0*x0*x0)) , cos(x3)) , (x0*x0*x0))),avg((x0 - (((-x1) - log(abs(min(x1 , x4))))*((-x1) - log(abs(min(x1 , x4))))*((-x1) - log(abs(min(x1 , x4)))))) , (((-x1) - (((-x1) - log(abs(min(x1 , x4))))*((-x1) - log(abs(min(x1 , x4))))*((-x1) - log(abs(min(x1 , x4)))))) - ((((x4*x4) * x4) - avg(cos(x3) , (x0*x0*x0))) - (x4*x4)))),avg((sign((if (x3 > log(abs(x1))) {sign(x1)} else {(2.177323394589 / -1.026959215617)})) - avg(avg(avg((avg(avg(cos(x3) , (x0*x0*x0)) , ninteger(x0)) - (avg(avg(cos(x3) , (x0*x0*x0)) , ninteger(x0)) - x3)) , (x0*x0*x0)) , cos(x3)) , avg((avg(avg(cos(x3) , (x0*x0*x0)) , ninteger(x0)) - (avg(avg(cos(x3) , (x0*x0*x0)) , ninteger(x0)) - x3)) , (((-x1) - log(abs(min(x1 , x4))))*((-x1) - log(abs(min(x1 , x4))))*((-x1) - log(abs(min(x1 , x4)))))))) , ((((x4*x4) * x4) - log(abs(x0))) - (sign((((x4*x4) * x4) - log(abs(x0)))) - (sign(x2) - (((-x1) - log(abs(min(x1 , x4))))*((-x1) - log(abs(min(x1 , x4))))*((-x1) - log(abs(min(x1 , x4))))))))),avg(ninteger(x3) , avg((avg(avg(cos(x3) , (x0*x0*x0)) , ninteger(x0)) - avg(cos(x3) , (x0*x0*x0))) , (ninteger(x3)*ninteger(x3)*ninteger(x3)))),avg((x0*x0*x0) , (x0*x0*x0)));
myBestRegressorChampions
myBestRegressorChampions[0],ErrorPct=[0.1205479528489,Score=[0.003384085730641],WFF= mvlregress(((avg(x2 , (x0*x0*x0)) - log(abs(x3))) - avg(avg(avg((avg(avg(cos(x3) , (x0*x0*x0)) , ninteger(x0)) - (avg(avg(cos(x3) , (x0*x0*x0)) , ninteger(x0)) - x3)) , (x0*x0*x0)) , cos(x3)) , (x0*x0*x0))),avg((x0 - (((-x1) - log(abs(min(x1 , x4))))*((-x1) - log(abs(min(x1 , x4))))*((-x1) - log(abs(min(x1 , x4)))))) , (((-x1) - (((-x1) - log(abs(min(x1 , x4))))*((-x1) - log(abs(min(x1 , x4))))*((-x1) - log(abs(min(x1 , x4)))))) - ((((x4*x4) * x4) - avg(cos(x3) , (x0*x0*x0))) - (x4*x4)))),avg((sign((if (x3 > log(abs(x1))) {sign(x1)} else {(2.177323394589 / -1.026959215617)})) - avg(avg(avg((avg(avg(cos(x3) , (x0*x0*x0)) , ninteger(x0)) - (avg(avg(cos(x3) , (x0*x0*x0)) , ninteger(x0)) - x3)) , (x0*x0*x0)) , cos(x3)) , avg((avg(avg(cos(x3) , (x0*x0*x0)) , ninteger(x0)) - (avg(avg(cos(x3) , (x0*x0*x0)) , ninteger(x0)) - x3)) , (((-x1) - log(abs(min(x1 , x4))))*((-x1) - log(abs(min(x1 , x4))))*((-x1) - log(abs(min(x1 , x4)))))))) , ((((x4*x4) * x4) - log(abs(x0))) - (sign((((x4*x4) * x4) - log(abs(x0)))) - (sign(x2) - (((-x1) - log(abs(min(x1 , x4))))*((-x1) - log(abs(min(x1 , x4))))*((-x1) - log(abs(min(x1 , x4))))))))),avg(ninteger(x3) , avg((avg(avg(cos(x3) , (x0*x0*x0)) , ninteger(x0)) - avg(cos(x3) , (x0*x0*x0))) , (ninteger(x3)*ninteger(x3)*ninteger(x3)))),avg((x0*x0*x0) , (x0*x0*x0)));

Show final results on training data, Score=[0.003384085730641], ErrorPct=[0.1205479528489]
X[0], ey=[-10034406.28038], y=[-10336372.24698], ErrorPct=[0.1187757189754]
X[50], ey=[-4643771.540665], y=[-5291851.755636], ErrorPct=[0.2549167853392]
X[100], ey=[-3584315.477793], y=[-4053975.485556], ErrorPct=[0.1847367295214]
X[150], ey=[-2856253.333659], y=[-3217999.295743], ErrorPct=[0.1422896666702]
X[200], ey=[-2741037.175518], y=[-2774680.249854], ErrorPct=[0.01323321428521]
X[250], ey=[-1846113.839898], y=[-2126238.647328], ErrorPct=[0.1101846866392]
X[300], ey=[-1319376.608108], y=[-1704503.865393], ErrorPct=[0.1514864982846]
X[350], ey=[-826324.0546273], y=[-1297323.19644], ErrorPct=[0.1852634663965]
X[400], ey=[-813422.1309573], y=[-932012.7720576], ErrorPct=[0.04664660994465]
X[450], ey=[-214608.6904349], y=[-539882.3185784], ErrorPct=[0.1279435874241]
X[500], ey=[-329569.4432804], y=[-187187.9915761], ErrorPct=[0.05600452092495]
X[550], ey=[-147722.1674656], y=[268123.7792543], ErrorPct=[0.1635694308904]
X[600], ey=[524587.6184828], y=[678461.7118233], ErrorPct=[0.06052505278702]
X[650], ey=[1049399.566684], y=[1034060.051954], ErrorPct=[0.006033666347512]
X[700], ey=[1790595.085558], y=[1457719.085826], ErrorPct=[0.1309339149818]
X[750], ey=[1804381.254279], y=[2097829.023497], ErrorPct=[0.1154251592103]
X[800], ey=[2987028.019897], y=[2636462.10489], ErrorPct=[0.1378920911926]
X[850], ey=[3499558.867754], y=[3213719.685207], ErrorPct=[0.1124323870034]
X[900], ey=[4737217.16862], y=[4090456.62416], ErrorPct=[0.25439770428]
X[950], ey=[4978755.723704], y=[5273320.98899], ErrorPct=[0.1158647166886]
Actual computed error on training data is ErrorPct=[0.1205479528489] versus reported ErrorPct=[0.1205479528489] while average Y is AvgY=[-41226.53641205]
yHistory=[#(num| -5591606.469873 -3290743.105378 -2182167.387681 -1310061.766492 -528585.3436906 260479.9470997 1062473.405484 2071858.272645 3284852.058829 5811235.024938 )]
eHistory=[#(num| -5551941.606523 -3282332.224447 -2168105.308468 -1280536.968519 -527637.3077225 246599.9436245 1066588.182528 2031797.614655 3280062.584767 5773239.725985 )]
aHistory=[#(num| -5551941.606523 -4417136.915485 -3667459.713146 -3070729.026989 -2562110.683136 2479657.610312 3037922.026984 3695033.308469 4526651.155376 5773239.725985 )]
dHistory=[#(num| 5041768.293448 6108651.053973 7362493.021615 8943788.070861 11325181.33251 )]


Final testing on test data returns Score=[0.003120106808263], ErrorPct=[0.1220443796586]
X[0], ey=[-12992474.87194], y=[-13591373.61233], ErrorPct=[0.2356790049318]
X[50], ey=[-5096453.880636], y=[-5201982.070384], ErrorPct=[0.04152751888537]
X[100], ey=[-4157836.56029], y=[-4362161.850714], ErrorPct=[0.080406215412]
X[150], ey=[-2957875.061485], y=[-3337017.901728], ErrorPct=[0.149200526382]
X[200], ey=[-2907524.970803], y=[-2702822.496509], ErrorPct=[0.08055464504298]
X[250], ey=[-2357153.31887], y=[-2175055.243522], ErrorPct=[0.07165934790605]
X[300], ey=[-1360239.975866], y=[-1678382.846895], ErrorPct=[0.125195780545]
X[350], ey=[-1306997.131932], y=[-1324515.79438], ErrorPct=[0.006893954945986]
X[400], ey=[-374784.3797526], y=[-891187.6684915], ErrorPct=[0.2032153434727]
X[450], ey=[-702445.2711569], y=[-545942.7239869], ErrorPct=[0.06158697973282]
X[500], ey=[125017.5522714], y=[-212977.3428097], ErrorPct=[0.1330079614011]
X[550], ey=[1010158.609624], y=[247651.437926], ErrorPct=[0.3000622966122]
X[600], ey=[707458.4320832], y=[574215.0538655], ErrorPct=[0.05243401709565]
X[650], ey=[856063.9074967], y=[1046246.287099], ErrorPct=[0.07484068834608]
X[700], ey=[1660677.476685], y=[1441857.687453], ErrorPct=[0.08611009960062]
X[750], ey=[1953548.368894], y=[1999990.322897], ErrorPct=[0.01827586663386]
X[800], ey=[2524447.023967], y=[2600359.365018], ErrorPct=[0.02987307168016]
X[850], ey=[3217856.305047], y=[3225154.755816], ErrorPct=[0.002872090887576]
X[900], ey=[3944329.746519], y=[4037924.848225], ErrorPct=[0.036831602655]
X[950], ey=[5105223.453434], y=[5383847.029551], ErrorPct=[0.1096441230238]
Actual computed error on testing data is ErrorPct=[0.1220443796586], Avg Y=[-101336.8575817], AvgDev Y=[2541162.886197]
yHistory=[#(num| -5727711.598454 -3427227.20547 -2182056.164352 -1304151.162644 -560931.9081185 213329.2195535 1018783.188004 2003847.330264 3279303.504236 5673446.221164 )]
eHistory=[#(num| -5691072.288638 -3405904.807177 -2165124.091715 -1290671.527218 -560814.9823427 190937.5455479 1004730.149505 2020781.675421 3242804.552412 5640965.198387 )]
aHistory=[#(num| -5691072.288638 -4548488.547907 -3754033.729177 -3138193.178687 -2622717.539418 2420043.824255 2977320.393931 3634850.475407 4441884.875399 5640965.198387 )]
dHistory=[#(num| 5042761.363673 6115513.572618 7388884.204583 8990373.423306 11332037.48702 )]

esm.selfTest[cubicRegression]: completed in [6.663283333333] minutes.

Starting test case: hyperTangent
Building test data as: y = 36.28472509997 + (36.28472509997*tanh(x0*x0*x0)) + (30.58638330984*tanh(x1*x1*x1)) + (13.02686266162*tanh(x2*x2*x2)) - (29.03445701301*tanh(x3*x3*x3)) + (35.00133469577*tanh(x4*x4*x4));
...starting new evolutionary process.
...starting generation of constant selector WFF.
...starting regress selector for each column name.
...starting a best-of-breed MVL selector WFF.
...starting a best-of-breed REG selector WFF.
...starting generation of root selector WFFs in each column island.
...starting generation of random selector WFFs.

Final results of training
esm: N = [1000], M = [5], Generations = [88], WFFs = [8838], Score=[0.004392293915191], ScoreHistory=[#(num| 0.004392293915191 )]
esm.myBest, Score=[0.004392293915191], ErrorPct=[0.2531995755105], lengthWFF=[554], WFF = mvlregress((sign(x1) - expt(abs(sign(x0)) ,expt(abs(sign(x0)) ,((-(ninteger(abs(x1)) % ninteger(abs(x1)))) - (-log(abs(x3))))))),ninteger(ninteger((sign(sign(x0)) - log(abs(sign(x1)))))),avg(sign(x1) , (sign(x1) - expt(abs(sign(x0)) ,expt(abs(sign(x0)) ,(-x0))))),avg(ninteger(avg(avg(ninteger(x3) , log(abs(x4))) , log(abs(x4)))) , log(abs(x4))),((sign(x4) + sign(x4)) + (sign((1.0 / x2)) - sign(x4))));
esm.myBestRegressor, Score=[0.004392293915191], ErrorPct=[0.2531995755105], lengthWFF=[554], WFF = mvlregress((sign(x1) - expt(abs(sign(x0)) ,expt(abs(sign(x0)) ,((-(ninteger(abs(x1)) % ninteger(abs(x1)))) - (-log(abs(x3))))))),ninteger(ninteger((sign(sign(x0)) - log(abs(sign(x1)))))),avg(sign(x1) , (sign(x1) - expt(abs(sign(x0)) ,expt(abs(sign(x0)) ,(-x0))))),avg(ninteger(avg(avg(ninteger(x3) , log(abs(x4))) , log(abs(x4)))) , log(abs(x4))),((sign(x4) + sign(x4)) + (sign((1.0 / x2)) - sign(x4))));
myBestSelectorChampions
myBestSelectorChampions[0],Score=[0.004392293915191,ErrorPct=[0.2531995755105],WFF= mvlregress((sign(x1) - expt(abs(sign(x0)) ,expt(abs(sign(x0)) ,((-(ninteger(abs(x1)) % ninteger(abs(x1)))) - (-log(abs(x3))))))),ninteger(ninteger((sign(sign(x0)) - log(abs(sign(x1)))))),avg(sign(x1) , (sign(x1) - expt(abs(sign(x0)) ,expt(abs(sign(x0)) ,(-x0))))),avg(ninteger(avg(avg(ninteger(x3) , log(abs(x4))) , log(abs(x4)))) , log(abs(x4))),((sign(x4) + sign(x4)) + (sign((1.0 / x2)) - sign(x4))));
myBestSelectorChampions[1],Score=[0.02628769673209,ErrorPct=[0.3778710381898],WFF= mvlregress((tanh(x1) - min(sign(x3) , sqrt(abs(avg(x2 , x4))))),max(expt(abs(sign((sqrt(abs((1.0 / (((min(x1 , -2.770833660886) % tanh(1.722896426655)) % x3) % x3)))) % x3))) ,max(max(((min(x1 , -2.770833660886) % tanh(1.722896426655)) % sign(x3)) , (1.0 / sign(x3))) , expt(abs(expt(abs(sign(sign(max(x1 , (1.0 / x0))))) ,x0)) ,max(sign(x0) , x0)))) , (if (x1 > x3) {expt(abs(x2) ,-0.3341016504478)} else {(x2 + x1)})),(expt(abs(expt(abs(sign(sign(x1))) ,((min(x1 , -2.770833660886) % tanh(1.722896426655)) % x3))) ,((1.0 / x2) * sign(max(avg(x0 , expt(abs(max((1.0 / x4) , x2)) ,sign(sqrt(abs(x2))))) , (1.0 / x0))))) / expt(abs(sign(min((1.0 / x0) , tanh(x1)))) ,sign(x3))),avg((avg(x2 , x4) - min(sign(x3) , sqrt(abs(x1)))) , avg(x0 , avg(x0 , expt(abs(max((1.0 / x4) , x2)) ,sign(sqrt(abs(x2))))))),(max(min((1.0 / x0) , sign(x0)) , max(avg(x0 , expt(abs(max((1.0 / x4) , x2)) ,sign(sqrt(abs(x2))))) , (1.0 / x0))) + (((avg(x2 , x4) - (avg(x2 , x4) - min(sign(x3) , sqrt(abs(x1))))) + min(sign(x3) , sqrt(abs(x1)))) / expt(abs(expt(abs(sign(sign(x1))) ,x0)) ,max(sign(x0) , sin(x0))))));
myBestSelectorChampions[2],Score=[0.02876794976006,ErrorPct=[0.3869903243043],WFF= mvlregress((x2 - (x2 - x3)),(avg((1.0 / x4) , sign(x2)) / avg((1.0 / x4) , sign(x2))),avg(x0 , x2),avg(sign(x1) , tanh(x2)),avg(ninteger(x0) , ninteger(x4)));
myBestSelectorChampions[3],Score=[0.09027618457912,ErrorPct=[19.50191809013],WFF= mvlregress(x4,x3,x2,x1,x0);
myBestRegressorChampions
myBestRegressorChampions[0],ErrorPct=[0.2531995755105,Score=[0.004392293915191],WFF= mvlregress((sign(x1) - expt(abs(sign(x0)) ,expt(abs(sign(x0)) ,((-(ninteger(abs(x1)) % ninteger(abs(x1)))) - (-log(abs(x3))))))),ninteger(ninteger((sign(sign(x0)) - log(abs(sign(x1)))))),avg(sign(x1) , (sign(x1) - expt(abs(sign(x0)) ,expt(abs(sign(x0)) ,(-x0))))),avg(ninteger(avg(avg(ninteger(x3) , log(abs(x4))) , log(abs(x4)))) , log(abs(x4))),((sign(x4) + sign(x4)) + (sign((1.0 / x2)) - sign(x4))));
myBestRegressorChampions[1],ErrorPct=[0.3759007613966,Score=[0.02718938705583],WFF= mvlregress((tanh(x1) - min(sign(x3) , sqrt(abs(avg(x2 , sqrt(abs(x2))))))),max(expt(abs(sign((sqrt(abs((1.0 / (((min(x1 , -2.770833660886) % tanh(1.722896426655)) % x3) % x3)))) % x3))) ,max(max(((min(x1 , -2.770833660886) % tanh(1.722896426655)) % sign(x3)) , (1.0 / sign(x3))) , expt(abs(expt(abs(sign(sign(max(x1 , (1.0 / x0))))) ,x0)) ,max(sign(x0) , x0)))) , (if (x1 > max(expt(abs(sign((sqrt(abs((1.0 / (((min(x1 , -2.770833660886) % tanh(1.722896426655)) % x3) % x3)))) % x3))) ,max(max(((min(x1 , -2.770833660886) % tanh(1.722896426655)) % sign(x3)) , (1.0 / sign(x3))) , expt(abs(expt(abs(sign(sign(max(x1 , (1.0 / x0))))) ,x0)) ,max(sign(x0) , x0)))) , ninteger(x2))) {expt(abs(x2) ,-0.3341016504478)} else {(x2 + x1)})),(expt(abs(expt(abs(sign(sign(x1))) ,((min(x1 , -2.770833660886) % tanh(1.722896426655)) % x3))) ,((1.0 / x2) * sign(max(avg(x0 , expt(abs(max((1.0 / x4) , x2)) ,sign(sqrt(abs(x2))))) , (1.0 / x0))))) / expt(abs(sign(min((1.0 / x0) , tanh(x1)))) ,sign(x3))),avg((avg(x2 , x4) - (tanh(x1) - min(sign(x3) , sqrt(abs(avg(x2 , x4)))))) , avg(x0 , avg(x0 , expt(abs(max((1.0 / x4) , x2)) ,sign(sqrt(abs(x2))))))),(max(min((1.0 / x0) , sign(x0)) , max(avg(x0 , max(min((1.0 / x0) , sign(x0)) , max(avg(x0 , expt(abs(max((1.0 / x4) , x2)) ,sign(sqrt(abs(x2))))) , (1.0 / x0)))) , (1.0 / x0))) + (((avg(x2 , x4) - (avg(x2 , x4) - min(sign(x3) , sqrt(abs(x1))))) + min(sign(x3) , sqrt(abs(x1)))) / expt(abs(expt(abs(sign(sign(x1))) ,x0)) ,max(sign(x0) , sin(x0))))));
myBestRegressorChampions[2],ErrorPct=[0.3869903243043,Score=[0.02876794976006],WFF= mvlregress((x2 - (x2 - x3)),(avg((1.0 / x4) , sign(x2)) / avg((1.0 / x4) , sign(x2))),avg(x0 , x2),avg(sign(x1) , tanh(x2)),avg(ninteger(x0) , ninteger(x4)));
myBestRegressorChampions[3],ErrorPct=[0.7174460099747,Score=[0.1412396510097],WFF= mvlregress(((1.0 / x3) * x3),(log(abs(x1)) % exp(x0)),min(log(abs(x2)) , x3),((1.0 / x4) / sin(x3)),avg(x1 , sqrt(abs(x2))));


Show final results on training data, Score=[0.004392293915191], ErrorPct=[0.2531995755105]
X[0], ey=[32.150229164], y=[31.28472509997], ErrorPct=[0.4653895698963]
X[50], ey=[34.1927351712], y=[33.28472509997], ErrorPct=[0.4882454445603]
X[100], ey=[33.14479475966], y=[33.28472509997], ErrorPct=[0.07524184299235]
X[150], ey=[34.02308819412], y=[33.28472509997], ErrorPct=[0.3970246901107]
X[200], ey=[35.66096718462], y=[35.28472509997], ErrorPct=[0.2023088616533]
X[250], ey=[34.84628397535], y=[35.28472509997], ErrorPct=[0.2357538628562]
X[300], ey=[34.51820247877], y=[35.28472509997], ErrorPct=[0.4121663292202]
X[350], ey=[35.42902603958], y=[35.28472509997], ErrorPct=[0.07759195480353]
X[400], ey=[36.85018129427], y=[35.28472509997], ErrorPct=[0.8417603281538]
X[450], ey=[35.74585617349], y=[35.28472509997], ErrorPct=[0.2479544590118]
X[500], ey=[35.77049712727], y=[35.28515520507], ErrorPct=[0.2609728571037]
X[550], ey=[37.3351668161], y=[37.28472509997], ErrorPct=[0.02712297902488]
X[600], ey=[37.04814412143], y=[37.28472509997], ErrorPct=[0.1272117883984]
X[650], ey=[37.83694577757], y=[37.28472509997], ErrorPct=[0.2969341847299]
X[700], ey=[36.46896745046], y=[37.28472509997], ErrorPct=[0.4386404610039]
X[750], ey=[37.15528077382], y=[37.28472509997], ErrorPct=[0.06960341583512]
X[800], ey=[36.22160516846], y=[37.28472509997], ErrorPct=[0.5716494563563]
X[850], ey=[38.50545040196], y=[39.28472509997], ErrorPct=[0.4190232393023]
X[900], ey=[39.10763730889], y=[39.28472509997], ErrorPct=[0.09522174921108]
X[950], ey=[39.22465634224], y=[39.28472509997], ErrorPct=[0.03229952866399]
Actual computed error on training data is ErrorPct=[0.2531995755105] versus reported ErrorPct=[0.2531995755105] while average Y is AvgY=[36.25300913762]
yHistory=[#(num| 32.57846787433 33.58092149169 35.28472509997 35.28472509997 35.28472522495 37.08574213055 37.28472509997 37.28472509997 38.93033877549 39.93099547927 )]
eHistory=[#(num| 32.5894116839 33.61720133067 35.28370172878 35.27925653884 35.42363046582 37.00655015138 37.24428205527 37.27618108801 38.90530974151 39.90456659201 )]
aHistory=[#(num| 32.5894116839 33.10330650728 33.83010491445 34.19239282055 34.4386403496 38.06737792563 38.3325848692 38.69535247384 39.40493816676 39.90456659201 )]
dHistory=[#(num| 3.628737576034 4.140192048651 4.86524755939 6.301631659471 7.315154908107 )]


Final testing on test data returns Score=[0.005014187386324], ErrorPct=[0.2528045009258]
X[0], ey=[31.13934625996], y=[31.28472509997], ErrorPct=[0.07732945595574]
X[50], ey=[33.7492677499], y=[33.28472509997], ErrorPct=[0.2470980672442]
X[100], ey=[33.21532042551], y=[33.28472509997], ErrorPct=[0.03691751644302]
X[150], ey=[33.67348590344], y=[33.28472509997], ErrorPct=[0.206788425504]
X[200], ey=[35.49732665068], y=[35.28472507118], ErrorPct=[0.1130863644008]
X[250], ey=[36.14018972702], y=[35.28472509997], ErrorPct=[0.4550360574521]
X[300], ey=[35.90164062011], y=[35.28472509997], ErrorPct=[0.3281477657723]
X[350], ey=[34.97218404082], y=[35.28472509997], ErrorPct=[0.1662458585105]
X[400], ey=[35.88450419602], y=[35.28472509997], ErrorPct=[0.3190326128949]
X[450], ey=[35.11232367487], y=[35.28472509997], ErrorPct=[0.091703224538]
X[500], ey=[36.194938755], y=[37.28240257218], ErrorPct=[0.5784403379662]
X[550], ey=[37.2323958224], y=[37.28472509997], ErrorPct=[0.02783482496468]
X[600], ey=[37.25681096248], y=[37.28472509997], ErrorPct=[0.01484800033763]
X[650], ey=[37.14887235488], y=[37.28472509997], ErrorPct=[0.07226236546189]
X[700], ey=[36.48849319072], y=[37.28472509997], ErrorPct=[0.423529176248]
X[750], ey=[37.08566397282], y=[37.28472509997], ErrorPct=[0.1058839695178]
X[800], ey=[38.20938563695], y=[37.29713942386], ErrorPct=[0.4852391403529]
X[850], ey=[39.40790854972], y=[39.28472509997], ErrorPct=[0.06552335367553]
X[900], ey=[38.87735246164], y=[39.28472509997], ErrorPct=[0.2166883742467]
X[950], ey=[38.86395135186], y=[39.28472509997], ErrorPct=[0.2238166504704]
Actual computed error on testing data is ErrorPct=[0.2528045009258], Avg Y=[36.28949770797], AvgDev Y=[1.879993053384]
yHistory=[#(num| 32.59683359193 33.41549488528 35.28472509968 35.28472509997 35.47996913092 37.28470186248 37.28472509997 37.28472807722 39.06886071433 39.91021351794 )]
eHistory=[#(num| 32.62357470724 33.51842137147 35.20094312426 35.29631347925 35.65789348067 37.1095713172 37.25136021663 37.37201995301 39.0091715929 39.85570783709 )]
aHistory=[#(num| 32.62357470724 33.07099803935 33.78097973432 34.15981317055 34.45942923258 38.11956618337 38.37206489991 38.74563312767 39.432439715 39.85570783709 )]
dHistory=[#(num| 3.660136950788 4.212251729354 4.964653393346 6.361441675646 7.232133129857 )]

esm.selfTest[hyperTangent]: completed in [25.04765] minutes.

Starting test case: elipsoid
Building test data as: y = 0.0 + (1.0*x0*x0) + (2.0*x1*x1) + (3.0*x2*x2) + (4.0*x3*x3) + (5.0*x4*x4);
...starting new evolutionary process.
...starting generation of constant selector WFF.
...starting regress selector for each column name.
...starting a best-of-breed MVL selector WFF.
...starting a best-of-breed REG selector WFF.
...starting generation of root selector WFFs in each column island.
...starting generation of random selector WFFs.
Starting generation [0 of 200],WFFs=[5784],Score=[0.05351914888901],ErrorPct=[0.456048087977,0.456048087977],length(BestWFF)=[187],BestWFF= mvlregress((log(abs(x2)) - cos(x1)),((1.0 / x1) - (x3*x3)),(abs(x4) - sign(x4)),(abs(x0) % x1),avg(n...
...locating the best selector and the best regressor from this training run.

Final results of training
esm: N = [1000], M = [5], Generations = [0], WFFs = [6330], Score=[0.00923663453289], ScoreHistory=[#(num| 0.00923663453289 )]
esm.myBest, Score=[0.00923663453289], ErrorPct=[0.200571764572], lengthWFF=[143], WFF = mvlregress(sqrt(abs(x1)),max((x3*x3) , x4),max((x2*x2) , x2),avg(x0 , (x2*x2)),avg((x4*x4) , x3));
esm.myBestRegressor, Score=[0.00923663453289], ErrorPct=[0.200571764572], lengthWFF=[143], WFF = mvlregress(sqrt(abs(x1)),max((x3*x3) , x4),max((x2*x2) , x2),avg(x0 , (x2*x2)),avg((x4*x4) , x3));
myBestSelectorChampions
myBestSelectorChampions[0],Score=[0.00923663453289,ErrorPct=[0.200571764572],WFF= mvlregress(sqrt(abs(x1)),max((x3*x3) , x4),max((x2*x2) , x2),avg(x0 , (x2*x2)),avg((x4*x4) , x3));
myBestRegressorChampions
myBestRegressorChampions[0],ErrorPct=[0.200571764572,Score=[0.00923663453289],WFF= mvlregress(sqrt(abs(x1)),max((x3*x3) , x4),max((x2*x2) , x2),avg(x0 , (x2*x2)),avg((x4*x4) , x3));

Show final results on training data, Score=[0.00923663453289], ErrorPct=[0.200571764572]
X[0], ey=[739.3287087813], y=[720.303170667], ErrorPct=[0.004344775788493]
X[50], ey=[4794.928378399], y=[4228.772747128], ErrorPct=[0.129290391919]
X[100], ey=[7115.875957461], y=[5726.97004252], ErrorPct=[0.3171781400077]
X[150], ey=[8314.004789549], y=[6893.618866379], ErrorPct=[0.3243670866097]
X[200], ey=[9345.354948183], y=[7893.500645021], ErrorPct=[0.3315533777237]
X[250], ey=[9783.752786691], y=[8674.805428101], ErrorPct=[0.2532452751336]
X[300], ey=[10287.04369746], y=[9513.117119698], ErrorPct=[0.1767380999645]
X[350], ey=[8488.77718295], y=[10145.15689431], ErrorPct=[0.3782599169215]
X[400], ey=[12477.4519189], y=[10799.72015949], ErrorPct=[0.3831359872236]
X[450], ey=[10410.65849573], y=[11366.3985778], ErrorPct=[0.2182580247527]
X[500], ey=[12576.29735786], y=[12024.68322272], ErrorPct=[0.1259696164462]
X[550], ey=[14216.25202459], y=[12960.49968744], ErrorPct=[0.2867704617881]
X[600], ey=[13529.13211529], y=[13562.30016843], ErrorPct=[0.007574437756559]
X[650], ey=[13792.30925023], y=[14415.00953845], ErrorPct=[0.1422032385884]
X[700], ey=[13683.01878202], y=[15146.48138184], ErrorPct=[0.3342043117449]
X[750], ey=[15161.90742561], y=[16035.89609102], ErrorPct=[0.1995888247723]
X[800], ey=[17742.51258005], y=[17209.18546788], ErrorPct=[0.1217934920099]
X[850], ey=[18602.31550142], y=[18394.43727728], ErrorPct=[0.04747220655632]
X[900], ey=[19058.37647231], y=[20062.16871478], ErrorPct=[0.2292314785316]
X[950], ey=[21894.08427916], y=[22728.44130924], ErrorPct=[0.1905383280866]
Actual computed error on training data is ErrorPct=[0.200571764572] versus reported ErrorPct=[0.200571764572] while average Y is AvgY=[12570.94648723]
yHistory=[#(num| 3985.914632167 6863.940616272 8675.85651053 10153.85828989 11379.14165899 12863.4896336 14363.64517328 16097.80963247 18382.50086655 22943.30785855 )]
eHistory=[#(num| 4173.981218397 7004.500506921 8876.744152689 9930.64697717 11438.24918083 12909.2419464 14247.29294045 16071.61024916 18241.87292001 22815.32478027 )]
aHistory=[#(num| 4173.981218397 5589.240862659 6685.075292669 7496.468213794 8284.824407201 16857.06856726 17844.02522247 19042.93598315 20528.59885014 22815.32478027 )]
dHistory=[#(num| 8572.244160059 10347.55700868 12357.86069048 14939.35798748 18641.34356188 )]


Final testing on test data returns Score=[0.009296100536756], ErrorPct=[0.2031668706518]
X[0], ey=[739.3287087813], y=[720.303170667], ErrorPct=[0.004449096460738]
X[50], ey=[4456.25560169], y=[4232.435774653], ErrorPct=[0.05233996506869]
X[100], ey=[7405.18761825], y=[5781.517787287], ErrorPct=[0.3796930028972]
X[150], ey=[8903.122590413], y=[6930.039813894], ErrorPct=[0.4614027495583]
X[200], ey=[7532.69904207], y=[7921.103219735], ErrorPct=[0.09082779376893]
X[250], ey=[7472.276032449], y=[8816.933706664], ErrorPct=[0.3144463858693]
X[300], ey=[9035.685101627], y=[9621.289578365], ErrorPct=[0.1369428180796]
X[350], ey=[12060.52888852], y=[10244.82004803], ErrorPct=[0.4246010665997]
X[400], ey=[12356.45721271], y=[10905.04399441], ErrorPct=[0.3394110260531]
X[450], ey=[11538.41723178], y=[11562.68397015], ErrorPct=[0.005674744081854]
X[500], ey=[11587.02980923], y=[12115.10949427], ErrorPct=[0.1234907230267]
X[550], ey=[13456.20395897], y=[13084.18461673], ErrorPct=[0.08699622207489]
X[600], ey=[14181.30401426], y=[13564.65144859], ErrorPct=[0.1442033718562]
X[650], ey=[13708.51562042], y=[14388.04596713], ErrorPct=[0.158907256255]
X[700], ey=[15191.16321063], y=[15145.05580149], ErrorPct=[0.01078215552153]
X[750], ey=[16955.38659933], y=[16258.18403832], ErrorPct=[0.1630398797643]
X[800], ey=[17208.82070993], y=[17186.85358979], ErrorPct=[0.005136981455507]
X[850], ey=[19223.76801288], y=[18304.29700287], ErrorPct=[0.2150170571683]
X[900], ey=[18679.16890065], y=[19404.42899287], ErrorPct=[0.1696010956453]
X[950], ey=[20008.83052253], y=[21732.94679268], ErrorPct=[0.4031822673996]
Actual computed error on testing data is ErrorPct=[0.2031668706518], Avg Y=[12562.04182174], AvgDev Y=[4276.270088133]
yHistory=[#(num| 4022.826310134 6921.581630653 8784.442717046 10251.60699061 11513.5458242 12946.00959171 14349.29891052 16158.85968588 18246.23067432 22426.01588234 )]
eHistory=[#(num| 4165.907102137 7073.099537264 8945.169966809 10163.53696257 11461.78146039 13055.41965441 14234.71189603 16095.16426625 18187.82955592 22237.79781564 )]
aHistory=[#(num| 4165.907102137 5619.503319701 6728.058868737 7586.928392195 8361.899005835 16762.18463765 17688.87588346 18840.26387927 20212.81368578 22237.79781564 )]
dHistory=[#(num| 8400.285631816 10101.94749127 12112.20501053 14593.31036608 18071.8907135 )]

esm.selfTest[elipsoid]: completed in [2.043216666667] minutes.

Starting test case: hiddenModel
Building test data as: y = 36.28472509997 + (13.02686266162*sin(x2));
...starting new evolutionary process.
...starting generation of constant selector WFF.
...starting regress selector for each column name.
...starting a best-of-breed MVL selector WFF.
...starting a best-of-breed REG selector WFF.
...starting generation of root selector WFFs in each column island.
...locating the best selector and the best regressor from this training run.

Final results of training
esm: N = [1000], M = [5], Generations = [0], WFFs = [4784], Score=[1.125342916927E-018], ScoreHistory=[#(num| 1.125342916927E-018 )]
esm.myBest, Score=[1.125342916927E-018], ErrorPct=[3.337448796336E-015], lengthWFF=[50], WFF = regress(sin(x2));
esm.myBestRegressor, Score=[1.125342916927E-018], ErrorPct=[3.337448796336E-015], lengthWFF=[50], WFF = regress(sin(x2));
myBestSelectorChampions
myBestSelectorChampions[0],Score=[1.125342916927E-018,ErrorPct=[3.337448796336E-015],WFF= regress(sin(x2));
myBestRegressorChampions
myBestRegressorChampions[0],ErrorPct=[3.337448796336E-015,Score=[1.125342916927E-018],WFF= regress(sin(x2));

Show final results on training data, Score=[1.125342916927E-018], ErrorPct=[3.337448796336E-015]
X[0], ey=[23.25787246009], y=[23.25787246009], ErrorPct=[6.828539736749E-015]
X[50], ey=[23.43596175858], y=[23.43596175858], ErrorPct=[6.828539736749E-015]
X[100], ey=[23.92624678468], y=[23.92624678468], ErrorPct=[6.828539736749E-015]
X[150], ey=[24.76582273516], y=[24.76582273516], ErrorPct=[6.401756003202E-015]
X[200], ey=[25.79897642671], y=[25.79897642671], ErrorPct=[6.401756003202E-015]
X[250], ey=[27.50535722515], y=[27.50535722515], ErrorPct=[5.974972269655E-015]
X[300], ey=[29.20625423205], y=[29.20625423205], ErrorPct=[5.121404802562E-015]
X[350], ey=[31.28865916152], y=[31.28865916152], ErrorPct=[4.694621069015E-015]
X[400], ey=[32.65963739298], y=[32.65963739298], ErrorPct=[4.267837335468E-015]
X[450], ey=[35.04057828095], y=[35.04057828095], ErrorPct=[3.414269868374E-015]
X[500], ey=[37.00995038227], y=[37.00995038227], ErrorPct=[3.414269868374E-015]
X[550], ey=[39.15404134627], y=[39.15404134626], ErrorPct=[2.560702401281E-015]
X[600], ey=[41.28062528251], y=[41.28062528251], ErrorPct=[1.707134934187E-015]
X[650], ey=[42.87057523749], y=[42.87057523749], ErrorPct=[1.707134934187E-015]
X[700], ey=[44.61966335572], y=[44.61966335572], ErrorPct=[8.535674670936E-016]
X[750], ey=[46.0821647803], y=[46.0821647803], ErrorPct=[8.535674670936E-016]
X[800], ey=[47.14895736407], y=[47.14895736407], ErrorPct=[0.0]
X[850], ey=[47.93963636865], y=[47.93963636865], ErrorPct=[8.535674670936E-016]
X[900], ey=[48.80721214807], y=[48.80721214807], ErrorPct=[0.0]
X[950], ey=[49.18857003833], y=[49.18857003833], ErrorPct=[8.535674670936E-016]
Actual computed error on training data is ErrorPct=[3.337448796336E-015] versus reported ErrorPct=[3.337448796336E-015] while average Y is AvgY=[36.67080716405]
yHistory=[#(num| 23.49129896275 24.78176509402 27.51441170507 31.07813857077 34.87906395304 39.07584441729 42.83669612157 45.97062619005 47.955730499 49.12449612692 )]
eHistory=[#(num| 23.49129896275 24.78176509402 27.51441170507 31.07813857077 34.87906395304 39.07584441729 42.83669612157 45.97062619005 47.955730499 49.12449612692 )]
aHistory=[#(num| 23.49129896275 24.13653202838 25.26249192061 26.71640358315 28.34893565713 44.99267867097 46.47188723439 47.68361760532 48.54011331296 49.12449612692 )]
dHistory=[#(num| 16.64374301384 19.75548365124 22.42112568471 24.40358128458 25.63319716418 )]


Final testing on test data returns Score=[1.145970979012E-018], ErrorPct=[3.440824956516E-015]
X[0], ey=[23.25818137544], y=[23.25818137544], ErrorPct=[7.251025692423E-015]
X[50], ey=[23.38948404263], y=[23.38948404263], ErrorPct=[7.251025692423E-015]
X[100], ey=[23.83698618623], y=[23.83698618623], ErrorPct=[6.824494769339E-015]
X[150], ey=[24.63000934365], y=[24.63000934365], ErrorPct=[6.824494769339E-015]
X[200], ey=[25.87191668516], y=[25.87191668516], ErrorPct=[5.971432923172E-015]
X[250], ey=[27.14146484919], y=[27.14146484919], ErrorPct=[5.971432923172E-015]
X[300], ey=[28.5633043377], y=[28.5633043377], ErrorPct=[5.544902000088E-015]
X[350], ey=[30.4718374612], y=[30.4718374612], ErrorPct=[5.118371077005E-015]
X[400], ey=[32.09561968884], y=[32.09561968884], ErrorPct=[5.118371077005E-015]
X[450], ey=[34.4463050512], y=[34.4463050512], ErrorPct=[4.265309230837E-015]
X[500], ey=[36.36487545068], y=[36.36487545068], ErrorPct=[3.41224738467E-015]
X[550], ey=[38.74920166594], y=[38.74920166594], ErrorPct=[2.559185538502E-015]
X[600], ey=[40.66671973995], y=[40.66671973995], ErrorPct=[2.559185538502E-015]
X[650], ey=[42.25438158664], y=[42.25438158664], ErrorPct=[1.706123692335E-015]
X[700], ey=[43.96615259993], y=[43.96615259993], ErrorPct=[1.706123692335E-015]
X[750], ey=[45.47701982123], y=[45.47701982123], ErrorPct=[8.530618461674E-016]
X[800], ey=[46.78692003324], y=[46.78692003324], ErrorPct=[8.530618461674E-016]
X[850], ey=[47.7039953335], y=[47.7039953335], ErrorPct=[0.0]
X[900], ey=[48.59225946256], y=[48.59225946256], ErrorPct=[0.0]
X[950], ey=[49.10163036696], y=[49.10163036696], ErrorPct=[8.530618461674E-016]
Actual computed error on testing data is ErrorPct=[3.440824956516E-015], Avg Y=[36.30268382014], AvgDev Y=[8.329322650547]
yHistory=[#(num| 23.44006680089 24.66985919281 27.14144868426 30.3704009068 34.24559722836 38.68485223569 42.24539358158 45.47409149293 47.70791240384 49.04721567421 )]
eHistory=[#(num| 23.44006680089 24.66985919281 27.14144868426 30.3704009068 34.24559722836 38.68485223569 42.24539358158 45.47409149293 47.70791240384 49.04721567421 )]
aHistory=[#(num| 23.44006680089 24.05496299685 25.08379155932 26.40544389619 27.97347456263 44.63189307765 46.11865328814 47.40973985699 48.37756403903 49.04721567421 )]
dHistory=[#(num| 16.65841851502 19.71320939195 22.32594829767 24.32260104217 25.60714887332 )]

esm.selfTest[hiddenModel]: completed in [1.249733333333] minutes.

Starting test case: linearRegression
Building test data as: y = 36.28472509997 + (36.28472509997*x0) + (30.58638330984*x1) + (13.02686266162*x2) - (29.03445701301*x3) + (35.00133469577*x4);
...starting new evolutionary process.
...starting generation of constant selector WFF.
...starting regress selector for each column name.
...starting a best-of-breed MVL selector WFF.
...locating the best selector and the best regressor from this training run.

Final results of training
esm: N = [1000], M = [5], Generations = [0], WFFs = [8], Score=[2.805047073921E-005], ScoreHistory=[#(num| 2.805047073921E-005 )]
esm.myBest, Score=[2.805047073921E-005], ErrorPct=[0.02301464075688], lengthWFF=[78], WFF = mvlregress(x4,x3,x2,x1,x0);
esm.myBestRegressor, Score=[2.805047073921E-005], ErrorPct=[0.02301464075688], lengthWFF=[78], WFF = mvlregress(x4,x3,x2,x1,x0);
myBestSelectorChampions
myBestSelectorChampions[0],Score=[2.805047073921E-005,ErrorPct=[0.02301464075688],WFF= mvlregress(x4,x3,x2,x1,x0);
myBestRegressorChampions
myBestRegressorChampions[0],ErrorPct=[0.02301464075688,Score=[2.805047073921E-005],WFF= mvlregress(x4,x3,x2,x1,x0);


Show final results on training data, Score=[2.805047073921E-005], ErrorPct=[0.02301464075688]
X[0], ey=[-5843.163750627], y=[-5808.843994914], ErrorPct=[0.02178099153448]
X[50], ey=[-3060.383583548], y=[-3025.383739751], ErrorPct=[0.02221260861667]
X[100], ey=[-2544.331592509], y=[-2508.973852964], ErrorPct=[0.02243974672117]
X[150], ey=[-2122.265937244], y=[-2086.549905584], ErrorPct=[0.02266713637871]
X[200], ey=[-1739.390229865], y=[-1703.731995936], ErrorPct=[0.02263045511856]
X[250], ey=[-1418.373048652], y=[-1382.407482634], ErrorPct=[0.02282550305842]
X[300], ey=[-1090.853942556], y=[-1055.615701569], ErrorPct=[0.02236390710521]
X[350], ey=[-816.5446456864], y=[-780.8900495508], ErrorPct=[0.0226281463977]
X[400], ey=[-610.3393271481], y=[-573.9496629182], ErrorPct=[0.02309465647645]
X[450], ey=[-337.8171650406], y=[-302.0340395261], ErrorPct=[0.02270971741289]
X[500], ey=[-41.35736908715], y=[-5.178759057364], ErrorPct=[0.0229607111831]
X[550], ey=[183.4619330142], y=[220.2310103636], ErrorPct=[0.02333545055471]
X[600], ey=[410.0478792595], y=[446.020085315], ErrorPct=[0.02282971715001]
X[650], ey=[696.07861249], y=[733.0760047121], ErrorPct=[0.02348035031306]
X[700], ey=[939.2025895943], y=[976.6369766491], ErrorPct=[0.02375768855615]
X[750], ey=[1222.181265982], y=[1258.197601923], ErrorPct=[0.02285772412896]
X[800], ey=[1650.86192273], y=[1687.688121192], ErrorPct=[0.02337170239994]
X[850], ey=[2003.908581394], y=[2041.888413328], ErrorPct=[0.0241038544903]
X[900], ey=[2591.889695326], y=[2628.666017283], ErrorPct=[0.02334004833617]
X[950], ey=[3391.951723443], y=[3429.611172845], ErrorPct=[0.02390052410342]
Actual computed error on training data is ErrorPct=[0.02301464075688] versus reported ErrorPct=[0.02301464075688] while average Y is AvgY=[-0.5984121923563]
yHistory=[#(num| -3283.281808261 -2091.508320913 -1396.777278985 -805.6204340371 -304.1315504475 219.4866579832 716.1484604536 1277.013920423 2085.484803221 3577.20142864 )]
eHistory=[#(num| -3283.281808261 -2091.508320913 -1396.777278985 -805.6204340371 -304.1315504475 219.4904608229 716.1446576139 1277.013920423 2085.484803221 3577.20142864 )]
aHistory=[#(num| -3283.281808261 -2687.395064587 -2257.189136053 -1894.296960549 -1576.263878529 1575.067054144 1913.961202474 2313.233384095 2831.34311593 3577.20142864 )]
dHistory=[#(num| 3151.330932673 3808.258163023 4570.422520148 5518.738180518 6860.483236901 )]


Final testing on test data returns Score=[4.33661031386E-005], ErrorPct=[0.02299119330935]
X[0], ey=[-6518.618720943], y=[-6484.623704412], ErrorPct=[0.02155033987663]
X[50], ey=[-3271.303887069], y=[-3236.437925615], ErrorPct=[0.02210245489317]
X[100], ey=[-2628.175261211], y=[-2593.162567164], ErrorPct=[0.02219547256389]
X[150], ey=[-2123.983228316], y=[-2087.961124077], ErrorPct=[0.02283536437485]
X[200], ey=[-1711.654778528], y=[-1675.368766003], ErrorPct=[0.02300266281603]
X[250], ey=[-1446.293822763], y=[-1409.961854118], ErrorPct=[0.02303179561565]
X[300], ey=[-1118.701300059], y=[-1082.131532354], ErrorPct=[0.0231825427274]
X[350], ey=[-837.6775684122], y=[-801.0312808508], ErrorPct=[0.02323105068783]
X[400], ey=[-576.0428723192], y=[-540.2923618324], ErrorPct=[0.02266319391404]
X[450], ey=[-275.0037064372], y=[-237.6116087693], ErrorPct=[0.02370383943506]
X[500], ey=[-25.06080852636], y=[10.30082433361], ErrorPct=[0.0224166741037]
X[550], ey=[153.2989159602], y=[190.7620580547], ErrorPct=[0.02374887637566]
X[600], ey=[488.6687779334], y=[524.3325539087], ErrorPct=[0.02260821061378]
X[650], ey=[727.335100493], y=[764.1127556697], ErrorPct=[0.02331432809277]
X[700], ey=[971.7974974384], y=[1007.200600128], ErrorPct=[0.0224429629251]
X[750], ey=[1344.327010143], y=[1380.437501899], ErrorPct=[0.02289139555896]
X[800], ey=[1630.753673893], y=[1667.834312768], ErrorPct=[0.02350639747083]
X[850], ey=[2072.065773895], y=[2109.188899938], ErrorPct=[0.02353333121008]
X[900], ey=[2586.913588067], y=[2623.683696846], ErrorPct=[0.02330954423191]
X[950], ey=[3164.033562841], y=[3201.358129815], ErrorPct=[0.02366102994308]
Actual computed error on testing data is ErrorPct=[0.02299119330935], Avg Y=[-8.890305832169], AvgDev Y=[1577.470087507]
yHistory=[#(num| -3366.450229611 -2107.733923626 -1375.136018612 -817.2387459612 -265.1521579873 215.549764995 756.1077597689 1354.709844058 2127.666211191 3388.774437463 )]
eHistory=[#(num| -3366.450229611 -2107.733923626 -1375.136018612 -817.2387459612 -265.1521579873 215.549764995 756.1077597689 1354.709844058 2127.666211191 3388.774437463 )]
aHistory=[#(num| -3366.450229611 -2737.092076618 -2283.10672395 -1916.639729452 -1586.342215159 1568.561603495 1906.81456312 2290.383497571 2758.220324327 3388.774437463 )]
dHistory=[#(num| 3154.903818655 3823.454292573 4573.49022152 5495.312400945 6755.224667073 )]

esm.selfTest[linearRegression]: completed in [0.01303333333338] minutes.

Starting test case: mixedModels
Building test data as:
if ((x0 % 4) == 0) y = + (36.28472509997*log(.000001+abs(x0))) + (30.58638330984*log(.000001+abs(x1))) + (13.02686266162*log(.000001+abs(x2))) - (29.03445701301*log(.000001+abs(x3))) + (35.00133469577*log(.000001+abs(x4)));
if ((x0 % 4) == 1) y = + (36.28472509997*x0*x0) + (30.58638330984*x1*x1) + (13.02686266162*x2*x2) - (29.03445701301*x3*x3) + (35.00133469577*x4*x4);
if ((x0 % 4) == 2) y = + (36.28472509997*sin(x0)) + (30.58638330984*sin(x1)) + (13.02686266162*sin(x2)) - (29.03445701301*sin(x3)) + (35.00133469577*sin(x4));
if ((x0 % 4) == 3) y = + (36.28472509997*x0) + (30.58638330984*x1) + (13.02686266162*x2) - (29.03445701301*x3) + (35.00133469577*x4);
...starting new evolutionary process.
...starting generation of constant selector WFF.
...starting regress selector for each column name.
...starting a best-of-breed MVL selector WFF.
...starting a best-of-breed REG selector WFF.
...starting generation of root selector WFFs in each column island.
...starting generation of random selector WFFs.

Final results of training
esm: N = [1000], M = [5], Generations = [17], WFFs = [13759], Score=[0.009135690004296], ScoreHistory=[#(num| 0.009135690004296 )]
esm.myBest, Score=[0.009135690004296], ErrorPct=[0.2305835733017], lengthWFF=[507], WFF = mvlregress((avg(avg(log(abs(x4)) , log(abs(x1))) , sqrt(abs(x0))) % avg(log(abs(x0)) , log(abs(x4)))),((tanh(max(((max(log(abs(x4)) , x0) + max((x1*x1) , x0))*(max(log(abs(x4)) , x0) + max((x1*x1) , x0))) , x2)) / ((1.0 / x3) / x3)) / min(sqrt(abs((tanh(x0) * x0))) , avg(abs(x2) , (x3*x3)))),avg(log(abs(x4)) , log(abs(x1))),avg(log(abs(sqrt(abs(x4)))) , log(abs(x0))),avg(log(abs((tanh((1.0 / x3)) * x1))) , log(abs(x2))));
esm.myBestRegressor, Score=[0.01202157964718], ErrorPct=[0.1887825355682], lengthWFF=[357], WFF = mvlregress((sign(avg(log(abs(avg(cos(x0) , (x3*x3)))) , abs(x0))) / x3),avg(log(abs(x2)) , log(abs((sign(x3) / x3)))),avg(log(abs(avg(log(abs(x1)) , log(abs(x2))))) , log(abs(x1))),avg(avg(avg(log(abs(x4)) , log(abs(x1))) , sqrt(abs(x0))) , log(abs(x4))),avg(abs(x3) , (1.0 / x3)));
myBestSelectorChampions
myBestSelectorChampions[0],Score=[0.009135690004296,ErrorPct=[0.2305835733017],WFF= mvlregress((avg(avg(log(abs(x4)) , log(abs(x1))) , sqrt(abs(x0))) % avg(log(abs(x0)) , log(abs(x4)))),((tanh(max(((max(log(abs(x4)) , x0) + max((x1*x1) , x0))*(max(log(abs(x4)) , x0) + max((x1*x1) , x0))) , x2)) / ((1.0 / x3) / x3)) / min(sqrt(abs((tanh(x0) * x0))) , avg(abs(x2) , (x3*x3)))),avg(log(abs(x4)) , log(abs(x1))),avg(log(abs(sqrt(abs(x4)))) , log(abs(x0))),avg(log(abs((tanh((1.0 / x3)) * x1))) , log(abs(x2))));
myBestRegressorChampions
myBestRegressorChampions[0],ErrorPct=[0.1887825355682,Score=[0.01202157964718],WFF= mvlregress((sign(avg(log(abs(avg(cos(x0) , (x3*x3)))) , abs(x0))) / x3),avg(log(abs(x2)) , log(abs((sign(x3) / x3)))),avg(log(abs(avg(log(abs(x1)) , log(abs(x2))))) , log(abs(x1))),avg(avg(avg(log(abs(x4)) , log(abs(x1))) , sqrt(abs(x0))) , log(abs(x4))),avg(abs(x3) , (1.0 / x3)));


Show final results on training data, Score=[0.009135690004296], ErrorPct=[0.2305835733017]
X[0], ey=[28.1540639096], y=[-13.23760403537], ErrorPct=[0.8124821244839]
X[50], ey=[147.9619622708], y=[135.1123012931], ErrorPct=[0.2522275706283]
X[100], ey=[178.2263663785], y=[166.551667462], ErrorPct=[0.2291640962864]
X[150], ey=[193.8245175288], y=[186.1430650744], ErrorPct=[0.1507801719308]
X[200], ey=[205.1599392505], y=[202.859475415], ErrorPct=[0.0451560866503]
X[250], ey=[226.3760324965], y=[212.9894490978], ErrorPct=[0.2627668866565]
X[300], ey=[240.7803190228], y=[222.4203895046], ErrorPct=[0.3603893073396]
X[350], ey=[239.2123788251], y=[232.3874084185], ErrorPct=[0.1339681808159]
X[400], ey=[255.3392299272], y=[241.3945403664], ErrorPct=[0.2737220209325]
X[450], ey=[260.6107352321], y=[248.8061091812], ErrorPct=[0.231714451936]
X[500], ey=[251.7049039524], y=[257.0376173531], ErrorPct=[0.1046764851027]
X[550], ey=[269.9468547634], y=[265.4798923283], ErrorPct=[0.08768255326225]
X[600], ey=[285.3973528474], y=[271.7973256368], ErrorPct=[0.2669566014052]
X[650], ey=[279.7420234154], y=[279.1544829591], ErrorPct=[0.01153290364629]
X[700], ey=[296.8732572104], y=[287.8396440448], ErrorPct=[0.1773219002981]
X[750], ey=[291.6063786562], y=[294.0879150633], ErrorPct=[0.04871038235793]
X[800], ey=[294.2424829346], y=[301.1214393603], ErrorPct=[0.1350278790016]
X[850], ey=[313.7034443416], y=[310.1516064979], ErrorPct=[0.06971946047017]
X[900], ey=[302.854172856], y=[322.9519291139], ErrorPct=[0.3945013214611]
X[950], ey=[337.8439743004], y=[342.3042575068], ErrorPct=[0.08755144586552]
Actual computed error on training data is ErrorPct=[0.2305835733017] versus reported ErrorPct=[0.2305835733017] while average Y is AvgY=[250.1877736409]
yHistory=[#(num| 118.3836150264 185.2073994833 212.7294357336 232.2800258421 249.2075077634 264.3604977838 279.6905666505 294.2951662029 310.9254756561 354.7980462664 )]
eHistory=[#(num| 121.2674615999 185.6357847117 212.2990401414 232.7914661342 249.5413854129 263.6637504089 280.1237259594 294.2391398093 309.7526891453 352.5632930855 )]
aHistory=[#(num| 121.2674615999 153.4516231558 173.0674288177 187.9984381468 200.3070276 300.0685196817 309.1697119999 318.8517073467 331.1579911154 352.5632930855 )]
dHistory=[#(num| 99.76149208169 121.1712738531 145.7842785291 177.7063679596 231.2958314856 )]


Final testing on test data returns Score=[0.1254684712536], ErrorPct=[1.838704799059]
X[0], ey=[-60.48995021504], y=[-55774.31598227], ErrorPct=[1.380509077231]
X[50], ey=[193.4871708664], y=[-8367.990182567], ErrorPct=[0.2121411872543]
X[100], ey=[168.0669043372], y=[8775.784540496], ErrorPct=[0.2132869554519]
X[150], ey=[145.5586519887], y=[19880.52687222], ErrorPct=[0.4890043407038]
X[200], ey=[229.696466047], y=[27691.76602164], ErrorPct=[0.6804708813073]
X[250], ey=[258.5496010615], y=[37549.52250006], ErrorPct=[0.9240170753344]
X[300], ey=[284.3330482208], y=[43831.326997], ErrorPct=[1.079032346438]
X[350], ey=[206.75963016], y=[50906.68909993], ErrorPct=[1.256271877787]
X[400], ey=[279.673173703], y=[57799.0778786], ErrorPct=[1.425248739268]
X[450], ey=[175.7460213905], y=[64618.72422034], ErrorPct=[1.596805007006]
X[500], ey=[288.1653210089], y=[69302.83463794], ErrorPct=[1.710084986791]
X[550], ey=[211.4703214503], y=[76669.2249284], ErrorPct=[1.894514015226]
X[600], ey=[261.7242487417], y=[83809.30104038], ErrorPct=[2.070189688196]
X[650], ey=[174.2720653134], y=[90553.80460854], ErrorPct=[2.239475799059]
X[700], ey=[266.9155561403], y=[97559.11515529], ErrorPct=[2.410761820829]
X[750], ey=[270.0068567819], y=[105659.8896113], ErrorPct=[2.611410849923]
X[800], ey=[333.9409332492], y=[114405.2755248], ErrorPct=[2.826524833617]
X[850], ey=[293.9069550448], y=[125746.8564634], ErrorPct=[3.108545003927]
X[900], ey=[327.9774461608], y=[138018.4447838], ErrorPct=[3.411773226601]
X[950], ey=[257.259602842], y=[156179.1992898], ErrorPct=[3.863523085875]
Actual computed error on testing data is ErrorPct=[1.838704799059], Avg Y=[71906.10039048], AvgDev Y=[40357.44998057]
yHistory=[#(num| -11531.78024178 18681.32300334 36524.97777961 50551.81911893 63739.37899042 76623.90476001 90912.81001399 105685.5122257 125555.6143644 162317.4438902 )]
eHistory=[#(num| 22648.16716219 30310.23957282 39826.85217838 50708.34529844 61714.628418 69250.32258596 82065.61067666 97557.69348431 117042.50061 147936.643918 )]
aHistory=[#(num| 22648.16716219 26479.20336751 30928.4196378 35873.40105296 41041.64652597 102770.554255 111150.6121723 120845.6126708 132489.572264 147936.643918 )]
dHistory=[#(num| 61728.90772903 75277.2111193 89917.19303299 106010.3688965 125288.4767558 )]

esm.selfTest[mixedModel]: completed in [4.839333333333] minutes.

Starting test case: ratioRegression
Building test data as:
if ((x0 % 4) == 0) y = + ((36.28472509997*x0)/(30.58638330984*x1)) + ((30.58638330984*x1)/(13.02686266162*x2)) + ((13.02686266162*x2)/(29.03445701301*x3)) + ((29.03445701301*x3)/(35.00133469577*x4));
if ((x0 % 4) == 1) y = + ((36.28472509997*x0)%(30.58638330984*x1)) + ((30.58638330984*x1)%(13.02686266162*x2)) + ((13.02686266162*x2)%(29.03445701301*x3)) + ((29.03445701301*x3)%(35.00133469577*x4));
if ((x0 % 4) == 2) y = + ((36.28472509997*sin(x0))/(30.58638330984*tan(x1))) + ((30.58638330984*sin(x1))/(13.02686266162*tan(x2))) + ((13.02686266162*sin(x2))/(29.03445701301*tan(x3))) + ((29.03445701301*sin(x3))/(35.00133469577*tan(x4)));
if ((x0 % 4) == 3) y = + (30.58638330984* log(.000001+abs(x1))) + (13.02686266162* log(.000001+abs(x2))) - (29.03445701301* log(.000001+abs(x3))) + (35.00133469577* log(.000001+abs(x4)));
...starting new evolutionary process.
...starting generation of constant selector WFF.
...starting regress selector for each column name.
...starting a best-of-breed MVL selector WFF.
...starting a best-of-breed REG selector WFF.
...starting generation of root selector WFFs in each column island.
...starting generation of random selector WFFs.

Final results of training
esm: N = [1000], M = [5], Generations = [13], WFFs = [11136], Score=[0.004492346385637], ScoreHistory=[#(num| 0.004492346385637 )]
esm.myBest, Score=[0.004492346385637], ErrorPct=[0.09439024296217], lengthWFF=[551], WFF = mvlregress((tanh((ninteger((log(abs(x2)) / x3)) / (x1 / x2))) - (ninteger(exp(x2)) / x3)),(x2 / x3),((1.0 / x4) / ((x2 / (x3*x3*x3)) / x2)),avg((x1 / x2) , avg((1.0 / x2) , (cos(x0) / x3))),avg(((x1 / x2) / x3) , ((((cos(x0) / x3) / ninteger(x3)) - ((cos(x0) / x3) / avg(abs(x3) , x1))) + avg((ninteger(x2) / x3) , ((cos(x0) / x3) / abs(ninteger(x4)))))));
esm.myBestRegressor, Score=[0.004492346385637], ErrorPct=[0.09439024296217], lengthWFF=[551], WFF = mvlregress((tanh((ninteger((log(abs(x2)) / x3)) / (x1 / x2))) - (ninteger(exp(x2)) / x3)),(x2 / x3),((1.0 / x4) / ((x2 / (x3*x3*x3)) / x2)),avg((x1 / x2) , avg((1.0 / x2) , (cos(x0) / x3))),avg(((x1 / x2) / x3) , ((((cos(x0) / x3) / ninteger(x3)) - ((cos(x0) / x3) / avg(abs(x3) , x1))) + avg((ninteger(x2) / x3) , ((cos(x0) / x3) / abs(ninteger(x4)))))));
myBestSelectorChampions
myBestSelectorChampions[0],Score=[0.004492346385637,ErrorPct=[0.09439024296217],WFF= mvlregress((tanh((ninteger((log(abs(x2)) / x3)) / (x1 / x2))) - (ninteger(exp(x2)) / x3)),(x2 / x3),((1.0 / x4) / ((x2 / (x3*x3*x3)) / x2)),avg((x1 / x2) , avg((1.0 / x2) , (cos(x0) / x3))),avg(((x1 / x2) / x3) , ((((cos(x0) / x3) / ninteger(x3)) - ((cos(x0) / x3) / avg(abs(x3) , x1))) + avg((ninteger(x2) / x3) , ((cos(x0) / x3) / abs(ninteger(x4)))))));
myBestRegressorChampions
myBestRegressorChampions[0],ErrorPct=[0.09439024296217,Score=[0.004492346385637],WFF= mvlregress((tanh((ninteger((log(abs(x2)) / x3)) / (x1 / x2))) - (ninteger(exp(x2)) / x3)),(x2 / x3),((1.0 / x4) / ((x2 / (x3*x3*x3)) / x2)),avg((x1 / x2) , avg((1.0 / x2) , (cos(x0) / x3))),avg(((x1 / x2) / x3) , ((((cos(x0) / x3) / ninteger(x3)) - ((cos(x0) / x3) / avg(abs(x3) , x1))) + avg((ninteger(x2) / x3) , ((cos(x0) / x3) / abs(ninteger(x4)))))));

Show final results on training data, Score=[0.004492346385637], ErrorPct=[0.09439024296217]
X[0], ey=[-5919.565703956], y=[-5920.093083128], ErrorPct=[0.02624721923181]
X[50], ey=[-16.09044453709], y=[-15.9307513781], ErrorPct=[0.007947794645269]
X[100], ey=[-9.234943120718], y=[-7.977775861121], ErrorPct=[0.06256816057364]
X[150], ey=[-6.282682533642], y=[-5.562853010373], ErrorPct=[0.03582531190954]
X[200], ey=[-4.179581209147], y=[-4.424171856367], ErrorPct=[0.01217307146144]
X[250], ey=[-1.906534482231], y=[-3.228629703173], ErrorPct=[0.0657995707779]
X[300], ey=[-2.289752480753], y=[-2.350785253532], ErrorPct=[0.003037549934856]
X[350], ey=[-1.474161091908], y=[-1.419437251584], ErrorPct=[0.002723559655585]
X[400], ey=[-0.4697257557486], y=[-0.7238386156872], ErrorPct=[0.01264698400558]
X[450], ey=[0.09080355999835], y=[-0.09678140798499], ErrorPct=[0.009335946596113]
X[500], ey=[0.3890572850939], y=[0.557884103256], ErrorPct=[0.008402369205261]
X[550], ey=[2.202951687372], y=[1.1333426962], ErrorPct=[0.05323354279216]
X[600], ey=[2.267835236131], y=[1.750943418446], ErrorPct=[0.02572527243391]
X[650], ey=[2.131206727436], y=[2.328209326095], ErrorPct=[0.009804654179643]
X[700], ey=[3.387401701791], y=[2.9358285196], ErrorPct=[0.02247441870476]
X[750], ey=[3.479346986046], y=[3.615728013845], ErrorPct=[0.006787569419564]
X[800], ey=[4.216662186732], y=[4.705172180757], ErrorPct=[0.02431273286407]
X[850], ey=[5.585186154426], y=[6.309329616105], ErrorPct=[0.03604001300763]
X[900], ey=[9.952475734026], y=[9.359059503825], ErrorPct=[0.02953382829117]
X[950], ey=[18.59091130018], y=[19.29510886351], ErrorPct=[0.03504732236845]
Actual computed error on training data is ErrorPct=[0.09439024296217] versus reported ErrorPct=[0.09439024296217] while average Y is AvgY=[-4.962236922537]
yHistory=[#(num| -104.6148640943 -5.70235829275 -3.332281677626 -1.494273842777 -0.08744276242159 1.1438482789 2.287297984271 3.673839714872 6.606267536593 51.89759792983 )]
eHistory=[#(num| -103.9752392467 -5.751502604578 -3.307331195073 -1.329797194637 -0.1422282117791 1.197066910822 2.182291040524 3.28366648839 6.437444275413 51.78326051222 )]
aHistory=[#(num| -103.9752392467 -54.86337092563 -37.67802434878 -28.59096756024 -22.90121969055 12.97674584547 15.92166557914 20.50145709201 29.11035239382 51.78326051222 )]
dHistory=[#(num| 35.87796553602 44.51263313938 58.17948144078 83.97372331944 155.7584997589 )]


Final testing on test data returns Score=[0.5054724482025], ErrorPct=[1.05203674258]
X[0], ey=[-0.7984955518041], y=[-2337.158918978], ErrorPct=[4.905690832785]
X[50], ey=[3.484773365808], y=[-1037.941744205], ErrorPct=[2.186698794006]
X[100], ey=[1.605752545394], y=[-788.7051109295], ErrorPct=[1.659427509183]
X[150], ey=[-0.08208196162508], y=[-536.0673749539], ErrorPct=[1.125416315041]
X[200], ey=[20.50286679754], y=[-399.4598653025], ErrorPct=[0.8818020132158]
X[250], ey=[1.552630903534], y=[-296.9562866444], ErrorPct=[0.6267836270622]
X[300], ey=[-1.785020846129], y=[-215.4767466396], ErrorPct=[0.4486917043091]
X[350], ey=[1.371669474255], y=[-143.5794291647], ErrorPct=[0.3043559840619]
X[400], ey=[-2.647579999083], y=[-88.64441676256], ErrorPct=[0.1805688409755]
X[450], ey=[-8.401987946535], y=[-22.40515276246], ErrorPct=[0.02940265405059]
X[500], ey=[-1.031920063041], y=[31.51283717435], ErrorPct=[0.06833471224469]
X[550], ey=[-5.358576611156], y=[93.49445972344], ErrorPct=[0.2075631949922]
X[600], ey=[3.880306472401], y=[155.9541046513], ErrorPct=[0.319311622536]
X[650], ey=[1.512430439809], y=[247.4319724498], ErrorPct=[0.5163609307644]
X[700], ey=[-9.574767952153], y=[304.4036787563], ErrorPct=[0.6592652282011]
X[750], ey=[0.00692872896971], y=[377.200142545], ErrorPct=[0.7919982176777]
X[800], ey=[0.2983645127984], y=[496.4257010333], ErrorPct=[1.041725969272]
X[850], ey=[1.814812599384], y=[627.7992279711], ErrorPct=[1.314388814827]
X[900], ey=[-12.59405325508], y=[845.7656188617], ErrorPct=[1.802310607779]
X[950], ey=[0.3099116854467], y=[1144.47988531], ErrorPct=[2.402430761315]
Actual computed error on testing data is ErrorPct=[1.05203674258], Avg Y=[40.61766458009], AvgDev Y=[476.2551296164]
yHistory=[#(num| -1148.355725631 -551.2607858727 -303.3673643078 -149.0641080012 -25.7416983953 94.85119435782 237.1392256867 387.967133277 649.8265591789 1214.182215509 )]
eHistory=[#(num| 27.56566657518 62.91467653002 59.30824652387 -20.97116532308 78.62667429812 -1.563830946669 22.44660883666 75.00515670322 14.00351261908 88.84109998453 )]
aHistory=[#(num| 27.56566657518 45.2401715526 49.92952987636 32.2043560765 41.48881972082 39.74650943936 50.07409453587 59.28325643561 51.4223063018 88.84109998453 )]
dHistory=[#(num| -1.74231028146 17.86973845937 9.35372655925 6.182134749202 61.27543340934 )]

esm.selfTest[ratioRegression]: completed in [4.024483333333] minutes.

Starting test case: cyclicSeries
Building test data as: y = 0.1161904777945 + (0.1161904777945*x0*sin(x0)) - (32.6501248441*x1*cos(x0)) - (48.72020620381*x2*tan(x0)) + (10.08585426787*x3*sin(x0)) + (37.41419583449*x4*cos(x0));
...starting new evolutionary process.
...starting generation of constant selector WFF.
...starting regress selector for each column name.
...starting a best-of-breed MVL selector WFF.
...starting a best-of-breed REG selector WFF.
...starting generation of root selector WFFs in each column island.
...starting generation of random selector WFFs.

Final results of training
esm: N = [1000], M = [5], Generations = [184], WFFs = [8227], Score=[0.009668384109879], ScoreHistory=[#(num| 0.009668384109879 )]
esm.myBest, Score=[0.009668384109879], ErrorPct=[0.1333507024905], lengthWFF=[1379], WFF = mvlregress((cos(x0) * x4),min((x3 % x4) , min(avg(((x3 * x4) % (expt(abs(x1) ,x3) / cos(x0))) , avg((tan(x0) * x2) , ((sign((x2 % sqrt(abs(x4)))) % tan(((min((tan(x0) * x2) , avg((tan(x0) * x2) , avg((tan(x0) * x2) , ((sign((x2 % sqrt(abs(x4)))) % tan(x0)) % x1)))) % expt(abs(x3) ,cos(((exp(x2) * avg(x2 , x3)) * x2)))) / (tan(x0) * x2)))) % x1))) , avg(((x3 * x4) % (expt(abs(x1) ,x3) / cos(x0))) , avg((tan(x0) * x2) , ((sign((x2 % sqrt(abs(x4)))) % tan(x0)) % x1))))),max((tan(x0) * x2) , min((tan(x0) * x2) , (tan(x0) * x2))),expt(abs(max((cos(x0) % x1) , (x3 * x4))) ,sign(x1)),((ninteger(x1) % x1) + ((abs(x3) * avg(((tan(x0) * x2) + sign(avg((tan(x0) * x2) , ((sign((x2 % sqrt(abs(x4)))) % tan(x0)) % x1)))) , avg((tan(x0) * x2) , ((sign((x2 % sqrt(abs(x4)))) % tan(x0)) % x1)))) / (x2 / avg(((tan(x0) * x2) - avg(ninteger(x3) , tanh(x2))) , sign(x4))))));
esm.myBestRegressor, Score=[0.009668384109879], ErrorPct=[0.1333507024905], lengthWFF=[1379], WFF = mvlregress((cos(x0) * x4),min((x3 % x4) , min(avg(((x3 * x4) % (expt(abs(x1) ,x3) / cos(x0))) , avg((tan(x0) * x2) , ((sign((x2 % sqrt(abs(x4)))) % tan(((min((tan(x0) * x2) , avg((tan(x0) * x2) , avg((tan(x0) * x2) , ((sign((x2 % sqrt(abs(x4)))) % tan(x0)) % x1)))) % expt(abs(x3) ,cos(((exp(x2) * avg(x2 , x3)) * x2)))) / (tan(x0) * x2)))) % x1))) , avg(((x3 * x4) % (expt(abs(x1) ,x3) / cos(x0))) , avg((tan(x0) * x2) , ((sign((x2 % sqrt(abs(x4)))) % tan(x0)) % x1))))),max((tan(x0) * x2) , min((tan(x0) * x2) , (tan(x0) * x2))),expt(abs(max((cos(x0) % x1) , (x3 * x4))) ,sign(x1)),((ninteger(x1) % x1) + ((abs(x3) * avg(((tan(x0) * x2) + sign(avg((tan(x0) * x2) , ((sign((x2 % sqrt(abs(x4)))) % tan(x0)) % x1)))) , avg((tan(x0) * x2) , ((sign((x2 % sqrt(abs(x4)))) % tan(x0)) % x1)))) / (x2 / avg(((tan(x0) * x2) - avg(ninteger(x3) , tanh(x2))) , sign(x4))))));
myBestSelectorChampions
myBestSelectorChampions[0],Score=[0.009668384109879,ErrorPct=[0.1333507024905],WFF= mvlregress((cos(x0) * x4),min((x3 % x4) , min(avg(((x3 * x4) % (expt(abs(x1) ,x3) / cos(x0))) , avg((tan(x0) * x2) , ((sign((x2 % sqrt(abs(x4)))) % tan(((min((tan(x0) * x2) , avg((tan(x0) * x2) , avg((tan(x0) * x2) , ((sign((x2 % sqrt(abs(x4)))) % tan(x0)) % x1)))) % expt(abs(x3) ,cos(((exp(x2) * avg(x2 , x3)) * x2)))) / (tan(x0) * x2)))) % x1))) , avg(((x3 * x4) % (expt(abs(x1) ,x3) / cos(x0))) , avg((tan(x0) * x2) , ((sign((x2 % sqrt(abs(x4)))) % tan(x0)) % x1))))),max((tan(x0) * x2) , min((tan(x0) * x2) , (tan(x0) * x2))),expt(abs(max((cos(x0) % x1) , (x3 * x4))) ,sign(x1)),((ninteger(x1) % x1) + ((abs(x3) * avg(((tan(x0) * x2) + sign(avg((tan(x0) * x2) , ((sign((x2 % sqrt(abs(x4)))) % tan(x0)) % x1)))) , avg((tan(x0) * x2) , ((sign((x2 % sqrt(abs(x4)))) % tan(x0)) % x1)))) / (x2 / avg(((tan(x0) * x2) - avg(ninteger(x3) , tanh(x2))) , sign(x4))))));
myBestSelectorChampions[1],Score=[0.01050829028717,ErrorPct=[0.1354767328461],WFF= mvlregress((cos(((log(abs(1.766945547385)) % (if (x0 > -2.230201610243) {x0} else {x2})) % expt(abs(x3) ,cos(((exp(x2) * avg(x2 , x3)) * x2))))) % (x4 % x3)),min((tan(x0) * x2) , (tan(x0) * x2)),min(avg(((x3 * x4) % (expt(abs(x1) ,x3) / cos(x0))) , avg((tan(x0) * x2) , ((sign((x2 % sqrt(abs(x4)))) % tan(x0)) % x1))) , avg(((x3 * x4) % (expt(abs(x1) ,x3) / cos(x0))) , avg((tan(x0) * x2) , ((sign((x2 % sqrt(abs(x4)))) % tan(x0)) % x1)))),max((cos(x0) % x1) , (cos(x0) % x1)),((abs(x3) * x4) / (x2 / avg(((tan(x0) * x2) - avg(ninteger(x3) , tanh(x2))) , sign(x4)))));
myBestRegressorChampions
myBestRegressorChampions[0],ErrorPct=[0.1333507024905,Score=[0.009668384109879],WFF= mvlregress((cos(x0) * x4),min((x3 % x4) , min(avg(((x3 * x4) % (expt(abs(x1) ,x3) / cos(x0))) , avg((tan(x0) * x2) , ((sign((x2 % sqrt(abs(x4)))) % tan(((min((tan(x0) * x2) , avg((tan(x0) * x2) , avg((tan(x0) * x2) , ((sign((x2 % sqrt(abs(x4)))) % tan(x0)) % x1)))) % expt(abs(x3) ,cos(((exp(x2) * avg(x2 , x3)) * x2)))) / (tan(x0) * x2)))) % x1))) , avg(((x3 * x4) % (expt(abs(x1) ,x3) / cos(x0))) , avg((tan(x0) * x2) , ((sign((x2 % sqrt(abs(x4)))) % tan(x0)) % x1))))),max((tan(x0) * x2) , min((tan(x0) * x2) , (tan(x0) * x2))),expt(abs(max((cos(x0) % x1) , (x3 * x4))) ,sign(x1)),((ninteger(x1) % x1) + ((abs(x3) * avg(((tan(x0) * x2) + sign(avg((tan(x0) * x2) , ((sign((x2 % sqrt(abs(x4)))) % tan(x0)) % x1)))) , avg((tan(x0) * x2) , ((sign((x2 % sqrt(abs(x4)))) % tan(x0)) % x1)))) / (x2 / avg(((tan(x0) * x2) - avg(ninteger(x3) , tanh(x2))) , sign(x4))))));
myBestRegressorChampions[1],ErrorPct=[0.1349791158634,Score=[0.0105425768153],WFF= mvlregress((cos(x0) % x1),((min((tan(x0) * x2) , avg((tan(x0) * x2) , avg((tan(x0) * x2) , ((sign((x2 % sqrt(abs(x4)))) % tan(x0)) % x1)))) % expt(abs(x3) ,cos(((exp(x2) * avg(x2 , x3)) * x2)))) / (tan(x0) * x2)),avg(((x3 * x4) % (expt(abs(x1) ,x3) / cos(x0))) , max((expt(abs(tan(x0)) ,x3) / cos(x0)) , ((abs(x3) * x4) % (expt(abs(x1) ,x3) / cos(x0))))),avg(((x3 * x4) % (expt(abs(x1) ,x3) / cos(x0))) , avg((tan(x0) * x2) , ((sign((x2 % sqrt(abs(x4)))) % tan(x0)) % x1))),avg(((tan(x0) * x2) + sign(avg((tan(x0) * x2) , ((sign((x2 % sqrt(abs(x4)))) % tan(x0)) % x1)))) , avg((tan(x0) * x2) , ((sign((x2 % sqrt(abs(x4)))) % tan(x0)) % x1))));

Show final results on training data, Score=[0.009668384109879], ErrorPct=[0.1333507024905]
X[0], ey=[-210032.739295], y=[-210034.6613361], ErrorPct=[0.0004514741127028]
X[50], ey=[-6942.534971778], y=[-7280.270747042], ErrorPct=[0.07933179179942]
X[100], ey=[-3186.869548012], y=[-3849.196435204], ErrorPct=[0.1555759933243]
X[150], ey=[-2587.407118542], y=[-2708.856315906], ErrorPct=[0.02852757434979]
X[200], ey=[-2059.66504659], y=[-2146.594139003], ErrorPct=[0.02041904105445]
X[250], ey=[-1856.241890615], y=[-1659.856089478], ErrorPct=[0.04612966297686]
X[300], ey=[-2449.178034148], y=[-1283.044855486], ErrorPct=[0.2739165978723]
X[350], ey=[-1044.171304917], y=[-976.4785495868], ErrorPct=[0.01590055885543]
X[400], ey=[-548.2282053723], y=[-680.9039078015], ErrorPct=[0.03116460254686]
X[450], ey=[844.4431686657], y=[-385.9762473263], ErrorPct=[0.2890169892698]
X[500], ey=[-501.8959940572], y=[-79.45256770082], ErrorPct=[0.09922903169072]
X[550], ey=[593.5656669746], y=[222.1857704291], ErrorPct=[0.0872345626051]
X[600], ey=[109.5584881869], y=[483.4846480469], ErrorPct=[0.08783266220229]
X[650], ey=[45.579927511], y=[864.6518324834], ErrorPct=[0.1923943111542]
X[700], ey=[1679.444069693], y=[1205.645013377], ErrorPct=[0.1112921130759]
X[750], ey=[1576.551774425], y=[1643.811908749], ErrorPct=[0.01579893918076]
X[800], ey=[2660.561718178], y=[2143.598706198], ErrorPct=[0.1214310269689]
X[850], ey=[3018.270932401], y=[2654.723769789], ErrorPct=[0.08539470771515]
X[900], ey=[2906.173096248], y=[3552.737637622], ErrorPct=[0.1518735275856]
X[950], ey=[6292.449511355], y=[6770.603007257], ErrorPct=[0.1123149407416]
Actual computed error on training data is ErrorPct=[0.1333507024905] versus reported ErrorPct=[0.1333507024905] while average Y is AvgY=[-550.0054315862]
yHistory=[#(num| -17954.97965501 -2865.593449349 -1668.782632133 -980.2406785609 -388.4637574437 209.4373356231 853.8051816679 1662.677364481 2737.07752389 12895.00845097 )]
eHistory=[#(num| -17920.12662525 -2631.892774306 -1553.635597849 -796.4362622616 -232.046216353 35.1150020342 816.8385326372 1419.512028974 2486.507878729 12876.10971778 )]
aHistory=[#(num| -17920.12662525 -10276.00969978 -7368.551665801 -5725.522814916 -4626.827495204 3526.816632031 4399.742039531 5594.043208495 7681.308798255 12876.10971778 )]
dHistory=[#(num| 8153.644127235 10125.26485445 12962.5948743 17957.31849803 30796.23634303 )]


Final testing on test data returns Score=[0.005267646608884], ErrorPct=[0.06678646841028]
X[0], ey=[-2376095.425223], y=[-2382351.127495], ErrorPct=[0.741204785521]
X[50], ey=[-8062.767482509], y=[-7339.018454094], ErrorPct=[0.08575316088036]
X[100], ey=[-3580.0489338], y=[-3757.660782079], ErrorPct=[0.02104428027086]
X[150], ey=[-1844.972621378], y=[-2711.152055436], ErrorPct=[0.1026289797207]
X[200], ey=[-2255.439446696], y=[-2016.012955785], ErrorPct=[0.02836836746991]
X[250], ey=[-2671.713042434], y=[-1552.418716064], ErrorPct=[0.1326192128393]
X[300], ey=[-55.03278345947], y=[-1078.917252874], ErrorPct=[0.1213146079392]
X[350], ey=[-297.254932746], y=[-761.2507140094], ErrorPct=[0.05497638451494]
X[400], ey=[214.284893915], y=[-443.0853608237], ErrorPct=[0.07788829414526]
X[450], ey=[-1213.719121109], y=[-157.7205767356], ErrorPct=[0.1251196333394]
X[500], ey=[-582.0481436039], y=[182.3712209202], ErrorPct=[0.09057197201302]
X[550], ey=[1309.895622705], y=[430.7731754085], ErrorPct=[0.104162528303]
X[600], ey=[-291.7550881145], y=[754.8906282917], ErrorPct=[0.1240114666547]
X[650], ey=[762.7472127912], y=[1079.418183949], ErrorPct=[0.03752065380354]
X[700], ey=[121.0234037246], y=[1449.799404595], ErrorPct=[0.1574395787806]
X[750], ey=[2086.841681647], y=[1816.058309937], ErrorPct=[0.03208367697406]
X[800], ey=[2076.302879029], y=[2260.572171284], ErrorPct=[0.02183308528737]
X[850], ey=[2859.263865598], y=[2769.674891842], ErrorPct=[0.01061491950659]
X[900], ey=[4406.935791357], y=[4235.166442287], ErrorPct=[0.02035203371166]
X[950], ey=[9052.368264524], y=[8904.590625325], ErrorPct=[0.01750938401469]
Actual computed error on testing data is ErrorPct=[0.06678646841028], Avg Y=[-2162.201679091], AvgDev Y=[8439.910797285]
yHistory=[#(num| -43736.19965408 -2780.814223596 -1548.892859661 -767.8724170563 -148.2273778234 432.7902154373 1084.843080652 1829.152760685 2942.489563135 21070.7141214 )]
eHistory=[#(num| -43691.51666581 -2575.461206402 -1303.716693524 -738.5765478194 -185.0407670564 506.8352630416 1103.928740641 1476.279892112 2731.570922789 21053.68027111 )]
aHistory=[#(num| -43691.51666581 -23133.4889361 -15856.89818858 -12077.31777839 -9698.862376122 5374.459017939 6591.364956664 8420.510362004 11892.62559695 21053.68027111 )]
dHistory=[#(num| 15073.32139406 18668.68273505 24277.40855058 35026.11453305 64745.19693692 )]

esm.selfTest[cyclicSeries]: completed in [49.04713333333] minutes.
true

(writeln "********** regressGaCMVL:")(esm.setOptions regressGaCMVL: 00% true)(esm.selfTest all: 1 10 1000 200 .99% checkpoint:)
********** regressGaCMVL:

Starting test case: crossCorrelation
Building test data as: y = -9.165146942478 - (9.165146942478*x0*x0*x0) - (19.5666514757*x0*x1*x1) + (21.87460482304*x0*x1*x2) - (17.48124453288*x1*x2*x3) + (38.81839452492*x2*x3*x4) - (38.63656433142*x3*x4*x5) + (13.18212824804*x4*x5*x6) - (2.045229508597*x5*x6*x7) + (45.1292360492*x6*x7*x8) + (26.03603502163*x7*x8*x9);
...starting new evolutionary process.
...starting generation of constant selector WFF.
...starting regress selector for each column name.
...starting a best-of-breed MVL selector WFF.
...starting a best-of-breed REG selector WFF.
...starting generation of root selector WFFs in each column island.
...starting generation of random selector WFFs.

Final results of training
esm: N = [1000], M = [10], Generations = [200], WFFs = [29960], Score=[0.1751680380524], ScoreHistory=[#(num| 0.1751680380524 )]
esm.myBest, Score=[0.1751680380524], ErrorPct=[0.7365682454002], lengthWFF=[3416], WFF = mvlregress((((sign(x3) - x0) - ((((((-x6) * x8) * sqrt(abs(x4))) - (-sqrt(abs(x4)))) * x7) - max((-(1.0 / x5)) , x0))) - (1.0 / x9)),(((x8*x8*x8) * ((ninteger(((ninteger(x9)*ninteger(x9)*ninteger(x9)) - ninteger(x9))) * x7) + ((log(abs(((x8*x8*x8) - ninteger((max(sqrt(abs(x2)) , x0) - ninteger(x9)))))) + min((cos(x2) + (1.0 / x2)) , x1)) * max(log(abs((x5 * x3))) , x0)))) % min(abs(x5) , (((-x7) * x7) - (sign(x1) - x0)))),((max(max(((log(abs(x6)) - exp(x7)) * (ninteger(x2) - (x6 * x8))) , (min(abs(x5) , (x6 * x8)) - (log(abs(tanh(x7))) - ((x6 * x8) * (sign(x3) - (1.0 / x5)))))) , (1.0 / x1)) % ninteger(x8)) % exp(x6)),max(sqrt(abs((x0*x0*x0))) , x9),((avg(ninteger(x4) , (avg(cos(x6) , x0)*avg(cos(x6) , x0)*avg(cos(x6) , x0))) % (sign(x3) - x0)) / cos(x6)),(avg(exp(x1) , max((min(avg(max(x4 , x9) , ((ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))*ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))*ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))) - ninteger(x9))) , avg(max(x4 , x9) , ((max(log(abs(max(x1 , ninteger(x9)))) , x0)*max(log(abs(max(x1 , ninteger(x9)))) , x0)*max(log(abs(max(x1 , ninteger(x9)))) , x0)) - ninteger(x9)))) / (1.0 / x5)) , ((exp(x1) + (x0*x0*x0)) % avg(abs(x4) , x0)))) / avg((-x5) , x0)),(avg((((expt(abs(min(max((max((x9 * (x1*x1)) , sqrt(abs(x5))) * (x1*x1)) , (x0 - x7)) , max((max((x9 * (x1*x1)) , sqrt(abs(x5))) * (x1*x1)) , (avg(max(x4 , x9) , ((ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))*ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))*ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))) - ninteger(x9))) - x7)))) ,tanh(max((x5*x5*x5) , log(abs(x9))))) / ninteger(cos(x0))) + min((x6*x6*x6) , sqrt(abs(x9))))*((expt(abs(min(max((max((x9 * (x1*x1)) , sqrt(abs(x5))) * (x1*x1)) , (x0 - x7)) , max((max((x9 * (x1*x1)) , sqrt(abs(x5))) * (x1*x1)) , (avg(max(x4 , x9) , ((ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))*ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))*ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))) - ninteger(x9))) - x7)))) ,tanh(max((x5*x5*x5) , log(abs(x9))))) / ninteger(cos(x0))) + min((x6*x6*x6) , sqrt(abs(x9))))*((expt(abs(min(max((max((x9 * (x1*x1)) , sqrt(abs(x5))) * (x1*x1)) , (x0 - x7)) , max((max((x9 * (x1*x1)) , sqrt(abs(x5))) * (x1*x1)) , (avg(max(x4 , x9) , ((ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))*ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))*ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))) - ninteger(x9))) - x7)))) ,tanh(max((x5*x5*x5) , log(abs(x9))))) / ninteger(cos(x0))) + min((x6*x6*x6) , sqrt(abs(x9))))) , (avg(avg((tanh(x7)*tanh(x7)*tanh(x7)) , cos(abs(x1))) , min(avg((avg(log(abs(x3)) , x0)*avg(log(abs(x3)) , x0)*avg(log(abs(x3)) , x0)) , cos(x7)) , min(log(abs(x7)) , (1.0 / x5)))) + x8)) / avg(avg((x0*x0*x0) , x4) , ((sign((x8 + max(sign(x8) , (x9*x9*x9)))) + ((log(abs(((x8*x8*x8) - ninteger((max(sqrt(abs(x2)) , x0) - ninteger(x9)))))) + min((cos(x2) + (1.0 / x2)) , x1)) * max(log(abs((x5 * x3))) , x0))) + (log(abs(tanh(expt(abs(abs((sign(x7)*sign(x7)*sign(x7)))) ,log(abs((1.0 / x5))))))) + ninteger(max((min(avg(max(x4 , x9) , ((ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))*ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))*ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))) - ninteger(x9))) , avg(max(x4 , x9) , ((max(log(abs(max(x1 , ninteger(x9)))) , x0)*max(log(abs(max(x1 , ninteger(x9)))) , x0)*max(log(abs(max(x1 , ninteger(x9)))) , x0)) - ninteger(x9)))) / (1.0 / x5)) , avg(max(x4 , x9) , ((ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))*ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))*ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))) - ninteger(x9))))))))),avg((-x0) , (avg((-x7) , x0) / avg((-x0) , cos(x2)))),avg((x3*x3*x3) , (sign(x8) - avg(sign(avg(sign((x9 * (x1*x1))) , x0)) , x0))),avg(avg(avg(ninteger(x1) , (x3*x3)) , avg(sign(x2) , (((x7*x7*x7) * (x6 * x8)) + (cos(expt(abs(x9) ,x8)) % exp(ninteger(x4)))))) , avg(tanh(x3) , x8)));
esm.myBestRegressor, Score=[0.1751680380524], ErrorPct=[0.7365682454002], lengthWFF=[3416], WFF = mvlregress((((sign(x3) - x0) - ((((((-x6) * x8) * sqrt(abs(x4))) - (-sqrt(abs(x4)))) * x7) - max((-(1.0 / x5)) , x0))) - (1.0 / x9)),(((x8*x8*x8) * ((ninteger(((ninteger(x9)*ninteger(x9)*ninteger(x9)) - ninteger(x9))) * x7) + ((log(abs(((x8*x8*x8) - ninteger((max(sqrt(abs(x2)) , x0) - ninteger(x9)))))) + min((cos(x2) + (1.0 / x2)) , x1)) * max(log(abs((x5 * x3))) , x0)))) % min(abs(x5) , (((-x7) * x7) - (sign(x1) - x0)))),((max(max(((log(abs(x6)) - exp(x7)) * (ninteger(x2) - (x6 * x8))) , (min(abs(x5) , (x6 * x8)) - (log(abs(tanh(x7))) - ((x6 * x8) * (sign(x3) - (1.0 / x5)))))) , (1.0 / x1)) % ninteger(x8)) % exp(x6)),max(sqrt(abs((x0*x0*x0))) , x9),((avg(ninteger(x4) , (avg(cos(x6) , x0)*avg(cos(x6) , x0)*avg(cos(x6) , x0))) % (sign(x3) - x0)) / cos(x6)),(avg(exp(x1) , max((min(avg(max(x4 , x9) , ((ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))*ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))*ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))) - ninteger(x9))) , avg(max(x4 , x9) , ((max(log(abs(max(x1 , ninteger(x9)))) , x0)*max(log(abs(max(x1 , ninteger(x9)))) , x0)*max(log(abs(max(x1 , ninteger(x9)))) , x0)) - ninteger(x9)))) / (1.0 / x5)) , ((exp(x1) + (x0*x0*x0)) % avg(abs(x4) , x0)))) / avg((-x5) , x0)),(avg((((expt(abs(min(max((max((x9 * (x1*x1)) , sqrt(abs(x5))) * (x1*x1)) , (x0 - x7)) , max((max((x9 * (x1*x1)) , sqrt(abs(x5))) * (x1*x1)) , (avg(max(x4 , x9) , ((ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))*ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))*ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))) - ninteger(x9))) - x7)))) ,tanh(max((x5*x5*x5) , log(abs(x9))))) / ninteger(cos(x0))) + min((x6*x6*x6) , sqrt(abs(x9))))*((expt(abs(min(max((max((x9 * (x1*x1)) , sqrt(abs(x5))) * (x1*x1)) , (x0 - x7)) , max((max((x9 * (x1*x1)) , sqrt(abs(x5))) * (x1*x1)) , (avg(max(x4 , x9) , ((ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))*ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))*ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))) - ninteger(x9))) - x7)))) ,tanh(max((x5*x5*x5) , log(abs(x9))))) / ninteger(cos(x0))) + min((x6*x6*x6) , sqrt(abs(x9))))*((expt(abs(min(max((max((x9 * (x1*x1)) , sqrt(abs(x5))) * (x1*x1)) , (x0 - x7)) , max((max((x9 * (x1*x1)) , sqrt(abs(x5))) * (x1*x1)) , (avg(max(x4 , x9) , ((ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))*ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))*ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))) - ninteger(x9))) - x7)))) ,tanh(max((x5*x5*x5) , log(abs(x9))))) / ninteger(cos(x0))) + min((x6*x6*x6) , sqrt(abs(x9))))) , (avg(avg((tanh(x7)*tanh(x7)*tanh(x7)) , cos(abs(x1))) , min(avg((avg(log(abs(x3)) , x0)*avg(log(abs(x3)) , x0)*avg(log(abs(x3)) , x0)) , cos(x7)) , min(log(abs(x7)) , (1.0 / x5)))) + x8)) / avg(avg((x0*x0*x0) , x4) , ((sign((x8 + max(sign(x8) , (x9*x9*x9)))) + ((log(abs(((x8*x8*x8) - ninteger((max(sqrt(abs(x2)) , x0) - ninteger(x9)))))) + min((cos(x2) + (1.0 / x2)) , x1)) * max(log(abs((x5 * x3))) , x0))) + (log(abs(tanh(expt(abs(abs((sign(x7)*sign(x7)*sign(x7)))) ,log(abs((1.0 / x5))))))) + ninteger(max((min(avg(max(x4 , x9) , ((ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))*ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))*ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))) - ninteger(x9))) , avg(max(x4 , x9) , ((max(log(abs(max(x1 , ninteger(x9)))) , x0)*max(log(abs(max(x1 , ninteger(x9)))) , x0)*max(log(abs(max(x1 , ninteger(x9)))) , x0)) - ninteger(x9)))) / (1.0 / x5)) , avg(max(x4 , x9) , ((ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))*ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))*ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))) - ninteger(x9))))))))),avg((-x0) , (avg((-x7) , x0) / avg((-x0) , cos(x2)))),avg((x3*x3*x3) , (sign(x8) - avg(sign(avg(sign((x9 * (x1*x1))) , x0)) , x0))),avg(avg(avg(ninteger(x1) , (x3*x3)) , avg(sign(x2) , (((x7*x7*x7) * (x6 * x8)) + (cos(expt(abs(x9) ,x8)) % exp(ninteger(x4)))))) , avg(tanh(x3) , x8)));
myBestSelectorChampions
myBestSelectorChampions[0],Score=[0.1751680380524,ErrorPct=[0.7365682454002],WFF= mvlregress((((sign(x3) - x0) - ((((((-x6) * x8) * sqrt(abs(x4))) - (-sqrt(abs(x4)))) * x7) - max((-(1.0 / x5)) , x0))) - (1.0 / x9)),(((x8*x8*x8) * ((ninteger(((ninteger(x9)*ninteger(x9)*ninteger(x9)) - ninteger(x9))) * x7) + ((log(abs(((x8*x8*x8) - ninteger((max(sqrt(abs(x2)) , x0) - ninteger(x9)))))) + min((cos(x2) + (1.0 / x2)) , x1)) * max(log(abs((x5 * x3))) , x0)))) % min(abs(x5) , (((-x7) * x7) - (sign(x1) - x0)))),((max(max(((log(abs(x6)) - exp(x7)) * (ninteger(x2) - (x6 * x8))) , (min(abs(x5) , (x6 * x8)) - (log(abs(tanh(x7))) - ((x6 * x8) * (sign(x3) - (1.0 / x5)))))) , (1.0 / x1)) % ninteger(x8)) % exp(x6)),max(sqrt(abs((x0*x0*x0))) , x9),((avg(ninteger(x4) , (avg(cos(x6) , x0)*avg(cos(x6) , x0)*avg(cos(x6) , x0))) % (sign(x3) - x0)) / cos(x6)),(avg(exp(x1) , max((min(avg(max(x4 , x9) , ((ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))*ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))*ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))) - ninteger(x9))) , avg(max(x4 , x9) , ((max(log(abs(max(x1 , ninteger(x9)))) , x0)*max(log(abs(max(x1 , ninteger(x9)))) , x0)*max(log(abs(max(x1 , ninteger(x9)))) , x0)) - ninteger(x9)))) / (1.0 / x5)) , ((exp(x1) + (x0*x0*x0)) % avg(abs(x4) , x0)))) / avg((-x5) , x0)),(avg((((expt(abs(min(max((max((x9 * (x1*x1)) , sqrt(abs(x5))) * (x1*x1)) , (x0 - x7)) , max((max((x9 * (x1*x1)) , sqrt(abs(x5))) * (x1*x1)) , (avg(max(x4 , x9) , ((ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))*ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))*ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))) - ninteger(x9))) - x7)))) ,tanh(max((x5*x5*x5) , log(abs(x9))))) / ninteger(cos(x0))) + min((x6*x6*x6) , sqrt(abs(x9))))*((expt(abs(min(max((max((x9 * (x1*x1)) , sqrt(abs(x5))) * (x1*x1)) , (x0 - x7)) , max((max((x9 * (x1*x1)) , sqrt(abs(x5))) * (x1*x1)) , (avg(max(x4 , x9) , ((ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))*ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))*ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))) - ninteger(x9))) - x7)))) ,tanh(max((x5*x5*x5) , log(abs(x9))))) / ninteger(cos(x0))) + min((x6*x6*x6) , sqrt(abs(x9))))*((expt(abs(min(max((max((x9 * (x1*x1)) , sqrt(abs(x5))) * (x1*x1)) , (x0 - x7)) , max((max((x9 * (x1*x1)) , sqrt(abs(x5))) * (x1*x1)) , (avg(max(x4 , x9) , ((ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))*ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))*ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))) - ninteger(x9))) - x7)))) ,tanh(max((x5*x5*x5) , log(abs(x9))))) / ninteger(cos(x0))) + min((x6*x6*x6) , sqrt(abs(x9))))) , (avg(avg((tanh(x7)*tanh(x7)*tanh(x7)) , cos(abs(x1))) , min(avg((avg(log(abs(x3)) , x0)*avg(log(abs(x3)) , x0)*avg(log(abs(x3)) , x0)) , cos(x7)) , min(log(abs(x7)) , (1.0 / x5)))) + x8)) / avg(avg((x0*x0*x0) , x4) , ((sign((x8 + max(sign(x8) , (x9*x9*x9)))) + ((log(abs(((x8*x8*x8) - ninteger((max(sqrt(abs(x2)) , x0) - ninteger(x9)))))) + min((cos(x2) + (1.0 / x2)) , x1)) * max(log(abs((x5 * x3))) , x0))) + (log(abs(tanh(expt(abs(abs((sign(x7)*sign(x7)*sign(x7)))) ,log(abs((1.0 / x5))))))) + ninteger(max((min(avg(max(x4 , x9) , ((ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))*ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))*ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))) - ninteger(x9))) , avg(max(x4 , x9) , ((max(log(abs(max(x1 , ninteger(x9)))) , x0)*max(log(abs(max(x1 , ninteger(x9)))) , x0)*max(log(abs(max(x1 , ninteger(x9)))) , x0)) - ninteger(x9)))) / (1.0 / x5)) , avg(max(x4 , x9) , ((ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))*ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))*ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))) - ninteger(x9))))))))),avg((-x0) , (avg((-x7) , x0) / avg((-x0) , cos(x2)))),avg((x3*x3*x3) , (sign(x8) - avg(sign(avg(sign((x9 * (x1*x1))) , x0)) , x0))),avg(avg(avg(ninteger(x1) , (x3*x3)) , avg(sign(x2) , (((x7*x7*x7) * (x6 * x8)) + (cos(expt(abs(x9) ,x8)) % exp(ninteger(x4)))))) , avg(tanh(x3) , x8)));
myBestSelectorChampions[1],Score=[0.1836345725288,ErrorPct=[0.7797861414162],WFF= mvlregress(((((x9 * ((max(tanh(x0) , (sign(x2) + x0)) - max(tanh(x0) , (sign(x2) + x0)))*(max(tanh(x0) , (sign(x2) + x0)) - max(tanh(x0) , (sign(x2) + x0))))) - x9) - avg(sign(avg(sign((x9 * (x1*x1))) , x0)) , x0)) - avg(sign(x1) , x0)),(((x7*x7*x7) * (log(abs(x9)) * x8)) * (log(abs(x9)) * x8)),(max((x7*x7*x7) , x0) % exp(x6)),ninteger(expt(abs(exp(x6)) ,tanh(tanh(x0)))),((avg(ninteger(x4) , (avg(cos(x6) , x0)*avg(cos(x6) , x0)*avg(cos(x6) , x0))) % max(exp(x8) , (x5*x5*x5))) / cos(x6)),(expt(abs(min(max((max((x9 * (x1*x1)) , sqrt(abs(x5))) * (x1*x1)) , (x0 - x7)) , max((max((x9 * (x1*x1)) , sqrt(abs(x5))) * (x1*x1)) , (x0 - x7)))) ,tanh(max((x5*x5*x5) , log(abs(x9))))) / ninteger(((cos(expt(abs(x9) ,x8)) % exp(x1)) % (cos(x2) % exp(x1))))),(((avg(ninteger(x4) , (avg(cos(x6) , x0)*avg(cos(x6) , x0)*avg(cos(x6) , x0))) % max(exp(x8) , (x5*x5*x5))) / ((((x7*x7*x7) * (x6 * x8))*((x7*x7*x7) * (x6 * x8))*((x7*x7*x7) * (x6 * x8))) * (max((x9 * (x1*x1)) , (x0 - max(exp(x8) , (x5*x5*x5)))) * x8))) / expt(abs(x9) ,expt(abs(exp(x6)) ,tanh(x0)))),avg(ninteger(x4) , (max(avg(sqrt(abs(x5)) , x0) , (x5*x5*x5)) / expt(abs(x9) ,expt(abs(exp(x6)) ,tanh(x0))))),avg((min(cos(x0) , cos(x0)) / max((-x3) , x0)) , avg(sign(x2) , (((x7*x7*x7) * (x6 * x8)) + (cos(expt(abs(x9) ,x8)) % exp(ninteger(x4)))))),(avg(x4 , ninteger(x4)) + (expt(abs(avg((1.0 / x3) , x0)) ,sin(x3)) / (expt(abs(avg((1.0 / expt(abs(avg((1.0 / x3) , x0)) ,sin(x3))) , x0)) ,sin(x3)) / (avg(ninteger(x4) , (avg(cos(x6) , x0)*avg(cos(x6) , x0)*avg(cos(x6) , x0))) + sin((avg(x4 , ninteger(x4)) + (expt(abs(avg((1.0 / x3) , x0)) ,sin(x3)) / (expt(abs(avg((1.0 / expt(abs(avg((1.0 / x3) , x0)) ,sin(x3))) , x0)) ,sin(x3)) / (avg(ninteger(x4) , (avg(cos(x6) , x0)*avg(cos(x6) , x0)*avg(cos(x6) , x0))) + sin(min(expt(abs(x7) ,x8) , min(exp(x3) , cos(x0))))))))))))));
myBestSelectorChampions[2],Score=[0.245522826125,ErrorPct=[0.8628883330962],WFF= mvlregress((max(expt(abs(x3) ,log(abs(x1))) , (exp(x6) - sin(x8))) * cos(x1)),((max(avg((-ninteger(x0)) , x0) , x1) * (x6 * x8)) % x4),(log(abs(x8)) % (max((1.0 / log(abs(x8))) , x0) - (ninteger(x7) * (((x6*x6*x6) % exp(x8))*((x6*x6*x6) % exp(x8))*((x6*x6*x6) % exp(x8)))))),min(((max(tan(x2) , (abs(x9) % log(abs(x6)))) % x7) - (avg(sign(x8) , ninteger(x3)) - avg(sign(x8) , ninteger(x3)))) , ((max(tan(x2) , (abs(x9) % log(abs(((avg(ninteger(x7) , x0) * x3) * min(min(ninteger(x0) , exp(x0)) , exp(x0))))))) % x7) - min(max(expt(abs(x2) ,log(abs(x1))) , (exp(x6) - sin(x8))) , max(expt(abs(x2) ,log(abs(x1))) , (exp(x6) - sin(x8)))))),min(((avg(ninteger(x7) , max((log(abs(x3)) + (x8*x8*x8)) , (x1 / x5))) * x3) * min(min(ninteger(ninteger(x0)) , exp(x0)) , exp(x0))) , (ninteger((max(avg((-ninteger(x0)) , x0) , x1) * (x6 * x8))) - x4)),max(max(expt(abs(x7) ,x8) , avg(avg(sign(x8) , ninteger(x3)) , log(abs((log(abs(x8)) - sin(x8)))))) , max(max((expt(abs(x1) ,x4) % min(ninteger(x0) , exp(x0))) , expt(abs(x1) ,x4)) , expt(abs(x1) ,x4))),max(expt(abs(x7) ,x8) , ninteger((ninteger((avg(sign(x8) , ninteger(x3)) - avg(sign(x8) , ninteger(x3)))) - x4))),(x8 / max((log(abs(x3)) + (x8*x8*x8)) , max((log(abs(x3)) + (x8*x8*x8)) , (x1 / (if (x3 < (if (x1 < -4.303440480149) {4.890264304325} else {-2.761845454534})) {sin(2.857591494582)} else {(x5 + x0)}))))),avg((x4 - (ninteger(x7) * (((x6*x6*x6) % exp(x8))*((x6*x6*x6) % exp(x8))*((x6*x6*x6) % exp(x8))))) , expt(abs(x7) ,x8)),(avg(tanh(abs(x7)) , (avg(tanh(abs(x7)) , x0) + avg(max((log(abs(x8)) - sin(x8)) , (x5 % sign(x5))) , log(abs(x5))))) + (avg(tanh(abs(x7)) , x0) + avg(max((log(abs(x8)) - sin(x8)) , (x5 % sign(x5))) , log(abs(x5))))));
myBestSelectorChampions[3],Score=[0.2464420219617,ErrorPct=[0.8689475722515],WFF= mvlregress(((max(sign(x8) , (x9*x9*x9)) % max((1.0 / x7) , (x9*x9*x9))) - (((1.0 / x1) * sqrt(abs(x8))) - (avg(tanh(x2) , x0) + sign(x6)))),(ninteger(x2) * tanh(avg(((((1.0 / x1) * sqrt(abs(x8)))*((1.0 / x1) * sqrt(abs(x8)))*((1.0 / x1) * sqrt(abs(x8)))) - x5) , ((((1.0 / x1) * sqrt(abs(x8)))*((1.0 / x1) * sqrt(abs(x8)))*((1.0 / x1) * sqrt(abs(x8)))) - x5)))),min((1.0 / x3) , avg(x0 , x4)),min((1.0 / (log(abs(max(x0 , x1))) + sign(x6))) , (max((x5 * x3) , avg(x0 , x7)) * tanh(x4))),max((((1.0 / x1) * sqrt(abs(x8))) - (exp(x1) - abs(avg(x2 , x8)))) , (exp(x1) - abs(avg(x2 , x8)))),max((ninteger(x2) * x3) , avg(x0 , x7)),max(max((max(sign(x8) , (x9*x9*x9)) % max((1.0 / x7) , (x9*x9*x9))) , avg(x0 , x4)) , (((1.0 / x1) * sqrt(abs(x8))) - (avg(tanh(x2) , x0) + sign(x6)))),max(max((1.0 / (1.0 / x3)) , (x9*x9*x9)) , ((1.0 / x3) * x7)),max((x8*x8*x8) , sin(x0)),(avg(tanh(x2) , x0) + (max((x5 * x3) , avg(sign(x6) , x7)) * tanh(x4))));
myBestRegressorChampions
myBestRegressorChampions[0],ErrorPct=[0.7365682454002,Score=[0.1751680380524],WFF= mvlregress((((sign(x3) - x0) - ((((((-x6) * x8) * sqrt(abs(x4))) - (-sqrt(abs(x4)))) * x7) - max((-(1.0 / x5)) , x0))) - (1.0 / x9)),(((x8*x8*x8) * ((ninteger(((ninteger(x9)*ninteger(x9)*ninteger(x9)) - ninteger(x9))) * x7) + ((log(abs(((x8*x8*x8) - ninteger((max(sqrt(abs(x2)) , x0) - ninteger(x9)))))) + min((cos(x2) + (1.0 / x2)) , x1)) * max(log(abs((x5 * x3))) , x0)))) % min(abs(x5) , (((-x7) * x7) - (sign(x1) - x0)))),((max(max(((log(abs(x6)) - exp(x7)) * (ninteger(x2) - (x6 * x8))) , (min(abs(x5) , (x6 * x8)) - (log(abs(tanh(x7))) - ((x6 * x8) * (sign(x3) - (1.0 / x5)))))) , (1.0 / x1)) % ninteger(x8)) % exp(x6)),max(sqrt(abs((x0*x0*x0))) , x9),((avg(ninteger(x4) , (avg(cos(x6) , x0)*avg(cos(x6) , x0)*avg(cos(x6) , x0))) % (sign(x3) - x0)) / cos(x6)),(avg(exp(x1) , max((min(avg(max(x4 , x9) , ((ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))*ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))*ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))) - ninteger(x9))) , avg(max(x4 , x9) , ((max(log(abs(max(x1 , ninteger(x9)))) , x0)*max(log(abs(max(x1 , ninteger(x9)))) , x0)*max(log(abs(max(x1 , ninteger(x9)))) , x0)) - ninteger(x9)))) / (1.0 / x5)) , ((exp(x1) + (x0*x0*x0)) % avg(abs(x4) , x0)))) / avg((-x5) , x0)),(avg((((expt(abs(min(max((max((x9 * (x1*x1)) , sqrt(abs(x5))) * (x1*x1)) , (x0 - x7)) , max((max((x9 * (x1*x1)) , sqrt(abs(x5))) * (x1*x1)) , (avg(max(x4 , x9) , ((ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))*ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))*ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))) - ninteger(x9))) - x7)))) ,tanh(max((x5*x5*x5) , log(abs(x9))))) / ninteger(cos(x0))) + min((x6*x6*x6) , sqrt(abs(x9))))*((expt(abs(min(max((max((x9 * (x1*x1)) , sqrt(abs(x5))) * (x1*x1)) , (x0 - x7)) , max((max((x9 * (x1*x1)) , sqrt(abs(x5))) * (x1*x1)) , (avg(max(x4 , x9) , ((ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))*ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))*ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))) - ninteger(x9))) - x7)))) ,tanh(max((x5*x5*x5) , log(abs(x9))))) / ninteger(cos(x0))) + min((x6*x6*x6) , sqrt(abs(x9))))*((expt(abs(min(max((max((x9 * (x1*x1)) , sqrt(abs(x5))) * (x1*x1)) , (x0 - x7)) , max((max((x9 * (x1*x1)) , sqrt(abs(x5))) * (x1*x1)) , (avg(max(x4 , x9) , ((ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))*ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))*ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))) - ninteger(x9))) - x7)))) ,tanh(max((x5*x5*x5) , log(abs(x9))))) / ninteger(cos(x0))) + min((x6*x6*x6) , sqrt(abs(x9))))) , (avg(avg((tanh(x7)*tanh(x7)*tanh(x7)) , cos(abs(x1))) , min(avg((avg(log(abs(x3)) , x0)*avg(log(abs(x3)) , x0)*avg(log(abs(x3)) , x0)) , cos(x7)) , min(log(abs(x7)) , (1.0 / x5)))) + x8)) / avg(avg((x0*x0*x0) , x4) , ((sign((x8 + max(sign(x8) , (x9*x9*x9)))) + ((log(abs(((x8*x8*x8) - ninteger((max(sqrt(abs(x2)) , x0) - ninteger(x9)))))) + min((cos(x2) + (1.0 / x2)) , x1)) * max(log(abs((x5 * x3))) , x0))) + (log(abs(tanh(expt(abs(abs((sign(x7)*sign(x7)*sign(x7)))) ,log(abs((1.0 / x5))))))) + ninteger(max((min(avg(max(x4 , x9) , ((ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))*ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))*ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))) - ninteger(x9))) , avg(max(x4 , x9) , ((max(log(abs(max(x1 , ninteger(x9)))) , x0)*max(log(abs(max(x1 , ninteger(x9)))) , x0)*max(log(abs(max(x1 , ninteger(x9)))) , x0)) - ninteger(x9)))) / (1.0 / x5)) , avg(max(x4 , x9) , ((ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))*ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))*ninteger(expt(abs(cos(x0)) ,(1.0 / x8)))) - ninteger(x9))))))))),avg((-x0) , (avg((-x7) , x0) / avg((-x0) , cos(x2)))),avg((x3*x3*x3) , (sign(x8) - avg(sign(avg(sign((x9 * (x1*x1))) , x0)) , x0))),avg(avg(avg(ninteger(x1) , (x3*x3)) , avg(sign(x2) , (((x7*x7*x7) * (x6 * x8)) + (cos(expt(abs(x9) ,x8)) % exp(ninteger(x4)))))) , avg(tanh(x3) , x8)));
myBestRegressorChampions[1],ErrorPct=[0.7587289688422,Score=[0.1879942999537],WFF= mvlregress((((x9 * ((max(tanh(x0) , (sign(x2) + x0)) - max(tanh(x0) , (sign(x2) + x0)))*(max(tanh(x0) , (sign(x2) + x0)) - max(tanh(x0) , (sign(x2) + x0))))) - log(abs(x9))) - avg(sign(avg(sign((x9 * (x1*x1))) , x0)) , x0)),(expt(abs(min(cos(x6) , max((max((x9 * (x1*x1)) , sqrt(abs(x5))) * (x1*x1)) , (x0 - x7)))) ,tanh(max((x5*x5*x5) , log(abs(x9))))) * ((max((x7*x7*x7) , x0) % exp(x6)) / (max(sqrt(abs(x2)) , x0) % exp(x6)))),((x7*x7*x7) * (log(abs(expt(abs(min(max((max((x9 * (x1*x1)) , sqrt(abs(x5))) * (x1*x1)) , (x0 - x7)) , max((max((x9 * (x1*x1)) , sqrt(abs(x5))) * (x1*x1)) , (x0 - x7)))) ,tanh(max((x5*x5*x5) , log(abs(x9))))))) * x8)),((max(sqrt(abs(x2)) , x0) % exp(x6)) % ((sign((x9 * (x1*x1)))*sign((x9 * (x1*x1)))*sign((x9 * (x1*x1)))) * (x6 * x8))),max(avg((min(cos(x0) , cos(x0)) / max((-x3) , x0)) , min(expt(abs(x7) ,x8) , cos(expt(abs((cos(expt(abs(x9) ,cos(expt(abs(x9) ,x8)))) % cos(x0))) ,x8)))) , expt(abs(exp(x6)) ,tanh(tanh(x0)))),ninteger((max((avg(x4 , ninteger(x4)) + (expt(abs(avg((1.0 / x3) , x0)) ,sin(x3)) / (expt(abs(avg((1.0 / expt(abs(avg((1.0 / x3) , x0)) ,sin(x3))) , x0)) ,sin(x3)) / (avg(ninteger(x4) , (avg(cos(x6) , x0)*avg(cos(x6) , x0)*avg(cos(x6) , x0))) + sin((avg(x4 , ninteger(x4)) + (expt(abs(avg((1.0 / x3) , x0)) ,sin(x3)) / (expt(abs(avg((1.0 / expt(abs(avg((1.0 / x3) , x0)) ,sin(x3))) , x0)) ,sin(x3)) / (avg(ninteger(x4) , (avg(cos(x6) , x0)*avg(cos(x6) , x0)*avg(cos(x6) , x0))) + sin(min(expt(abs(x7) ,x8) , min(exp(x3) , cos(x0))))))))))))) , (((x9 * ((max(tanh(x0) , (sign(x2) + x0)) - max(tanh(x0) , (sign(x2) + x0)))*(max(tanh(x0) , (sign(x2) + x0)) - max(tanh(x0) , (sign(x2) + x0))))) - x9) - avg(sign(x1) , x0))) % (cos(x2) % exp(x1)))),((avg(ninteger(x4) , (avg(cos(x6) , x0)*avg(cos(x6) , x0)*avg(cos(x6) , x0))) % max(exp(x8) , (x5*x5*x5))) / ((((x7*x7*x7) * (x6 * x8))*((x7*x7*x7) * (x6 * x8))*((x7*x7*x7) * (x6 * x8))) * exp(x8))),(max(avg(sqrt(abs(x5)) , x0) , (x5*x5*x5)) / expt(abs(x9) ,expt(abs(exp(expt(abs(expt(abs(x9) ,expt(abs(exp(x6)) ,tanh(x0)))) ,expt(abs(exp(x6)) ,tanh(x0))))) ,tanh(x0)))),avg(avg(ninteger(x4) , exp(ninteger(x4))) , (((x7*x7*x7) * (x6 * x8)) + (cos(expt(abs(x9) ,x8)) % exp(ninteger(x4))))),(avg(avg((min(cos(x0) , cos(x0)) / max((-x3) , x0)) , avg(sign(x2) , (((x7*x7*x7) * (x6 * x8)) + (cos(expt(abs(x9) ,x8)) % exp(ninteger(avg((min(cos(x0) , cos(x0)) / max((-x3) , x0)) , min(expt(abs(x7) ,x8) , cos(expt(abs(x7) ,x8)))))))))) , ninteger(x4)) + (expt(abs(avg((1.0 / x3) , x0)) ,sin(x3)) / (expt(abs(avg((1.0 / expt(abs(avg((1.0 / x3) , x0)) ,sin(x3))) , x0)) ,sin(x3)) / (avg(ninteger(x4) , (avg(cos(x6) , x0)*avg(cos(x6) , x0)*avg(cos(x6) , x0))) + sin(min(expt(abs(x7) ,x8) , min(exp(x3) , cos(x0)))))))));
myBestRegressorChampions[2],ErrorPct=[0.83699572824,Score=[0.2483217765388],WFF= mvlregress((((avg(ninteger(x7) , x0) * x3) * ninteger(ninteger(x7))) - (avg(sign(x8) , ninteger(x3)) - avg(sign(x8) , ninteger(x3)))),((max(tan(x2) , (abs(x9) % log(abs(x6)))) % x7) - (((avg(ninteger(x7) , max((log(abs(x3)) + (x8*x8*x8)) , (x1 / x5))) * x3) * min(min(ninteger(ninteger(x0)) , exp(x0)) , exp(x0))) * avg((x4 - (ninteger(x7) * (((x6*x6*x6) % exp(x8))*((x6*x6*x6) % exp(x8))*((x6*x6*x6) % exp(x8))))) , expt(abs(x7) ,x8)))),(((avg(ninteger(x7) , max((log(abs(x3)) + (x8*x8*x8)) , (x1 / x5))) * x3) * min(min(ninteger(x0) , exp(x0)) , exp((max(expt(abs(x7) ,x8) , avg(avg(sign(x8) , ninteger(x3)) , log(abs(x5)))) * (ninteger(ninteger(x7)) * (((x6*x6*x6) % exp(x8))*((x6*x6*x6) % exp(x8))*((x6*x6*x6) % exp(x8)))))))) * (max((log(abs(x8)) - sin(x8)) , (log(abs(x5)) % sign(x5))) - x4)),max((ninteger((avg(sign(x8) , ninteger(x3)) - avg(sign(x8) , ninteger(x3)))) - x4) , (exp(x0) % min(ninteger(x0) , exp((ninteger((max(avg((-ninteger(x0)) , x0) , x1) * (x6 * x8))) - x4))))),max(max(expt(abs(x7) ,x8) , avg(avg(sign(x8) , ninteger(x3)) , log(abs((log(abs(x8)) - sin(x8)))))) , max(max((expt(abs(x1) ,x4) % min(ninteger(x0) , exp(x0))) , expt(abs(x1) ,x4)) , expt(abs(x1) ,x4))),(max(expt(abs(x7) ,x8) , avg(avg(sign((ninteger(ninteger(x7)) * (((x6*x6*x6) % exp(x8))*((x6*x6*x6) % exp(x8))*((x6*x6*x6) % exp(x8))))) , ninteger(x3)) , log(abs((log(abs(x8)) - sin(x8)))))) / ((x1 / max((log(abs(x3)) + (x8*x8*x8)) , max((log(abs(x3)) + (x8*x8*x8)) , (x1 / (if (x3 < (if (x1 < -4.303440480149) {4.890264304325} else {-2.761845454534})) {sin(2.857591494582)} else {(x5 + x0)}))))) * (x8 / log(abs(x3))))),avg((((avg(ninteger(x7) , x0) * x3) * max((log(abs(x3)) + (x8*x8*x8)) , max((log(abs(x3)) + (x8*x8*x8)) , (x1 / (if (x3 < (if (x1 < -4.303440480149) {4.890264304325} else {-2.761845454534})) {sin(2.857591494582)} else {(x5 + x0)}))))) - (ninteger(x7) * (((x6*x6*x6) % exp(x8))*((x6*x6*x6) % exp(x8))*((x6*x6*x6) % exp(x8))))) , (max(log(abs(x8)) , ninteger(x7)) - cos(log(abs(x8))))),avg(((((max(tan(x2) , (abs(x9) % log(abs(x6)))) % x7) - (avg(sign(x8) , ninteger(x3)) - avg(sign(x8) , ninteger(x3)))) / (max(expt(abs(x3) ,log(abs(x1))) , (exp(x6) - sin(x8))) * (max(tan(x2) , (abs(x9) % log(abs(x6)))) % x7))) % min(ninteger(x0) , exp(x0))) , (max(avg((-x9) , x0) , x1) * (x6 * x8))),avg(avg(tanh(abs(x7)) , x0) , ((max(tan(x2) , (abs(x9) % log(abs(x6)))) % sin(x8)) / min(log(abs(x3)) , max(expt(abs(x2) ,log(abs(x1))) , (exp(x6) - sin(x8)))))),(avg(tanh(abs(x7)) , x0) + avg((avg(tanh(abs(x7)) , x0) + avg(max((log(abs(x8)) - sin(x8)) , (x5 % sign(x5))) , log(abs(avg(max((log(abs(x8)) - sin(x8)) , (x5 % sign(x5))) , (exp(x1) / x0)))))) , avg(tanh(abs(sign(x5))) , x0))));
myBestRegressorChampions[3],ErrorPct=[0.8638621851202,Score=[0.2522138860794],WFF= mvlregress((((1.0 / x1) * sqrt(abs(x8))) - (avg(tanh(x2) , x0) + sign(x6))),(exp(x1) - abs(avg(x2 , x8))),((((1.0 / x1) * sqrt(abs(x8)))*((1.0 / x1) * sqrt(abs(x8)))*((1.0 / x1) * sqrt(abs(x8)))) - x5),(max((x5 * x3) , avg(x0 , x7)) * tanh(x4)),((1.0 / x3) * x7),(ninteger(x2) * x3),(max(sign(x8) , (x9*x9*x9)) % max((1.0 / x7) , (x9*x9*x9))),min((1.0 / x3) , avg(x0 , x4)),max((x8*x8*x8) , sin(x0)),(log(abs(max(x0 , x1))) + sign(x6)));

Show final results on training data, Score=[0.1751680380524], ErrorPct=[0.7365682454002]
X[0], ey=[-12972.96572878], y=[-9327848.03011], ErrorPct=[5.424911431422]
X[50], ey=[-1097986.980045], y=[-3740383.407601], ErrorPct=[1.538911309827]
X[100], ey=[-1091026.803614], y=[-2748608.694192], ErrorPct=[0.9653629151824]
X[150], ey=[571771.853714], y=[-2135383.822265], ErrorPct=[1.576626596895]
X[200], ey=[-1092248.71621], y=[-1710846.004311], ErrorPct=[0.3602662919759]
X[250], ey=[-3414459.029357], y=[-1362747.922949], ErrorPct=[1.194900732237]
X[300], ey=[320805.9871631], y=[-1016051.643005], ErrorPct=[0.7785755783044]
X[350], ey=[-876407.3886317], y=[-744398.886778], ErrorPct=[0.07688073385864]
X[400], ey=[-87816.33517873], y=[-498721.4795291], ErrorPct=[0.2393079885032]
X[450], ey=[-58700.49543393], y=[-275472.3775958], ErrorPct=[0.1262462731301]
X[500], ey=[-221534.4084221], y=[-71169.29039405], ErrorPct=[0.08757148561191]
X[550], ey=[636458.8119944], y=[123022.3131536], ErrorPct=[0.2990214589696]
X[600], ey=[-611010.7017671], y=[353190.4073948], ErrorPct=[0.5615432931874]
X[650], ey=[1100422.426982], y=[566364.1809499], ErrorPct=[0.3110314055659]
X[700], ey=[847236.2459651], y=[830605.2372171], ErrorPct=[0.009685771290452]
X[750], ey=[-123923.5853622], y=[1128804.266495], ErrorPct=[0.7295790439519]
X[800], ey=[756181.9915889], y=[1600383.076346], ErrorPct=[0.4916562040249]
X[850], ey=[243127.2051513], y=[2100721.076953], ErrorPct=[1.081848351205]
X[900], ey=[375854.6866819], y=[2824996.584045], ErrorPct=[1.426361361194]
X[950], ey=[418044.8006312], y=[3967898.993502], ErrorPct=[2.067407716978]
Actual computed error on training data is ErrorPct=[0.7365682454002] versus reported ErrorPct=[0.7365682454002] while average Y is AvgY=[-41888.88936835]
yHistory=[#(num| -4208214.422889 -2208265.076842 -1345133.751529 -740653.1693255 -291032.1101871 121608.758834 560424.0186039 1155485.713095 2117709.186045 4419181.960511 )]
eHistory=[#(num| -2764568.991009 -1389127.590774 -827053.528056 -666058.2730748 -344647.0036841 26839.05167246 346786.5277398 976452.0778353 1283458.707439 2939030.128227 )]
aHistory=[#(num| -2764568.991009 -2076848.290891 -1660250.036613 -1411702.095728 -1198291.077319 1114513.298583 1386431.86031 1732980.3045 2111244.417833 2939030.128227 )]
dHistory=[#(num| 2312804.375902 2798133.956039 3393230.341113 4188092.708724 5703599.119236 )]


Final testing on test data returns Score=[0.1841751518277], ErrorPct=[0.8149881663065]
X[0], ey=[-1158543.460565], y=[-8157851.626121], ErrorPct=[4.240228424234]
X[50], ey=[-6969.728989869], y=[-3367988.182219], ErrorPct=[2.036127806158]
X[100], ey=[-912151.7706323], y=[-2562380.467859], ErrorPct=[0.9997197527179]
X[150], ey=[-1114371.705529], y=[-1976912.778106], ErrorPct=[0.5225332399294]
X[200], ey=[779120.2776129], y=[-1498449.335949], ErrorPct=[1.379767140576]
X[250], ey=[-2982533.466233], y=[-1119689.568157], ErrorPct=[1.128523485421]
X[300], ey=[-566681.6080367], y=[-837145.8338154], ErrorPct=[0.1638490649017]
X[350], ey=[-246074.3824543], y=[-554874.6239524], ErrorPct=[0.187073283593]
X[400], ey=[-563227.8432532], y=[-343870.3259071], ErrorPct=[0.1328882738294]
X[450], ey=[-273765.8303856], y=[-104897.5504706], ErrorPct=[0.1023015508833]
X[500], ey=[59289.2464575], y=[125955.9840453], ErrorPct=[0.0403871624144]
X[550], ey=[497074.2861934], y=[317131.1497907], ErrorPct=[0.1090107741612]
X[600], ey=[664107.7383912], y=[561715.7029299], ErrorPct=[0.06202979050341]
X[650], ey=[1168774.443146], y=[807741.0534895], ErrorPct=[0.2187164795017]
X[700], ey=[594331.6553464], y=[1020233.764908], ErrorPct=[0.2580143905926]
X[750], ey=[1240979.21659], y=[1354506.341669], ErrorPct=[0.06877550342046]
X[800], ey=[75233.06829447], y=[1747735.227878], ErrorPct=[1.01321316749]
X[850], ey=[411009.302136], y=[2233450.702542], ErrorPct=[1.10404737793]
X[900], ey=[1073725.20794], y=[2815955.068832], ErrorPct=[1.055454682515]
X[950], ey=[1648461.965764], y=[3781669.213584], ErrorPct=[1.292311438936]
Actual computed error on testing data is ErrorPct=[0.8149881663065], Avg Y=[146544.142109], AvgDev Y=[1650691.298977]
yHistory=[#(num| -3712137.275662 -1986750.565584 -1145440.851725 -566409.727065 -109561.366964 320512.5631229 799473.8415728 1355075.542192 2245436.863778 4265242.397425 )]
eHistory=[#(num| -2143527.08703 -1499736.167008 -631866.7255299 -515281.0451304 -185366.0519137 645035.8799369 475631.9572302 1245808.460335 1535023.514622 2539718.685578 )]
aHistory=[#(num| -2143527.08703 -1821631.627019 -1425043.326523 -1197602.756175 -995155.4153224 1288243.69954 1449045.654441 1773516.886845 2037371.1001 2539718.685578 )]
dHistory=[#(num| 2283399.114863 2646648.410616 3198560.213368 3859002.727119 4683245.772609 )]

esm.selfTest[crossCorrelation]: completed in [97.62943333333] minutes.

Starting test case: cubicRegression
Building test data as: y = -13.99864194741 - (13.99864194741*x0*x0*x0) - (32.66789144459*x1*x1*x1) - (30.73171497235*x2*x2*x2) + (10.01135458484*x3*x3*x3) - (2.83159613203*x4*x4*x4) + (5.499486281783*x5*x5*x5) - (46.33521181329*x6*x6*x6) + (10.35455431571*x7*x7*x7) + (18.14155680594*x8*x8*x8) - (3.363675194861*x9*x9*x9);
...starting new evolutionary process.
...starting generation of constant selector WFF.
...starting regress selector for each column name.
...starting a best-of-breed MVL selector WFF.
...starting a best-of-breed REG selector WFF.
...starting generation of root selector WFFs in each column island.
...starting generation of random selector WFFs.

Final results of training
esm: N = [1000], M = [10], Generations = [200], WFFs = [29987], Score=[0.01018869110383], ScoreHistory=[#(num| 0.01018869110383 )]
esm.myBest, Score=[0.01018869110383], ErrorPct=[0.1939311955088], lengthWFF=[3990], WFF = mvlregress((((cos(x5) - avg((-x3) , x0)) - cos(cos((-x3)))) - avg((-x3) , x0)),(((avg(tanh(((avg(sin(ninteger(x7)) , (x6*x6*x6)) * expt(abs((1.0 / x6)) ,cos(x2))) / (abs(ninteger(x8)) * x6))) , x6) - (x9 - ((x1*x1*x1) - (x9 - ((x1*x1*x1) - avg((-sin(x9)) , (((-(x1*x1*x1)) - x1) - x1))))))) - max((1.0 / x8) , x6)) - (x2*x2*x2)),((avg(tanh(((avg(sin(ninteger(x7)) , (x6*x6*x6)) * expt(abs((1.0 / x6)) ,cos(x2))) / (abs(ninteger(x8)) * x6))) , x6) - (x9 - ((x1*x1*x1) - (x9 - ((x1*x1*x1) - avg((-sin(x9)) , (((-(x1*x1*x1)) - x1) - x1))))))) - (((x8*x8*x8) - ((x0 + ((x6*x6*x6) + (x2*x2*x2))) - x9)) + avg(x8 , x2))),(min(avg(min(x8 , x5) , (x1*x1)) , cos(x5)) - avg(tan(log(abs(x8))) , avg(avg(avg(cos(tanh(avg(exp(x6) , (1.0 / x4)))) , x2) , avg((avg(sqrt(abs(x7)) , x2)*avg(sqrt(abs(x7)) , x2)*avg(sqrt(abs(x7)) , x2)) , (x1*x1))) , avg(sin(x7) , (ninteger(x0) + avg((x6*x6*x6) , avg((1.0 / x0) , x6))))))),(min(avg(min(x8 , x5) , avg((-x3) , ((1.0 / ninteger(x1)) + (x4*x4*x4)))) , (avg((avg((x6*x6*x6) , x1) - max((1.0 / x8) , x6)) , x1) - max((1.0 / x8) , x6))) - min((1.0 / (max(avg(max(x5 , avg((x6*x6*x6) , x5)) , (x4*x4*x4)) , min(x4 , x4)) - ((x1*x1*x1) - avg((-sin(x9)) , (((-(x1*x1*x1)) - x1) - x1))))) , (min(abs(x0) , x6) % exp((x6 % min(log(abs(x2)) , sqrt(abs(x9)))))))),((x6*x6*x6) - max((1.0 / x8) , x6)),min((1.0 / (max(avg(max(x5 , avg((x6*x6*x6) , x5)) , (x4*x4*x4)) , min(x4 , x4)) - ((x1*x1*x1) - avg((-sin(x9)) , (((-(x1*x1*x1)) - x1) - x1))))) , (min(abs(x0) , x6) % exp((x6 % min(log(abs(x2)) , sqrt(abs(x9))))))),min((avg((cos(((x6*x6*x6) + (x2*x2*x2))) + avg(cos(x4) , x2)) , avg((cos(x3) + max((1.0 / x8) , x6)) , (cos(min(avg(min(x8 , x5) , avg((-x3) , ((1.0 / ninteger(x1)) + (x4*x4*x4)))) , (avg((avg((x6*x6*x6) , x1) - max((1.0 / x8) , x6)) , x1) - max((1.0 / x8) , x6)))) + (x0*x0)))) + (cos(x3) + (x0*x0))) , max(avg((x6*x6*x6) , (x4*x4*x4)) , max(avg(max(x5 , avg((x6*x6*x6) , x5)) , (x4*x4*x4)) , min(x4 , x4)))),max(max(avg(max(x5 , avg((x6*x6*x6) , x5)) , ((1.0 / x0)*(1.0 / x0)*(1.0 / x0))) , max(avg(max(x5 , avg((x6*x6*x6) , x5)) , (x4*x4*x4)) , min(x4 , avg((-x3) , max(expt(abs(cos(x5)) ,(1.0 / x7)) , expt(abs(log(abs(x4))) ,ninteger(min(x8 , (((((x4*x4*x4) % exp(x9))*((x4*x4*x4) % exp(x9))*((x4*x4*x4) % exp(x9))) % exp(x9)) % exp(x9)))))))))) , max(avg((min((x8*x8*x8) , min(x4 , ninteger(x8)))*min((x8*x8*x8) , min(x4 , ninteger(x8)))*min((x8*x8*x8) , min(x4 , ninteger(x8)))) , (x4*x4*x4)) , max(avg(max(x5 , avg((x6*x6*x6) , x5)) , ((if (x5 >= tanh(-2.534504273257)) {(if (x0 < -3.109395818684) {-0.190022410201} else {-3.640206438313})} else {(if (x8 <= x5) {-3.56291172268} else {x9})})*(if (x5 >= tanh(-2.534504273257)) {(if (x0 < -3.109395818684) {-0.190022410201} else {-3.640206438313})} else {(if (x8 <= x5) {-3.56291172268} else {x9})})*(if (x5 >= tanh(-2.534504273257)) {(if (x0 < -3.109395818684) {-0.190022410201} else {-3.640206438313})} else {(if (x8 <= x5) {-3.56291172268} else {x9})}))) , min(x4 , x4)))),avg((((x8*x8*x8) - ((x0 + ((x6*x6*x6) + (x2*x2*x2))) - x9)) + avg(x8 , x2)) , (x3 - min(x4 , (-x6)))));
esm.myBestRegressor, Score=[0.01018869110383], ErrorPct=[0.1939311955088], lengthWFF=[3990], WFF = mvlregress((((cos(x5) - avg((-x3) , x0)) - cos(cos((-x3)))) - avg((-x3) , x0)),(((avg(tanh(((avg(sin(ninteger(x7)) , (x6*x6*x6)) * expt(abs((1.0 / x6)) ,cos(x2))) / (abs(ninteger(x8)) * x6))) , x6) - (x9 - ((x1*x1*x1) - (x9 - ((x1*x1*x1) - avg((-sin(x9)) , (((-(x1*x1*x1)) - x1) - x1))))))) - max((1.0 / x8) , x6)) - (x2*x2*x2)),((avg(tanh(((avg(sin(ninteger(x7)) , (x6*x6*x6)) * expt(abs((1.0 / x6)) ,cos(x2))) / (abs(ninteger(x8)) * x6))) , x6) - (x9 - ((x1*x1*x1) - (x9 - ((x1*x1*x1) - avg((-sin(x9)) , (((-(x1*x1*x1)) - x1) - x1))))))) - (((x8*x8*x8) - ((x0 + ((x6*x6*x6) + (x2*x2*x2))) - x9)) + avg(x8 , x2))),(min(avg(min(x8 , x5) , (x1*x1)) , cos(x5)) - avg(tan(log(abs(x8))) , avg(avg(avg(cos(tanh(avg(exp(x6) , (1.0 / x4)))) , x2) , avg((avg(sqrt(abs(x7)) , x2)*avg(sqrt(abs(x7)) , x2)*avg(sqrt(abs(x7)) , x2)) , (x1*x1))) , avg(sin(x7) , (ninteger(x0) + avg((x6*x6*x6) , avg((1.0 / x0) , x6))))))),(min(avg(min(x8 , x5) , avg((-x3) , ((1.0 / ninteger(x1)) + (x4*x4*x4)))) , (avg((avg((x6*x6*x6) , x1) - max((1.0 / x8) , x6)) , x1) - max((1.0 / x8) , x6))) - min((1.0 / (max(avg(max(x5 , avg((x6*x6*x6) , x5)) , (x4*x4*x4)) , min(x4 , x4)) - ((x1*x1*x1) - avg((-sin(x9)) , (((-(x1*x1*x1)) - x1) - x1))))) , (min(abs(x0) , x6) % exp((x6 % min(log(abs(x2)) , sqrt(abs(x9)))))))),((x6*x6*x6) - max((1.0 / x8) , x6)),min((1.0 / (max(avg(max(x5 , avg((x6*x6*x6) , x5)) , (x4*x4*x4)) , min(x4 , x4)) - ((x1*x1*x1) - avg((-sin(x9)) , (((-(x1*x1*x1)) - x1) - x1))))) , (min(abs(x0) , x6) % exp((x6 % min(log(abs(x2)) , sqrt(abs(x9))))))),min((avg((cos(((x6*x6*x6) + (x2*x2*x2))) + avg(cos(x4) , x2)) , avg((cos(x3) + max((1.0 / x8) , x6)) , (cos(min(avg(min(x8 , x5) , avg((-x3) , ((1.0 / ninteger(x1)) + (x4*x4*x4)))) , (avg((avg((x6*x6*x6) , x1) - max((1.0 / x8) , x6)) , x1) - max((1.0 / x8) , x6)))) + (x0*x0)))) + (cos(x3) + (x0*x0))) , max(avg((x6*x6*x6) , (x4*x4*x4)) , max(avg(max(x5 , avg((x6*x6*x6) , x5)) , (x4*x4*x4)) , min(x4 , x4)))),max(max(avg(max(x5 , avg((x6*x6*x6) , x5)) , ((1.0 / x0)*(1.0 / x0)*(1.0 / x0))) , max(avg(max(x5 , avg((x6*x6*x6) , x5)) , (x4*x4*x4)) , min(x4 , avg((-x3) , max(expt(abs(cos(x5)) ,(1.0 / x7)) , expt(abs(log(abs(x4))) ,ninteger(min(x8 , (((((x4*x4*x4) % exp(x9))*((x4*x4*x4) % exp(x9))*((x4*x4*x4) % exp(x9))) % exp(x9)) % exp(x9)))))))))) , max(avg((min((x8*x8*x8) , min(x4 , ninteger(x8)))*min((x8*x8*x8) , min(x4 , ninteger(x8)))*min((x8*x8*x8) , min(x4 , ninteger(x8)))) , (x4*x4*x4)) , max(avg(max(x5 , avg((x6*x6*x6) , x5)) , ((if (x5 >= tanh(-2.534504273257)) {(if (x0 < -3.109395818684) {-0.190022410201} else {-3.640206438313})} else {(if (x8 <= x5) {-3.56291172268} else {x9})})*(if (x5 >= tanh(-2.534504273257)) {(if (x0 < -3.109395818684) {-0.190022410201} else {-3.640206438313})} else {(if (x8 <= x5) {-3.56291172268} else {x9})})*(if (x5 >= tanh(-2.534504273257)) {(if (x0 < -3.109395818684) {-0.190022410201} else {-3.640206438313})} else {(if (x8 <= x5) {-3.56291172268} else {x9})}))) , min(x4 , x4)))),avg((((x8*x8*x8) - ((x0 + ((x6*x6*x6) + (x2*x2*x2))) - x9)) + avg(x8 , x2)) , (x3 - min(x4 , (-x6)))));
myBestSelectorChampions
myBestSelectorChampions[0],Score=[0.01018869110383,ErrorPct=[0.1939311955088],WFF= mvlregress((((cos(x5) - avg((-x3) , x0)) - cos(cos((-x3)))) - avg((-x3) , x0)),(((avg(tanh(((avg(sin(ninteger(x7)) , (x6*x6*x6)) * expt(abs((1.0 / x6)) ,cos(x2))) / (abs(ninteger(x8)) * x6))) , x6) - (x9 - ((x1*x1*x1) - (x9 - ((x1*x1*x1) - avg((-sin(x9)) , (((-(x1*x1*x1)) - x1) - x1))))))) - max((1.0 / x8) , x6)) - (x2*x2*x2)),((avg(tanh(((avg(sin(ninteger(x7)) , (x6*x6*x6)) * expt(abs((1.0 / x6)) ,cos(x2))) / (abs(ninteger(x8)) * x6))) , x6) - (x9 - ((x1*x1*x1) - (x9 - ((x1*x1*x1) - avg((-sin(x9)) , (((-(x1*x1*x1)) - x1) - x1))))))) - (((x8*x8*x8) - ((x0 + ((x6*x6*x6) + (x2*x2*x2))) - x9)) + avg(x8 , x2))),(min(avg(min(x8 , x5) , (x1*x1)) , cos(x5)) - avg(tan(log(abs(x8))) , avg(avg(avg(cos(tanh(avg(exp(x6) , (1.0 / x4)))) , x2) , avg((avg(sqrt(abs(x7)) , x2)*avg(sqrt(abs(x7)) , x2)*avg(sqrt(abs(x7)) , x2)) , (x1*x1))) , avg(sin(x7) , (ninteger(x0) + avg((x6*x6*x6) , avg((1.0 / x0) , x6))))))),(min(avg(min(x8 , x5) , avg((-x3) , ((1.0 / ninteger(x1)) + (x4*x4*x4)))) , (avg((avg((x6*x6*x6) , x1) - max((1.0 / x8) , x6)) , x1) - max((1.0 / x8) , x6))) - min((1.0 / (max(avg(max(x5 , avg((x6*x6*x6) , x5)) , (x4*x4*x4)) , min(x4 , x4)) - ((x1*x1*x1) - avg((-sin(x9)) , (((-(x1*x1*x1)) - x1) - x1))))) , (min(abs(x0) , x6) % exp((x6 % min(log(abs(x2)) , sqrt(abs(x9)))))))),((x6*x6*x6) - max((1.0 / x8) , x6)),min((1.0 / (max(avg(max(x5 , avg((x6*x6*x6) , x5)) , (x4*x4*x4)) , min(x4 , x4)) - ((x1*x1*x1) - avg((-sin(x9)) , (((-(x1*x1*x1)) - x1) - x1))))) , (min(abs(x0) , x6) % exp((x6 % min(log(abs(x2)) , sqrt(abs(x9))))))),min((avg((cos(((x6*x6*x6) + (x2*x2*x2))) + avg(cos(x4) , x2)) , avg((cos(x3) + max((1.0 / x8) , x6)) , (cos(min(avg(min(x8 , x5) , avg((-x3) , ((1.0 / ninteger(x1)) + (x4*x4*x4)))) , (avg((avg((x6*x6*x6) , x1) - max((1.0 / x8) , x6)) , x1) - max((1.0 / x8) , x6)))) + (x0*x0)))) + (cos(x3) + (x0*x0))) , max(avg((x6*x6*x6) , (x4*x4*x4)) , max(avg(max(x5 , avg((x6*x6*x6) , x5)) , (x4*x4*x4)) , min(x4 , x4)))),max(max(avg(max(x5 , avg((x6*x6*x6) , x5)) , ((1.0 / x0)*(1.0 / x0)*(1.0 / x0))) , max(avg(max(x5 , avg((x6*x6*x6) , x5)) , (x4*x4*x4)) , min(x4 , avg((-x3) , max(expt(abs(cos(x5)) ,(1.0 / x7)) , expt(abs(log(abs(x4))) ,ninteger(min(x8 , (((((x4*x4*x4) % exp(x9))*((x4*x4*x4) % exp(x9))*((x4*x4*x4) % exp(x9))) % exp(x9)) % exp(x9)))))))))) , max(avg((min((x8*x8*x8) , min(x4 , ninteger(x8)))*min((x8*x8*x8) , min(x4 , ninteger(x8)))*min((x8*x8*x8) , min(x4 , ninteger(x8)))) , (x4*x4*x4)) , max(avg(max(x5 , avg((x6*x6*x6) , x5)) , ((if (x5 >= tanh(-2.534504273257)) {(if (x0 < -3.109395818684) {-0.190022410201} else {-3.640206438313})} else {(if (x8 <= x5) {-3.56291172268} else {x9})})*(if (x5 >= tanh(-2.534504273257)) {(if (x0 < -3.109395818684) {-0.190022410201} else {-3.640206438313})} else {(if (x8 <= x5) {-3.56291172268} else {x9})})*(if (x5 >= tanh(-2.534504273257)) {(if (x0 < -3.109395818684) {-0.190022410201} else {-3.640206438313})} else {(if (x8 <= x5) {-3.56291172268} else {x9})}))) , min(x4 , x4)))),avg((((x8*x8*x8) - ((x0 + ((x6*x6*x6) + (x2*x2*x2))) - x9)) + avg(x8 , x2)) , (x3 - min(x4 , (-x6)))));
myBestSelectorChampions[1],Score=[0.01031723694115,ErrorPct=[0.1957289823962],WFF= mvlregress((((cos(x5) - avg((-x3) , x0)) - cos(cos(x5))) - avg((-x3) , x0)),(((avg(tanh(((avg(sin(ninteger(x7)) , (x6*x6*x6)) * expt(abs((1.0 / x6)) ,cos(x2))) / (abs(ninteger(x8)) * x6))) , x6) - (x9 - ((x1*x1*x1) - (x9 - ((x1*x1*x1) - avg((-sin(x9)) , (((-(x1*x1*x1)) - x1) - x1))))))) - max((1.0 / x8) , x6)) - ((x0 + ((x6*x6*x6) + (x2*x2*x2))) - x9)),((avg(tanh(((avg(sin(ninteger(x7)) , (x6*x6*x6)) * expt(abs((1.0 / x6)) ,cos(x2))) / (abs(ninteger(x8)) * x6))) , x6) - (x9 - ((x1*x1*x1) - (x9 - ((x1*x1*x1) - avg((-sin(x9)) , (((-(x1*x1*x1)) - x1) - x1))))))) - (((x8*x8*x8) - ((x0 + ((x6*x6*x6) + (x2*x2*x2))) - x9)) + avg(x8 , x2))),(min((1.0 / x8) , (min(abs(x0) , x6) % exp((x6 % min(log(abs(x2)) , sqrt(abs(x9))))))) - ((min(abs(x0) , x6) % exp(((((x4*x4*x4) % exp(x9))*((x4*x4*x4) % exp(x9))*((x4*x4*x4) % exp(x9))) % cos(x5)))) - ((x1*x1*x1) - avg((-sin(x9)) , (((-(x1*x1*x1)) - x1) - x1))))),(min(avg(min(x8 , x5) , (x1*x1)) , cos(x5)) - ((x6*x6*x6) - max((1.0 / x8) , x6))),(max(avg(max(x5 , avg((x6*x6*x6) , x5)) , (x4*x4*x4)) , min(x4 , x4)) - (max((1.0 / x8) , x6) - avg((-sin(x9)) , (((-(x1*x1*x1)) - x1) - x1)))),(avg((avg((x6*x6*x6) , x1) - max((1.0 / x8) , x6)) , x1) - min((1.0 / (max(avg(max(x5 , avg((x6*x6*x6) , x5)) , (x4*x4*x4)) , min(x4 , x4)) - ((x1*x1*x1) - avg((-sin(x9)) , (((-(x1*x1*x1)) - x1) - x1))))) , (min(abs(x0) , x6) % exp((x6 % min(log(abs(x2)) , sqrt(abs(x9)))))))),min((avg((cos(((x6*x6*x6) + (x2*x2*x2))) + avg(cos(x4) , x2)) , avg((cos(x3) + max((1.0 / x8) , x6)) , (cos(min(avg(min(x8 , x5) , avg((-x3) , ((1.0 / ninteger(x1)) + (x4*x4*x4)))) , (avg((avg((x6*x6*x6) , x1) - max((1.0 / x8) , x6)) , x1) - max((1.0 / x8) , x6)))) + (x0*x0)))) + (cos(x3) + (x0*x0))) , min(x8 , sqrt(abs(x7)))),max(max(avg(max(x5 , avg((x6*x6*x6) , x5)) , ((1.0 / x0)*(1.0 / x0)*(1.0 / x0))) , max(avg(max(x5 , avg((x6*x6*x6) , x5)) , (x4*x4*x4)) , min(x4 , avg((-x3) , max(expt(abs(cos(x5)) ,(1.0 / x7)) , expt(abs(log(abs(x4))) ,ninteg5)) , (x4*x4*x4)) , min(x4 , avg((-x3) , max(expt(abs(cos(x5)) ,(1.0 / x7)) , expt(abs(log(abs(x4))) ,ninteger(min(x8 , (((((x4*x4*x4) % exp(x9))*((x4*x4*x4) % exp(x9))*((x4*x4*x4) % exp(x9))) % exp(x9)) % exp(x9)))))))))))*(min(avg((-x3) , x0) , avg((-x3) , ((1.0 / ninteger(x1)) + (x4*x4*x4)))) - max(avg(max(x5 , avg((x6*x6*x6) , x5)) , ((1.0 / x0)*(1.0 / x0)*(1.0 / x0))) , max(avg(max(x5 , avg((x6*x6*x6) , x5)) , (x4*x4*x4)) , min(x4 , avg((-x3) , max(expt(abs(cos(x5)) ,(1.0 / x7)) , expt(abs(log(abs(x4))) ,ninteger(min(x8 , (((((x4*x4*x4) % exp(x9))*((x4*x4*x4) % exp(x9))*((x4*x4*x4) % exp(x9))) % exp(x9)) % exp(x9)))))))))))*(min(avg((-x3) , x0) , avg((-x3) , ((1.0 / ninteger(x1)) + (x4*x4*x4)))) - max(avg(max(x5 , avg((x6*x6*x6) , x5)) , ((1.0 / x0)*(1.0 / x0)*(1.0 / x0))) , max(avg(max(x5 , avg((x6*x6*x6) , x5)) , (x4*x4*x4)) , min(x4 , avg((-x3) , max(expt(abs(cos(x5)) ,(1.0 / x7)) , expt(abs(log(abs(x4))) ,ninteger(min(x8 , (((((x4*x4*x4) % exp(x9))*((x4*x4*x4) % exp(x9))*((x4*x4*x4) % exp(x9))) % exp(x9)) % exp(x9)))))))))))) , (x3 - min(x4 , (-x6)))));
(x1*x1)) , cos(x5)) - avg(tan(log(abs(x8))) , avg(avg(avg(cos(tanh(avg(exp(x6) , (1.0 / x4)))) , x2) , avg((avg(sqrt(abs(x7)) , x2)*avg(sqrt(abs(x7)) , x2)*avg(sqrt(abs(x7)) , x2)) , (x1*x1))) , avg(sin(x7) , (ninteger(x0) + avg((x6*x6*x6) , avg((1.0 / x0) , x6))))))),(min(avg(min(x8 , x5) , avg((-x3) , ((1.0 / ninteger(x1)) + (x4*x4*x4)))) , (avg((avg((x6*x6*x6) , x1) - max((1.0 / x8) , x6)) , x1) - max((1.0 / x8myBestSelectorChampions[2],Score=[0.01677895942023,ErrorPct=[0.2461228392074],WFF= mvlregress((avg((ninteger(x1)*ninteger(x1)*ninteger(x1)) , (avg(ninteger(x1) , avg(avg(avg(ninteger(x1) , avg((-x8) , x2)) , x2) , max(min((1.0 / x9) , abs(x8)) , x9))) - (avg(ninteger(x1) , avg(avg(avg((-x8) , x2) , x2) , x2)) / max((avg((x6*x6*x6) , avg(avg((x6*x6*x6) , x8) , avg(abs(x4) , x6))) + x0) , (avg((x6*x6*x6) , avg(avg((x6*x6*x6) , x8) , avg(abs(x4) , x6))) + x0))))) - (avg(sign(((avg(cos(x7) , avg(x6 , max(x1 , avg(ninteger(x1) , x2)))) + x2) + avg(max(tanh(x5) , x6) , x8))) , x0) - avg((avg((x6*x6*x6) , avg(avg((x6*x6*x6) , x8) , avg(abs(x4) , x6))) + x0) , avg(avg(max(tanh(x5) , x6) , x8) , x3)))),(avg(cos(x7) , x6) % min((max(ninteger(x2) , avg(sqrt(abs(avg(sqrt(abs(x4)) , min(cos(ninteger((avg(avg(avg(ninteger(x1) , avg((-x8) , x2)) , x2) , x2) + x0))) , x6)))) , min(cos(x3) , x6))) % (((min(exp(x9) , x8) % x0) % (ninteger(avg((-x8) , x2)) + max(x1 , (x9 / min((1.0 / x9) , x6))))) - (x2 - (x9 - (x6*x6*x6))))) , max(max(min((abs(x7) % x6) , max(tanh(x3) , x5)) , x9) , x5))),min(max(ninteger(max(x0 , x6)) , x6) , min(min((avg(cos(x7) , avg(x6 , max(x1 , avg(ninteger(x1) , x2)))) + x2) , ((avg(cos(x7) , avg(x6 , max(x1 , avg(ninteger(x1) , x2)))) + x2) + avg(max(tanh(x5) , x6) , x8))) , abs(x8))),max(max(max(x7 , (min(max((avg((x6*x6*x6) , avg(avg((x6*x6*x6) , x8) , avg(abs(x4) , x6))) + (x2 - avg(log(abs(x2)) , x6))) , x9) , x8) % x0)) , (abs(x7) % max(ninteger(max(x0 , x6)) , x6))) , max(x7 , (min(max((avg((x6*x6*x6) , avg(avg((x6*x6*x6) , x8) , avg(abs(x4) , x6))) + (x2 - avg(log(abs(x2)) , x6))) , x9) , x8) % x0))),ninteger(ninteger(x2)),avg(((min(exp(ninteger(x1)) , x8) % x0) % (x2 - (x9 - (x6*x6*x6)))) , max(log(abs(x4)) , (-x8))),avg(avg(sign(x4) , (x2*x2*x2)) , sign(x4)),avg(avg(ninteger(x1) , sin((x6*x6*x6))) , max(ninteger(max(x0 , ninteger(avg((-ninteger(x1)) , (-sin((x6*x6*x6))))))) , avg(sqrt(abs(x4)) , min(cos(x3) , x6)))),avg(avg(avg(cos(x7) , x6) , avg(avg(sign(x4) , x6) , x3)) , avg(max(tanh(x5) , x6) , x8)),(avg(avg(avg(avg(ninteger(x1) , x1) , avg(sqrt(abs(x7)) , x2)) , avg((x6*x6*x6) , (x2*x2*x2))) , avg(ninteger(x1) , avg(sqrt(abs(x7)) , x2))) + avg(avg(avg(ninteger(x1) , x1) , avg(sqrt(abs(x7)) , x2)) , avg((x6*x6*x6) , (x2*x2*x2)))));
myBestRegressorChampions
myBestRegressorChampions[0],ErrorPct=[0.1939311955088,Score=[0.01018869110383],WFF= mvlregress((((cos(x5) - avg((-x3) , x0)) - cos(cos((-x3)))) - avg((-x3) , x0)),(((avg(tanh(((avg(sin(ninteger(x7)) , (x6*x6*x6)) * expt(abs((1.0 / x6)) ,cos(x2))) / (abs(ninteger(x8)) * x6))) , x6) - (x9 - ((x1*x1*x1) - (x9 - ((x1*x1*x1) - avg((-sin(x9)) , (((-(x1*x1*x1)) - x1) - x1))))))) - max((1.0 / x8) , x6)) - (x2*x2*x2)),((avg(tanh(((avg(sin(ninteger(x7)) , (x6*x6*x6)) * expt(abs((1.0 / x6)) ,cos(x2))) / (abs(ninteger(x8)) * x6))) , x6) - (x9 - ((x1*x1*x1) - (x9 - ((x1*x1*x1) - avg((-sin(x9)) , (((-(x1*x1*x1)) - x1) - x1))))))) - (((x8*x8*x8) - ((x0 + ((x6*x6*x6) + (x2*x2*x2))) - x9)) + avg(x8 , x2))),(min(avg(min(x8 , x5) , (x1*x1)) , cos(x5)) - avg(tan(log(abs(x8))) , avg(avg(avg(cos(tanh(avg(exp(x6) , (1.0 / x4)))) , x2) , avg((avg(sqrt(abs(x7)) , x2)*avg(sqrt(abs(x7)) , x2)*avg(sqrt(abs(x7)) , x2)) , (x1*x1))) , avg(sin(x7) , (ninteger(x0) + avg((x6*x6*x6) , avg((1.0 / x0) , x6))))))),(min(avg(min(x8 , x5) , avg((-x3) , ((1.0 / ninteger(x1)) + (x4*x4*x4)))) , (avg((avg((x6*x6*x6) , x1) - max((1.0 / x8) , x6)) , x1) - max((1.0 / x8) , x6))) - min((1.0 / (max(avg(max(x5 , avg((x6*x6*x6) , x5)) , (x4*x4*x4)) , min(x4 , x4)) - ((x1*x1*x1) - avg((-sin(x9)) , (((-(x1*x1*x1)) - x1) - x1))))) , (min(abs(x0) , x6) % exp((x6 % min(log(abs(x2)) , sqrt(abs(x9)))))))),((x6*x6*x6) - max((1.0 / x8) , x6)),min((1.0 / (max(avg(max(x5 , avg((x6*x6*x6) , x5)) , (x4*x4*x4)) , min(x4 , x4)) - ((x1*x1*x1) - avg((-sin(x9)) , (((-(x1*x1*x1)) - x1) - x1))))) , (min(abs(x0) , x6) % exp((x6 % min(log(abs(x2)) , sqrt(abs(x9))))))),min((avg((cos(((x6*x6*x6) + (x2*x2*x2))) + avg(cos(x4) , x2)) , avg((cos(x3) + max((1.0 / x8) , x6)) , (cos(min(avg(min(x8 , x5) , avg((-x3) , ((1.0 / ninteger(x1)) + (x4*x4*x4)))) , (avg((avg((x6*x6*x6) , x1) - max((1.0 / x8) , x6)) , x1) - max((1.0 / x8) , x6)))) + (x0*x0)))) + (cos(x3) + (x0*x0))) , max(avg((x6*x6*x6) , (x4*x4*x4)) , max(avg(max(x5 , avg((x6*x6*x6) , x5)) , (x4*x4*x4)) , min(x4 , x4)))),max(max(avg(max(x5 , avg((x6*x6*x6) , x5)) , ((1.0 / x0)*(1.0 / x0)*(1.0 / x0))) , max(avg(max(x5 , avg((x6*x6*x6) , x5)) , (x4*x4*x4)) , min(x4 , avg((-x3) , max(expt(abs(cos(x5)) ,(1.0 / x7)) , expt(abs(log(abs(x4))) ,ninteger(min(x8 , (((((x4*x4*x4) % exp(x9))*((x4*x4*x4) % exp(x9))*((x4*x4*x4) % exp(x9))) % exp(x9)) % exp(x9)))))))))) , max(avg((min((x8*x8*x8) , min(x4 , ninteger(x8)))*min((x8*x8*x8) , min(x4 , ninteger(x8)))*min((x8*x8*x8) , min(x4 , ninteger(x8)))) , (x4*x4*x4)) , max(avg(max(x5 , avg((x6*x6*x6) , x5)) , ((if (x5 >= tanh(-2.534504273257)) {(if (x0 < -3.109395818684) {-0.190022410201} else {-3.640206438313})} else {(if (x8 <= x5) {-3.56291172268} else {x9})})*(if (x5 >= tanh(-2.534504273257)) {(if (x0 < -3.109395818684) {-0.190022410201} else {-3.640206438313})} else {(if (x8 <= x5) {-3.56291172268} else {x9})})*(if (x5 >= tanh(-2.534504273257)) {(if (x0 < -3.109395818684) {-0.190022410201} else {-3.640206438313})} else {(if (x8 <= x5) {-3.56291172268} else {x9})}))) , min(x4 , x4)))),avg((((x8*x8*x8) - ((x0 + ((x6*x6*x6) + (x2*x2*x2))) - x9)) + avg(x8 , x2)) , (x3 - min(x4 , (-x6)))));
myBestRegressorChampions[1],ErrorPct=[0.1947698639731,Score=[0.01048720904089],WFF= mvlregress((((cos(x5) - avg((-x3) , x0)) - cos(x5)) - avg((-x3) , x0)),(((avg(tanh(((avg(sin(ninteger(x7)) , (x6*x6*x6)) * expt(abs((1.0 / x6)) ,cos(x2))) / (abs(ninteger(x8)) * x6))) , x6) - (x9 - ((x1*x1*x1) - (x9 - ((x1*x1*x1) - avg((-sin(x9)) , (((-(x1*x1*x1)) - x1) - x1))))))) - max((1.0 / x8) , x6)) - ((x0 + ((x6*x6*x6) + (x2*x2*x2))) - x9)),((min(abs(x0) , x6) % exp(((((x4*x4*x4) % exp(x9))*((x4*x4*x4) % exp(x9))*((x4*x4*x4) % exp(x9))) % cos(x5)))) - ((x1*x1*x1) - avg((-sin(x9)) , (((-(x1*x1*x1)) - x1) - x1)))),((x6*x6*x6) - max((1.0 / x8) , x6)),((avg((cos(((x6*x6*x6) + (x2*x2*x2))) + avg(cos(x4) , x2)) , avg((cos(x3) + max((1.0 / x8) , x6)) , (cos(x3) + (x0*x0)))) + avg(avg(sin(x4) , x6) , avg(tan(log(abs(x8))) , avg(avg(avg(cos(tanh(avg(exp(x6) , (1.0 / x4)))) , x2) , avg((avg(sqrt(abs(x7)) , x2)*avg(sqrt(abs(x7)) , x2)*avg(sqrt(abs(x7)) , x2)) , (x1*x1))) , avg(sin(x7) , (ninteger(x0) + avg((x6*x6*x6) , avg((1.0 / x0) , x6)))))))) - max((1.0 / x8) , x6)),min((1.0 / (max(avg(max(x5 , avg((x6*x6*x6) , x5)) , (x4*x4*x4)) , min(x4 , x4)) - ((x1*x1*x1) - avg((-sin(x9)) , (((-(x1*x1*x1)) - x1) - x1))))) , (min(abs(x0) , x6) % exp((x6 % min(log(abs(x2)) , sqrt(abs(x9))))))),min(avg(min(x8 , x5) , avg((-x3) , ((1.0 / ninteger(x1)) + (x4*x4*x4)))) , (avg((avg((x6*x6*x6) , x1) - max((1.0 / x8) , x6)) , x1) - max((1.0 / x8) , x6))),max(avg((x6*x6*x6) , (x4*x4*x4)) , max(avg(max(x5 , avg((x6*x6*x6) , x5)) , (x4*x4*x4)) , min(x4 , x4))),(min((x8*x8*x8) , min(x4 , ninteger(x8))) / (min(avg((-x3) , x0) , avg((-x3) , ((1.0 / ninteger(x1)) + (x4*x4*x4)))) - max(avg(max(x5 , avg((x6*x6*x6) , x5)) , ((1.0 / x0)*(1.0 / x0)*(1.0 / x0))) , max(avg(max(x5 , avg((x6*x6*x6) , x5)) , (x4*x4*x4)) , min(x4 , avg((-x3) , max(expt(abs(cos(x5)) ,(1.0 / x7)) , expt(abs(log(abs(x4))) ,ninteger(min(x8 , (((((x4*x4*x4) % exp(x9))*((x4*x4*x4) % exp(x9))*((x4*x4*x4) % exp(x9))) % exp(x9)) % exp(x9)))))))))))),(((x8*x8*x8) - ((x0 + ((x6*x6*x6) + (x2*x2*x2))) - x9)) + avg(x8 , x2)));
myBestRegressorChampions[2],ErrorPct=[0.2461228392074,Score=[0.01677895942023],WFF= mvlregress((avg((ninteger(x1)*ninteger(x1)*ninteger(x1)) , (avg(ninteger(x1) , avg(avg(avg(ninteger(x1) , avg((-x8) , x2)) , x2) , max(min((1.0 / x9) , abs(x8)) , x9))) - (avg(ninteger(x1) , avg(avg(avg((-x8) , x2) , x2) , x2)) / max((avg((x6*x6*x6) , avg(avg((x6*x6*x6) , x8) , avg(abs(x4) , x6))) + x0) , (avg((x6*x6*x6) , avg(avg((x6*x6*x6) , x8) , avg(abs(x4) , x6))) + x0))))) - (avg(sign(((avg(cos(x7) , avg(x6 , max(x1 , avg(ninteger(x1) , x2)))) + x2) + avg(max(tanh(x5) , x6) , x8))) , x0) - avg((avg((x6*x6*x6) , avg(avg((x6*x6*x6) , x8) , avg(abs(x4) , x6))) + x0) , avg(avg(max(tanh(x5) , x6) , x8) , x3)))),(avg(cos(x7) , x6) % min((max(ninteger(x2) , avg(sqrt(abs(avg(sqrt(abs(x4)) , min(cos(ninteger((avg(avg(avg(ninteger(x1) , avg((-x8) , x2)) , x2) , x2) + x0))) , x6)))) , min(cos(x3) , x6))) % (((min(exp(x9) , x8) % x0) % (ninteger(avg((-x8) , x2)) + max(x1 , (x9 / min((1.0 / x9) , x6))))) - (x2 - (x9 - (x6*x6*x6))))) , max(max(min((abs(x7) % x6) , max(tanh(x3) , x5)) , x9) , x5))),min(max(ninteger(max(x0 , x6)) , x6) , min(min((avg(cos(x7) , avg(x6 , max(x1 , avg(ninteger(x1) , x2)))) + x2) , ((avg(cos(x7) , avg(x6 , max(x1 , avg(ninteger(x1) , x2)))) + x2) + avg(max(tanh(x5) , x6) , x8))) , abs(x8))),max(max(max(x7 , (min(max((avg((x6*x6*x6) , avg(avg((x6*x6*x6) , x8) , avg(abs(x4) , x6))) + (x2 - avg(log(abs(x2)) , x6))) , x9) , x8) % x0)) , (abs(x7) % max(ninteger(max(x0 , x6)) , x6))) , max(x7 , (min(max((avg((x6*x6*x6) , avg(avg((x6*x6*x6) , x8) , avg(abs(x4) , x6))) + (x2 - avg(log(abs(x2)) , x6))) , x9) , x8) % x0))),ninteger(ninteger(x2)),avg(((min(exp(ninteger(x1)) , x8) % x0) % (x2 - (x9 - (x6*x6*x6)))) , max(log(abs(x4)) , (-x8))),avg(avg(sign(x4) , (x2*x2*x2)) , sign(x4)),avg(avg(ninteger(x1) , sin((x6*x6*x6))) , max(ninteger(max(x0 , ninteger(avg((-ninteger(x1)) , (-sin((x6*x6*x6))))))) , avg(sqrt(abs(x4)) , min(cos(x3) , x6)))),avg(avg(avg(cos(x7) , x6) , avg(avg(sign(x4) , x6) , x3)) , avg(max(tanh(x5) , x6) , x8)),(avg(avg(avg(avg(ninteger(x1) , x1) , avg(sqrt(abs(x7)) , x2)) , avg((x6*x6*x6) , (x2*x2*x2))) , avg(ninteger(x1) , avg(sqrt(abs(x7)) , x2))) + avg(avg(avg(ninteger(x1) , x1) , avg(sqrt(abs(x7)) , x2)) , avg((x6*x6*x6) , (x2*x2*x2)))));

Show final results on training data, Score=[0.01018869110383], ErrorPct=[0.1939311955088]
X[0], ey=[-9568018.753796], y=[-10194198.99524], ErrorPct=[0.2330722209409]
X[50], ey=[-6207841.849936], y=[-5731791.910134], ErrorPct=[0.1771918201907]
X[100], ey=[-4580022.857909], y=[-4385357.764859], ErrorPct=[0.07245681446651]
X[150], ey=[-2730683.354162], y=[-3532644.865784], ErrorPct=[0.2985002372359]
X[200], ey=[-3063003.405727], y=[-2777598.968877], ErrorPct=[0.1062311480957]
X[250], ey=[-2075406.715184], y=[-2205104.118], ErrorPct=[0.0482750168787]
X[300], ey=[-459133.5217235], y=[-1744737.63376], ErrorPct=[0.4785181419226]
X[350], ey=[-1375173.349603], y=[-1144617.126808], ErrorPct=[0.08581594777731]
X[400], ey=[-586552.9866829], y=[-659048.2485587], ErrorPct=[0.02698365514407]
X[450], ey=[-536719.3274957], y=[-323186.381529], ErrorPct=[0.07947966841937]
X[500], ey=[571547.7155194], y=[83553.74103903], ErrorPct=[0.1816375412551]
X[550], ey=[770513.4454244], y=[406918.3309076], ErrorPct=[0.1353347091703]
X[600], ey=[1443177.366046], y=[766308.7135659], ErrorPct=[0.2519390898629]
X[650], ey=[1228792.126611], y=[1267185.886493], ErrorPct=[0.01429064396133]
X[700], ey=[1559876.110157], y=[1732960.073939], ErrorPct=[0.06442404467301]
X[750], ey=[1295173.555015], y=[2241051.24796], ErrorPct=[0.3520676636583]
X[800], ey=[2942172.632618], y=[2896045.960064], ErrorPct=[0.01716893205078]
X[850], ey=[2825561.104153], y=[3506938.740411], ErrorPct=[0.2536173907636]
X[900], ey=[4264747.995178], y=[4343102.178887], ErrorPct=[0.02916442009579]
X[950], ey=[5666892.677219], y=[5448808.612965], ErrorPct=[0.08117365231884]
Actual computed error on training data is ErrorPct=[0.1939311955088] versus reported ErrorPct=[0.1939311955088] while average Y is AvgY=[18042.13258201]
yHistory=[#(num| -6021096.793893 -3604493.73283 -2228000.268101 -1170959.882476 -317044.0687077 398417.9373582 1241721.84014 2311074.987531 3540892.172944 6029909.133854 )]
eHistory=[#(num| -5859246.247508 -3594647.876512 -2192230.333791 -1096340.139889 -343528.015013 445791.584543 1183107.34105 2266278.41238 3431692.948216 5939543.652345 )]
aHistory=[#(num| -5859246.247508 -4726947.06201 -3882041.485937 -3185616.149425 -2617198.522543 2653282.787707 3205155.588498 3879171.67098 4685618.30028 5939543.652345 )]
dHistory=[#(num| 5270481.310249 6390771.737923 7761213.156917 9412565.36229 11798789.89985 )]


Final testing on test data returns Score=[0.01064326398717], ErrorPct=[0.1984220868552]
X[0], ey=[-9568018.753796], y=[-10194198.99524], ErrorPct=[0.2369137853109]
X[50], ey=[-5428678.254656], y=[-5509579.817996], ErrorPct=[0.03060891152431]
X[100], ey=[-4552974.006491], y=[-4263282.87499], ErrorPct=[0.1096039414734]
X[150], ey=[-2730683.354162], y=[-3532644.865784], ErrorPct=[0.3034202052663]
X[200], ey=[-1703275.111107], y=[-2817822.871745], ErrorPct=[0.4216864592771]
X[250], ey=[-602978.0599363], y=[-2207651.057995], ErrorPct=[0.6071241616968]
X[300], ey=[-2316740.994743], y=[-1766318.376059], ErrorPct=[0.2082510713097]
X[350], ey=[-1490492.42435], y=[-1246808.292409], ErrorPct=[0.09219730406257]
X[400], ey=[-518474.7818241], y=[-753220.739967], ErrorPct=[0.08881556754656]
X[450], ey=[-418582.8068586], y=[-425469.1113184], ErrorPct=[0.002605416696966]
X[500], ey=[421850.5886917], y=[-56562.53395165], ErrorPct=[0.1810064520192]
X[550], ey=[668188.9938199], y=[275776.7930872], ErrorPct=[0.1484682104689]
X[600], ey=[1887520.077048], y=[724641.3884283], ErrorPct=[0.4399723494054]
X[650], ey=[1071765.023026], y=[1221336.188606], ErrorPct=[0.05658989004384]
X[700], ey=[1109155.619352], y=[1696678.017549], ErrorPct=[0.2222876834813]
X[750], ey=[2499226.243861], y=[2228000.035083], ErrorPct=[0.1026177824601]
X[800], ey=[2013180.974394], y=[2847450.093262], ErrorPct=[0.3156437106097]
X[850], ey=[3666471.231684], y=[3494225.585163], ErrorPct=[0.06516872526474]
X[900], ey=[4318250.500462], y=[4303447.303033], ErrorPct=[0.005600754072855]
X[950], ey=[5285530.179785], y=[5253143.613387], ErrorPct=[0.01225337934792]
Actual computed error on testing data is ErrorPct=[0.1984220868552], Avg Y=[-18106.40931101], AvgDev Y=[2643072.207131]
yHistory=[#(num| -5827359.493433 -3555885.183883 -2246056.129171 -1262070.674526 -413503.396365 320869.0671257 1195784.806861 2243763.743858 3534506.214917 5828886.951508 )]
eHistory=[#(num| -5675570.069985 -3511136.636928 -2243937.949827 -1155139.106549 -419587.7523799 296313.5643818 1111031.521472 2252572.572558 3435325.855581 5729063.908566 )]
aHistory=[#(num| -5675570.069985 -4593353.353456 -3810214.88558 -3146445.940822 -2601074.303134 2564861.484512 3131998.464544 3805654.112235 4582194.882073 5729063.908566 )]
dHistory=[#(num| 5165935.787645 6278444.405366 7615868.997815 9175548.23553 11404633.97855 )]

esm.selfTest[cubicRegression]: completed in [93.84348333333] minutes.

Starting test case: hyperTangent
Building test data as: y = -13.99864194741 - (13.99864194741*tanh(x0*x0*x0)) - (32.66789144459*tanh(x1*x1*x1)) - (30.73171497235*tanh(x2*x2*x2)) + (10.01135458484*tanh(x3*x3*x3)) - (2.83159613203*tanh(x4*x4*x4)) + (5.499486281783*tanh(x5*x5*x5)) - (46.33521181329*tanh(x6*x6*x6)) + (10.35455431571*tanh(x7*x7*x7)) + (18.14155680594*tanh(x8*x8*x8)) - (3.363675194861*tanh(x9*x9*x9));
...starting new evolutionary process.
...starting generation of constant selector WFF.
...starting regress selector for each column name.
...starting a best-of-breed MVL selector WFF.
...starting a best-of-breed REG selector WFF.
...starting generation of root selector WFFs in each column island.
...starting generation of random selector WFFs.

Final results of training
esm: N = [1000], M = [10], Generations = [200], WFFs = [18174], Score=[0.05138309738076], ScoreHistory=[#(num| 0.05138309738076 )]
esm.myBest, Score=[0.05138309738076], ErrorPct=[5.717045963064], lengthWFF=[215], WFF = mvlregress((x8 - x6),(x2 - x7),(x1 - x7),avg(x3 , x5),avg(x2 , x6),avg(x0 , x6),avg(tanh(x4) , x0),avg(sign(x9) , x0),avg(ninteger(x3) , x7),avg(ninteger(x0) , x1));
esm.myBestRegressor, Score=[0.06380864589195], ErrorPct=[0.5089072748814], lengthWFF=[619], WFF = mvlregress(x7,x6,tanh(x9),((x1 - x5) - min(max(tanh(x6) , x9) , min(ninteger(x0) , min(exp(x7) , x8)))),min(x0 , avg(x0 , x0)),max(tanh(x8) , max(tanh(x4) , sign(x3))),ninteger(expt(abs(sign(expt(abs(sign(avg(x0 , x2))) ,(x2*x2)))) ,avg((x1*x1*x1) , x2))),avg(x2 , tanh(x4)),avg(x2 , expt(abs(sign(expt(abs(sign(avg(x0 , x2))) ,(x2*x2)))) ,avg(x3 , x2))),avg(avg(expt(abs(sign(expt(abs(sign(avg(x0 , x2))) ,(x2*x2)))) ,avg(x0 , x2)) , x9) , x2));
myBestSelectorChampions
myBestSelectorChampions[0],Score=[0.05138309738076,ErrorPct=[5.717045963064],WFF= mvlregress((x8 - x6),(x2 - x7),(x1 - x7),avg(x3 , x5),avg(x2 , x6),avg(x0 , x6),avg(tanh(x4) , x0),avg(sign(x9) , x0),avg(ninteger(x3) , x7),avg(ninteger(x0) , x1));
myBestSelectorChampions[1],Score=[0.06358288516982,ErrorPct=[0.5095972731371],WFF= mvlregress((x9 - (x8 - x6)),((x2 - x7) - tanh(x7)),x2,((min(max(x4 , x2) , avg(sin(avg(x3 , x5)) , x0)) * x6) * avg(x0 , x6)),(((x1 - x7) % avg(x3 , x7)) % avg(x2 , x6)),max(avg(x3 , x5) , x5),max(avg(tan(x9) , x0) , x5),expt(abs(sign(tan(x9))) ,sign(avg(sin(x4) , x0))),avg(avg(ninteger(x1) , x7) , avg(ninteger(x3) , x7)),avg(avg(avg((avg(ninteger(x0) , x1) + avg(abs(x6) , x0)) , (-x7)) , avg(abs(x6) , avg(ninteger(x0) , x1))) , avg(ninteger(x4) , x1)));
myBestSelectorChampions[2],Score=[0.06380864589195,ErrorPct=[0.5089072748814],WFF= mvlregress(x7,x6,tanh(x9),((x1 - x5) - min(max(tanh(x6) , x9) , min(ninteger(x0) , min(exp(x7) , x8)))),min(x0 , avg(x0 , x0)),max(tanh(x8) , max(tanh(x4) , sign(x3))),ninteger(expt(abs(sign(expt(abs(sign(avg(x0 , x2))) ,(x2*x2)))) ,avg((x1*x1*x1) , x2))),avg(x2 , tanh(x4)),avg(x2 , expt(abs(sign(expt(abs(sign(avg(x0 , x2))) ,(x2*x2)))) ,avg(x3 , x2))),avg(avg(expt(abs(sign(expt(abs(sign(avg(x0 , x2))) ,(x2*x2)))) ,avg(x0 , x2)) , x9) , x2));
myBestSelectorChampions[3],Score=[0.06775868707422,ErrorPct=[5.719097934796],WFF= mvlregress((x7 - x6),(x1 - x5),(x0 - x3),expt(abs(x1) ,avg(x2 , x6)),(x6 / (x2 - x3)),avg(x3 , x8),avg(x2 , x9),avg(sin(x1) , x0),(x9 + (x8 - x9)),(x7 + (x4 - x7)));
myBestSelectorChampions[4],Score=[0.06780749203057,ErrorPct=[5.723696651588],WFF= mvlregress(x9,x8,x7,x6,x5,x4,x3,x2,x1,x0);
myBestRegressorChampions
myBestRegressorChampions[0],ErrorPct=[0.5089072748814,Score=[0.06380864589195],WFF= mvlregress(x7,x6,tanh(x9),((x1 - x5) - min(max(tanh(x6) , x9) , min(ninteger(x0) , min(exp(x7) , x8)))),min(x0 , avg(x0 , x0)),max(tanh(x8) , max(tanh(x4) , sign(x3))),ninteger(expt(abs(sign(expt(abs(sign(avg(x0 , x2))) ,(x2*x2)))) ,avg((x1*x1*x1) , x2))),avg(x2 , tanh(x4)),avg(x2 , expt(abs(sign(expt(abs(sign(avg(x0 , x2))) ,(x2*x2)))) ,avg(x3 , x2))),avg(avg(expt(abs(sign(expt(abs(sign(avg(x0 , x2))) ,(x2*x2)))) ,avg(x0 , x2)) , x9) , x2));
myBestRegressorChampions[1],ErrorPct=[0.5094032611048,Score=[0.06423249221213],WFF= mvlregress(tanh(x7),((x2 - x7) - tanh(tan(x1))),(max(avg(x3 , x5) , x5) * avg(((avg((x2*x2*x2) , tan(x1)) * avg(ninteger(max(avg(tan(x9) , x0) , avg(x0 , x6))) , x7)) / avg(sin(x4) , x0)) , (avg(avg(expt(abs(sign(log(abs(x0)))) ,sign(avg(sin(x4) , x0))) , avg(sin(x4) , x0)) , (min(max(x4 , x2) , avg(sin(avg(x3 , x5)) , x0)) * x6))*avg(avg(expt(abs(sign(log(abs(x0)))) ,sign(avg(sin(x4) , x0))) , avg(sin(x4) , x0)) , (min(max(x4 , x2) , avg(sin(avg(x3 , x5)) , x0)) * x6))))),(avg(sin(x4) , (x2 - x7)) * avg(x0 , max(min(avg((1.0 / x1) , x2) , expt(abs(sign(tan(x9))) ,sign(x3))) , min(max(x4 , x2) , avg(sin(avg(x3 , x5)) , x0))))),min(expt(abs(log(abs(((x1 - x7) % avg(x3 , x7))))) ,sin((avg(sin(x4) , (x2 - x7)) % avg(max(avg(avg(tan(x9) , x0) , x6) , tanh(((x9 - x6) / (x9 - x6)))) , ((ninteger((avg(x3 , ((x2 - x7) - tanh(x7))) - sin(min(avg(avg((avg(max(avg(avg(tan(x9) , x0) , x6) , tanh(x4)) , ((x1 - x7) % avg(max((expt(abs(avg(ninteger(x3) , x7)) ,avg(ninteger(avg(avg(x5 , x0) , x0)) , x7)) % avg(ninteger(x3) , x7)) , x8) , x7))) + avg(abs(x6) , x0)) , (-x7)) , avg(abs(x6) , avg(ninteger(x4) , x1))) , max(cos(avg(x0 , exp(x8))) , abs(x6)))))) % log(abs(x0))) - max((1.0 / x5) , (1.0 / x6))))))) , tanh(tan(x7))),max(max(avg(x3 , x5) , x5) , avg(avg(ninteger(x3) , min(max(x4 , x2) , avg(sin(avg(x3 , x5)) , x0))) , (x3 - x9))),expt(abs(sign(tan(x9))) ,tan(x9)),(max(avg(tan(sign(avg(avg(avg(avg(ninteger(x3) , x7) , (x3 - x9)) , x2) , x2))) , x0) , x5) / max(max(min(avg((1.0 / x1) , x2) , expt(abs(sign(tan(x9))) ,sign(x3))) , min(max(x4 , x2) , avg(sin(avg(x3 , x5)) , x0))) , (x1 - x7))),avg(avg(ninteger(x1) , x7) , avg(ninteger(x3) , x7)),avg((avg(avg(avg((avg(ninteger(x0) , x1) + avg(abs(x6) , x0)) , (-x7)) , avg(abs(x6) , x0)) , avg(ninteger(x4) , x1)) + avg(((((x9 - sign((avg(abs(x6) , x0) - x6))) - tanh(x7)) + (sign((x8 - x6)) - (x8 - x6))) + (x0 - avg(x3 , x5))) , avg(ninteger(x4) , avg(avg(avg((avg(ninteger(x0) , x1) + avg(abs(x6) , x0)) , (-x7)) , avg(abs(x6) , avg(ninteger(x0) , x1))) , avg(ninteger(x4) , x1))))) , (avg(avg(avg((avg(ninteger(x0) , x1) + avg(abs(x6) , x0)) , (-x7)) , avg(abs(x6) , x0)) , avg(ninteger(x4) , x1)) + avg(((((x9 - sign((x8 - x6))) - tanh(x7)) + (sign((x8 - x6)) - (x8 - x6))) + (x0 - avg(x3 , x5))) , avg(ninteger(x4) , x1)))));
myBestRegressorChampions[2],ErrorPct=[0.5256821032165,Score=[0.06815766698856],WFF= mvlregress(x8,x7,x6,x5,max(x7 , ninteger(x4)),expt(abs(sign(x1)) ,tanh(x4)),avg(x2 , x5),avg(x0 , x1),(x2 + sign(x9)),(max(avg(x1 , x5) , x1) + ninteger(x3)));
myBestRegressorChampions[3],ErrorPct=[0.52904151495,Score=[0.07278718654908],WFF= mvlregress(((x2 - x8) - (x5 - x9)),max((1.0 / avg(x3 , x6)) , tanh(x9)),max(ninteger(((x8 - x6) - x7)) , (x5*x5*x5)),max(avg((-x7) , (x1 - x7)) , max(x4 , x1)),max(avg(ninteger(x2) , log(abs(x1))) , avg(x1 , x7)),expt(abs(sign(min(x7 , x8))) ,log(abs(avg(x2 , x5)))),avg(avg(x0 , x4) , sign(x6)),avg(abs(x5) , avg(tanh(x3) , x0)),((-x6) + sign(x9)),(avg((-ninteger(x0)) , (-x0)) + avg(x3 , x7)));
myBestRegressorChampions[4],ErrorPct=[0.5293095496179,Score=[0.06800704553753],WFF= mvlregress((x9 - (x4 - (x8 - x9))),((x4 - x7) - x8),((x0 - x3) - (x4 - (x1 - x5))),(min(exp(x8) , exp((x7 - avg(x2 , x6)))) - x6),(max(avg(x2 , x9) , max(((x1 - x5)*(x1 - x5)*(x1 - x5)) , (x1 - x5))) - x7),max(max(((x1 - x5)*(x1 - x5)*(x1 - x5)) , abs(x6)) , max(((ninteger(x9) * (-x6)) + ((-x7) - sign(x8))) , (x4 - x5))),expt(abs(sign(x9)) ,x3),avg((x7 - x6) , x2),avg(avg(x2 , x1) , avg(x2 , x9)),avg(((ninteger(x9) * (-x6)) + ((-x7) - sign(x8))) , (avg((x1*x1*x1) , (1.0 / x9)) + x2)));
myBestRegressorChampions[5],ErrorPct=[0.5427313530093,Score=[0.07417683700572],WFF= mvlregress(x9,(x8 - sin(x0)),((min(exp(x7) , x3) - expt(abs(sign(x1)) ,tanh(x8))) - x9),sign(x6),min(x5 , log(abs(x9))),expt(abs(expt(abs(sign(x1)) ,tanh(max((x3*x3*x3) , ninteger(x7))))) ,x3),(x2*x2*x2),avg(sign(x8) , x1),(x0 + x2),(min(exp(x5) , x4) + x4));
myBestRegressorChampions[6],ErrorPct=[0.5653030841619,Score=[0.07895946965623],WFF= mvlregress(((ninteger(x5) - (ninteger(x0) - (ninteger(x2) - x9))) - (avg((x7 + log(abs(x7))) , x3) - expt(abs(sign(x1)) ,x1))),(sign(x5) - x4),(avg((x7 + log(abs(x7))) , sign(x8)) - expt(abs(sign(x1)) ,x1)),((log(abs(x1)) / sign(x8)) * (x4*x4*x4)),min(sign(x2) , (x0*x0)),min(exp(exp(x3)) , x3),max(ninteger(x5) , ninteger(sign(sign(x8)))),(expt(abs(sign(x1)) ,min(expt(abs(log(abs(x3))) ,log(abs(x9))) , x2)) / expt(abs(sign(x1)) ,min(expt(abs(log(abs(x3))) ,log(abs(x9))) , x2))),avg(sign(ninteger(x9)) , sign(x1)),avg(avg((avg((x8*x8*x8) , x6) + log(abs(sign(x5)))) , x3) , min((x5 / sign(expt(abs(log(abs(x2))) ,log(abs(x5))))) , min(exp(x5) , x5))));
myBestRegressorChampions[7],ErrorPct=[0.5740249608914,Score=[0.08228134915713],WFF= mvlregress(x8,tanh(x9),tanh(x4),(x5 % (x8 - x8)),max(min(tanh(x3) , x7) , x6),expt(abs(sign(avg((-ninteger(x6)) , x8))) ,x6),(min(((cos(x4) * cos(x5)) - x4) , ((cos(x4) * cos(x5)) - x4)) / avg(x0 , x1)),avg(x3 , x7),avg(expt(abs(sign(min(expt(abs(sign(min(exp(x4) , exp(x0)))) ,x2) , exp(tanh(x3))))) ,x2) , x2),(expt(abs(sign(log(abs(x3)))) ,log(abs(x6))) + min(avg((-x1) , x8) , x1)));
myBestRegressorChampions[8],ErrorPct=[0.5754710073698,Score=[0.08215924043133],WFF= mvlregress((ninteger(x5) - tan(x5)),(x1 * abs(((-x6) - sign(x8)))),min(sign(x4) , (ninteger(x8)*ninteger(x8))),min((avg((x2*x2*x2) , x6) % (x9 / sign(x8))) , x0),ninteger(x3),expt(abs(sign(x1)) ,tanh(expt(abs(sign(x1)) ,tanh(x0)))),(expt(abs(sign(x1)) ,tanh(x8)) / sign(sign(x8))),(((ninteger(x2) - sin(x0)) - ((-x6) - sqrt(abs(x9)))) + (ninteger(x9) - x8)),(sign(x0) + max((-x7) , sin(x5))),((((x4*x4*x4) + log(abs(x7)))*((x4*x4*x4) + log(abs(x7)))*((x4*x4*x4) + log(abs(x7)))) + log(abs(avg(min(sign(x4) , (x7*x7)) , x6)))));
myBestRegressorChampions[9],ErrorPct=[0.5808627982462,Score=[0.08544379188282],WFF= mvlregress((x7 - x9),((x1 - x8) - avg((-x0) , tanh(x9))),((x4 / expt(abs(sign(log(abs(x1)))) ,tanh(min((-x9) , x7)))) - expt(abs(sign(x3)) ,exp(x0))),(avg(sign(x2) , ((-(x8*x8*x8)) + (-sqrt(abs(x2))))) - avg(((avg(sign(x2) , ((-(x8*x8*x8)) + (-sqrt(abs(x2))))) - x7) - x5) , x8)),(avg(x6 , min(((-x5) % (ninteger(x8)*ninteger(x8))) , exp(x2))) - avg(sin(cos(x3)) , x0)),sin(cos(x0)),x4,x3,x1,abs(x4));
myBestRegressorChampions[10],ErrorPct=[0.5809076411514,Score=[0.08236804285533],WFF= mvlregress((max(x7 , (x3*x3*x3)) - x4),min(x8 , (x7 - x6)),min((cos((abs(x3) % x9)) * (x8*x8*x8)) , (x8 - x9)),max(max(ninteger(x9) , ((x0 - avg(sin(x1) , x0))*(x0 - avg(sin(x1) , x0))*(x0 - avg(sin(x1) , x0)))) , max(x5 , log(abs(x1)))),expt(abs(sign((x1 - x5))) ,max((1.0 / x5) , (1.0 / x7))),avg(x3 , ninteger(expt(abs(sign(x3)) ,x4))),avg(x1 , abs(x3)),avg((-x5) , avg(x2 , x9)),(x0 + tanh(avg(x2 , x6))),((-x4) + (1.0 / avg(log(abs(x4)) , x3))));
myBestRegressorChampions[11],ErrorPct=[0.5849630206177,Score=[0.08596795965656],WFF= mvlregress((x8 - ((1.0 / x9) % tanh(x3))),min((log(abs(x3)) * (cos(x9) % expt(abs(abs(x4)) ,x8))) , (avg((x3*x3*x3) , ninteger(x5)) % x9)),min(expt(abs(sign(x1)) ,tanh(x5)) , sign(tanh(x5))),max(max(min(ninteger(x4) , log(abs(x6))) , min(avg((1.0 / x2) , sin(x7)) , sign(x2))) , expt(abs(sign(x5)) ,(x3*x3*x3))),expt(abs(expt(abs(sign(x3)) ,(x2*x2*x2))) ,(x1*x1*x1)),avg(min(ninteger(x5) , log(abs(x6))) , ninteger(abs(x4))),avg(ninteger(x8) , sign(x2)),((((-x6) - x9) - ((-x5) % (((-x5) % (x4*x4))*((-x5) % (x4*x4))))) + ((-x0) - (-x7))),(ninteger(x7) + sign(x1)),((x0 + avg((1.0 / x2) , (ninteger(x9) + x9))) + avg((1.0 / x2) , sin(x7))));
myBestRegressorChampions[12],ErrorPct=[0.6109192146364,Score=[0.09962316519667],WFF= mvlregress((((-x6) - x9) - (ninteger(x4) - x7)),(ninteger(x2) - x4),min(sign(max(x7 , abs(x1))) , x6),max(x0 , ninteger(x8)),max(log(abs(max((exp(x8) % sign(x0)) , x5))) , ((x7 % min(sign(x4) , (x0*x0))) / ninteger(x2))),max((expt(abs(sign(x0)) ,log(abs(x3))) + (1.0 / x3)) , x0),expt(abs(sign(x4)) ,exp(x2)),(avg(min(ninteger(min(x7 , ninteger(x8))) , (x6*x6*x6)) , x6) / (log(abs(x1)) / sign(x5))),(x1 + (log(abs(x1)) / sign(x5))),((x6 + tanh(x8)) + ((-x1) + x8)));
myBestRegressorChampions[13],ErrorPct=[0.6150133713007,Score=[0.09438398950961],WFF= mvlregress(x9,(x2 - ((-x7) - sign(x8))),(max(tan(x6) , log(abs(x5))) - (((-x9) - x0) - x0)),(x7 * (((-x8) - sign(x8)) % x8)),min(x9 , exp(x8)),min(max(expt(abs((1.0 / max(abs(x5) , (x4*x4*x4)))) ,cos(max((tanh(x4)*tanh(x4)*tanh(x4)) , (1.0 / x9)))) , x3) , log(abs(x5))),expt(abs(sign(sign(avg(((ninteger(x5) - tan(x7)) % x9) , x9)))) ,ninteger(x7)),avg(avg(ninteger(x7) , (avg(avg(ninteger(x5) , x3) , log(abs(x2))) + avg(avg(ninteger(x5) , x3) , log(abs(x2))))) , sign(x8)),avg(avg(ninteger(x5) , x1) , (x6 + x0)),((x8*x8*x8) + ((x0 + x1) / (((-x8) - sign(x8)) % x8))));
myBestRegressorChampions[14],ErrorPct=[0.6273712644482,Score=[0.09867746993302],WFF= mvlregress((min(sign(x2) , (x0*x0)) - (ninteger(x2) - ((x3 - x6) - x3))),(ninteger((x3 - (x8 - x4))) - sin(sin((ninteger(x0) - sin(sin(x0)))))),min(min(x5 , ninteger((x0*x0))) , min(min(((x2 - x3) % x6) , ninteger(x8)) , (x0*x0))),max(max(min(exp(min((-x1) , exp(x7))) , x3) , ninteger(((ninteger(x7) - x0) % x2))) , max(min(exp(min((-x1) , exp(x7))) , x3) , ninteger(x7))),expt(abs(sign(x1)) ,(min(avg(x4 , (x5*x5*x5)) , x0)*min(avg(x4 , (x5*x5*x5)) , x0)*min(avg(x4 , (x5*x5*x5)) , x0))),expt(abs(log(abs((log(abs((-x0))) / sign(x0))))) ,expt(abs(sign(x1)) ,sign(x1))),(avg(avg((-x0) , x8) , x5) / sign(expt(abs(log(abs(x3))) ,log(abs(log(abs(x3))))))),avg(sign(x1) , (sign(x2) / sign(avg((x8*x8*x8) , (sign(x2) / sign(x8)))))),avg(ninteger(sign(x5)) , ninteger(ninteger(x3))),(min(exp(x3) , x3) + min(avg((-x0) , x8) , (avg(x4 , x9)*avg(x4 , x9)))));
myBestRegressorChampions[15],ErrorPct=[0.6345007598846,Score=[0.09991645019941],WFF= mvlregress(x1,x4,min(sign(ninteger(min(sign(x2) , (x7*x7)))) , (x6*x6)),min(exp(min((min(sign(x2) , (x0*x0)) - max(x0 , x5)) , ninteger(x8))) , x5),max((x4*x4*x4) , ninteger(x7)),ninteger(x8),expt(abs(expt(abs(sign(x1)) ,expt(abs(sign(x1)) ,x2))) ,x4),(tanh(x6) / x8),avg(ninteger(ninteger(x9)) , x0),(x3 + x3));

Show final results on training data, Score=[0.05138309738076], ErrorPct=[5.717045963064]
X[0], ey=[-6.02609129896], y=[-21.99864194741], ErrorPct=[6.540997561439]
X[50], ey=[-4.115266846619], y=[-18.26762979799], ErrorPct=[5.795603569587]
X[100], ey=[-1.726374204204], y=[-17.99864194741], ErrorPct=[6.663736179026]
X[150], ey=[-0.6298393153082], y=[-17.99864194741], ErrorPct=[7.112783559883]
X[200], ey=[-1.29864648152], y=[-15.99864194741], ErrorPct=[6.019867246743]
X[250], ey=[-1.728276678157], y=[-15.99864194741], ErrorPct=[5.843927277581]
X[300], ey=[0.03009032996022], y=[-15.99864194741], ErrorPct=[6.564004775869]
X[350], ey=[-3.077459004313], y=[-15.99864194741], ErrorPct=[5.291417005455]
X[400], ey=[-2.214082647993], y=[-13.99883022967], ErrorPct=[4.826029786383]
X[450], ey=[0.08953687272442], y=[-13.99864194741], ErrorPct=[5.769319211178]
X[500], ey=[-0.3540994661202], y=[-13.99864194741], ErrorPct=[5.58764351802]
X[550], ey=[0.1399895838059], y=[-13.99864194741], ErrorPct=[5.789980348364]
X[600], ey=[-0.3759806881645], y=[-13.27074498779], ErrorPct=[5.280598177184]
X[650], ey=[0.197844985281], y=[-11.99864194741], ErrorPct=[4.994643187598]
X[700], ey=[1.413684511208], y=[-11.99864194741], ErrorPct=[5.492547595556]
X[750], ey=[0.2695811737076], y=[-11.99864194741], ErrorPct=[5.024020225987]
X[800], ey=[-1.125762308504], y=[-11.99864194741], ErrorPct=[4.452606272424]
X[850], ey=[3.412393217989], y=[-9.998641995316], ErrorPct=[5.492018811355]
X[900], ey=[4.610505010012], y=[-9.998641947408], ErrorPct=[5.982663428428]
X[950], ey=[3.915129366217], y=[-8.044746129055], ErrorPct=[4.897747277282]
Actual computed error on training data is ErrorPct=[5.717045963064] versus reported ErrorPct=[5.717045963064] while average Y is AvgY=[-13.98682092264]
yHistory=[#(num| -19.05303714243 -17.44233667681 -15.99864194741 -15.63966882049 -13.99864618517 -13.98786596885 -12.32170360044 -11.99864194741 -10.78612173614 -8.641545201263 )]
eHistory=[#(num| -18.63569111849 -17.00342142723 -15.97288463017 -15.14078446254 -14.04680271524 -13.85235385351 -13.06172826913 -12.01056065207 -11.04577274879 -9.098209349252 )]
aHistory=[#(num| -18.63569111849 -17.81955627286 -17.20399905863 -16.68819540961 -16.15991687073 -11.81372497455 -11.30406775481 -10.7181809167 -10.07199104902 -9.098209349252 )]
dHistory=[#(num| 4.346191896182 5.384127654795 6.485818141925 7.747565223837 9.537481769234 )]


Final testing on test data returns Score=[0.05229992062224], ErrorPct=[5.672584628393]
X[0], ey=[-7.503161990256], y=[-21.99864194741], ErrorPct=[5.877629424109]
X[50], ey=[-3.889267717873], y=[-19.02994022122], ErrorPct=[6.139242196154]
X[100], ey=[-2.353289080196], y=[-17.99864194741], ErrorPct=[6.343880067077]
X[150], ey=[-3.666460117775], y=[-17.99864194741], ErrorPct=[5.811415274454]
X[200], ey=[-0.5241302708712], y=[-15.99864194741], ErrorPct=[6.274607355023]
X[250], ey=[-1.169972445832], y=[-15.99864194741], ErrorPct=[6.012731171406]
X[300], ey=[-1.305321726073], y=[-15.99864194741], ErrorPct=[5.957849724608]
X[350], ey=[-1.096678128825], y=[-15.99864194741], ErrorPct=[6.042450562246]
X[400], ey=[-2.214082647993], y=[-13.99883022967], ErrorPct=[4.778481247017]
X[450], ey=[-1.97146242675], y=[-13.99864194741], ErrorPct=[4.876782586615]
X[500], ey=[-2.346279177084], y=[-13.99864194741], ErrorPct=[4.724801833516]
X[550], ey=[-0.8013770116988], y=[-13.99864194741], ErrorPct=[5.35122899919]
X[600], ey=[0.129964418376], y=[-13.99864194741], ErrorPct=[5.72886946432]
X[650], ey=[0.4459020172258], y=[-12.0185833338], ErrorPct=[5.054101421416]
X[700], ey=[1.604186746419], y=[-11.99864194741], ErrorPct=[5.515677053695]
X[750], ey=[-0.1893966407053], y=[-11.99864194741], ErrorPct=[4.788414588298]
X[800], ey=[1.056463736373], y=[-11.99864194741], ErrorPct=[5.293586244034]
X[850], ey=[3.412393217989], y=[-9.998641995316], ErrorPct=[5.437908603957]
X[900], ey=[4.686961796742], y=[-9.998641947408], ErrorPct=[5.954720846262]
X[950], ey=[3.915129366217], y=[-8.044746129055], ErrorPct=[4.849492140135]
Actual computed error on testing data is ErrorPct=[5.672584628393], Avg Y=[-14.03477757154], AvgDev Y=[2.46621195574]
yHistory=[#(num| -19.20184672009 -17.63582520105 -15.99864194741 -15.63276856356 -13.99864530789 -13.99864194741 -12.59095805117 -11.99864194741 -10.68363513307 -8.608170896369 )]
eHistory=[#(num| -18.69570975799 -17.26395695209 -15.89787014974 -15.16949003866 -14.04066134549 -13.85438129629 -13.06397839725 -12.33211717378 -11.07316555933 -8.956445044818 )]
aHistory=[#(num| -18.69570975799 -17.97983335504 -17.28584561994 -16.75675672462 -16.21353764879 -11.85601749429 -11.3564265438 -10.78724259264 -10.01480530207 -8.956445044818 )]
dHistory=[#(num| 4.357520154499 5.400330180823 6.498603027296 7.965028052967 9.739264713169 )]

esm.selfTest[hyperTangent]: completed in [121.9372333333] minutes.

Starting test case: elipsoid
Building test data as: y = 0.0 + (1.0*x0*x0) + (2.0*x1*x1) + (3.0*x2*x2) + (4.0*x3*x3) + (5.0*x4*x4) + (6.0*x5*x5) + (7.0*x6*x6) + (8.0*x7*x7) + (9.0*x8*x8) + (10.0*x9*x9);
...starting new evolutionary process.
...starting generation of constant selector WFF.
...starting regress selector for each column name.
...starting a best-of-breed MVL selector WFF.
...starting a best-of-breed REG selector WFF.
...starting generation of root selector WFFs in each column island.
...starting generation of random selector WFFs.

Final results of training
esm: N = [1000], M = [10], Generations = [10], WFFs = [21650], Score=[0.008975388910574], ScoreHistory=[#(num| 0.008975388910574 )]
esm.myBest, Score=[0.008975388910574], ErrorPct=[0.2023382613793], lengthWFF=[889], WFF = mvlregress(max(sqrt(abs(x2)) , min(min(max(x7 , min(x6 , x7)) , (cos(x6)*cos(x6))) , x7)),max(max((x9*x9) , (ninteger(x3) / exp(x3))) , abs(x6)),max(ninteger(x0) , max(x0 , max(x1 , x7))),(max(x1 , x8) / (avg(x1 , abs(x8)) % avg((x7*x7) , x9))),avg((x8*x8) , abs(max(abs(avg((x8*x8) , avg((x8*x8) , x3))) , x1))),avg((x4*x4) , min(max(x0 , x8) , max(max(x1 , x9) , x8))),avg(avg((x7*x7) , avg((x5*x5) , x5)) , sqrt(abs(avg(abs(x6) , sqrt(abs(x4)))))),avg(avg(ninteger(x0) , ninteger(x0)) , x4),avg(avg(avg((max(x7 , avg((x5*x5) , x2))*max(x7 , avg((x5*x5) , x2))) , x6) , x4) , avg((x9*x9) , x4)),avg(abs(x6) , abs(avg(abs(x3) , abs(x4)))));
esm.myBestRegressor, Score=[0.01003473724311], ErrorPct=[0.2022149402264], lengthWFF=[426], WFF = mvlregress(max((x9*x9) , min(x0 , x7)),max((x2*x2) , min(x6 , x7)),max(max(x1 , x9) , x8),(ninteger(x3) / exp(x3)),avg((x8*x8) , avg((x8*x8) , x3)),avg((x7*x7) , x9),avg((max(x7 , avg((x5*x5) , x2))*max(x7 , avg((x5*x5) , x2))) , x6),avg(avg(ninteger(x0) , x1) , x4),avg(abs(x6) , sqrt(abs(x4))),avg(abs(x3) , abs(x4)));
myBestSelectorChampions
myBestSelectorChampions[0],Score=[0.008975388910574,ErrorPct=[0.2023382613793],WFF= mvlregress(max(sqrt(abs(x2)) , min(min(max(x7 , min(x6 , x7)) , (cos(x6)*cos(x6))) , x7)),max(max((x9*x9) , (ninteger(x3) / exp(x3))) , abs(x6)),max(ninteger(x0) , max(x0 , max(x1 , x7))),(max(x1 , x8) / (avg(x1 , abs(x8)) % avg((x7*x7) , x9))),avg((x8*x8) , abs(max(abs(avg((x8*x8) , avg((x8*x8) , x3))) , x1))),avg((x4*x4) , min(max(x0 , x8) , max(max(x1 , x9) , x8))),avg(avg((x7*x7) , avg((x5*x5) , x5)) , sqrt(abs(avg(abs(x6) , sqrt(abs(x4)))))),avg(avg(ninteger(x0) , ninteger(x0)) , x4),avg(avg(avg((max(x7 , avg((x5*x5) , x2))*max(x7 , avg((x5*x5) , x2))) , x6) , x4) , avg((x9*x9) , x4)),avg(abs(x6) , abs(avg(abs(x3) , abs(x4)))));
myBestRegressorChampions
myBestRegressorChampions[0],ErrorPct=[0.2022149402264,Score=[0.01003473724311],WFF= mvlregress(max((x9*x9) , min(x0 , x7)),max((x2*x2) , min(x6 , x7)),max(max(x1 , x9) , x8),(ninteger(x3) / exp(x3)),avg((x8*x8) , avg((x8*x8) , x3)),avg((x7*x7) , x9),avg((max(x7 , avg((x5*x5) , x2))*max(x7 , avg((x5*x5) , x2))) , x6),avg(avg(ninteger(x0) , x1) , x4),avg(abs(x6) , sqrt(abs(x4))),avg(abs(x3) , abs(x4)));

Show final results on training data, Score=[0.008975388910574], ErrorPct=[0.2023382613793]
X[0], ey=[11662.43559659], y=[6840.806451978], ErrorPct=[0.4225242730266]
X[50], ey=[26214.43268188], y=[24181.12317701], ErrorPct=[0.1781809829454]
X[100], ey=[30986.47838693], y=[28898.05165456], ErrorPct=[0.1830109617309]
X[150], ey=[32454.85846706], y=[32126.08759339], ErrorPct=[0.02881052653081]
X[200], ey=[35795.36572435], y=[34377.56080021], ErrorPct=[0.1242436896116]
X[250], ey=[38142.27382633], y=[36606.60346594], ErrorPct=[0.1345723578425]
X[300], ey=[38095.70792681], y=[38746.34795917], ErrorPct=[0.05701624874673]
X[350], ey=[35884.94980318], y=[40267.53347182], ErrorPct=[0.3840502707761]
X[400], ey=[46230.09351498], y=[42008.12725866], ErrorPct=[0.3699752033365]
X[450], ey=[47262.63121892], y=[43964.79334855], ErrorPct=[0.2889928916023]
X[500], ey=[46852.3806901], y=[45640.0423557], ErrorPct=[0.106238443074]
X[550], ey=[49350.4170699], y=[47589.42141362], ErrorPct=[0.1543178430272]
X[600], ey=[51224.68564044], y=[49333.01089517], ErrorPct=[0.1657693846987]
X[650], ey=[48728.91626188], y=[50996.87086128], ErrorPct=[0.1987431715771]
X[700], ey=[45172.07253939], y=[53823.13708991], ErrorPct=[0.7581015981287]
X[750], ey=[52987.22990214], y=[55795.45654465], ErrorPct=[0.2460877610102]
X[800], ey=[59502.44631677], y=[58427.58697192], ErrorPct=[0.09419101919108]
X[850], ey=[61239.42300755], y=[62458.81019386], ErrorPct=[0.106856141147]
X[900], ey=[63038.52417128], y=[65976.43383302], ErrorPct=[0.2574520160742]
X[950], ey=[67708.73122252], y=[72245.72862275], ErrorPct=[0.397581703354]
Actual computed error on training data is ErrorPct=[0.2023382613793] versus reported ErrorPct=[0.2023382613793] while average Y is AvgY=[46494.86938341]
yHistory=[#(num| 22695.79043542 31921.37254937 36673.02980415 40332.29643948 43874.19872954 47550.1260796 51299.10294314 55897.02282559 62285.54113998 72420.21288785 )]
eHistory=[#(num| 23138.57219582 32312.37755078 36801.55380546 40080.96165913 44288.49303726 47341.87929336 51413.95692811 55444.98710168 62148.34428844 71977.56797408 )]
aHistory=[#(num| 23138.57219582 27725.4748733 30750.83451735 33083.3663028 35324.39164969 57665.34711713 60246.21407308 63190.29978807 67062.95613126 71977.56797408 )]
dHistory=[#(num| 22340.95546744 27162.84777028 32439.46527071 39337.48125796 48838.99577826 )]


Final testing on test data returns Score=[0.008742347977787], ErrorPct=[0.1995861081602]
X[0], ey=[11501.76461002], y=[5987.992113759], ErrorPct=[0.4738494181939]
X[50], ey=[24754.11622253], y=[21491.91425969], ErrorPct=[0.2803511576094]
X[100], ey=[30318.18250365], y=[26826.52111088], ErrorPct=[0.3000707266419]
X[150], ey=[31676.52557177], y=[30479.34240567], ErrorPct=[0.1028850115072]
X[200], ey=[32020.27174439], y=[33595.37766004], ErrorPct=[0.135363405406]
X[250], ey=[37082.77603229], y=[36116.60082224], ErrorPct=[0.0830323633167]
X[300], ey=[39040.11655671], y=[38427.67434067], ErrorPct=[0.0526328186277]
X[350], ey=[39992.41843515], y=[40081.57086388], ErrorPct=[0.007661691975171]
X[400], ey=[42388.99378711], y=[41617.28791043], ErrorPct=[0.06631981659143]
X[450], ey=[48418.72777098], y=[43485.74176793], ErrorPct=[0.4239370683294]
X[500], ey=[47477.67224738], y=[45834.51328339], ErrorPct=[0.1412118326636]
X[550], ey=[48972.55827597], y=[47728.71349528], ErrorPct=[0.1068950752057]
X[600], ey=[50905.33209436], y=[49088.95837067], ErrorPct=[0.1560977774799]
X[650], ey=[44548.81831886], y=[51279.2970711], ErrorPct=[0.5784122292091]
X[700], ey=[53027.05118371], y=[53654.46937328], ErrorPct=[0.05391984241122]
X[750], ey=[59219.76293026], y=[56095.02892222], ErrorPct=[0.2685372660411]
X[800], ey=[60761.57466275], y=[58294.35883639], ErrorPct=[0.2120306531817]
X[850], ey=[61746.43864463], y=[60939.02953882], ErrorPct=[0.06938812497168]
X[900], ey=[58833.21183964], y=[63861.97412705], ErrorPct=[0.4321680094231]
X[950], ey=[69663.78659681], y=[69861.87077398], ErrorPct=[0.01702320365442]
Actual computed error on testing data is ErrorPct=[0.1995861081602], Avg Y=[45876.5578783], AvgDev Y=[11636.12802835]
yHistory=[#(num| 20916.59000118 30695.26365089 36058.43061933 39980.00987219 43552.75488484 47605.54452916 51214.78410407 55931.78957335 60969.27629555 71841.1352524 )]
eHistory=[#(num| 21357.14407729 30736.12981617 36819.65534778 39600.65571593 43408.46701907 47891.36947529 51041.16425311 55618.56462491 60947.0296307 71345.3988227 )]
aHistory=[#(num| 21357.14407729 26046.63694673 29637.64308041 32128.39623929 34384.41039525 57368.70536134 59738.03933286 62636.99769277 66146.2142267 71345.3988227 )]
dHistory=[#(num| 22984.29496609 27609.64309356 32999.35461236 40099.57727997 49988.25474541 )]

esm.selfTest[elipsoid]: completed in [7.313283333333] minutes.

Starting test case: hiddenModel
Building test data as: y = -13.99864194741 + (5.499486281783*sin(x5));
...starting new evolutionary process.
...starting generation of constant selector WFF.
...starting regress selector for each column name.
...starting a best-of-breed MVL selector WFF.
...starting a best-of-breed REG selector WFF.
...starting generation of root selector WFFs in each column island.
...locating the best selector and the best regressor from this training run.

Final results of training
esm: N = [1000], M = [10], Generations = [0], WFFs = [9114], Score=[5.018167673163E-019], ScoreHistory=[#(num| 5.018167673163E-019 )]
esm.myBest, Score=[5.018167673163E-019], ErrorPct=[1.304078256625E-015], lengthWFF=[80], WFF = regress(sin(x5));
esm.myBestRegressor, Score=[5.018167673163E-019], ErrorPct=[1.304078256625E-015], lengthWFF=[80], WFF = regress(sin(x5));
myBestSelectorChampions
myBestSelectorChampions[0],Score=[5.018167673163E-019,ErrorPct=[1.304078256625E-015],WFF= regress(sin(x5));
myBestRegressorChampions
myBestRegressorChampions[0],ErrorPct=[1.304078256625E-015,Score=[5.018167673163E-019],WFF= regress(sin(x5));

Show final results on training data, Score=[5.018167673163E-019], ErrorPct=[1.304078256625E-015]
X[0], ey=[-19.49812581752], y=[-19.49812581752], ErrorPct=[2.004732139316E-015]
X[50], ey=[-19.42818147734], y=[-19.42818147734], ErrorPct=[2.004732139316E-015]
X[100], ey=[-19.20335103182], y=[-19.20335103182], ErrorPct=[2.004732139316E-015]
X[150], ey=[-18.87569640492], y=[-18.87569640492], ErrorPct=[2.004732139316E-015]
X[200], ey=[-18.48936626612], y=[-18.48936626612], ErrorPct=[1.002366069658E-015]
X[250], ey=[-17.85778290542], y=[-17.85778290542], ErrorPct=[1.002366069658E-015]
X[300], ey=[-17.20168913903], y=[-17.20168913903], ErrorPct=[1.002366069658E-015]
X[350], ey=[-16.54262805481], y=[-16.54262805481], ErrorPct=[0.0]
X[400], ey=[-15.81461185388], y=[-15.81461185388], ErrorPct=[5.011830348289E-016]
X[450], ey=[-15.24326299302], y=[-15.24326299302], ErrorPct=[5.011830348289E-016]
X[500], ey=[-14.25378698281], y=[-14.25378698281], ErrorPct=[0.0]
X[550], ey=[-13.15766775198], y=[-13.15766775198], ErrorPct=[5.011830348289E-016]
X[600], ey=[-12.28685096199], y=[-12.28685096199], ErrorPct=[5.011830348289E-016]
X[650], ey=[-11.46456697328], y=[-11.46456697328], ErrorPct=[1.002366069658E-015]
X[700], ey=[-10.78539995614], y=[-10.78539995614], ErrorPct=[1.002366069658E-015]
X[750], ey=[-9.965035175473], y=[-9.965035175473], ErrorPct=[1.503549104487E-015]
X[800], ey=[-9.446822652228], y=[-9.446822652228], ErrorPct=[1.503549104487E-015]
X[850], ey=[-9.095393642971], y=[-9.095393642971], ErrorPct=[1.503549104487E-015]
X[900], ey=[-8.712130701559], y=[-8.712130701559], ErrorPct=[2.004732139316E-015]
X[950], ey=[-8.579789229671], y=[-8.579789229671], ErrorPct=[2.004732139316E-015]
Actual computed error on training data is ErrorPct=[1.304078256625E-015] versus reported ErrorPct=[1.304078256625E-015] while average Y is AvgY=[-14.02891621428]
yHistory=[#(num| -19.41088655989 -18.89155283568 -17.85712410448 -16.53217438309 -15.15874696673 -13.21775897777 -11.50605042786 -10.02838949939 -9.10011403115 -8.58636435675 )]
eHistory=[#(num| -19.41088655989 -18.89155283568 -17.85712410448 -16.53217438309 -15.15874696673 -13.21775897777 -11.50605042786 -10.02838949939 -9.10011403115 -8.58636435675 )]
aHistory=[#(num| -19.41088655989 -19.15121969779 -18.71985450002 -18.17293447079 -17.57009696998 -10.48773545858 -9.805229578787 -9.238289295764 -8.84323919395 -8.58636435675 )]
dHistory=[#(num| 7.082361511393 8.367704892001 9.481565204255 10.30798050384 10.82452220314 )]


Final testing on test data returns Score=[4.970301141135E-019], ErrorPct=[1.298142629663E-015]
X[0], ey=[-19.49812399836], y=[-19.49812399836], ErrorPct=[2.044319101832E-015]
X[50], ey=[-19.43280804506], y=[-19.43280804506], ErrorPct=[2.044319101832E-015]
X[100], ey=[-19.20018785487], y=[-19.20018785487], ErrorPct=[2.044319101832E-015]
X[150], ey=[-18.88082269654], y=[-18.88082269654], ErrorPct=[1.022159550916E-015]
X[200], ey=[-18.21744697639], y=[-18.21744697639], ErrorPct=[2.044319101832E-015]
X[250], ey=[-17.57504076892], y=[-17.57504076892], ErrorPct=[1.022159550916E-015]
X[300], ey=[-16.98981688248], y=[-16.98981688248], ErrorPct=[1.022159550916E-015]
X[350], ey=[-16.18305410655], y=[-16.18305410655], ErrorPct=[1.022159550916E-015]
X[400], ey=[-15.66233253171], y=[-15.66233253171], ErrorPct=[5.110797754579E-016]
X[450], ey=[-14.78721398385], y=[-14.78721398385], ErrorPct=[5.110797754579E-016]
X[500], ey=[-13.92289786372], y=[-13.92289786372], ErrorPct=[0.0]
X[550], ey=[-13.0130609995], y=[-13.0130609995], ErrorPct=[5.110797754579E-016]
X[600], ey=[-12.08119652434], y=[-12.08119652434], ErrorPct=[5.110797754579E-016]
X[650], ey=[-11.24475457948], y=[-11.24475457948], ErrorPct=[1.022159550916E-015]
X[700], ey=[-10.58356911665], y=[-10.58356911665], ErrorPct=[1.533239326374E-015]
X[750], ey=[-10.05295557416], y=[-10.05295557416], ErrorPct=[2.044319101832E-015]
X[800], ey=[-9.550865337403], y=[-9.550865337403], ErrorPct=[1.533239326374E-015]
X[850], ey=[-9.216108779542], y=[-9.216108779542], ErrorPct=[2.044319101832E-015]
X[900], ey=[-8.804942463085], y=[-8.804942463085], ErrorPct=[2.044319101832E-015]
X[950], ey=[-8.585001326275], y=[-8.585001326275], ErrorPct=[2.044319101832E-015]
Actual computed error on testing data is ErrorPct=[1.298142629663E-015], Avg Y=[-13.91424489213], AvgDev Y=[3.47569386366]
yHistory=[#(num| -19.4126914188 -18.84207250059 -17.59572103908 -16.25794061039 -14.84118168037 -13.01861571757 -11.31093115832 -10.04826669707 -9.208082588635 -8.606945510467 )]
eHistory=[#(num| -19.4126914188 -18.84207250059 -17.59572103908 -16.25794061039 -14.84118168037 -13.01861571757 -11.31093115832 -10.04826669707 -9.208082588635 -8.606945510467 )]
aHistory=[#(num| -19.4126914188 -19.1273819597 -18.61682831949 -18.02710639222 -17.38992144985 -10.43856833441 -9.793556488625 -9.287764932059 -8.907514049551 -8.606945510467 )]
dHistory=[#(num| 6.951353115434 8.233549903591 9.329063387431 10.21986791014 10.80574590833 )]

esm.selfTest[hiddenModel]: completed in [2.3664] minutes.

Starting test case: linearRegression
Building test data as: y = -13.99864194741 - (13.99864194741*x0) - (32.66789144459*x1) - (30.73171497235*x2) + (10.01135458484*x3) - (2.83159613203*x4) + (5.499486281783*x5) - (46.33521181329*x6) + (10.35455431571*x7) + (18.14155680594*x8) - (3.363675194861*x9);
...starting new evolutionary process.
...starting generation of constant selector WFF.
...starting regress selector for each column name.
...starting a best-of-breed MVL selector WFF.
...locating the best selector and the best regressor from this training run.

Final results of training
esm: N = [1000], M = [10], Generations = [0], WFFs = [13], Score=[1.084135930752E-005], ScoreHistory=[#(num| 1.084135930752E-005 )]
esm.myBest, Score=[1.084135930752E-005], ErrorPct=[0.008453294585231], lengthWFF=[143], WFF = mvlregress(x9,x8,x7,x6,x5,x4,x3,x2,x1,x0);
esm.myBestRegressor, Score=[1.084135930752E-005], ErrorPct=[0.008453294585231], lengthWFF=[143], WFF = mvlregress(x9,x8,x7,x6,x5,x4,x3,x2,x1,x0);
myBestSelectorChampions
myBestSelectorChampions[0],Score=[1.084135930752E-005,ErrorPct=[0.008453294585231],WFF= mvlregress(x9,x8,x7,x6,x5,x4,x3,x2,x1,x0);
myBestRegressorChampions
myBestRegressorChampions[0],ErrorPct=[0.008453294585231,Score=[1.084135930752E-005],WFF= mvlregress(x9,x8,x7,x6,x5,x4,x3,x2,x1,x0);

Show final results on training data, Score=[1.084135930752E-005], ErrorPct=[0.008453294585231]
X[0], ey=[-5271.300653522], y=[-5285.500628724], ErrorPct=[0.008585175109116]
X[50], ey=[-3402.505729188], y=[-3416.080223117], ErrorPct=[0.008207014852879]
X[100], ey=[-2646.268829136], y=[-2659.350566711], ErrorPct=[0.007909098869212]
X[150], ey=[-2192.603339924], y=[-2206.474139513], ErrorPct=[0.008386158544494]
X[200], ey=[-1756.917386028], y=[-1771.088433646], ErrorPct=[0.008567685755998]
X[250], ey=[-1391.28695472], y=[-1405.94240124], ErrorPct=[0.00886054890135]
X[300], ey=[-1085.743041012], y=[-1100.779243844], ErrorPct=[0.009090750684688]
X[350], ey=[-756.8222546839], y=[-770.4349755639], ErrorPct=[0.00823012651814]
X[400], ey=[-519.5970701835], y=[-534.409497473], ErrorPct=[0.00895545803866]
X[450], ey=[-298.2109634814], y=[-312.5967548381], ErrorPct=[0.008697517856492]
X[500], ey=[15.4243894984], y=[1.590983392676], ErrorPct=[0.008363550786851]
X[550], ey=[296.2270225528], y=[282.4465236807], ErrorPct=[0.008331563557377]
X[600], ey=[572.0583991882], y=[558.2556040414], ErrorPct=[0.008345043681097]
X[650], ey=[834.4190556387], y=[820.6976608719], ErrorPct=[0.008295829755971]
X[700], ey=[1164.196956457], y=[1150.085697013], ErrorPct=[0.008531538373554]
X[750], ey=[1468.59789349], y=[1454.898373924], ErrorPct=[0.008282604209429]
X[800], ey=[1783.470504649], y=[1770.35410917], ErrorPct=[0.007930052720686]
X[850], ey=[2245.469061545], y=[2232.277680592], ErrorPct=[0.007975388252107]
X[900], ey=[2627.501241278], y=[2613.031621475], ErrorPct=[0.008748199768104]
X[950], ey=[3354.400134333], y=[3340.747139625], ErrorPct=[0.008254475705619]
Actual computed error on training data is ErrorPct=[0.008453294585231] versus reported ErrorPct=[0.008453294585231] while average Y is AvgY=[-0.5812652573506]
yHistory=[#(num| -3538.304305014 -2229.536937711 -1415.059968214 -802.0816553292 -287.9791207543 262.3309746378 847.2612958013 1439.01874349 2173.9425955 3544.595725019 )]
eHistory=[#(num| -3538.304305014 -2229.536937711 -1415.059968214 -802.0816553292 -287.9791207543 262.3309746378 847.2612958013 1439.01874349 2173.9425955 3544.595725019 )]
aHistory=[#(num| -3538.304305014 -2883.920621362 -2394.300403646 -1996.245716567 -1654.592397404 1653.42986689 2001.204589953 2385.85235467 2859.269160259 3544.595725019 )]
dHistory=[#(num| 3308.022264294 3997.45030652 4780.152758316 5743.189781622 7082.900030033 )]


Final testing on test data returns Score=[1.038399539726E-005], ErrorPct=[0.008589611097953]
X[0], ey=[-5095.038292232], y=[-5108.966090193], ErrorPct=[0.008546533419231]
X[50], ey=[-3293.14395504], y=[-3307.462504956], ErrorPct=[0.008786311067624]
X[100], ey=[-2661.613687427], y=[-2676.035617198], ErrorPct=[0.008849748187586]
X[150], ey=[-2194.195800101], y=[-2208.448198229], ErrorPct=[0.008745718257167]
X[200], ey=[-1814.923655149], y=[-1828.605520788], ErrorPct=[0.008395621637146]
X[250], ey=[-1432.016430573], y=[-1445.853768039], ErrorPct=[0.008491024024041]
X[300], ey=[-1099.618728668], y=[-1114.486266539], ErrorPct=[0.009123187285993]
X[350], ey=[-786.7713896249], y=[-801.1536258616], ErrorPct=[0.008825390990444]
X[400], ey=[-548.5185229389], y=[-562.5271495134], ErrorPct=[0.008596132390249]
X[450], ey=[-318.1012651748], y=[-332.0366406839], ErrorPct=[0.008551183240335]
X[500], ey=[-57.76100305961], y=[-72.43594013856], ErrorPct=[0.009005001402406]
X[550], ey=[191.1255842498], y=[177.2424111912], ErrorPct=[0.008519150180346]
X[600], ey=[488.5399962157], y=[474.4114974509], ErrorPct=[0.008669689723768]
X[650], ey=[795.782767669], y=[782.2281743184], ErrorPct=[0.008317523371621]
X[700], ey=[1135.954269859], y=[1121.622717917], ErrorPct=[0.008794289517919]
X[750], ey=[1389.034916525], y=[1374.966955061], ErrorPct=[0.008632542138498]
X[800], ey=[1728.544331247], y=[1714.884199416], ErrorPct=[0.0083822850913]
X[850], ey=[2184.163771908], y=[2169.723248543], ErrorPct=[0.008861157799109]
X[900], ey=[2599.218794358], y=[2585.872370859], ErrorPct=[0.008189783825682]
X[950], ey=[3295.633418564], y=[3281.760538568], ErrorPct=[0.008512834034416]
Actual computed error on testing data is ErrorPct=[0.008589611097953], Avg Y=[-30.26230369959], AvgDev Y=[1629.642953195]
yHistory=[#(num| -3448.467750488 -2258.875243186 -1437.066889673 -835.5128534142 -317.7039042148 199.5831247087 791.5502003465 1394.296766265 2130.299898787 3479.273613872 )]
eHistory=[#(num| -3448.467750488 -2258.875243186 -1437.066889673 -835.5128534142 -317.7039042148 199.5831247087 791.5502003465 1394.296766265 2130.299898787 3479.273613872 )]
aHistory=[#(num| -3448.467750488 -2853.671496837 -2381.469961116 -1994.98068419 -1659.525328195 1599.000720796 1948.855119818 2334.623426308 2804.78675633 3479.273613872 )]
dHistory=[#(num| 3258.526048991 3943.835804008 4716.093387424 5658.458253167 6927.74136436 )]

esm.selfTest[linearRegression]: completed in [0.03828333333331] minutes.

Starting test case: mixedModels
Building test data as:
if ((x0 % 4) == 0) y = - (13.99864194741*log(.000001+abs(x0))) - (32.66789144459*log(.000001+abs(x1))) - (30.73171497235*log(.000001+abs(x2))) + (10.01135458484*log(.000001+abs(x3))) - (2.83159613203*log(.000001+abs(x4))) + (5.499486281783*log(.000001+abs(x5))) - (46.33521181329*log(.000001+abs(x6))) + (10.35455431571*log(.000001+abs(x7))) + (18.14155680594*log(.000001+abs(x8))) - (3.363675194861*log(.000001+abs(x9)));
if ((x0 % 4) == 1) y = - (13.99864194741*x0*x0) - (32.66789144459*x1*x1) - (30.73171497235*x2*x2) + (10.01135458484*x3*x3) - (2.83159613203*x4*x4) + (5.499486281783*x5*x5) - (46.33521181329*x6*x6) + (10.35455431571*x7*x7) + (18.14155680594*x8*x8) - (3.363675194861*x9*x9);
if ((x0 % 4) == 2) y = - (13.99864194741*sin(x0)) - (32.66789144459*sin(x1)) - (30.73171497235*sin(x2)) + (10.01135458484*sin(x3)) - (2.83159613203*sin(x4)) + (5.499486281783*sin(x5)) - (46.33521181329*sin(x6)) + (10.35455431571*sin(x7)) + (18.14155680594*sin(x8)) - (3.363675194861*sin(x9));
if ((x0 % 4) == 3) y = - (13.99864194741*x0) - (32.66789144459*x1) - (30.73171497235*x2) + (10.01135458484*x3) - (2.83159613203*x4) + (5.499486281783*x5) - (46.33521181329*x6) + (10.35455431571*x7) + (18.14155680594*x8) - (3.363675194861*x9);
...starting new evolutionary process.
...starting generation of constant selector WFF.
...starting regress selector for each column name.
...starting a best-of-breed MVL selector WFF.
...starting a best-of-breed REG selector WFF.
...starting generation of root selector WFFs in each column island.
...starting generation of random selector WFFs.

Final results of training
esm: N = [1000], M = [10], Generations = [171], WFFs = [26164], Score=[0.00939469174857], ScoreHistory=[#(num| 0.00939469174857 )]
esm.myBest, Score=[0.00939469174857], ErrorPct=[0.1953482518649], lengthWFF=[1863], WFF = mvlregress(((max(min((1.0 / x7) , x6) , max(min(min((1.0 / x7) , x6) , x6) , sqrt(abs(4.472122186797)))) - expt(abs(avg(abs(x4) , abs(x6))) ,(avg(log(abs(x1)) , abs(x6)) - avg(log(abs(avg(cos(x5) , sqrt(abs(x1))))) , log(abs(avg(log(abs(x8)) , log(abs(x7))))))))) * log(abs(x6))),min(max(max(min(abs(max(log(abs(x7)) , (x6*x6))) , (x6*x6)) , avg(sqrt(abs(4.472122186797)) , (if (x7 > x3) {x3} else {x0}))) , log(abs(-1.908797964953))) , avg(avg(log(abs(x6)) , log(abs(x2))) , avg(log(abs(x1)) , log(abs(x6))))),max(log(abs(x8)) , (x6*x6)),((max(max(x2 , avg(sqrt(abs(4.472122186797)) , (if (x7 > x3) {x3} else {x0}))) , avg(sqrt(abs(4.472122186797)) , (if (x7 > x3) {x3} else {x0}))) % max((x6*x6) , (x6*x6))) / (sqrt(abs((min(abs(x0) , (x6*x6)) * log(abs(x6))))) % max(x9 , (avg(cos(x7) , abs(x6)) * abs(x6))))),(avg(avg(x3 , (x6*x6)) , sqrt(abs(x1))) / max(avg(log(abs(x6)) , log(abs(x2))) , log(abs((x1*x1))))),avg(max(abs(x6) , max((max((x7*x7) , x2)*max((x7*x7) , x2)) , log(abs(x6)))) , avg(cos(x5) , sqrt(abs(x1)))),avg(log(abs(x8)) , log(abs(log(abs(avg(x3 , (avg(log(abs(x6)) , avg(cos(x5) , sqrt(abs(x1))))*avg(log(abs(x6)) , avg(cos(x5) , sqrt(abs(x1))))))))))),avg(log(abs(x6)) , log(abs(abs(x0)))),avg(avg(log(abs(x6)) , log(abs(x2))) , avg(max(log(abs(max(((log(abs(x5)) / (sign(x8) / x8))*(log(abs(x5)) / (sign(x8) / x8))) , sqrt(abs(x4))))) , avg(log(abs(x6)) , log(abs(x2)))) , avg(log(abs(avg(x3 , avg(log(abs(x8)) , log(abs(x7)))))) , log(abs(x6))))),avg(avg(cos(abs(x6)) , abs(x6)) , avg(avg(log(abs(x8)) , log(abs(x7))) , log(abs(x7)))));
esm.myBestRegressor, Score=[0.00939469174857], ErrorPct=[0.1953482518649], lengthWFF=[1863], WFF = mvlregress(((max(min((1.0 / x7) , x6) , max(min(min((1.0 / x7) , x6) , x6) , sqrt(abs(4.472122186797)))) - expt(abs(avg(abs(x4) , abs(x6))) ,(avg(log(abs(x1)) , abs(x6)) - avg(log(abs(avg(cos(x5) , sqrt(abs(x1))))) , log(abs(avg(log(abs(x8)) , log(abs(x7))))))))) * log(abs(x6))),min(max(max(min(abs(max(log(abs(x7)) , (x6*x6))) , (x6*x6)) , avg(sqrt(abs(4.472122186797)) , (if (x7 > x3) {x3} else {x0}))) , log(abs(-1.908797964953))) , avg(avg(log(abs(x6)) , log(abs(x2))) , avg(log(abs(x1)) , log(abs(x6))))),max(log(abs(x8)) , (x6*x6)),((max(max(x2 , avg(sqrt(abs(4.472122186797)) , (if (x7 > x3) {x3} else {x0}))) , avg(sqrt(abs(4.472122186797)) , (if (x7 > x3) {x3} else {x0}))) % max((x6*x6) , (x6*x6))) / (sqrt(abs((min(abs(x0) , (x6*x6)) * log(abs(x6))))) % max(x9 , (avg(cos(x7) , abs(x6)) * abs(x6))))),(avg(avg(x3 , (x6*x6)) , sqrt(abs(x1))) / max(avg(log(abs(x6)) , log(abs(x2))) , log(abs((x1*x1))))),avg(max(abs(x6) , max((max((x7*x7) , x2)*max((x7*x7) , x2)) , log(abs(x6)))) , avg(cos(x5) , sqrt(abs(x1)))),avg(log(abs(x8)) , log(abs(log(abs(avg(x3 , (avg(log(abs(x6)) , avg(cos(x5) , sqrt(abs(x1))))*avg(log(abs(x6)) , avg(cos(x5) , sqrt(abs(x1))))))))))),avg(log(abs(x6)) , log(abs(abs(x0)))),avg(avg(log(abs(x6)) , log(abs(x2))) , avg(max(log(abs(max(((log(abs(x5)) / (sign(x8) / x8))*(log(abs(x5)) / (sign(x8) / x8))) , sqrt(abs(x4))))) , avg(log(abs(x6)) , log(abs(x2)))) , avg(log(abs(avg(x3 , avg(log(abs(x8)) , log(abs(x7)))))) , log(abs(x6))))),avg(avg(cos(abs(x6)) , abs(x6)) , avg(avg(log(abs(x8)) , log(abs(x7))) , log(abs(x7)))));
myBestSelectorChampions
myBestSelectorChampions[0],Score=[0.00939469174857,ErrorPct=[0.1953482518649],WFF= mvlregress(((max(min((1.0 / x7) , x6) , max(min(min((1.0 / x7) , x6) , x6) , sqrt(abs(4.472122186797)))) - expt(abs(avg(abs(x4) , abs(x6))) ,(avg(log(abs(x1)) , abs(x6)) - avg(log(abs(avg(cos(x5) , sqrt(abs(x1))))) , log(abs(avg(log(abs(x8)) , log(abs(x7))))))))) * log(abs(x6))),min(max(max(min(abs(max(log(abs(x7)) , (x6*x6))) , (x6*x6)) , avg(sqrt(abs(4.472122186797)) , (if (x7 > x3) {x3} else {x0}))) , log(abs(-1.908797964953))) , avg(avg(log(abs(x6)) , log(abs(x2))) , avg(log(abs(x1)) , log(abs(x6))))),max(log(abs(x8)) , (x6*x6)),((max(max(x2 , avg(sqrt(abs(4.472122186797)) , (if (x7 > x3) {x3} else {x0}))) , avg(sqrt(abs(4.472122186797)) , (if (x7 > x3) {x3} else {x0}))) % max((x6*x6) , (x6*x6))) / (sqrt(abs((min(abs(x0) , (x6*x6)) * log(abs(x6))))) % max(x9 , (avg(cos(x7) , abs(x6)) * abs(x6))))),(avg(avg(x3 , (x6*x6)) , sqrt(abs(x1))) / max(avg(log(abs(x6)) , log(abs(x2))) , log(abs((x1*x1))))),avg(max(abs(x6) , max((max((x7*x7) , x2)*max((x7*x7) , x2)) , log(abs(x6)))) , avg(cos(x5) , sqrt(abs(x1)))),avg(log(abs(x8)) , log(abs(log(abs(avg(x3 , (avg(log(abs(x6)) , avg(cos(x5) , sqrt(abs(x1))))*avg(log(abs(x6)) , avg(cos(x5) , sqrt(abs(x1))))))))))),avg(log(abs(x6)) , log(abs(abs(x0)))),avg(avg(log(abs(x6)) , log(abs(x2))) , avg(max(log(abs(max(((log(abs(x5)) / (sign(x8) / x8))*(log(abs(x5)) / (sign(x8) / x8))) , sqrt(abs(x4))))) , avg(log(abs(x6)) , log(abs(x2)))) , avg(log(abs(avg(x3 , avg(log(abs(x8)) , log(abs(x7)))))) , log(abs(x6))))),avg(avg(cos(abs(x6)) , abs(x6)) , avg(avg(log(abs(x8)) , log(abs(x7))) , log(abs(x7)))));
myBestSelectorChampions[1],Score=[0.01511757692827,ErrorPct=[0.2187583199833],WFF= mvlregress(max(x9 , (x6*x6)),max(x2 , abs(x6)),avg(x3 , (x6*x6)),avg(log(abs(x8)) , log(abs(x7))),avg(log(abs(x6)) , log(abs(x2))),avg(log(abs(x1)) , log(abs(x6))),avg(cos(x7) , abs(x6)),avg(cos(x5) , sqrt(abs(x1))),avg(abs(x4) , abs(x6)),avg(abs(x0) , (x6*x6)));
myBestRegressorChampions
myBestRegressorChampions[0],ErrorPct=[0.1953482518649,Score=[0.00939469174857],WFF= mvlregress(((max(min((1.0 / x7) , x6) , max(min(min((1.0 / x7) , x6) , x6) , sqrt(abs(4.472122186797)))) - expt(abs(avg(abs(x4) , abs(x6))) ,(avg(log(abs(x1)) , abs(x6)) - avg(log(abs(avg(cos(x5) , sqrt(abs(x1))))) , log(abs(avg(log(abs(x8)) , log(abs(x7))))))))) * log(abs(x6))),min(max(max(min(abs(max(log(abs(x7)) , (x6*x6))) , (x6*x6)) , avg(sqrt(abs(4.472122186797)) , (if (x7 > x3) {x3} else {x0}))) , log(abs(-1.908797964953))) , avg(avg(log(abs(x6)) , log(abs(x2))) , avg(log(abs(x1)) , log(abs(x6))))),max(log(abs(x8)) , (x6*x6)),((max(max(x2 , avg(sqrt(abs(4.472122186797)) , (if (x7 > x3) {x3} else {x0}))) , avg(sqrt(abs(4.472122186797)) , (if (x7 > x3) {x3} else {x0}))) % max((x6*x6) , (x6*x6))) / (sqrt(abs((min(abs(x0) , (x6*x6)) * log(abs(x6))))) % max(x9 , (avg(cos(x7) , abs(x6)) * abs(x6))))),(avg(avg(x3 , (x6*x6)) , sqrt(abs(x1))) / max(avg(log(abs(x6)) , log(abs(x2))) , log(abs((x1*x1))))),avg(max(abs(x6) , max((max((x7*x7) , x2)*max((x7*x7) , x2)) , log(abs(x6)))) , avg(cos(x5) , sqrt(abs(x1)))),avg(log(abs(x8)) , log(abs(log(abs(avg(x3 , (avg(log(abs(x6)) , avg(cos(x5) , sqrt(abs(x1))))*avg(log(abs(x6)) , avg(cos(x5) , sqrt(abs(x1))))))))))),avg(log(abs(x6)) , log(abs(abs(x0)))),avg(avg(log(abs(x6)) , log(abs(x2))) , avg(max(log(abs(max(((log(abs(x5)) / (sign(x8) / x8))*(log(abs(x5)) / (sign(x8) / x8))) , sqrt(abs(x4))))) , avg(log(abs(x6)) , log(abs(x2)))) , avg(log(abs(avg(x3 , avg(log(abs(x8)) , log(abs(x7)))))) , log(abs(x6))))),avg(avg(cos(abs(x6)) , abs(x6)) , avg(avg(log(abs(x8)) , log(abs(x7))) , log(abs(x7)))));
myBestRegressorChampions[1],ErrorPct=[0.2187583199833,Score=[0.01511757692827],WFF= mvlregress(max(x9 , (x6*x6)),max(x2 , abs(x6)),avg(x3 , (x6*x6)),avg(log(abs(x8)) , log(abs(x7))),avg(log(abs(x6)) , log(abs(x2))),avg(log(abs(x1)) , log(abs(x6))),avg(cos(x7) , abs(x6)),avg(cos(x5) , sqrt(abs(x1))),avg(abs(x4) , abs(x6)),avg(abs(x0) , (x6*x6)));

Show final results on training data, Score=[0.00939469174857], ErrorPct=[0.1953482518649]
X[0], ey=[-416.3818488454], y=[-442.1540158257], ErrorPct=[0.4637982024556]
X[50], ey=[-337.4133991663], y=[-344.9757762834], ErrorPct=[0.1360932092314]
X[100], ey=[-329.0236508097], y=[-328.56690699], ErrorPct=[0.008219602283216]
X[150], ey=[-300.4747732851], y=[-316.4793507135], ErrorPct=[0.2880197946881]
X[200], ey=[-311.1058970399], y=[-306.8662887642], ErrorPct=[0.07629636649975]
X[250], ey=[-286.1691468812], y=[-300.6746932272], ErrorPct=[0.2610430983967]
X[300], ey=[-293.6566026182], y=[-292.6564079634], ErrorPct=[0.01799959170488]
X[350], ey=[-303.0196368048], y=[-286.4143307292], ErrorPct=[0.2988305606954]
X[400], ey=[-286.9294608024], y=[-278.0566072551], ErrorPct=[0.1596766592831]
X[450], ey=[-274.9776776639], y=[-269.0276574358], ErrorPct=[0.1070770916726]
X[500], ey=[-257.7630339902], y=[-260.7617842454], ErrorPct=[0.05396577552157]
X[550], ey=[-252.265112994], y=[-253.3954399912], ErrorPct=[0.02034146487922]
X[600], ey=[-251.1536081549], y=[-244.8933174142], ErrorPct=[0.1126607473316]
X[650], ey=[-242.8110666789], y=[-236.8301447833], ErrorPct=[0.1076332008213]
X[700], ey=[-237.8257955819], y=[-225.4810582371], ErrorPct=[0.2221569879896]
X[750], ey=[-215.180827824], y=[-211.7854710109], ErrorPct=[0.06110314230788]
X[800], ey=[-222.7079280537], y=[-199.4295532521], ErrorPct=[0.4189196972571]
X[850], ey=[-189.3896441351], y=[-182.6983877618], ErrorPct=[0.1204164430752]
X[900], ey=[-185.3166496022], y=[-157.3531907696], ErrorPct=[0.5032328849549]
X[950], ey=[-77.17165803292], y=[-112.605874638], ErrorPct=[0.637677304335]
Actual computed error on training data is ErrorPct=[0.1953482518649] versus reported ErrorPct=[0.1953482518649] while average Y is AvgY=[-250.5255193461]
yHistory=[#(num| -353.889203165 -317.3134385468 -300.3091004233 -285.8905446946 -269.5610676413 -253.0163528728 -235.9529395876 -212.0759905006 -181.8577655025 -95.38879052681 )]
eHistory=[#(num| -350.3380258394 -316.7345149449 -300.3013389324 -286.8878241945 -268.7486739538 -251.3294297052 -239.0136980469 -211.6646959167 -182.8366081867 -97.40038374085 )]
aHistory=[#(num| -350.3380258394 -333.5362703921 -322.4579599055 -313.5654259778 -304.602075573 -196.4489631193 -182.7288464728 -163.9672292814 -140.1184959638 -97.40038374085 )]
dHistory=[#(num| 108.1531124537 130.836579505 158.4907306241 193.4177744283 252.9376420985 )]


Final testing on test data returns Score=[0.09870870207575], ErrorPct=[1.785187219036]
X[0], ey=[-394.1253093768], y=[-236544.394484], ErrorPct=[5.756961015735]
X[50], ey=[-338.561544381], y=[-159952.1219483], ErrorPct=[3.891120039964]
X[100], ey=[-318.6179593071], y=[-134231.6974115], ErrorPct=[3.264583947323]
X[150], ey=[-303.4692017251], y=[-123218.0039703], ErrorPct=[2.996457244802]
X[200], ey=[-258.2894376363], y=[-113368.7600128], ErrorPct=[2.757450041656]
X[250], ey=[-252.4147222554], y=[-104357.6568433], ErrorPct=[2.537917159779]
X[300], ey=[-290.8020886195], y=[-97010.4800072], ErrorPct=[2.357869068615]
X[350], ey=[-313.3516673992], y=[-89717.80030923], ErrorPct=[2.179535629002]
X[400], ey=[-284.8954057399], y=[-80824.02487137], ErrorPct=[1.963413508676]
X[450], ey=[-321.0651197281], y=[-75892.75707881], ErrorPct=[1.842315429164]
X[500], ey=[4.470193237652], y=[-68912.23845871], ErrorPct=[1.680077716739]
X[550], ey=[-268.062293036], y=[-61710.81211235], ErrorPct=[1.497874707684]
X[600], ey=[-229.0155428235], y=[-56822.37960704], ErrorPct=[1.379654538637]
X[650], ey=[-235.1059636834], y=[-50335.55243298], ErrorPct=[1.22136772574]
X[700], ey=[-281.1511185928], y=[-41587.97330989], ErrorPct=[1.006993410892]
X[750], ey=[-255.169758225], y=[-34712.78581706], ErrorPct=[0.8400208606124]
X[800], ey=[-272.8590118378], y=[-25758.74377498], ErrorPct=[0.6213045851939]
X[850], ey=[-247.4756139086], y=[-16731.9186346], ErrorPct=[0.4018640172123]
X[900], ey=[-162.2390397802], y=[-3575.452886615], ErrorPct=[0.0832086244207]
X[950], ey=[-236.2250208973], y=[9895.426240873], ErrorPct=[0.2469932452032]
Actual computed error on testing data is ErrorPct=[1.785187219036], Avg Y=[-70682.87296825], AvgDev Y=[41019.95280654]
yHistory=[#(num| -165551.3345727 -123422.4378105 -104911.527941 -89194.43805917 -75299.0408074 -62227.49334444 -49679.31692681 -34213.50245759 -15871.0944263 13541.45666341 )]
eHistory=[#(num| -155663.3776099 -115527.2354471 -100025.5455654 -79794.12372734 -70144.32285257 -63872.4070884 -47468.29281633 -36404.78411945 -19592.39209028 -18336.24836565 )]
aHistory=[#(num| -155663.3776099 -135595.3065285 -123738.7195408 -112752.5705875 -104230.9210405 -37134.82489602 -30450.42934793 -24777.8081918 -18964.32022797 -18336.24836565 )]
dHistory=[#(num| 67096.09614445 82302.14123952 98960.91134903 116630.9863005 137327.1292443 )]

esm.selfTest[mixedModel]: completed in [79.72683333333] minutes.

Starting test case: ratioRegression
Building test data as:
if ((x0 % 4) == 0) y = + ((13.99864194741*x0)/(32.66789144459*x1)) + ((32.66789144459*x1)/(30.73171497235*x2)) + ((30.73171497235*x2)/(10.01135458484*x3)) + ((10.01135458484*x3)/(2.83159613203*x4)) + ((2.83159613203*x4)/(5.499486281783*x5)) + ((5.499486281783*x5)/(46.33521181329*x6)) + ((46.33521181329*x6)/(10.35455431571*x7)) + ((10.35455431571*x7)/(18.14155680594*x8)) + ((18.14155680594*x8)/(3.363675194861*x9));
if ((x0 % 4) == 1) y = + ((13.99864194741*x0)%(32.66789144459*x1)) + ((32.66789144459*x1)%(30.73171497235*x2)) + ((30.73171497235*x2)%(10.01135458484*x3)) + ((10.01135458484*x3)%(2.83159613203*x4)) + ((2.83159613203*x4)%(5.499486281783*x5)) + ((5.499486281783*x5)%(46.33521181329*x6)) + ((46.33521181329*x6)%(10.35455431571*x7)) + ((10.35455431571*x7)%(18.14155680594*x8)) + ((18.14155680594*x8)%(3.363675194861*x9));
if ((x0 % 4) == 2) y = + ((13.99864194741*sin(x0))/(32.66789144459*tan(x1))) + ((32.66789144459*sin(x1))/(30.73171497235*tan(x2))) + ((30.73171497235*sin(x2))/(10.01135458484*tan(x3))) + ((10.01135458484*sin(x3))/(2.83159613203*tan(x4))) + ((2.83159613203*sin(x4))/(5.499486281783*tan(x5))) + ((5.499486281783*sin(x5))/(46.33521181329*tan(x6))) + ((46.33521181329*sin(x6))/(10.35455431571*tan(x7))) + ((10.35455431571*sin(x7))/(18.14155680594*tan(x8))) + ((18.14155680594*sin(x8))/(3.363675194861*tan(x9)));
if ((x0 % 4) == 3) y = - (32.66789144459* log(.000001+abs(x1))) - (30.73171497235* log(.000001+abs(x2))) + (10.01135458484* log(.000001+abs(x3))) - (2.83159613203* log(.000001+abs(x4))) + (5.499486281783* log(.000001+abs(x5))) - (46.33521181329* log(.000001+abs(x6))) + (10.35455431571* log(.000001+abs(x7))) + (18.14155680594* log(.000001+abs(x8))) - (3.363675194861* log(.000001+abs(x9)));
...starting new evolutionary process.
...starting generation of constant selector WFF.
...starting regress selector for each column name.
...starting a best-of-breed MVL selector WFF.
...starting a best-of-breed REG selector WFF.
...starting generation of root selector WFFs in each column island.
...starting generation of random selector WFFs.

Final results of training
esm: N = [1000], M = [10], Generations = [200], WFFs = [25052], Score=[0.04947157880955], ScoreHistory=[#(num| 0.04947157880955 )]
esm.myBest, Score=[0.04947157880955], ErrorPct=[0.131191510911], lengthWFF=[1628], WFF = mvlregress((expt(abs((x4 % x8)) ,(x4 % x8)) - ((x4 % x8) * x1)),((((x3 / x7) * min(ninteger(x6) , x8)) / (x2 / x7)) - (x1 / x7)),((avg(x0 , x3) / x7) - ((ninteger(ninteger((min(x7 , x6) / x7))) / (if (x6 > -1.010183128751) {1.327267689878} else {x7})) / min(x7 , x6))),(((ninteger(x8) / x9) + (ninteger(x8) / x3)) - (((x3 / x7) / ((1.0 / x9) / x4)) + (((x3 / x7) / ((1.0 / x9) / x4)) * (((1.0 / x7) / x2) / (sin(x5) - max(max(x0 , x2) , (x4 % x8))))))),max((ninteger(x8) / (sin(x5) / x3)) , (x8 / x9)),(x2 / ((((x5*x5)*(x5*x5)) / x2) / x2)),((((x3 / x4) / x9) - (ninteger(((log(abs(x3)) / x3) / x3)) / x9)) / max(log(abs(x5)) , x1)),((avg((x4 % x8) , ninteger(avg(((x2 / x3) / x9) , avg((ninteger(x8) / x9) , (avg(x0 , x3) / x9))))) - avg(x0 , avg(avg(ninteger(x6) , (cos(x0) / x7)) , ((x8*x8*x8) / x9)))) / abs(x6)),((x4 / x7) / (1.0 / avg((sign((sign(x6) / x3)) / (log(abs((log(abs(avg(x0 , x3))) / avg(x0 , ((x5 / x3) / ((1.0 / x7) / x9)))))) / x3)) , ((sign((sign(x6) / x3)) / x3) / x3)))),avg(sqrt(abs(x7)) , ((sqrt(abs(x2)) / min(ninteger(x6) , x8)) + ((x3 / x4) + (x3 / x4)))));
esm.myBestRegressor, Score=[0.07424158155064], ErrorPct=[0.09572680858147], lengthWFF=[1214], WFF = mvlregress(((avg((x4 % x8) , ninteger(x8)) - avg(x0 , avg(avg(ninteger(x6) , (cos(x0) / x7)) , ((x8*x8*x8) / x9)))) - avg(x0 , avg(avg(ninteger(x6) , (cos(x0) / x7)) , ((x8*x8*x8) / x9)))),min(((sqrt(abs(x2)) / min(ninteger(x6) , x8)) + (sqrt(abs(x2)) / x9)) , ((sqrt(abs(x2)) / min(ninteger(x6) , x8)) + (sqrt(abs(x2)) / x9))),(x1 / (x2 / x7)),(ninteger(ninteger((min(x7 , x6) / x7))) / (if (x6 > -1.010183128751) {1.327267689878} else {x7})),((((x5*x5)*(x5*x5)) / x2) / x2),((sign((sign(x6) / x3)) / x3) / x3),(((x4*x4*x4) / (ninteger(ninteger(x8)) / x9)) / abs(x5)),avg(ninteger(max(ninteger(x9) , max(((x8*x8*x8) / x9) , ninteger(x6)))) , (((log(abs((1.0 / x9))) / x3) / x7) / x4)),(ninteger(avg(((x2 / x3) / x9) , avg((ninteger(x8) / x9) , (avg(x0 , x3) / x9)))) + (((x3 / x7) / ((1.0 / x9) / x4)) * (((1.0 / x7) / x2) / x7))),((x3 / x4) + (x3 / x4)));
myBestSelectorChampions
myBestSelectorChampions[0],Score=[0.04947157880955,ErrorPct=[0.131191510911],WFF= mvlregress((expt(abs((x4 % x8)) ,(x4 % x8)) - ((x4 % x8) * x1)),((((x3 / x7) * min(ninteger(x6) , x8)) / (x2 / x7)) - (x1 / x7)),((avg(x0 , x3) / x7) - ((ninteger(ninteger((min(x7 , x6) / x7))) / (if (x6 > -1.010183128751) {1.327267689878} else {x7})) / min(x7 , x6))),(((ninteger(x8) / x9) + (ninteger(x8) / x3)) - (((x3 / x7) / ((1.0 / x9) / x4)) + (((x3 / x7) / ((1.0 / x9) / x4)) * (((1.0 / x7) / x2) / (sin(x5) - max(max(x0 , x2) , (x4 % x8))))))),max((ninteger(x8) / (sin(x5) / x3)) , (x8 / x9)),(x2 / ((((x5*x5)*(x5*x5)) / x2) / x2)),((((x3 / x4) / x9) - (ninteger(((log(abs(x3)) / x3) / x3)) / x9)) / max(log(abs(x5)) , x1)),((avg((x4 % x8) , ninteger(avg(((x2 / x3) / x9) , avg((ninteger(x8) / x9) , (avg(x0 , x3) / x9))))) - avg(x0 , avg(avg(ninteger(x6) , (cos(x0) / x7)) , ((x8*x8*x8) / x9)))) / abs(x6)),((x4 / x7) / (1.0 / avg((sign((sign(x6) / x3)) / (log(abs((log(abs(avg(x0 , x3))) / avg(x0 , ((x5 / x3) / ((1.0 / x7) / x9)))))) / x3)) , ((sign((sign(x6) / x3)) / x3) / x3)))),avg(sqrt(abs(x7)) , ((sqrt(abs(x2)) / min(ninteger(x6) , x8)) + ((x3 / x4) + (x3 / x4)))));
myBestSelectorChampions[1],Score=[0.06476346139399,ErrorPct=[0.09608672080563],WFF= mvlregress((sin(x5) - max(max(x0 , x2) , (x4 % x8))),(min(min(max(sin(x5) , sqrt(abs(x1))) , x8) , abs(x9)) % (sign(x2) / x7)),max((min(x7 , x6) / x7) , (log(abs(x8)) / x9)),(x3 / x4),(log(abs((log(abs(avg(x0 , x3))) / avg(x0 , ((x5 / x3) / ((1.0 / x7) / x9)))))) / x3),(((x4*x4*x4) / x3) / abs(x5)),(((x1*x1*x1) / x9) / max(((-(x3*x3*x3)) / x9) , x4)),avg(ninteger(max(ninteger(x9) , max(((x8*x8*x8) / x9) , ninteger(x6)))) , ((x2 / x3) / (tan(avg(x8 , x9)) / x9))),avg(((x2 / x3) / x9) , avg((ninteger(x8) / x9) , (avg(x0 , x3) / x9))));
myBestSelectorChampions[2],Score=[0.07649056679716,ErrorPct=[0.1928772452751],WFF= mvlregress(max(avg(min((1.0 / (x2 * x7)) , (x7 % x8)) , min((1.0 / x6) , x8)) , (cos(ninteger(x8)) / x3)),((min(min(ninteger(x6) , (max(x1 , x3) * (x8 / x7))) , (sqrt(abs(x5)) / x7)) - ((x2 * x7) * x8)) / min(x0 , min(x0 , x9))),(sqrt(abs(x1)) / log(abs(x2))),(log(abs((log(abs(x4)) / x9))) / x9),((1.0 / x7) / sign((sign((sign(x6) / x3)) / x3))),((tanh(x2) / x9) / ((ninteger(x8) / (log(abs(x4)) / x9)) / (sqrt(abs(x5)) / x7))),(((log(abs((sign(x7) / ((sqrt(abs(x5)) / x7) / x3)))) / x9)*(log(abs((sign(x7) / ((sqrt(abs(x5)) / x7) / x3)))) / x9)*(log(abs((sign(x7) / ((sqrt(abs(x5)) / x7) / x3)))) / x9)) / (min(cos((sign((sign(x6) / x3)) / x3)) , x8) / x9)),((((1.0 / x9) / x3)*((1.0 / x9) / x3)*((1.0 / x9) / x3)) / max((1.0 / (x7 % x8)) , x1)),avg((max(max((sin(x4) % x3) , (x7 % x8)) , x1) * min(x0 , min(x0 , x9))) , ((x3 / x4) * max(x1 , x3))),avg((cos(x0) / x9) , avg(((x8*x8*x8) / x9) , abs(x8))));
myBestSelectorChampions[3],Score=[0.08706187865316,ErrorPct=[0.1757150366937],WFF= mvlregress(((x2 % x8) % x1),((ninteger(x2) / x9) % (log(abs(x6)) / x9)),(x8 / x9),(x5 / (x3 / x7)),((x4*x4) / x2),(ninteger(x1) / x7),((x1 / x7) / x7),((sqrt(abs(x4)) / x9) / x4),(((x8*x8*x8) / x3) / x9),(cos(x0) / avg(x0 , x7)));
myBestSelectorChampions[4],Score=[0.1033471303003,ErrorPct=[0.1805357186159],WFF= mvlregress((((x8*x8*x8) / x9) - min(x6 , x8)),((x4 * x1) * x1),min(max(x1 , x3) , x8),max(x1 , (if (x2 > x7) {expt(abs(x4) ,-0.3341016504478)} else {(x5 + x2)})),max(x0 , max(x0 , x1)),max(max(x0 , x4) , x4),max((x2 / x3) , (x2 / x3)),max(cos(x5) , max(cos(x5) , x1)),((x4 / x7) / x7),avg(abs(x9) , avg(abs(x9) , x4)));
myBestRegressorChampions
myBestRegressorChampions[0],ErrorPct=[0.09572680858147,Score=[0.07424158155064],WFF= mvlregress(((avg((x4 % x8) , ninteger(x8)) - avg(x0 , avg(avg(ninteger(x6) , (cos(x0) / x7)) , ((x8*x8*x8) / x9)))) - avg(x0 , avg(avg(ninteger(x6) , (cos(x0) / x7)) , ((x8*x8*x8) / x9)))),min(((sqrt(abs(x2)) / min(ninteger(x6) , x8)) + (sqrt(abs(x2)) / x9)) , ((sqrt(abs(x2)) / min(ninteger(x6) , x8)) + (sqrt(abs(x2)) / x9))),(x1 / (x2 / x7)),(ninteger(ninteger((min(x7 , x6) / x7))) / (if (x6 > -1.010183128751) {1.327267689878} else {x7})),((((x5*x5)*(x5*x5)) / x2) / x2),((sign((sign(x6) / x3)) / x3) / x3),(((x4*x4*x4) / (ninteger(ninteger(x8)) / x9)) / abs(x5)),avg(ninteger(max(ninteger(x9) , max(((x8*x8*x8) / x9) , ninteger(x6)))) , (((log(abs((1.0 / x9))) / x3) / x7) / x4)),(ninteger(avg(((x2 / x3) / x9) , avg((ninteger(x8) / x9) , (avg(x0 , x3) / x9)))) + (((x3 / x7) / ((1.0 / x9) / x4)) * (((1.0 / x7) / x2) / x7))),((x3 / x4) + (x3 / x4)));
myBestRegressorChampions[1],ErrorPct=[0.09608672080563,Score=[0.06476346139399],WFF= mvlregress((sin(x5) - max(max(x0 , x2) , (x4 % x8))),(min(min(max(sin(x5) , sqrt(abs(x1))) , x8) , abs(x9)) % (sign(x2) / x7)),max((min(x7 , x6) / x7) , (log(abs(x8)) / x9)),(x3 / x4),(log(abs((log(abs(avg(x0 , x3))) / avg(x0 , ((x5 / x3) / ((1.0 / x7) / x9)))))) / x3),(((x4*x4*x4) / x3) / abs(x5)),(((x1*x1*x1) / x9) / max(((-(x3*x3*x3)) / x9) , x4)),avg(ninteger(max(ninteger(x9) , max(((x8*x8*x8) / x9) , ninteger(x6)))) , ((x2 / x3) / (tan(avg(x8 , x9)) / x9))),avg(((x2 / x3) / x9) , avg((ninteger(x8) / x9) , (avg(x0 , x3) / x9))));
myBestRegressorChampions[2],ErrorPct=[0.1539668051928,Score=[0.1151662126861],WFF= mvlregress((avg((cos(x0) / x9) , avg(((x8*x8*x8) / x9) , abs(x8))) % avg((cos(x0) / x9) , avg(((x8*x8*x8) / x9) , abs(x8)))),max((((log(abs((sign(x7) / ((sqrt(abs(x5)) / x7) / x3)))) / x9)*(log(abs((sign(x7) / ((sqrt(abs(x5)) / x7) / x3)))) / x9)*(log(abs((sign(x7) / ((sqrt(abs(x5)) / x7) / x3)))) / x9)) / (min(cos((sign((sign(x6) / x3)) / x3)) , x8) / x9)) , (((log(abs((sign(x7) / ((sqrt(abs(x5)) / x7) / x3)))) / x9)*(log(abs((sign(x7) / ((sqrt(abs(x5)) / x7) / x3)))) / x9)*(log(abs((sign(x7) / ((sqrt(abs(x5)) / x7) / x3)))) / x9)) / (min(cos((sign((sign(x6) / x3)) / x3)) , x8) / x9))),max(avg(min((1.0 / (x2 * x7)) , (x7 % x8)) , min(min(x0 , x9) , x8)) , (cos(ninteger(x8)) / x3)),((min(min(ninteger(x6) , (max(x1 , x3) * (x8 / x7))) , (sqrt(abs(x5)) / x7)) - ((x2 * x7) * x8)) / min(x0 , (cos(ninteger(x8)) / x3))),(sqrt(abs(log(abs(x2)))) / log(abs(x2))),((tanh(x2) / ((tanh(x2) / x9) / ((ninteger(x8) / (log(abs(x4)) / x9)) / (sqrt(abs(x5)) / x7)))) / ((ninteger(x8) / (log(abs(x4)) / x9)) / (sqrt(abs(x5)) / x7))),((log(abs(x4)) / x9) / x9),(((1.0 / x7) / sign((sign((sign(x6) / x3)) / x3))) / ((((1.0 / x9) / x3)*((1.0 / x9) / x3)*((1.0 / x9) / x3)) / max((1.0 / (x7 % x8)) , x1))),(((((1.0 / x9) / x3)*((1.0 / x9) / x3)*((1.0 / x9) / x3)) / max((1.0 / (x7 % x8)) , x1)) / ((1.0 / x7) / sign((sign((sign(x6) / x3)) / x3)))),avg(max(x1 , x3) , ((x3 / x4) * max(x1 , x3))));
myBestRegressorChampions[3],ErrorPct=[0.1618656336691,Score=[0.1224309204683],WFF= mvlregress(tan(x5),min(x0 , x8),min(sign(x6) , x8),(x8 / x9),(x4 / x3),(x3 / x9),(x3 / x7),(x2 / x3),((1.0 / x1) / x9),(ninteger(x8) / x2));
myBestRegressorChampions[4],ErrorPct=[0.1701742512899,Score=[0.1211132143944],WFF= mvlregress(tan(min(x8 , x5)),((x8 / x9) * (ninteger(x4) / x3)),((cos(x1) / x9) % avg(abs(x9) , x4)),min(x8 , ((x4*x4) / (abs(x7) / x9))),max((ninteger(x3) / x9) , (ninteger((log(abs(x3)) / x9)) / x9)),(x2 / min(x8 , x1)),(ninteger(x8) / x9),(((x8*x8*x8) / x9) / x2),(cos((abs(x6) / x9)) / x2),(avg((x2 % x1) , max((1.0 / x5) , x7)) / x3));

Show final results on training data, Score=[0.04947157880955], ErrorPct=[0.131191510911]
X[0], ey=[-71072.93627035], y=[-71179.56145901], ErrorPct=[0.275872292693]
X[50], ey=[-91.34593178122], y=[-114.8147199555], ErrorPct=[0.06072100300329]
X[100], ey=[-14.64787913638], y=[-49.28016210361], ErrorPct=[0.08960441171702]
X[150], ey=[-28.53931581479], y=[-32.51769078888], ErrorPct=[0.01029328472165]
X[200], ey=[22.38700849], y=[-22.12764639609], ErrorPct=[0.1151731598991]
X[250], ey=[-7.232113617238], y=[-16.11297146212], ErrorPct=[0.02297752196949]
X[300], ey=[-23.78636265069], y=[-11.15357482029], ErrorPct=[0.03268492357147]
X[350], ey=[-11.04878323835], y=[-7.238191515635], ErrorPct=[0.009859177632915]
X[400], ey=[-10.6811187362], y=[-4.80564883589], ErrorPct=[0.01520165518618]
X[450], ey=[0.9229513211011], y=[-1.896917344149], ErrorPct=[0.007295871112744]
X[500], ey=[-9.388844941547], y=[0.8290581178592], ErrorPct=[0.02643687086658]
X[550], ey=[-9.620157532584], y=[3.738868717944], ErrorPct=[0.03456392665258]
X[600], ey=[-6.152925506541], y=[7.154224624542], ErrorPct=[0.03442970711039]
X[650], ey=[-13.09485899534], y=[10.48227875541], ErrorPct=[0.06100133681995]
X[700], ey=[-6.119663942373], y=[14.00827826036], ErrorPct=[0.05207720270293]
X[750], ey=[-4.602961427489], y=[18.29806558979], ErrorPct=[0.05925202954539]
X[800], ey=[-8.557794101603], y=[23.61519293605], ErrorPct=[0.08324145362915]
X[850], ey=[448.8522106704], y=[32.79885639086], ErrorPct=[1.076458519595]
X[900], ey=[-2.939539970931], y=[51.47245791856], ErrorPct=[0.1407806429004]
X[950], ey=[24.70403620278], y=[124.6117907367], ErrorPct=[0.2584922160473]
Actual computed error on training data is ErrorPct=[0.131191510911] versus reported ErrorPct=[0.131191510911] while average Y is AvgY=[-180.2655570221]
yHistory=[#(num| -2052.993855673 -33.86004041827 -16.26548681454 -7.597674543073 -1.866735786955 3.77485228041 10.50658631701 18.55457434829 34.07514835822 243.017061711 )]
eHistory=[#(num| -1985.257957396 -21.40827025985 -6.958759164555 9.769251701156 -10.80836105879 6.168462423447 9.30476645572 1.267956203879 30.05140484393 165.2159360304 )]
aHistory=[#(num| -1985.257957396 -1003.333113828 -671.2083289403 -500.9639337799 -402.9328192357 42.40170519148 51.46001588349 65.51176569275 97.63367043718 165.2159360304 )]
dHistory=[#(num| 445.3345244272 552.4239496634 736.720094633 1100.966784265 2150.473893427 )]


Final testing on test data returns Score=[0.5019084467968], ErrorPct=[1.297542878964]
X[0], ey=[-7411.022197496], y=[-1917.949432036], ErrorPct=[11.48478004707]
X[50], ey=[-17.6242060674], y=[-1021.263219104], ErrorPct=[2.098383510202]
X[100], ey=[-7.890869651406], y=[-800.5887622851], ErrorPct=[1.657353057094]
X[150], ey=[25.55578104363], y=[-584.6385922986], ErrorPct=[1.275779233776]
X[200], ey=[-1.754251241728], y=[-495.5545097802], ErrorPct=[1.032425310686]
X[250], ey=[-14.99940415286], y=[-391.5709154912], ErrorPct=[0.7873263589201]
X[300], ey=[-3.512262107292], y=[-296.2740805764], ErrorPct=[0.6120991355584]
X[350], ey=[-0.3071223499268], y=[-195.8638395641], ErrorPct=[0.4088651251907]
X[400], ey=[-30.77941975901], y=[-123.9308858665], ErrorPct=[0.1947587707254]
X[450], ey=[29.31921817936], y=[-56.49026755919], ErrorPct=[0.1794083406024]
X[500], ey=[-8.826132693129], y=[22.4255624939], ErrorPct=[0.06534026776014]
X[550], ey=[-34.50276540139], y=[86.51843835752], ErrorPct=[0.2530281257044]
X[600], ey=[1.178008780211], y=[159.8462923438], ErrorPct=[0.3317397047118]
X[650], ey=[6.015160298037], y=[240.3586262005], ErrorPct=[0.4899594955816]
X[700], ey=[-0.5159252408088], y=[305.1340276587], ErrorPct=[0.6390453267832]
X[750], ey=[-7.188258415152], y=[377.6054595258], ErrorPct=[0.8045171441807]
X[800], ey=[11.47763348478], y=[462.6368956594], ErrorPct=[0.9432725750245]
X[850], ey=[-1.708633701436], y=[599.9774383572], ErrorPct=[1.257990288865]
X[900], ey=[0.9747630995283], y=[755.3983570285], ErrorPct=[1.577330104395]
X[950], ey=[-8.298210439734], y=[997.2891557234], ErrorPct=[2.102457078507]
Actual computed error on testing data is ErrorPct=[1.297542878964], Avg Y=[-0.4268696855259], AvgDev Y=[478.2915077997]
yHistory=[#(num| -1110.398210524 -620.4012095863 -398.9322562533 -207.1861502344 -55.44539875985 89.14827843896 236.6814025212 380.0614508862 604.7151926039 1077.488204053 )]
eHistory=[#(num| -22.76076329165 31.16930368969 24.48284090906 49.75857498511 -19.35328097258 -45.53820635683 -64.21521085921 -7.724812295738 22.58137233039 27.3314850065 )]
aHistory=[#(num| -22.76076329165 4.204270199018 10.96379376903 20.66248907305 12.65933506392 -13.51307443498 -5.506791454514 14.06268168039 24.95642866845 27.3314850065 )]
dHistory=[#(num| -26.1724094989 -26.16928052757 3.098887911353 20.75215846943 50.09224829816 )]

esm.selfTest[ratioRegression]: completed in [96.44401666667] minutes.

Starting test case: cyclicSeries
Building test data as: y = -33.6351407746 - (33.6351407746*x0*sin(x0)) + (7.641134588007*x1*cos(x0)) - (31.28769360846*x2*tan(x0)) + (26.43031024046*x3*sin(x0)) - (36.13429316222*x4*cos(x0)) + (9.002513930695*x5*tan(x0)) + (32.64322896195*x6*sin(x0)) + (9.975736140446*x7*cos(x0)) + (12.15586616836*x8*tan(x0)) - (8.644356209179*x9*sin(x0));
...starting new evolutionary process.
...starting generation of constant selector WFF.
...starting regress selector for each column name.
...starting a best-of-breed MVL selector WFF.
...starting a best-of-breed REG selector WFF.
...starting generation of root selector WFFs in each column island.
...starting generation of random selector WFFs.

Final results of training
esm: N = [1000], M = [10], Generations = [200], WFFs = [29220], Score=[0.02711762728426], ScoreHistory=[#(num| 0.02711762728426 )]
esm.myBest, Score=[0.02711762728426], ErrorPct=[0.1608366966644], lengthWFF=[3770], WFF = mvlregress((sign(sign(expt(abs(x3) ,x3))) - (sign(x5) - (sign(tanh(sqrt(abs(max(cos(x6) , (sign(x5) - (sign(tanh(sqrt(abs(max(cos(x6) , x5))))) - (sign(tanh(sqrt(abs(max(cos(x6) , x5))))) - (tan(x0) * x2))))))))) - (sign(tanh(sqrt(abs(max(cos(x6) , x5))))) - (tan(x0) * x2))))),(min(max(avg(avg(x0 , x4) , min(avg(min(abs(avg(x2 , x3)) , ((x7*x7) % x1)) , min(abs(avg(x2 , x3)) , ((x7*x7) % x1))) , ((tan(x0) * (sign(x9) % x2)) / (tan((avg(abs(x7) , x3) / x5)) * x2)))) , max(avg(avg(x0 , x4) , x4) , avg(max(avg(avg(max(avg(avg(x0 , x4) , x4) , avg(x0 , x4)) , x4) , x4) , avg(x0 , x4)) , avg(log(abs(avg((1.0 / x7) , x1))) , x6)))) , ((sign(x3) * min(avg(min(avg(x2 , x9) , exp(x5)) , max(max(avg(abs((sqrt(abs(((tan(x0) % x2) - min(sin(x7) , x1)))) + tanh(x1))) , avg(cos(x3) , x1)) , avg(avg(abs(x2) , x4) , max(cos(x6) , x5))) , expt(abs(x7) ,x8))) , exp(x5))) % (sqrt(abs(((tan(x0) % x2) - min(sin(x7) , x1)))) + tanh(x1)))) - (((sign(x3) * x1) % (sqrt(abs(((tan(x0) % x2) - min(sin(x7) , x1)))) + tanh(x1))) - (tan(x0) * avg(sign(x2) , x8)))),(avg((tan(x0) * ninteger(x8)) , (avg((avg((1.0 / avg(x4 , avg((1.0 / avg(x4 , x9)) , (sqrt(abs(x2)) % x5)))) , avg(x1 , x8)) * ninteger(x8)) , avg(cos(x3) , max(avg(avg(x0 , x4) , x4) , avg(x0 , x4)))) - avg(abs(x2) , avg(abs(x8) , x9)))) - avg(abs(x2) , avg(abs(x8) , x9))),((avg(((tan(x0) * x2) - min(sign(x6) , (tan(x0) * x2))) , (min(sign(x6) , sign(x6)) - min(sign(x6) , (tan(x0) * x2)))) - max(avg(avg(x0 , x4) , x4) , avg(((tan(x0) * x2) - min(sign(x6) , (tan(x0) * x2))) , (min(sign(x6) , sign(x6)) - min(sign(x6) , (tan(x0) * x2)))))) % max(avg(avg(x0 , avg(x2 , x9)) , min(avg(min(abs(avg(x2 , x3)) , ((x7*x7) % x1)) , min(abs(avg(x2 , x3)) , ((x7*x7) % x1))) , ((tan(x0) * (sign(x9) % x2)) / (tan((avg(abs(x7) , x3) / x5)) * x2)))) , max(avg(avg(x0 , x4) , x4) , avg(max(avg(avg(max(avg(avg(x0 , x4) , x4) , avg(x0 , x4)) , x4) , x4) , avg(x0 , x4)) , avg(log(abs(avg((1.0 / x7) , x1))) , x6))))),min(exp(x5) , tanh(x1)),max((tan(x0) * avg(x5 , x8)) , (avg((tan(x0) * ninteger(x8)) , avg(cos(x3) , max(avg(avg(x0 , x4) , x4) , avg(x0 , x4)))) - avg(abs(x2) , avg(abs(x8) , x9)))),(sign(x9) / avg(x1 , x8)),avg(max(avg(avg(x0 , x4) , x4) , avg(max(avg(avg(max(avg(avg(x0 , x4) , x4) , avg(x0 , x4)) , x4) , x4) , avg(x0 , x4)) , x4)) , avg((1.0 / avg(x1 , x8)) , avg(x1 , x8))),((avg(x5 , x8) * avg(sign(x2) , x8)) + sign(x6)),(((sign(x3) * x1) % (sqrt(abs(((tan(x0) % x2) - min(sin(x7) , x1)))) + tanh(avg(x5 , x8)))) + (tan(x0) * avg(x5 , x8))));
esm.myBestRegressor, Score=[0.02736840545727], ErrorPct=[0.159884340631], lengthWFF=[2968], WFF = mvlregress((avg((tan(x0) * ninteger(x8)) , avg(cos(x3) , max(avg(avg(x0 , x4) , x4) , avg(x0 , x4)))) - avg((tan(x0) * avg(sign(x2) , x8)) , avg(abs(x8) , x9))),(tan(x0) * avg(x5 , x8)),(avg(x5 , x8) * avg(sign(x2) , (tan(x0) * avg(x5 , x8)))),((tan((avg(abs(x3) , avg(cos(x3) , max(avg(avg(x0 , x4) , x4) , avg(x0 , x4)))) - avg(abs(x2) , avg(abs(x8) , x9)))) * avg(x5 , x8)) % (sqrt(abs(((tan(x0) % x2) - min(sin(x7) , x1)))) + tanh(x1))),min(avg(min(avg(x2 , x9) , exp(x5)) , max(max(avg(abs((sqrt(abs(((tan(x0) % x2) - min(sin(x7) , x1)))) + tanh(x1))) , avg(cos(x3) , x1)) , avg(avg(abs(x2) , x4) , max(cos(x6) , x5))) , expt(abs(x7) ,x8))) , exp(x5)),max(avg(avg(x0 , x4) , min(avg(min(abs(avg(x2 , x3)) , ((x7*x7) % x1)) , min(abs(avg(x2 , x3)) , ((x7*x7) % x1))) , ((tan(x0) * (sign(x9) % x2)) / (tan((avg(abs(x7) , x3) / x5)) * x2)))) , max(avg(avg(x0 , x4) , x4) , avg(max(avg(avg(max(avg(avg(x0 , x4) , x4) , avg(x0 , x4)) , x4) , x4) , avg(x0 , x4)) , avg(log(abs(avg((1.0 / x7) , x1))) , x6)))),(avg(x2 , x3) / avg(x1 , x8)),avg(max(avg(avg(x0 , x4) , x4) , avg(max(avg(avg(max(avg(avg(x0 , x4) , x4) , avg(x0 , x4)) , x4) , x4) , avg(x0 , x4)) , x4)) , avg(x1 , sign(x6))),avg(avg(((tan(x0) * x2) - min(sign(max(avg(avg(x0 , x4) , x4) , avg(max(avg(avg(max(avg(avg(x0 , x4) , x4) , avg(x0 , x4)) , x4) , x4) , avg(x0 , x4)) , x4))) , (tan(x0) * x2))) , (min(sign(x6) , sign(x6)) - min(sign(x6) , (tan(x0) * x2)))) , (min(sign(x6) , sign(x6)) - min(sign(x6) , (tan(x0) * x2)))),((sign(sign(expt(abs(x3) ,x3))) - (sign(tanh(sqrt(abs(max(cos(x6) , (sign(x5) - (sign(tanh(sqrt(abs(max(cos(x6) , x5))))) - (sign(tanh(sqrt(abs(max(cos(x6) , x5))))) - (tan(x0) * x2))))))))) - (sign(tanh(sqrt(abs(max(cos(x6) , x5))))) - (tan(x0) * x2)))) + (sign(x5) - (sign(tanh(sqrt(abs(max(cos(x6) , (sign(x5) - (sign(tanh(sqrt(abs(max(cos(x6) , x5))))) - (sign(tanh(sqrt(abs(max(cos(x6) , x5))))) - (tan(x0) * x2))))))))) - (sign(tanh(sqrt(abs(max(cos(x6) , x5))))) - (tan(x0) * x2))))));
myBestSelectorChampions
myBestSelectorChampions[0],Score=[0.02711762728426,ErrorPct=[0.1608366966644],WFF= mvlregress((sign(sign(expt(abs(x3) ,x3))) - (sign(x5) - (sign(tanh(sqrt(abs(max(cos(x6) , (sign(x5) - (sign(tanh(sqrt(abs(max(cos(x6) , x5))))) - (sign(tanh(sqrt(abs(max(cos(x6) , x5))))) - (tan(x0) * x2))))))))) - (sign(tanh(sqrt(abs(max(cos(x6) , x5))))) - (tan(x0) * x2))))),(min(max(avg(avg(x0 , x4) , min(avg(min(abs(avg(x2 , x3)) , ((x7*x7) % x1)) , min(abs(avg(x2 , x3)) , ((x7*x7) % x1))) , ((tan(x0) * (sign(x9) % x2)) / (tan((avg(abs(x7) , x3) / x5)) * x2)))) , max(avg(avg(x0 , x4) , x4) , avg(max(avg(avg(max(avg(avg(x0 , x4) , x4) , avg(x0 , x4)) , x4) , x4) , avg(x0 , x4)) , avg(log(abs(avg((1.0 / x7) , x1))) , x6)))) , ((sign(x3) * min(avg(min(avg(x2 , x9) , exp(x5)) , max(max(avg(abs((sqrt(abs(((tan(x0) % x2) - min(sin(x7) , x1)))) + tanh(x1))) , avg(cos(x3) , x1)) , avg(avg(abs(x2) , x4) , max(cos(x6) , x5))) , expt(abs(x7) ,x8))) , exp(x5))) % (sqrt(abs(((tan(x0) % x2) - min(sin(x7) , x1)))) + tanh(x1)))) - (((sign(x3) * x1) % (sqrt(abs(((tan(x0) % x2) - min(sin(x7) , x1)))) + tanh(x1))) - (tan(x0) * avg(sign(x2) , x8)))),(avg((tan(x0) * ninteger(x8)) , (avg((avg((1.0 / avg(x4 , avg((1.0 / avg(x4 , x9)) , (sqrt(abs(x2)) % x5)))) , avg(x1 , x8)) * ninteger(x8)) , avg(cos(x3) , max(avg(avg(x0 , x4) , x4) , avg(x0 , x4)))) - avg(abs(x2) , avg(abs(x8) , x9)))) - avg(abs(x2) , avg(abs(x8) , x9))),((avg(((tan(x0) * x2) - min(sign(x6) , (tan(x0) * x2))) , (min(sign(x6) , sign(x6)) - min(sign(x6) , (tan(x0) * x2)))) - max(avg(avg(x0 , x4) , x4) , avg(((tan(x0) * x2) - min(sign(x6) , (tan(x0) * x2))) , (min(sign(x6) , sign(x6)) - min(sign(x6) , (tan(x0) * x2)))))) % max(avg(avg(x0 , avg(x2 , x9)) , min(avg(min(abs(avg(x2 , x3)) , ((x7*x7) % x1)) , min(abs(avg(x2 , x3)) , ((x7*x7) % x1))) , ((tan(x0) * (sign(x9) % x2)) / (tan((avg(abs(x7) , x3) / x5)) * x2)))) , max(avg(avg(x0 , x4) , x4) , avg(max(avg(avg(max(avg(avg(x0 , x4) , x4) , avg(x0 , x4)) , x4) , x4) , avg(x0 , x4)) , avg(log(abs(avg((1.0 / x7) , x1))) , x6))))),min(exp(x5) , tanh(x1)),max((tan(x0) * avg(x5 , x8)) , (avg((tan(x0) * ninteger(x8)) , avg(cos(x3) , max(avg(avg(x0 , x4) , x4) , avg(x0 , x4)))) - avg(abs(x2) , avg(abs(x8) , x9)))),(sign(x9) / avg(x1 , x8)),avg(max(avg(avg(x0 , x4) , x4) , avg(max(avg(avg(max(avg(avg(x0 , x4) , x4) , avg(x0 , x4)) , x4) , x4) , avg(x0 , x4)) , x4)) , avg((1.0 / avg(x1 , x8)) , avg(x1 , x8))),((avg(x5 , x8) * avg(sign(x2) , x8)) + sign(x6)),(((sign(x3) * x1) % (sqrt(abs(((tan(x0) % x2) - min(sin(x7) , x1)))) + tanh(avg(x5 , x8)))) + (tan(x0) * avg(x5 , x8))));
myBestSelectorChampions[1],Score=[0.02724681775426,ErrorPct=[0.160087253817],WFF= mvlregress((sign(sign(expt(abs(x3) ,x3))) - (sign(tanh(sqrt(abs(max(cos(x6) , (sign(x5) - (sign(tanh(sqrt(abs(max(cos(x6) , x5))))) - (sign(tanh(sqrt(abs(max(cos(x6) , x5))))) - (tan(x0) * x2))))))))) - (sign(tanh(sqrt(abs(max(cos(x6) , x5))))) - (tan(x0) * x2)))),(avg((tan(x0) * ninteger(x8)) , avg(cos(x3) , max(avg(avg(x0 , x4) , x4) , avg(x0 , x4)))) - avg(abs(x2) , avg(abs(x8) , x9))),(tan(x0) * avg(x5 , x8)),(avg(x5 , x8) * avg(sign(x2) , x8)),((sign(x3) * x1) % (sqrt(abs(((tan(x0) % x2) - min(sin(x7) , x1)))) + tanh(x1))),min(avg(min(avg(x2 , x9) , exp(x5)) , max(max(avg(abs((sqrt(abs(((tan(x0) % x2) - min(sin(x7) , x1)))) + tanh(x1))) , avg(cos(x3) , x1)) , avg(avg(abs(x2) , x4) , max(cos(x6) , x5))) , expt(abs(x7) ,x8))) , exp(x5)),max(avg(avg(x0 , x4) , min(avg(min(abs(avg(x2 , x3)) , ((x7*x7) % x1)) , min(abs(avg(x2 , x3)) , ((x7*x7) % x1))) , ((tan(x0) * (sign(x9) % x2)) / (tan((avg(abs(x7) , x3) / x5)) * x2)))) , max(avg(avg(x0 , x4) , x4) , avg(max(avg(avg(max(avg(avg(x0 , x4) , x4) , avg(x0 , x4)) , x4) , x4) , avg(x0 , x4)) , avg(log(abs(avg((1.0 / x7) , x1))) , x6)))),(min(avg(min(abs(avg(x2 , x3)) , ((x7*x7) % x1)) , min(abs(avg(x2 , x3)) , ((x7*x7) % x1))) , ((tan(x0) * (sign(x9) % x2)) / (tan((avg(abs(x7) , x3) / x5)) * x2))) / avg(x1 , x8)),avg(((tan(x0) * x2) - min(sign(x6) , (tan(x0) * x2))) , (min(sign(x6) , sign(x6)) - min(sign(x6) , (tan(x0) * x2)))),avg(max(avg(avg(x0 , x4) , x4) , avg(max(avg(avg(max(avg(avg(x0 , x4) , x4) , avg(x0 , x4)) , x4) , x4) , avg(x0 , x4)) , x4)) , avg(x1 , x8)));
myBestRegressorChampions
myBestRegressorChampions[0],ErrorPct=[0.159884340631,Score=[0.02736840545727],WFF= mvlregress((avg((tan(x0) * ninteger(x8)) , avg(cos(x3) , max(avg(avg(x0 , x4) , x4) , avg(x0 , x4)))) - avg((tan(x0) * avg(sign(x2) , x8)) , avg(abs(x8) , x9))),(tan(x0) * avg(x5 , x8)),(avg(x5 , x8) * avg(sign(x2) , (tan(x0) * avg(x5 , x8)))),((tan((avg(abs(x3) , avg(cos(x3) , max(avg(avg(x0 , x4) , x4) , avg(x0 , x4)))) - avg(abs(x2) , avg(abs(x8) , x9)))) * avg(x5 , x8)) % (sqrt(abs(((tan(x0) % x2) - min(sin(x7) , x1)))) + tanh(x1))),min(avg(min(avg(x2 , x9) , exp(x5)) , max(max(avg(abs((sqrt(abs(((tan(x0) % x2) - min(sin(x7) , x1)))) + tanh(x1))) , avg(cos(x3) , x1)) , avg(avg(abs(x2) , x4) , max(cos(x6) , x5))) , expt(abs(x7) ,x8))) , exp(x5)),max(avg(avg(x0 , x4) , min(avg(min(abs(avg(x2 , x3)) , ((x7*x7) % x1)) , min(abs(avg(x2 , x3)) , ((x7*x7) % x1))) , ((tan(x0) * (sign(x9) % x2)) / (tan((avg(abs(x7) , x3) / x5)) * x2)))) , max(avg(avg(x0 , x4) , x4) , avg(max(avg(avg(max(avg(avg(x0 , x4) , x4) , avg(x0 , x4)) , x4) , x4) , avg(x0 , x4)) , avg(log(abs(avg((1.0 / x7) , x1))) , x6)))),(avg(x2 , x3) / avg(x1 , x8)),avg(max(avg(avg(x0 , x4) , x4) , avg(max(avg(avg(max(avg(avg(x0 , x4) , x4) , avg(x0 , x4)) , x4) , x4) , avg(x0 , x4)) , x4)) , avg(x1 , sign(x6))),avg(avg(((tan(x0) * x2) - min(sign(max(avg(avg(x0 , x4) , x4) , avg(max(avg(avg(max(avg(avg(x0 , x4) , x4) , avg(x0 , x4)) , x4) , x4) , avg(x0 , x4)) , x4))) , (tan(x0) * x2))) , (min(sign(x6) , sign(x6)) - min(sign(x6) , (tan(x0) * x2)))) , (min(sign(x6) , sign(x6)) - min(sign(x6) , (tan(x0) * x2)))),((sign(sign(expt(abs(x3) ,x3))) - (sign(tanh(sqrt(abs(max(cos(x6) , (sign(x5) - (sign(tanh(sqrt(abs(max(cos(x6) , x5))))) - (sign(tanh(sqrt(abs(max(cos(x6) , x5))))) - (tan(x0) * x2))))))))) - (sign(tanh(sqrt(abs(max(cos(x6) , x5))))) - (tan(x0) * x2)))) + (sign(x5) - (sign(tanh(sqrt(abs(max(cos(x6) , (sign(x5) - (sign(tanh(sqrt(abs(max(cos(x6) , x5))))) - (sign(tanh(sqrt(abs(max(cos(x6) , x5))))) - (tan(x0) * x2))))))))) - (sign(tanh(sqrt(abs(max(cos(x6) , x5))))) - (tan(x0) * x2))))));
myBestRegressorChampions[1],ErrorPct=[0.160087253817,Score=[0.02724681775426],WFF= mvlregress((sign(sign(expt(abs(x3) ,x3))) - (sign(tanh(sqrt(abs(max(cos(x6) , (sign(x5) - (sign(tanh(sqrt(abs(max(cos(x6) , x5))))) - (sign(tanh(sqrt(abs(max(cos(x6) , x5))))) - (tan(x0) * x2))))))))) - (sign(tanh(sqrt(abs(max(cos(x6) , x5))))) - (tan(x0) * x2)))),(avg((tan(x0) * ninteger(x8)) , avg(cos(x3) , max(avg(avg(x0 , x4) , x4) , avg(x0 , x4)))) - avg(abs(x2) , avg(abs(x8) , x9))),(tan(x0) * avg(x5 , x8)),(avg(x5 , x8) * avg(sign(x2) , x8)),((sign(x3) * x1) % (sqrt(abs(((tan(x0) % x2) - min(sin(x7) , x1)))) + tanh(x1))),min(avg(min(avg(x2 , x9) , exp(x5)) , max(max(avg(abs((sqrt(abs(((tan(x0) % x2) - min(sin(x7) , x1)))) + tanh(x1))) , avg(cos(x3) , x1)) , avg(avg(abs(x2) , x4) , max(cos(x6) , x5))) , expt(abs(x7) ,x8))) , exp(x5)),max(avg(avg(x0 , x4) , min(avg(min(abs(avg(x2 , x3)) , ((x7*x7) % x1)) , min(abs(avg(x2 , x3)) , ((x7*x7) % x1))) , ((tan(x0) * (sign(x9) % x2)) / (tan((avg(abs(x7) , x3) / x5)) * x2)))) , max(avg(avg(x0 , x4) , x4) , avg(max(avg(avg(max(avg(avg(x0 , x4) , x4) , avg(x0 , x4)) , x4) , x4) , avg(x0 , x4)) , avg(log(abs(avg((1.0 / x7) , x1))) , x6)))),(min(avg(min(abs(avg(x2 , x3)) , ((x7*x7) % x1)) , min(abs(avg(x2 , x3)) , ((x7*x7) % x1))) , ((tan(x0) * (sign(x9) % x2)) / (tan((avg(abs(x7) , x3) / x5)) * x2))) / avg(x1 , x8)),avg(((tan(x0) * x2) - min(sign(x6) , (tan(x0) * x2))) , (min(sign(x6) , sign(x6)) - min(sign(x6) , (tan(x0) * x2)))),avg(max(avg(avg(x0 , x4) , x4) , avg(max(avg(avg(max(avg(avg(x0 , x4) , x4) , avg(x0 , x4)) , x4) , x4) , avg(x0 , x4)) , x4)) , avg(x1 , x8)));

Show final results on training data, Score=[0.02711762728426], ErrorPct=[0.1608366966644]
X[0], ey=[-2251302.201029], y=[-2251402.101689], ErrorPct=[0.01412646717084]
X[50], ey=[-6041.983042469], y=[-6195.704159027], ErrorPct=[0.02173695655287]
X[100], ey=[-722.0712091117], y=[-3130.582215556], ErrorPct=[0.3405758445968]
X[150], ey=[-1022.52387046], y=[-2172.025756685], ErrorPct=[0.1625454792273]
X[200], ey=[-343.1261725993], y=[-1662.221442415], ErrorPct=[0.1865268559783]
X[250], ey=[-691.3360238576], y=[-1364.602017333], ErrorPct=[0.09520327445153]
X[300], ey=[-2083.485605516], y=[-1069.97333687], ErrorPct=[0.1433158478329]
X[350], ey=[10.15589149018], y=[-852.5351052423], ErrorPct=[0.1219889442283]
X[400], ey=[-505.5912916789], y=[-568.2603011271], ErrorPct=[0.008861720276869]
X[450], ey=[-1149.744822665], y=[-337.6258769846], ErrorPct=[0.1148377960898]
X[500], ey=[-1140.399869717], y=[-89.72301605376], ErrorPct=[0.148571111312]
X[550], ey=[1063.132554443], y=[180.6652119063], ErrorPct=[0.1247854212455]
X[600], ey=[2178.99536047], y=[470.9245459906], ErrorPct=[0.241529998707]
X[650], ey=[-808.8667819151], y=[788.0600618419], ErrorPct=[0.2258136695729]
X[700], ey=[2575.899561658], y=[1101.245484268], ErrorPct=[0.2085236714931]
X[750], ey=[-411.5231043893], y=[1438.240442598], ErrorPct=[0.2615660798867]
X[800], ey=[345.3729999495], y=[1744.353970525], ErrorPct=[0.1978231049615]
X[850], ey=[900.9655294302], y=[2201.753226929], ErrorPct=[0.1839380710869]
X[900], ey=[3660.620959795], y=[3060.30368903], ErrorPct=[0.08488794984517]
X[950], ey=[3841.861827247], y=[5499.583932154], ErrorPct=[0.2344104322027]
Actual computed error on training data is ErrorPct=[0.1608366966644] versus reported ErrorPct=[0.1608366966644] while average Y is AvgY=[-2349.810741959]
yHistory=[#(num| -37554.32322009 -2268.270468314 -1364.445593527 -843.2004156157 -330.0431935462 171.2113983184 789.3068920029 1430.129069022 2280.671275416 14190.85683675 )]
eHistory=[#(num| -37193.63261251 -1571.371870693 -906.5615050737 -376.755269683 -218.1026622422 242.5820283816 667.4418433337 704.6777929833 1523.654580034 13629.96025588 )]
aHistory=[#(num| -37193.63261251 -19382.5022416 -13223.85532943 -10012.08031449 -8053.284784041 3353.663300124 4131.433618059 5286.097542967 7576.807417959 13629.96025588 )]
dHistory=[#(num| 11406.94808416 14143.51393255 18509.95287239 26959.30965956 50823.5928684 )]


Final testing on test data returns Score=[0.04873258574713], ErrorPct=[0.327626392021]
X[0], ey=[-173197.3000184], y=[-171655.7339826], ErrorPct=[0.4411588480098]
X[50], ey=[-3813.767864705], y=[-4851.266724674], ErrorPct=[0.296907035599]
X[100], ey=[-5580.769827481], y=[-2811.888697706], ErrorPct=[0.7923866906152]
X[150], ey=[-2399.783622507], y=[-2022.147634432], ErrorPct=[0.1080702698393]
X[200], ey=[-1535.555821806], y=[-1618.303311911], ErrorPct=[0.02368032673411]
X[250], ey=[699.859064466], y=[-1256.885878756], ErrorPct=[0.5599729917128]
X[300], ey=[-716.2034290355], y=[-940.6731661318], ErrorPct=[0.06423779995763]
X[350], ey=[-1903.326715931], y=[-707.0154415674], ErrorPct=[0.3423553006464]
X[400], ey=[-3672.762035059], y=[-418.1521989741], ErrorPct=[0.931390477376]
X[450], ey=[338.540483103], y=[-112.5559431431], ErrorPct=[0.129092867331]
X[500], ey=[-39.24169162728], y=[126.6598752662], ErrorPct=[0.04747700872564]
X[550], ey=[325.0209055365], y=[358.8532680125], ErrorPct=[0.009682002397926]
X[600], ey=[737.5168502117], y=[633.1017861331], ErrorPct=[0.02988106140992]
X[650], ey=[1981.650656296], y=[921.4705338383], ErrorPct=[0.3033978633668]
X[700], ey=[-175.2123323618], y=[1207.844859513], ErrorPct=[0.3957974574698]
X[750], ey=[-243.5353679577], y=[1512.980941834], ErrorPct=[0.5026724082736]
X[800], ey=[-21.93359176999], y=[1888.403066127], ErrorPct=[0.5466920649043]
X[850], ey=[221.662892417], y=[2542.105926954], ErrorPct=[0.6640545732085]
X[900], ey=[4844.028123971], y=[3472.991781559], ErrorPct=[0.3923573816132]
X[950], ey=[5525.944415088], y=[6549.200680891], ErrorPct=[0.2928311502401]
Actual computed error on testing data is ErrorPct=[0.327626392021], Avg Y=[186.3005789062], AvgDev Y=[3494.355928197]
yHistory=[#(num| -12354.71821192 -2106.558711947 -1251.241260038 -694.7076499594 -130.2072084136 377.6118524223 926.0780292027 1510.858067185 2574.083284415 13011.80759812 )]
eHistory=[#(num| -11879.02851754 -1347.432805097 -550.9288536066 -336.3088860865 -17.5144515119 273.7250843666 408.6007856057 697.8642736218 2036.47269957 12577.55645974 )]
aHistory=[#(num| -11879.02851754 -6613.230661318 -4592.463392081 -3528.424765582 -2826.242702768 3198.843860581 3930.123554634 5103.964477644 7307.014579655 12577.55645974 )]
dHistory=[#(num| 6025.086563349 7458.548320216 9696.427869724 13920.24524097 24456.58497728 )]

esm.selfTest[cyclicSeries]: completed in [99.00988333333] minutes.
true

(writeln "********** regressGaCMVL:")(esm.setOptions regressGaCMVL: 00% true)(esm.selfTest cyclicSeries: 1 20 1000 200 .01% checkpoint:)
********** regressGaCMVL:

Starting test case: cyclicSeries
Building test data as: y = -9.165146942478 - (9.165146942478*x0*sin(x0)) - (19.5666514757*x1*cos(x0)) + (21.87460482304*x2*tan(x0)) - (17.48124453288*x3*sin(x0)) + (38.81839452492*x4*cos(x0)) - (38.63656433142*x5*tan(x0)) + (13.18212824804*x6*sin(x0)) - (2.045229508597*x7*cos(x0)) + (45.1292360492*x8*tan(x0)) + (26.03603502163*x9*sin(x0)) + (21.98935009253*x10*cos(x0)) + (39.25956682671*x11*tan(x0)) - (36.9527398792*x12*sin(x0)) + (8.601886956836*x13*cos(x0)) + (7.533313498536*x14*tan(x0)) - (23.03633770636*x15*sin(x0)) - (41.11091670539*x16*cos(x0)) + (46.07594342939*x17*tan(x0)) - (4.158230358161*x18*sin(x0)) + (35.61243398897*x19*cos(x0));
...starting new evolutionary process.
...starting generation of constant selector WFF.
...starting regress selector for each column name.
...starting a best-of-breed MVL selector WFF.
...starting a best-of-breed REG selector WFF.
...starting generation of root selector WFFs in each column island.
...starting generation of random selector WFFs.

Final results of training
esm: N = [1000], M = [20], Generations = [200], WFFs = [15625], Score=[0.1158047561399], ScoreHistory=[#(num| 0.1158047561399 )]
esm.myBest, Score=[0.1158047561399], ErrorPct=[0.6801518928869], lengthWFF=[7227], WFF = mvlregress((max((ninteger(x11) * tan(x0)) , ((1.0 / x2) % log(abs((((((x10 * x16) * (min(min(cos(x9) , (-x9)) , (x19*x19*x19)) % tanh(x15))) % x10) % (((x10 * (((avg(sign(x7) , (x4*x4*x4)) + abs(x2)) + (x10 % (x15 % x8))) - ((min(min(cos(x9) , (-x9)) , (x19*x19*x19)) % tanh(x15)) % (x3 % x11)))) % abs(x19)) % x14)) - min(ninteger((ninteger(x12) % abs((x18 % x11)))) , ((sign(x1) + sign(x9)) / (ninteger(x16) + x7)))))))) - max(max(tanh(x1) , tan(x0)) , min(((1.0 / cos(x19)) - cos(x2)) , min((x11*x11*x11) , (x12 % x16))))),(max(avg(x13 , avg((x19*x19) , ((1.0 / x17) / (1.0 / x4)))) , ((-x7) / log(abs(x13)))) * tan(x0)),(ninteger(((((-x4) / log(abs(x13))) - sign(((1.0 / x13) - ((-x4) - (avg((ninteger(x8) * tan(x0)) , (cos(x19) * (1.0 / x12))) - avg(x10 , x13)))))) * x14)) * sin(((((min(min(cos(x9) , (-x9)) , (x19*x19*x19)) % tanh(x15)) % (x3 % x11)) % abs(x19)) / sign(x3)))),(((min((abs(abs(x16)) / (x5 / x4)) , (abs(abs(x16)) / (((x8*x8*x8) / x15)*((x8*x8*x8) / x15)*((x8*x8*x8) / x15)))) - max(((x7 % x10) % x11) , ((x10 * x16) * (min(min(cos(x9) , (-x9)) , (x19*x19*x19)) % tanh(x15))))) * avg(cos(x6) , x16)) % avg(x9 , (max(avg(x13 , avg((x19*x19) , ((1.0 / (x4 % x16)) / (1.0 / x4)))) , tanh(x1)) - ((avg(tan(x19) , x13) / (x19*x19*x19)) / (ninteger(avg(avg((ninteger(x8) * tan(x0)) , (cos(x19) * (1.0 / x12))) , avg((cos(x1) + x16) , x13))) / avg(tan(x19) , x13)))))),(((1.0 / x19) * (x19*x19)) % ((-x2) % (1.0 / x10))),((((x10 * x16) * (min(min(cos(x9) , (-x9)) , (x19*x19*x19)) % tanh(x15))) % x10) % (x4 % (abs(x2) * ((cos(avg(x12 , x16)) * (avg(sign(avg(x13 , x18)) , log(abs(x3)))*avg(sign(avg(x13 , x18)) , log(abs(x3)))*avg(sign(avg(x13 , x18)) , log(abs(x3))))) / ((-x2) % (1.0 / (((1.0 / x2) / x14) / x16))))))),((max((-max((ninteger(x11) * tan(x0)) , ((1.0 / x2) % log(abs(x5))))) , (x10 % x5)) % (ninteger(x5) / tanh(x10))) % (x4 % x14)),(avg(sign(x13) , log(abs((min((tan(x0) / (ninteger(x11) / x15)) , min((((1.0 / (((min((abs(abss(x19) * (1.0 / x12))) , avg((cos(x1) + x16) , cos(x19)))) , avg(expt(abs((1.0 / x17)) ,sign(x0)) , avg(avg((avg(sign(x13) , log(abs(x3))) % (x3*x3*x3)) , (x7 % x11)) , x13)))) / (x19*x19*x19))),cos((abs(x13) / sign(sin(x15)))),avg(expt(abs((1.0 / x17)) ,sign(x0)) , avg((x3 % x11) , avg((ninteger(x8) * tan(x0)) , (cos(avg(abs(x18) , x19)) * (1.0 / x12))))),avg(cos(x19) , avg(avg((ninteger(x8) * tan(x0)) , (cos(x19) * (1.0 / x12))) , avg((cos(x1) + (((abs(abs(x16)) / (x9*x9*x9)) / (ninteger(x16) / tanh((1.0 / x17)))) / cos((abs(avg(((-x0) - sqrt(abs(x16))) , exp(x10))) / avg(((-x0) - sqrt(abs(x16))) , exp(x10)))))) , cos(x19)))),avg(avg(((-x4) / log(abs(x13))) , avg(abs(x15) , avg(avg(((-x4) / log(abs(x13))) , avg(abs(x15) , avg((avg((sqrt(abs(x16)) - (-sqrt(abs(((x8*x8*x8) / expt(abs(avg(sign(avg(sign(x10) , ((ninteger(x13) / (x11*x11*x11))*(ninteger(x13) / (x11*x11*x11))*(ninteger(x13) / (x11*x11*x11))))) , log(abs(avg(x12 , x16))))) ,avg(sqrt(abs(x6)) , x18))))))) , avg((-x13) , (-x19))) / log(abs(x13))) , ((-x7) / avg(abs(x15) , x15))))) , (abs((x11*x11*x11)) / (x9*x9*x9))))) , (abs((1.0 / x17)) / (x9*x9*x9))),(min((x7 % x11) , (x11*x11*x11)) + (avg(sign(x13) , log(abs(x3))) % (ninteger(x16) + x7))),(min(abs((x15 * x16)) , (x12 % x16)) + log(abs(x3))));
bs(abs(x16)))) , ((1.0 / x19) - sign(x10))))))))))))),expt(abs(min(avg((x3 % x11) , (ninteger(x16) + x7)) , avg((x3 % x11) , avg((ninteger(x8) * tan(x0)) , (cos(avg(abs(x18) , x19)) * (1.0 / x12)))))) ,(avg((avg(sign(x13) , log(abs(x3))) % avg(x10 , avg(tan(x19) , x13))) , avg(x13 , x18)) % (x4 % min(min((x11*x11*x11) , (x12 % x16)) , (x12 % x16))))),(x7 / (avg(tan(x19) , max(avg(cos(max(expt(abs(abs(x13)) ,tanh((x19*x19*x19))) , min(x19 , x6))) , avg(avg((ninteger(x8) * tan(x0)) , (co









;;**EXPORTKEY**:esm:%%Document_Esm_Ref_Guide
;#text#
<?xml version="1.0" encoding="UTF-8"?>
<Document>
	<KnowledgeBase>
	    <Title>Evolutionary Sequencing Machine</Title>
		<Topic>ESM</Topic>
		<SubTopic>Reference Guide</SubTopic>
		<HumanKeywords>ESM Lambdas Machine-Learning  Genetic-Programming Artificial-Intelligence</HumanKeywords>
	</KnowledgeBase>
	<Essay>Essay_ESM_Introduction</Essay>
	<Essay>Essay_ESM_Design_Notes</Essay>
	<Essay>Essay_Selector_Introduction</Essay>
	<Essay>Essay_Selector_Statements</Essay>
	<Essay>Essay_ESM_Performance_Tests</Essay>
 
</Document>






































;;**EXPORTKEY**:esm:%%Essay_ESM_Design_Notes
;#text#
<?xml version="1.0" encoding="UTF-8"?>
<Essay>
	<KnowledgeBase>
	    <Title>Design Notes</Title>
		<Topic>ESM</Topic>
		<SubTopic>Design</SubTopic>
		<HumanKeywords>ESM Machine-Learning Genetic-Programming Artificial-Intelligence</HumanKeywords>
	</KnowledgeBase>
	<Section>
	    <Heading>Introduction</Heading>
		<Description><![CDATA[	
             <p>Let X be a time series of vectors such as the numeric vector x = #(num| xtime x1 x2 x3 ... xm),
             and let Y be a numeric vector of "score" values. Let both X and Y be of length N.</p>

             <p>The time series, X, together with the "scores", Y, are used to train this learning machine. 
             There is also a testing set, TX and TY, of vectors similar to those in X and Y
             but for the testing time period (a time period not present in X or Y).
             After training, the machine is presented with the testing set, TX, 
             and outputs an estimator Lambda, Selector, which attempts to estimate ty in TY when presented with the corresponding tx in TX.</p> 

             <p>Running this Selector on every tx in TX returns a Vector EY, of estimates for TY. 
             Each element in EY contains a numeric estimate for the corresponding value in TY. 
             The Selector Lambda attempts to:</p>
             
             <ol>
             <li>Make the natural ordering of EY be predictive of the natural ordering of TY</li>
             <li>Minimize the least squared error between EY and TY</li> 
             </ol>

             <p>The internal model used by the Selector Lambda to compute ey is important. 
             The range of possible X and Y training sets and the power of the possible estimates is determined by the internal model used in the Selector Lambda.
             </p>       

             <p><b>Linear Regression</b></p>

             <p>Using the technique of <i>linear regression</i> the Selector Lambda could provide accurate estimates for all X and Y training sets
             which had been constructed from the following model, <b>y = c0 + c1*F1(x)</b>,             
             where c0 and c1 are internal numeric constants and F1 is an arbitrary numeric function on members of X.
             If the choice of F1 is known, linear regression can estimate the values of C0 and C1 quite accurately just by training on X and Y.       
             If the choice of F1 is unknown, linear regression works very poorly.</p>       

             <p><b>Multiple Linear Regression</b></p>

             <p>Using the technique of <i>multiple linear regression</i> the Selector Lambda could provide accurate estimates for all X and Y training sets
             which had been constructed from the following model, <b>y = c0 + c1*F1(x) + c2*F2(x) ... + cm*Fm(x)</b>,             
             where c0 through cm are internal numeric constants and F1 through Fm are arbitrary numeric functions on members of X.       
             If the choices of F1 through Fm are known, multiple linear regression can estimate the values of C0 through Cm quite accurately just by training on X and Y.       
             If the choices of F1 through Fm are unknown, multiple linear regression works very poorly.</p>       

             <p><b>Evolutionary Sequencing Regression</b></p>

             <p>The Evolutionary Sequencing Regression uses the techniques of <i>linear regression</i>, <i>multiple linear regression</i>,
             <i>support vector regression</i> and <i>percentile grid regression</i>.
             The Selector Lambda can provide accurate estimates for all X and Y training sets which have been constructed from the following models,
             <b>y = c0 + c1*E1(x)</b> or <b>y = c0 + c1*E1(x) + c2*E2(x) ... + cm*Em(x)</b>,             
             where c0 through cm are internal numeric constants and E1 through Em are valid Selector language regression expressions.
             Even when the choices of E1 through Em are unknown, evolutionary sequencing regression works very well,
             performing natural order preservation as its first priority and minimizing the least squared error as its second priority.      
             An explanation of the power and limitations of valid Selector regression expressions is contained in the next section.</p>       

	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Selector Language</Heading>
		<Description><![CDATA[	
             <p>The Evolutionary Sequencing Machine evolves a population of well-formed-formulas (WFFs) in a regression-oriented grammar known as <b>Selector</b>.
             Each WFF is known as a <i>Selector</i>, and is trained to map number vectors in X and XT to numbers in Y and YT.
             The Selector language is a dialect of JavaScript, to make learning and reading easy by the current generation of programmers. 
             Each Selector Lambda, wff, supports the following methods. 
             </p>

             <ul>
               <li>wff(x) ==> ey : invoking the main wff maps a number vector, x, to a number, y.</li>
               <li>wff.run(X) ==> EY : invoking wff.run outputs a number vector, EY, of estimates for Y.</li>
               <li>wff.train(X,Y) ==> true : invoking wff.train trains the Selector wff on the specified training data.</li>
             </ul>

             <p>The Selector grammar is defined, within the ESM Lambda, in esm:selector:%DECLARATION, which is a feature-based grammar specification understood by the ParseLib.
             The esm.selector child Lambda is a "Selector" parser, generated from esm:selector:%DECLARATION by ParseLib.
             The Selector parser translates ASCII text strings into Selector WFFs which are annotated s-expressions.
             Each WFF s-expression, in the population, may be annotated with grammar notes taken from the rules defined in esm:selector:%DECLARATION.
             </p> 

             <p>Each Selector WFF must chose one of the following regression model syntax to remain gramatically correct.
             </p> 

             <p><b>Linear Regression</b></p>

             <pre>         
             regress <i>expression</i>;         

                         <b>for example:</b>

             regress (x10 / sin(x12));
             </pre>

             <p>Trains a linear regression Selector WFF for the model <i>expression</i>. The <i>expression</i> must be a valid Selector regression expression.</p>

             <p><b>Support Vector Regression</b></p>

             <pre>         
             svmregress(<i>expression1</i>, <i>expression2</i>,..., <i>expressionM</i>);         

                         <b>for example:</b>

             svmregress (x10,cos(x12)/log(x3));
             </pre>

             <p>Trains a support vector regression Selector WFF, with a cubic kernel, for the models <i>expression1</i> through <i>expressionM</i>. 
             The models <i>expression1</i> through <i>expressionM</i> must each be valid Selector regression expressions.
             There must be AT LEAST one model expression and there may be up to M model expressions (where M is the number of elements in each x vector).
             The special case of <b>svmregress();</b> is assumed to mean <b>svmregress(xtime,x1,x2,...,xm);</b></p>

             <p><b>Percentile Grid Regression</b></p>

             <pre>         
             pgmregress(<i>expression1</i>, <i>expression2</i>,..., <i>expressionM</i>);         

                         <b>for example:</b>

             pgmregress (x1,exp(x12)/log(x3));
             </pre>

             <p>Trains a percentile grid regression Selector WFF for the models <i>expression1</i> through <i>expressionM</i>. 
             The models <i>expression1</i> through <i>expressionM</i> must each be valid Selector regression expressions.
             There must be AT LEAST one model expression and there may be up to M model expressions (where M is the number of elements in each x vector).</p>


             <p><b>Basis Grid Regression</b></p>

             <pre>         
             bgmregress(<i>expression1</i>, <i>expression2</i>, <i>expression2</i>, <i>expression4</i>);         

                         <b>for example:</b>

             bgmregress (x1,exp(x12)/log(x3));
             </pre>

             <p>Trains a basis grid regression Selector WFF, with a basis grid, for the models <i>expression1</i> through <i>expressionM</i>. 
             The models <i>expression1</i> through <i>expressionM</i> must each be valid Selector regression expressions.
             There must be AT LEAST one model expression and there may be up to four model expressions.</p>


             <p><b>Selector Expressions</b></p>

             <p>Valid Selector regression expression well-formed-formulas (WFFs) can be constructed by recursively applying the following production grammar rules.</p> 

             <pre>
             <b>TERMINAL</b>: xtime x1 x2 ... xm

                                  <b>explanation:</b>

                              A TERMIAL is any one of the elements of the input vector, x.

                                  <b>example:</b>

                              svmregress (xtime, x3, x10);
             </pre>

             <pre>
             <b>UNARY</b>:    abs(EXP) cos(EXP) cube(EXP) exp(EXP) integer(EXP) log(EXP) (- EXP) sign(EXP) sin(EXP) sqrt(EXP) square(EXP) tan(EXP) tanh(EXP)

                                  <b>explanation:</b>

                              A UNARY is any one of the unary operators, shown above, enclosing a valid Selector regression expression WFF.

                                  <b>example:</b>

                              svmregress (sin(xtime), abs(x3), log(sqrt(x10)));
             </pre>

             <pre>             
             <b>BINARY</b>:   avg(EXP1,EXP2) (EXP1+EXP2) (EXP1-EXP2)  (EXP1/EXP2)  (EXP1*EXP2) expt(EXP1,EXP2) max(EXP1,EXP2) min(EXP1,EXP2) mod(EXP1,EXP2) 

                                  <b>explanation:</b>

                              A BINARY is any one of the binary operators, shown above, enclosing two valid Selector regression expression WFFs.

                                  <b>example:</b>

                              svmregress (sin(xtime)-abs(x3), log(sqrt(x10))*x2);
             </pre>

             <pre>             
             <b>EXP</b>:      TERMINAL UNARY BINARY  

                                  <b>explanation:</b>

                              An EXP is any WFF resulting from invoking the TERMINAL, UNARY, or BINARY Selector grammar rules <u>up to five times recursively</u>.

                                  <b>example:</b>

                              pgmregress (sin(xtime)*abs(x3), tan(sqrt(x10))*x2);
             </pre>

             <p>A more detailed description of the Selector language is provided in the chapters on <i>Selector Introduction</i> and <i>Selector Statements</i>.</p>
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Evolutionary Computation</Heading>
		<Description><![CDATA[	
             <p>There are many specialties in the umbrella field of Evolutionary Computation including (but not limited to): 
             Genetic Algorithms; Genetic programming; Artificial Life; Grammatical Evolution; Evolvable Hardware, etc.
             However different, these various specialties all have a <i>least common denominator</i> of similarities; and,
             each of these specialties possess at least some techniques which have proven useful when applied to selected problem domains.
             </p>       

             <p><b>Populations</b></p>

             <p>One of the primary <i>least common denominator</i> areas across all specialties in the umbrella field of Evolutionary Computation,
             is the commonality of large populations of <i>things</i> which change, over time, becoming a population of <i>fitter things</i>. 
             In the specialties of Genetic Algorithms, Genetic Programming, and Grammatical Evolution, the <i>things</i> are called <i>genomes</i>.
             In other specialties the <i>things</i> are called <i>Lambdas</i>, <i>organisms</i>, <i>robots</i>, <i>rules</i>, <i>DNA sequences</i>, etc.
             </p>       

             <p><b>Fitness</b></p>

             <p>Another important <i>least common denominator</i> area across all specialties in the umbrella field of Evolutionary Computation,
             is the commonality of <i>fitness</i>. The population is said to become <i>fitter</i> over time. 
             In each specialty, the concept of <i>fitness</i> is expressed via a mapping from things in the population to a <i>fitness score</i>. 
             In our observations, of each area of specialty, the concept of fitness mapping can be generalized to a mapping from elements of a population
             of things to a real vector space.
             </p>       

             <p><b>Survival</b></p>

             <p>Survival is another <i>least common denominator</i> area across all specialties and is related to the fitness measure in some manner.
             In all specialties there are discrete time steps during which the things in the population are promoted (<i>survive</i>) into the next time frame. 
             Normally, this survival mechanism is related to the fitness measure in that the population becomes <i>fitter</i> over time. 
             </p>       

             <p><b>Operators</b></p>

             <p>In almost all areas of specialty which we studied the concept of population operators morphing the population is a <i>least common denominator</i>.
             In the specialties of Genetic Algorithms, Genetic Programming, and Grammatical Evolution, the operators are called <i>mutation</i>, <i>recombination</i>, <i>reproduction</i>, etc.
             In many cases the population operators incorporate the concept of survival. 
             In these architectures, the population operators map things (in the current population) into things (in the population one time step in the future).
             Things which are not promoted do not survive. 
             </p>       

             <p><b>Data</b></p>

             <p>All areas of specialty which we studied supported the concept of <i>training data</i> in one form or another.
             In many cases the training data can be numeric vectors.
             In other cases the training data is the initial environment for an ant colony, or a geographic playing field for robot traversal, etc. 
             In all these architectures, the training data, population operators, and fitness mapping work together to produce populations of things which better solve a training problem. 
             In many architectures, there is a concept of problem generalization. 
             In these architectures, once trained, things in the population can be presented with <i>testing data</i> (not seen during the training period) 
             and better solve a testing problem (presumably because the previous training experience was generalized).             
             </p>       

             <p><b>Evaluation</b></p>

             <p>All areas of specialty which we studied supported the concept of <i>evaluation</i> in one form or another.
             There must be some process by which each <i>thing</i> in the population can be <i>evaluated</i> so as to produce a fitness score.
             In the specialty of Genetic Programming, the genome is often an Lisp s-expression and the evaluation is the Lisp eval function.
             In the specialty of Genetic Algorithms, a translator is normally required to convert the genome into a program which can be evaluated.
             In the specialty of Grammatical Evolution, the genome is run through a grammar production resulting in a program which can be evaluated.
             </p>       

             <p><b><u>Design Principle</u></b></p>

             <p>The Evolutionary Sequencing Machine Lambda (ESM) is a symbolic regression engine, designed to accept an X Y training matrix as input 
             and to output an estimator Lambda (Selector) which attempts to map elements in X onto elements in Y, in an order preserving manner, with a high degree of accuracy.
             In order to accomplish this task, the Evolutionary Sequencing Machine Lambda (ESM) is designed, 
             to take advantage of practical research results in the general field of Evolutionary Computation, regardless of their specialty of origin. 
             </p>       

	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Design Ideas</Heading>
		<Description><![CDATA[	
             <p>The Evolutionary Sequencing Machine (ESM) is designed, from its basic foundation,
             to take advantage of any practical research results in the general field of Evolutionary Computation, regardless of the specialty of origin. 
             The Evolutionary Sequencing Machine architecture is a merger  
             of a wide range of evolutionary techniques and theories from throughout the whole field of Evolutionary Computation.</p>       

             <p><b>Population of WFFs</b></p>

             <p>The Evolutionary Sequencing Machine evolves a population of well-formed-formulas (WFFs) in a problem-oriented grammar known as <b>Selector</b>.
             The Selector grammar is defined, within the ESM Lambda, in esm:selector:%DECLARATION, which is a feature-based grammar specification understood by the ParseLib.
             The esm.selector child Lambda is a "Selector" parser, generated from esm:selector:%DECLARATION by ParseLib.
             The Selector parser translates ASCII text strings into Selector WFFs which are annotated s-expressions.
             </p> 

             <p>By implementing its <i>genomes</i> as annotated s-expressions, the Evolutionary Sequencing Machine takes advantage the large body of Genetic Programming tools 
             while also facilitating the use of many interesting techniques available in other specialties.
             Bit string annotations to the WFF s-expressions, facilitate the use of many interesting techniques available in Genetic Algorithm.
             Grammar annotations to the WFF s-expressions, facilitate the use of many interesting techniques available in the Gramatical Evolution.
             Goal and plan annotations to the WFF s-expressions, facilitate the use of many interesting techniques available in MultiLambda Systems and Swarm Learning.
             Nevertheless, since each WFF in the population is essentially an s-expression, evaluation requires only a call to the AIS Lisp eval function.
             </p> 

             <p><b>Operators on WFFs</b></p>

             <p>The Evolutionary Sequencing Machine applies a set of <i>grammar preserving operators</i> to evolve the population of well-formed-formulas (WFFs) into a population of <i>fitter</i> WFFs.
             The ESM population operators are <i>grammar preserving</i> such that each operator maps from a population of <b>Selector</b> WFFs to a population of <b>Selector</b> WFFs.
             Each Selector WFF evaluates to an AIS Lambda which accepts a set of Number Vectors and returns a proper subset of those same Number Vectors (attempting to pick the fittest in the set).
             </p>

             <p>The Evolutionary Sequencing Machine raises the grammar concepts embodied in Grammatical Evolution far above the WFF genome's grammar annotations,
             making the understanding and preservation of WFF grammar an essential task of the population operators.
             Thus ESM operators can understand that two WFF s-expressions cannot be combined because their are grammatically incompatible, 
             that two ESM WFF s-expressions must be combined in specific ways because of their grammar annotations,
             that two ESM WFF s-expressions are essentially equivalent, or
             that two ESM WFF s-expressions can be combined and reduced to a more primal form.
             </p>
      
             <p>Furthermore, the task of fitness improvement is raised far above any requirement for operational <i>purity</i>.
             Evolutionary Sequencing Machine WFFs can be annotated with a wide variety of information to help the population operators raise the population fitness level.
             Annotations can aid operators in establishing protected or unprotected sub-populations. 
             Annotations can aid operators in enhancing populations with WFFs generated via exhaustive parital-search, statistical learning methods, etc. 
             In general the ESM uses annotations to extend the style and type of population operator far beyond those listed in any one specialty while requiring only that the operator be grammar preserving.


             <p><b>Data</b></p>

             <p>The Evolutionary Sequencing Machine learns to select and score the best individuals from a universe of individuals over time.
             Over a series of discrete time steps, a universe of individuals is collected for each time step (the training data). 
             During prediction the ESM attempts to select the best individuals from the testing data (the ESM is <b>NOT</b> given the "score" values for the testing data).  
             The <i>individuals</i> are Number vectors and represent quantitative information about things such as Stocks, People, Cities, etc. 
             The <i>score values</i> are single Numbers and represent some quantitative value about these same Stocks, People, Cities, etc.
             <p>
 
             <p>Each individual followed by the system is given a unique identifier which remains
             unique across all time periods studied (no two individuals ever have the same identifier).
             Furthermore, each time period studied is given a unique ascending integer index (i.e. week 1,
             week 2, etc.). So, for a series of time periods, historical information about groups of
             individuals is collected for each time period. The historical information collected for each
             individual for each time period is stored in a Number Vector and includes: the time period index;
             the unique identifier of the individual; and other numeric information about the individual
             pertinent to the current investigation. Finally, each individual in each time period is given
             a numeric "score" which determines the value of the individual in that time period. The "best"
             individuals have the highest "score" values.
             </p>

             <p><b>Fitness of WFFs</b></p>
 
             <p>During training, the ESM is given historical information and the "score" values for time periods 0 through T for all individuals.
             For each time period 0 through T, we can calculate the average score for both the theoretic best and worst selections of N individuals.
             The ESM currently computes the theoretic best and worst scores for the each of the top and bottom 5% partitions (5%, 10%, 15%, ... 45%, 50%).
             The order preserving "fitness" of a Selector WFF are measured by what percentage of the theoretical "best" averages it obtained on each of these top and bottom percentile partitions.
             In addition, the ESM also computes the least squares error between the predictions and the "score" values for each individual.
             </p> 

             <p>During training, each WFF in the population (without knowing the score values), attempts to score the N individuals for time periods 0 through T.
             Associated with each WFF in the population {w in P} is the least squares error and the order preservation measure for time periods 0 through T.
             These two fittness measures are normalized into a single fittness score as follows S[w,t] =  g(Error[w,t],Order[w,t]).
             </p>

             <p>The fitness measure for each WFF {F[w] == (S[w,0] * S[w,1] * ... * S[w,t])} is the product of its normalized scores for time periods 0 through T.
             The fitness measure tends to favor those WFFs which have done consistantly well during the training period (as opposed to those with only an occasional big success).
             </p>

	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Population Operators</Heading>
		<Description><![CDATA[	
             <p>The Evolutionary Sequencing Machine applies a set of <i>grammar preserving operators</i> to evolve the population of well-formed-formulas (WFFs) into a population of <i>fitter</i> WFFs.
             The population operators are <i>grammar preserving</i> such that each operator maps from a population of <b>Selector</b> WFFs to a population of <b>Selector</b> WFFs.
             The understanding and preservation of WFF grammar is an essential task of the population operators.
             WFFs can be annotated with a wide variety of information to help the population operators raise the population fitness level.
             Annotations can aid operators in establishing protected or unprotected sub-populations <i>evolved with different grammars</i>. 
             Annotations can aid operators in enhancing populations with WFFs generated via exhaustive parital-search, statistical learning methods, etc. 
             </p>

             <p>Here follows an overview of the population operators supported during Evolutionary Sequencing Regression.
             </p>

             <p>The <b>grow</b> operator generates a new WFF, from the empty set, by randomly invoking Selector grammar rules so as to create a WFF.</p>
 
             <p>The <b>mutate</b> operator generates a new WFF, from a parent WFF, by randomly mutating the parent WFF in a grammar preserving manner.</p>
 
             <p>The <b>children</b> operator generates two new WFFs, from two parent WFFs, by randomly invoking Selector grammar rules so as to cut and recombine child WFFs in a grammar preserving manner.</p>
 
             <p>The <b>greed</b> operator generates a new WFF, from the empty set, by using greedy search to invoke Selector grammar rules so as to create a WFF.</p>
 
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Multiple Grammars</Heading>
		<Description><![CDATA[	
             <p>The Evolutionary Sequencing Machine supports multiple <i>grammars</i> in evolving the population of well-formed-formulas (WFFs) problem solutions.
             All ESM population operators are <i>grammar preserving</i> such that each operator maps from a population of <b>Selector</b> WFFs to a population of <b>Selector</b> WFFs,
             in a grammar preserving manner.
             The design concept of <i>grammar preservation</i> extends to establishing protected and unprotected sub-populations <i>evolved with different grammars</i>.
             The understanding and preservation of multiple WFF grammars is an essential task of the Evolutionary Sequencing Machine.
             The Evolutionary Sequencing Machine certainly attempts to evolve general programs, with evolved functions and variables; but, 
             there are also attempts to evolve program solutions using Support Vectors, Multiple Regression, and Decision Tree models.
             In fact the ESM supports the co-evolution of a wide variety of Selector WFF grammars, providing potentially unique strategies for approaching the problem.
             </p>

             <p>There Selector grammars currently supported during Evolutionary Sequencing Regression.
             </p>

             <p>The <b>REG</b> grammar forms valid Selector WFFs which train and run a simple linear regression model on a the training data set.</p>

             <p>The <b>MVL</b> grammar forms valid Selector WFFs which train and run a multiple linear regression model on a the training data set.</p>

             <p>The <b>SVM</b> grammar forms valid Selector WFFs which train and run a support vector regression model with a cubic kernel on a the training data set.</p>

             <p>The <b>BGM</b> grammar forms valid Selector WFFs which train and run a basis grid regression model on the training data set.</p>

             <p>The <b>FRM</b> grammar forms valid Selector WFFs which train and run a multiple factor regression model on the training data set.</p>

             <p>The <b>ENN</b> grammar forms valid Selector WFFs which train and run a neural net regression model on a the training data set.</p>

	    ]]></Description>
	</Section>
</Essay>




































;;**EXPORTKEY**:esm:%%Essay_ESM_Introduction
;#text#
<?xml version="1.0" encoding="UTF-8"?>
<Essay>
	<KnowledgeBase>
	    <Title>Introduction</Title>
		<Topic>ESM</Topic>
		<SubTopic>Overview</SubTopic>
		<HumanKeywords>ESM Machine-Learning Genetic-Programming Artificial-Intelligence</HumanKeywords>
	</KnowledgeBase>
	<Section>
	    <Heading>Linear Regression</Heading>
		<Description><![CDATA[
             <P>A classic statistical problem is to try to determine the relationship between two random variables X and Y. 
             For example, we might consider height and weight of a sample of adults. 
             Linear regression attempts to explain this relationship with a straight line fit to the data. 
             The linear regression model postulates that <B>Y = a + bX + e</B>.             
             Where the "error" <B>e</B> is a random variable with mean zero.  
             The coefficients <B>a</B> and <B>b</B> are determined by the condition that the sum of the square residuals is as small as possible.
             </P>

             <P>For instance if &nbsp; <B>X = #(1 2 3 4)</B> and &nbsp; <B>Y = #(4 5 6 7)</B>, then &nbsp; <B>a = 3.0</B>, <B>b = 1.0</B> and <B>e = 0.0</B>.
             <BR><BR> 
             If &nbsp; <B>X = #(1 2 3 4)</B> and &nbsp; <B>Y = #(2 4 6 8)</B>, then &nbsp; <B>a = 0.0</B>, <B>b = 2.0</B> and <B>e = 0.0</B>. 
             <BR><BR> 
             Also if &nbsp; <B>X = #(1 2 3 4)</B> and &nbsp; <B>Y = #(1.5 4 6.2 8)</B>, then &nbsp; <B>a = -0.5</B>, <B>b = 2.17</B> and <B>e = 0.123</B>. 
             </P>

             <P><B>Multiple Linear Regression</B>. The general purpose of multiple regression (the term was first used by Pearson, 1908) is to learn more about the relationship between several independent 
             or predictor variables and a dependent or criterion variable. 
             For example, a real estate Lambda might record for each listing the size of the house (in square feet), the number of bedrooms, 
             the average income in the respective neighborhood according to census data, and a subjective rating of appeal of the house. 
             Once this information has been compiled for various houses it would be interesting to see whether and how these measures relate to the price for which a house is sold. 
             For example, one might learn that the number of bedrooms is a better predictor of the price for which a house sells in a particular neighborhood than how "pretty" 
             the house is (subjective rating). One may also detect "outliers," that is, houses that should really sell for more, given their location and characteristics. 
             </P>

			 <P>Personnel professionals customarily use multiple regression procedures to determine equitable compensation.
             One can determine a number of factors or dimensions such as "amount of responsibility" (Resp) or "number of people to supervise" (No_Super) that one believes 
             to contribute to the value of a job. The personnel analyst then usually conducts a salary survey among comparable companies in the market, 
             recording the salaries and respective characteristics (i.e., values on dimensions) for different positions. 
             This information can be used in a multiple regression analysis to build a regression equation of the form: 
             <BR><BR>
             <B>Salary = .5*Resp + .8*No_Super</B> 
             <BR><BR>
             Once this so-called regression line has been determined, the analyst can now easily construct a graph of the expected (predicted) salaries 
             and the actual salaries of job incumbents in his or her company. Thus, the analyst is able to determine which position is underpaid (below the regression line) 
             or overpaid (above the regression line), or paid equitably. 
             </P>

             <P>In the social and natural sciences multiple regression procedures are very widely used in research. 
             In general, multiple regression allows the researcher to ask (and hopefully answer) the general question "what is the best predictor of ...". 
             For example, educational researchers might want to learn what are the best predictors of success in high-school. 
             Psychologists may want to determine which personality variable best predicts social adjustment. 
             Sociologists may want to find out which of the multiple social indicators best predict whether or not a new immigrant group will adapt and be absorbed into society. 
             </P>

             <P>The general computational problem that needs to be solved in multiple regression analysis is to fit a straight line to a number of points. 
             </P>

             <P><B>Least Squares</B>. In our problem, we have an independent or X variable, and a dependent or Y variable. 
             These variables may, for example, represent IQ (intelligence as measured by a test) and school achievement (grade point average; GPA), respectively. 
             Each point in the plot represents one student, that is, the respective student's IQ and GPA. 
             The goal of linear regression procedures is to fit a line through the points. 
             Specifically, the program will compute a line so that the squared deviations of the observed points from that line are minimized. 
             Thus, this general procedure is sometimes also referred to as least squares estimation.
             </P> 

             <P><B>The Regression Equation</B>. A line in a two dimensional or two-variable space is defined by the equation Y=a+b*X; 
             in full text: the Y variable can be expressed in terms of a constant (a) and a slope (b) times the X variable. 
             The constant is also referred to as the intercept, and the slope as the regression coefficient or B coefficient. 
             For example, GPA may best be predicted as 1+.02*IQ. 
             Thus, knowing that a student has an IQ of 130 would lead us to predict that her GPA would be 3.6 (since, 1+.02*130=3.6).
             In the multivariate case, when there is more than one independent variable, the regression line cannot be visualized in the two dimensional space, 
             but can be computed just as easily. 
             For example, if in addition to IQ we had additional predictors of achievement (e.g., Motivation, Self- discipline) we could construct 
             a linear equation containing all those variables. 
             In general then, multiple regression procedures will estimate a linear equation of the form: &nbsp; <B>Y = a + b1*X1 + b2*X2 + ... + bp*Xp</B> 
             </P>

             <P><B>Unique Prediction and Partial Correlation</B>. Note that in this equation, the regression coefficients (or B coefficients) represent the independent 
             contributions of each independent variable to the prediction of the dependent variable. 
             Another way to express this fact is to say that, for example, variable X1 is correlated with the Y variable, after controlling for all other independent variables. 
             This type of correlation is also referred to as a partial correlation (this term was first used by Yule, 1907). 
             Perhaps the following example will clarify this issue. One would probably find a significant negative correlation between hair length 
             and height in the population (i.e., short people have longer hair). 
             At first this may seem odd; however, if we were to add the variable Gender into the multiple regression equation, this correlation would probably disappear. 
             This is because women, on the average, have longer hair than men; they also are shorter on the average than men. 
             Thus, after we remove this gender difference by entering Gender into the equation, the relationship between hair length and height disappears 
             because hair length does not make any unique contribution to the prediction of height, above and beyond what it shares in the prediction with variable Gender. 
             Put another way, after controlling for the variable Gender, the partial correlation between hair length and height is zero. 
             </P>

             <P><B>Predicted and Residual Scores</B>. The regression line expresses the best prediction of the dependent variable (Y), given the independent variables (X). 
             However, nature is rarely (if ever) perfectly predictable, and usually there is substantial variation of the observed points around the fitted regression line 
             (as in the scatterplot shown earlier). 
             The deviation of a particular point from the regression line (its predicted value) is called the residual value. 
             </P>

             <P><B>Residual Variance and R-square</B>. The smaller the variability of the residual values around the regression line relative to the overall variability, 
             the better is our prediction. 
             For example, if there is no relationship between the X and Y variables, then the ratio of the residual variability of the Y variable to the original 
             variance is equal to 1.0. If X and Y are perfectly related then there is no residual variance and the ratio of variance would be 0.0. 
             In most cases, the ratio would fall somewhere between these extremes, that is, between 0.0 and 1.0. 1.0 minus this ratio is referred to as R-square 
             or the coefficient of determination. This value is immediately interpretable in the following manner. 
             If we have an R-square of 0.4 then we know that the variability of the Y values around the regression line is 1-0.4 times the original variance; 
             in other words we have explained 40% of the original variability, and are left with 60% residual variability. 
             Ideally, we would like to explain most if not all of the original variability. 
             The R-square value is an indicator of how well the model fits the data (e.g., an R-square close to 1.0 indicates that we have accounted 
             for almost all of the variability with the variables specified in the model). 
             </P>

             <P><B>Interpreting the Correlation Coefficient R</B>. Customarily, the degree to which two or more predictors (independent or X variables) 
             are related to the dependent (Y) variable is expressed in the correlation coefficient R, which is the square root of R-square. 
             In multiple regression, R can assume values between 0 and 1. 
             To interpret the direction of the relationship between variables, one looks at the signs (plus or minus) of the regression or B coefficients. 
             If a B coefficient is positive, then the relationship of this variable with the dependent variable is positive (e.g., the greater the IQ the 
             better the grade point average); if the B coefficient is negative then the relationship is negative (e.g., the lower the class size the better the average test scores). 
             Of course, if the B coefficient is equal to 0 then there is no relationship between the variables. 
             </P>
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>NonLinear Regression</Heading>
		<Description><![CDATA[
             <P>Nonlinear regression in statistics is the problem of fitting a model <B>y = f(x,P) + e</B> to multidimensional x, y data, 
             where f is a nonlinear function of x with parameters P, and where the "error" <B>e</B> is a random variable with mean zero.
             In general, there is no algebraic expression for the best-fitting parameters <B>P</B>, as there is in linear regression. 
             Usually numerical optimization algorithms are applied to determine the best-fitting parameters. 
             There may be many local maxima of the goodness of fit, again in contrast to linear regression, in which there is usually a unique global maximum of the goodness of fit. 
             To determine which maximum is to be located using numerical optimization, guess values of parameters are used.
             Some nonlinear regression problems can be linearized if the exact solution to the guess-regression equation can be found.
             <BR><BR>         
             For example:
             <BR><BR>         
             If we take a logarithm of <B>y = A*exp(B*x)</B> and cast it as a linear regression, it will look like <B>log(y) = log(A) + B*x</B>, 
             a usual linear regression problem of optimizing parameters log(A) and B, the exact solution of which is well known. 
             </P>

             <P>However, performing such a linearization may bias some data towards being more "relevant" than others, which may not be a desired effect.
             More complex problems, such as transcendental regression are optimized by more complex algorithms.
             Other nonlinear regressions may have several goodness of fit maxima, and will require the scientist to input guess values for the optimized parameters. 
             </P>

             <P>Nonlinear regression fits a mathematical model to your data, and therefore Nonlinear regression requires that you choose a model.  
             What is a model? 
             A mathematical model is a simple description of a physical, chemical or biological state or process. 
             Using a model can help you think about chemical and physiological processes or mechanisms, enabling you to design better experiments and make sense of the results.           
             Your model must be expressed as a mathematical function.
             You can express the model as a single algebraic equation.
             You can express the model as a set of differential equations or you can write an equation in a manner that lets you have different models for different portions of your data.
             </P>

		     <P>Choosing a model for NonLinear regression obviously requires some understanding of the problem data and some preference for one choice of model over another.
             </P>
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Neural Net Regression</Heading>
		<Description><![CDATA[
             <P>Neural Net regression is the problem of training a neural net model <B>y = Nf(x,S,Ch,Wh,Co,Wo) + e</B> 
             on multidimensional M-Vectors <B>x</B> in <B>X</B> and real numbers <B>y</B> in <B>Y</B>,
             such that the trained numeric coefficients, <B>Ch</B>, <B>Wh</B>, <B>Co</B> and <B>Wo</B>, optimize the least squares error component, <B>e</B>
             which is a random variable with mean zero. 
             Normally, a neural net is a complex learning machine receiving M dimensional inputs <B>x</B>, containing hidden layer coefficients <B>Ch</B> and <B>Wh</B>, 
             also containing output coefficients <B>Co</B> and <B>Wo</B>, producing hidden layer internal signals <B>S</B>, and producing one real number output signal <B>y</B>.
             </P>

             <P>A standard neural net, <B>Nf</B>, is defined by <i>M</i>, the number of input dimensions, and by <i>K</i>, the number of hidden layers.
             There are 1 thru M inputs, 0 thru K layers (with 1 thru M internal signals produced for each layer), and one real number output signal.
             The number of components inside the neural net learning machine is complex, and they are as follows.
             </P>

             <UL>
               <LI><B>x</B>: The input vector with 1 thru M dimensions.</LI>
               <LI><B>S</B>: The internal signals vector with 0 thru K rows (each containing a signal vector of 1 thru M dimensions).</LI>
               <LI><B>Ch</B>: The internal signals axis coefficient vector with 1 thru K rows (each containing an axis coefficient vector of 1 thru M dimensions).</LI>
               <LI><B>Wh</B>: The internal signals weight matrix of K by M elements (each containing a vector of 1 thru M dimensions).</LI>
               <LI><B>Co</B>: The output signal axis coefficient.</LI>
               <LI><B>Wo</B>: The output signal weight vector with 1 thru M dimensions.</LI>
               <LI><B>y</B>: The output signal (a real number).</LI>
             </UL>

             <P>A standard Neural Net operates by generating internal signals which propagate up through each layer until the final output signal is produced.
             The number and composition of these signals inside the neural net is complex, and they are as follows.
             </P>

             <UL>
               <LI><B>S[0]</B> = <B>x</B> (the input vector with 1 thru M dimensions is automatically assigned as the 0th layer signals)</LI>
               <LI><B>S[k][m]</B> = <B>tanh(sum(Ch[k,m] , dotProduct(S[k-1],Wh[k,m])))</B> (the mth internal signal for the kth layer is the dot product of its weight vector (Wh[k,m]) and all M signals from the previous layer)</LI>
               <LI><B>y</B> = <B>sum(Co , dotProduct(S[k],Wo))</B> (the output signal is the dot product of its weight vector (Wo) and all M signals from the final layer)</LI>
             </UL>

             <P>Normally each internal signal, S[k][m], is either a weighted function of the outputs from the layer below or an original input element x[m].
             All of the neural nets, which we will consider in this document, are fully connected neural nets, 
             receiving M input signals, with zero or more hidden layers, and have one real number output signal.
             </P>
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Symbolic Regression</Heading>
		<Description><![CDATA[
             <P>Symbolic regression is the induction of mathematical expressions from data. 
             This is called symbolic regression (first mentioned by Koza in 1992), to emphasize the fact that the object of search is a symbolic description of a model, 
             not just discovering a set of optimized coefficients in a prespecified model. 
             This is in sharp contrast with other methods of regression, including NonLinear regression, where a specific model is assumed and often only the complexity of this model can be varied.
             </P>

	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Combining All Of These Tools</Heading>
		<Description><![CDATA[	
             <p>The Evolutionary Sequencing Machine Lambda (ESM) is a learning machine which learns to
             select and score individuals from a universe of individuals over time. Over a series  
             of discrete time steps, a universe of individuals is collected for each timestep.
             The individuals are things such as Stocks, People, Cities, etc. The discrete time
             steps are weeks, days, seconds, years, microseconds, etc.</p>
         
             <p>Each individual followed by the system is given a unique identifier which remains
             unique across all time periods studied (no two individuals ever have the same identifier).
             Furthermore, each time period studied is given a unique ascending integer index (i.e. week 1,
             week 2, etc.). So, for a series of time periods, historical information about groups of
             individuals is collected for each time period. The historical information collected for each
             individual for each time period is stored in a Number Vector and includes: the time period index;
             the unique identifier of the individual; and other numeric information about the individual
             pertinent to the current investigation. Finally, each individual in each time period is given
             a numeric "score" which determines the value of the individual in that time period.</p>

             <p>During training, the ESM is given historical information for time periods 0 through T for
             all individuals. The ESM is also given the "score" values for each individual in each training
             time period from 0 through T. During training the ESM attempts to "learn" any patterns in 
             the available historical data. The machine (ESM) is free to discover static as well as time
             varying patterns.</p>

             <p>During forward prediction, the ESM is given new information for time period T+1 for
             all individuals. The ESM is <b>NOT</b> given the "score" values for each individual in the new
             time period T+1. During prediction the ESM attempts to use any patterns it has learned to 
             select and score the individuals, from the universe of individuals, seen in time period T+1. 
             Once the machine scores the individuals, in the new time period, the accuracy of the machine 
             is determined by: (a) the least squares error on the scored individuals in time period T+1; 
             and (b) by the "order preserving" property of the estimated scores in time period T+1 
             (the degree to which the estimated scores preserve the natural sort ordering of the actual scores).  
             Order preservation is a simple idea where if the estimated score for individual <b>x</b> is
             less than the estimated score for individual <b>y</b>, then the actual score for individual <b>x</b>
             should also be less than the actual actual score for individual <b>y</b> in time period T+1.
             Normally these two measures should be coincident -- especially if the least squares error is excellent. 
             However, in cases where there is insufficient information in the training data, they may not be coincident. 
             If a tradeoff is required, the Evolutionary Sequencing Machine prefers that at least natural ordering be preserved.</p>

             <p>A time series set of vectors, X, together with a set, Y, of scores for the vectors
             in X are used to train a learning machine. There is also a testing set, TX and TY, of 
             of vectors similar to those in X and Y but for the testing time period (a time period
             not present in X or Y). After training, the machine is presented with the testing set,
             TX and attempts to estimate TY. The learning machine returns a Vector EY, of estimates
             for TY. Each element in EY contains a numeric estimate for the corresponding value in TY. 
             The learning machine attempts to: (a) minimize the least squared error between EY and TY; 
             and to, as much as possible, have the natural ordering of EY be predictive of the natural
             ordering of TY.</p>

             <p>The <i>order preservation</i> mission of the Evolutionary Sequencing Machine is an important
             distinguishing feature between this learning machine and general regression learning machines.
             The ESM is trying to fit a function to the testing data, using least squares error; but, 
             the ESM is also trying to predict the natural order of the individuals in the testing data.</p>

             <p>In many instances the Evolutionary Sequencing Machine may not have an exact estimate
             for the scores of the individuals in the testing data. However, if the learning machine 
             is able to predict the natural ordering of individuals in the testingdata, then the machine
             has been partially successful even if its estimated scores are incorrect.</p>

             <p>Let X be a set of vectors such as the numeric vector x = #(num| xtime x1 x2 x3 ... xm),
             and let Y be a numeric vector of "score" values. Let both X and Y be of length N.</p>

             <p>Furthermore, let the first prefix element, xtime, of every vector, in X, contain a 
             non-negative integer value indicating some time span of integral length, for
             example, if the time span were weeks, a value of 1 would indicate week one, 
             and a value of 10 would indicate week ten, etc. (i.e. the vectors contained
             in X are time sequenced).</p>

	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Example</Heading>
		<Description><![CDATA[	
             <p>An example, but by no means the only example, of X and Y would be a set of vectors
             of stock market data taken from the Open High Low Close and Volume numbers for all
             NASDQ traded stocks over a 52 week period. In this example we have the following:</p>

		    <table border="3" cellpadding="2" width="100%" bgcolor="#99CCCC">
		      <tr align="top"><th>xtime</th> <td>The sequential index of the current week (from 1 through 52).</td><tr>
		      <tr align="top"><th>x1</th> <td>The unique integer identifier of the stock (stocks not traded would not appear).</td></tr>
		      <tr align="top"><th>x2</th> <td>The current week opening price.</td></tr>
		      <tr align="top"><th>x3</th> <td>The current week high price.</td></tr>
		      <tr align="top"><th>x4</th> <td>The current week low price.</td></tr>
		      <tr align="top"><th>x5</th> <td>The current week closing price.</td></tr>
		      <tr align="top"><th>x6</th> <td>The current week share volume traded.</td></tr>
		      <tr align="top"><th>Y</th> <td>The "score" vector of next week profits (next_week_closing_price - the_current_week_closing_price).</td></tr>
            </table>

            <p>Similar examples can be constructed for oil exploration data over time, for
            the height and weight of individuals over time, etc. However, continuing with our
            securities example, we train our machine on the market data for four stocks
            over a four week period as follows:</p>

            <p><b>Training Data</b></p>

		    <table border="3" cellpadding="2" width="100%" bgcolor="#99CCCC">
		      <tr align="top"><th><i>Time</i></th><th><i>Stock</i></th><th><i>Open</i></th><th><i>High</i></th><th><i>Low</i></th><th><i>Close</i></th><th><i>Vol</i></th><th><i>Score</i></th><tr>
		      <tr align="top"><th>x.xtime</th><th>x.x1</th><th>x.x2</th><th>x.x3</th><th>x.x4</th><th>x.x5</th><th>x.x6</th><th>y</th><tr>
		      <tr align="top"><th></th><th></th><th></th><th><i>(first week)</i></th><th></th><th></th><th></th><th>Note: (<i>next week's profit</i>)</th><tr>
		      <tr align="top"><td align="center">0</td><td align="center">1 <i>(Apple)</i></td><td align="center">$23.45</td><td align="center">$25.67</td><td align="center">$23.35</td><td align="center">$24.56</td><td align="center">19367</td><td align="center">3.4%</td><tr>
		      <tr align="top"><td align="center">0</td><td align="center">2 <i>(IBM)</i></td><td align="center">$143.45</td><td align="center">$145.27</td><td align="center">$143.15</td><td align="center">$144.96</td><td align="center">894676</td><td align="center">-1.2%</td><tr>
		      <tr align="top"><td align="center">0</td><td align="center">3 <i>(Xerox)</i></td><td align="center">$13.95</td><td align="center">$15.27</td><td align="center">$13.35</td><td align="center">$14.72</td><td align="center">56832</td><td align="center">4.8%</td><tr>
		      <tr align="top"><td align="center">0</td><td align="center">4 <i>(GM)</i></td><td align="center">$57.15</td><td align="center">$62.17</td><td align="center">$53.65</td><td align="center">$62.05</td><td align="center">3419647</td><td align="center">9.1%</td><tr>
		      <tr align="top"><th></th><th></th><th></th><th><i>(second week)</i></th><th></th><th></th><th></th><th>Note: (<i>next week's profit</i>)</th><tr>
		      <tr align="top"><td align="center">1</td><td align="center">1 <i>(Apple)</i></td><td align="center">$24.56</td><td align="center">$25.38</td><td align="center">$22.75</td><td align="center">$25.40</td><td align="center">12046</td><td align="center">1.2%</td><tr>
		      <tr align="top"><td align="center">1</td><td align="center">2 <i>(IBM)</i></td><td align="center">$144.96</td><td align="center">$144.96</td><td align="center">$143.15</td><td align="center">$143.23</td><td align="center">864023</td><td align="center">-3.2%</td><tr>
		      <tr align="top"><td align="center">1</td><td align="center">3 <i>(Xerox)</i></td><td align="center">$14.72</td><td align="center">$16.12</td><td align="center">$14.39</td><td align="center">$15.43</td><td align="center">59204</td><td align="center">3.4%</td><tr>
		      <tr align="top"><td align="center">1</td><td align="center">4 <i>(GM)</i></td><td align="center">$62.05</td><td align="center">$62.05</td><td align="center">$68.00</td><td align="center">$67.70</td><td align="center">3219382</td><td align="center">6.5%</td><tr>
		      <tr align="top"><th></th><th></th><th></th><th><i>(third week)</i></th><th></th><th></th><th></th><th>Note: (<i>next week's profit</i>)</th><tr>
		      <tr align="top"><td align="center">2</td><td align="center">1 <i>(Apple)</i></td><td align="center">$25.40</td><td align="center">$26.98</td><td align="center">$24.75</td><td align="center">$25.71</td><td align="center">22056</td><td align="center">0.8%</td><tr>
		      <tr align="top"><td align="center">2</td><td align="center">2 <i>(IBM)</i></td><td align="center">$143.23</td><td align="center">$143.23</td><td align="center">$136.75</td><td align="center">$138.64</td><td align="center">824093</td><td align="center">-4.3%</td><tr>
		      <tr align="top"><td align="center">2</td><td align="center">3 <i>(Xerox)</i></td><td align="center">$15.43</td><td align="center">$16.45</td><td align="center">$15.09</td><td align="center">$15.96</td><td align="center">61205</td><td align="center">-1.4%</td><tr>
		      <tr align="top"><td align="center">2</td><td align="center">4 <i>(GM)</i></td><td align="center">$67.70</td><td align="center">$75.35</td><td align="center">$66.39</td><td align="center">$72.10</td><td align="center">3619582</td><td align="center">7.8%</td><tr>
            </table>

            <p>We train the ESM on the training data shown above. After training, we
            show the machine the following testing data, TX, and ask it to return an estimate
            of the next week's profit, TY, for each of the four individuals. <u>We do not show the machine the scores, TY.</u></p>

            <p><b>Testing Data</b></p>

		    <table border="3" cellpadding="2" width="100%" bgcolor="#99CCCC">
		      <tr align="top"><th><i>Time</i></th><th><i>Stock</i></th><th><i>Open</i></th><th><i>High</i></th><th><i>Low</i></th><th><i>Close</i></th><th><i>Vol</i></th><th><i>Score</i></th><tr>
		      <tr align="top"><th>tx.xtime</th><th>tx.x1</th><th>tx.x2</th><th>tx.x3</th><th>tx.x4</th><th>tx.x5</th><th>tx.x6</th><th>ty</th><tr>
		      <tr align="top"><th></th><th></th><th></th><th><i>(fourth week)</i></th><th></th><th></th><th></th><th>Note: (<i>next week's profit</i>)</th><tr>
		      <tr align="top"><td align="center">3</td><td align="center">1 <i>(Apple)</i></td><td align="center">$25.71</td><td align="center">$26.18</td><td align="center">$25.55</td><td align="center">$25.92</td><td align="center">25046</td><td align="center">-1.2%</td><tr>
		      <tr align="top"><td align="center">3</td><td align="center">2 <i>(IBM)</i></td><td align="center">$138.64</td><td align="center">$139.23</td><td align="center">$131.15</td><td align="center">$132.67</td><td align="center">774593</td><td align="center">-6.1%</td><tr>
		      <tr align="top"><td align="center">3</td><td align="center">3 <i>(Xerox)</i></td><td align="center">$15.96</td><td align="center">$16.13</td><td align="center">$15.00</td><td align="center">$15.73</td><td align="center">59205</td><td align="center">2.4%</td><tr>
		      <tr align="top"><td align="center">3</td><td align="center">4 <i>(GM)</i></td><td align="center">$72.10</td><td align="center">$77.87</td><td align="center">$71.39</td><td align="center">$77.73</td><td align="center">3710582</td><td align="center">5.8%</td><tr>
            </table>

            <p><b>Resulting Estimates</b></p>

            <p>After testing, the learning machine returns the following estimated scores, EY.</u></p>

		    <table border="3" cellpadding="2" width="100%" bgcolor="#99CCCC">
		      <tr align="top"><th><i>Estimate</i></th><th><i>Score</i></th><tr>
		      <tr align="top"><th>ey</th><th>ty</th><tr>
		      <tr align="top"><td align="center">-2.3%</td><td align="center">-1.2%</td><tr>
		      <tr align="top"><td align="center">-7.6%</td><td align="center">-6.1%</td><tr>
		      <tr align="top"><td align="center">1.9%</td><td align="center">2.4%</td><tr>
		      <tr align="top"><td align="center">4.9%</td><td align="center">5.8%</td><tr>
            </table>

            <p>We calculate the least squares error on the four individuals as 1.13%, so we did not a mediocre job of minimizing least squares error; 
           however, we did a perfect job of preserving the sort order of the corresponding TY values.</p>

	    ]]></Description>
	</Section>
	<Section>
	    <Heading>FAQ</Heading>
		<Description><![CDATA[	
             <p><font color=blue><b>Question 1:</b></font> Why must we train the learning machine on collections of individuals in each time period? 
             Why not simply train the machine on each individual separately and perform a normal regression estimate for each individual?</p>

             <p><font color=blue><b>Answer:</b></font> Many <i>real world</i> estimates cannot be made unless one is aware of the competitive land scape.</p>
             <p>For instance, suppose one is estimating the <i>social popularity</i> of students in the senior high school class. We can perform any
             number of individual regressions correlating high scores for <i>intelligence</i>, <i>appearance</i>, and <i>social skills</i>
             with <i>social popularity</i>. However, all of these individual regression models are greatly skewed in the case where all
             students in the senior high school class are male except one student who is female.</p>

             <p>Also, suppose one is estimating the <i>financial popularity</i> of our Bank's Certificates of Deposit. We perform any
             number of individual regressions correlating our Bank's previous Certificates of Deposit with their <i>financial popularity</i>.
             However, all of these individual regression models are greatly skewed in the case where one of our competitors is advertising
             an aggressive interest rate two percentage points higher than ours.</p>

             <p><font color=blue><b>Question 2:</b></font> Why must we ask the learning machine to preserve the natural order of the individuals in the testing time period? 
             Why not simply have the machine provide more accurate regression estimates for each individual in the testing time period?</p>

             <p><font color=blue><b>Answer:</b></font> Many <i>real world</i> estimates cannot be made for all individuals; but, only for a few individuals.</p>
             <p>For instance, suppose one is estimating currency <i>conversion rates</i>. Normally these rates have very small random daily changes.
             However, every so often, a central bank will pre-announce its intention to buy or sell its own currency. In those special cases the 
             learning machine will want to provide its normal estimates on most currencies; yet, make a special higher than normal estimate for the currency whose central bank has pre-announced.</p>

             <p>Many <i>real world</i> situations do not allow us to accurately estimate the score of the best individuals; but, only to guess which might be the better individuals than others.</p>

             <p>For instance, if ten monkeys and one five year old human child are taking a simple IQ test (this being all the information we have). 
             We cannot, with the meager information provided, accurately estimate the IQ scores of the contestants after they take the IQ test.
             However, we can reasonably make the guess that the human child will have the better IQ score (whatever that score may be).</p>

	    ]]></Description>
	</Section>
	<Section>
	    <Heading>References</Heading>
		<Description><![CDATA[	
             <ol>
             <li><b>Genetic Programming: On the Programming of Computers by Means of Natural Selection</b>; John R. Koza; The MIT Press, Cambridge Massachusetts; 1992.</li>
             <li><b>Genetic Programming II: Automatic Discovery of Reusable Programs</b>; John R Koza; The MIT Press, Cambridge Massachusetts; 1994.</li>
             <li><b>Genetic Programming III: Darwinian Invention and Problem Solving</b>; John R Koza, Forrest H Bennett III, David Andre, Martin A Keane; Morgan Kaufmann Publishers, San Francisco, California; 1999.</li>
             <li><b>Genetic Programming IV: Routine Human-Competitive Machine Intelligence</b>; John R Koza, Martin A Keane, Mathew J Streeter, William Mydlowec, Jessen Yu, Guido Lanza; Kluwer Academic Publishers, Dordrecht Netherlands; 2003.</li>
             <li><b>Grammatical Evolution</b>; Michael O'Neill, Conor Ryan; Kluwer Academic Publishers, Dordrecht Netherlands; 2003.</li>
             <li><b>An Introduction to Genetic Algorithms</b>; Melanie Mitchell; The MIT Press, Cambridge Massachusetts; 1996.</li>
             <li><b>Datamining using Grammar based Genetic Programming and Applications</b>; Man Leung Wong, Kwong Sak Leung; Kluwer Academic Publishers, Dordrecht Netherlands; 2000.</li>
             <li><b>Genetic Algorithms and Genetic Programming in Computational Finance</b>; edited by Shu-Heng Chen; Kluwer Academic Publishers, Dordrecht Netherlands; 2002.</li>
             </ol>

	    ]]></Description>
	</Section>
</Essay>




































;;**EXPORTKEY**:esm:%%Essay_ESM_Performance_Tests
;#text#
<?xml version="1.0" encoding="UTF-8"?>
<Essay>
	<KnowledgeBase>
	    <Title>Performance Tests</Title>
		<Topic>ESM</Topic>
		<SubTopic>Testing</SubTopic>
		<HumanKeywords>Selector Programming Artificial-Intelligence</HumanKeywords>
	</KnowledgeBase>
	<Section>
	    <Heading>Overview</Heading>
		<Description><![CDATA[
			<p>The Evolutionary Sequencing Machine contains a self testing child Lambda.
            The selfTest child Lambda allows the ESM to be tested
            on a variety of industrial scale tests.
            This chapter contains a history of selected ESM selfTest runs over a number
            of illustrative problems. 
            In these test problems we vary the number of independent variables, add random noise
            and record the effects on training time, regression, and sequencing accuracy. 
		    These testing logs contain data which may aid the user in understanding the response range
		    of this learning machine on a variety of candidate problems.</p>

            <p>All testing has performed on a DELL XPS laptop, containing a Pentium M running at 3.4ghz,
            2 gig of 533mhz RAM over an 800mhz front size bus.
            In this testing environment, the ESM performs very high quality predictive order preservation 
            and ball-park level regression on even the most complex text cases.
            </p>

            <p>A collection of test problems are used to profile this learning machine.
            For each test problem, we vary the number of independent variables, add random noise
            and record the effects on training time, regression, and sequencing accuracy.
            All of the performance tests are run using the <B>esm.selfTest</B> method; 
            and, are therefore reproducable by any ESM user.</p>

            <P><B>Performance References:</B></P>
            <ol>
               <li>"Breeding Swarms: A GP/PSO Hybrid", Settles & Soule, University of Idaho,</li>
            </ol>
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Full Test Suite (defaultMVL) 5 Columns</Heading>
		<Description><![CDATA[
			<p>The full ESM test suite is executed, with one million rows and five columns per test.
            The individual tests and the formulas they use are shown below. No random noise was added.
            </p>
 		    <table border="3" cellpadding="2" width="100%" bgcolor="#99CCCC">
			<colgroup><col class="member"><col class="description"></colgroup>
		      <tr>
		        <th>Name</th>
		        <th>Formula</th>
		      </tr>
		      <tr>
		        <td>linearRegression</td>
		        <td>y = 1.576246453868 + (1.576246453868*x0) - (39.34690816454*x1) + (2.139554491495*x2) + (46.59470912803*x3) + (11.54421404243*x4);</td>
		      </tr>
		      <tr>
		        <td>crossCorrelation</td>
		        <td>y = -9.165146942478 - (9.165146942478*x0*x0*x0) - (19.5666514757*x0*x1*x1) + (21.87460482304*x0*x1*x2) - (17.48124453288*x1*x2*x3) + (38.81839452492*x2*x3*x4);</td>
		      </tr>
		      <tr>
		        <td>cubicRegression</td>
		        <td>y = 1.576246453868 + (1.576246453868*x0*x0*x0) - (39.34690816454*x1*x1*x1) + (2.139554491495*x2*x2*x2) + (46.59470912803*x3*x3*x3) + (11.54421404243*x4*x4*x4);</td>
		      </tr>
		      <tr>
		        <td>hyperTangent</td>
		        <td>y = 1.576246453868 + (1.576246453868*tanh(x0*x0*x0)) - (39.34690816454*tanh(x1*x1*x1)) + (2.139554491495*tanh(x2*x2*x2)) + (46.59470912803*tanh(x3*x3*x3)) + (11.54421404243*tanh(x4*x4*x4));</td>
		      </tr>
		      <tr>
		        <td>elipsoid</td>
		        <td>y = 0.0 + (1.0*x0*x0) + (2.0*x1*x1) + (3.0*x2*x2) + (4.0*x3*x3) + (5.0*x4*x4);</td>
		      </tr>
		      <tr>
		        <td>hiddenModel</td>
		        <td>y = 1.576246453868 + (2.139554491495*sin(x2));</td>
		      </tr>
		      <tr>
		        <td>cyclicSeries</td>
		        <td>y = 14.65350538719 + (14.65350538719*x0*sin(x0)) - (6.739011522396*x1*cos(x0)) - (18.35326211779*x2*tan(x0)) - (40.32047802994*x3*sin(x0)) - (4.433481378615*x4*cos(x0));</td>
		      </tr>
		      <tr>
		        <td>mixedModels</td>
		        <td>
                if ((x0 % 4) == 0) y = + (1.576246453868*log(.000001+abs(x0))) - (39.34690816454*log(.000001+abs(x1))) + (2.139554491495*log(.000001+abs(x2))) + (46.59470912803*log(.000001+abs(x3))) + (11.54421404243*log(.000001+abs(x4)));
                <br>
                if ((x0 % 4) == 1) y = + (1.576246453868*x0*x0) - (39.34690816454*x1*x1) + (2.139554491495*x2*x2) + (46.59470912803*x3*x3) + (11.54421404243*x4*x4);
                <br>
                if ((x0 % 4) == 2) y = + (1.576246453868*sin(x0)) - (39.34690816454*sin(x1)) + (2.139554491495*sin(x2)) + (46.59470912803*sin(x3)) + (11.54421404243*sin(x4));
                <br>
                if ((x0 % 4) == 3) y = + (1.576246453868*x0) - (39.34690816454*x1) + (2.139554491495*x2) + (46.59470912803*x3) + (11.54421404243*x4);
		      </tr>
		      <tr>
		        <td>ratioRegression</td>
		        <td>
                if ((x0 % 4) == 0) y = + ((1.576246453868*x0)/(39.34690816454*x1)) + ((39.34690816454*x1)/(2.139554491495*x2)) + ((2.139554491495*x2)/(46.59470912803*x3)) + ((46.59470912803*x3)/(11.54421404243*x4));
                <br>
                if ((x0 % 4) == 1) y = + ((1.576246453868*x0)%(39.34690816454*x1)) + ((39.34690816454*x1)%(2.139554491495*x2)) + ((2.139554491495*x2)%(46.59470912803*x3)) + ((46.59470912803*x3)%(11.54421404243*x4));
                <br>
                if ((x0 % 4) == 2) y = + ((1.576246453868*sin(x0))/(39.34690816454*tan(x1))) + ((39.34690816454*sin(x1))/(2.139554491495*tan(x2))) + ((2.139554491495*sin(x2))/(46.59470912803*tan(x3))) + ((46.59470912803*sin(x3))/(11.54421404243*tan(x4)));
                <br>
                if ((x0 % 4) == 3) y = - (39.34690816454* log(.000001+abs(x1))) + (2.139554491495* log(.000001+abs(x2))) + (46.59470912803* log(.000001+abs(x3))) + (11.54421404243* log(.000001+abs(x4)));
                </td>
		      </tr>
            </table>

			<p>The following are the ESM performance results for these tests executed with one million rows and five columns per test.
            For each test, using the defaultMVL option setting, the ESM was trained for a maximum of 10 generations with a training cutoff at .01% error rate. 
            No random noise was added, so an exact solution is theoretical possible.
            </p>

            <P><B>Performance Results 10 Generations no random noise:</B></P>
		    <table border="3" cellpadding="2" width="100%" bgcolor="#99CCCC">
			<colgroup><col class="member"><col class="description"></colgroup>
		      <tr>
		        <th>Name</th>
		        <th>Rows</th>
		        <th>Cols</th>
		        <th>Option</th>
		        <th>Gens</th>
		        <th>Minutes</th>
		        <th>Train Err</th>
		        <th>Test Err</th>
		        <th>Bot 10%</th>
		        <th>Bot 50%</th>
		        <th>Avg Y</th>
		        <th>Top 50%</th>
		        <th>Top 10%</th>
		        <th>Error</th>
		        <th>Sort</th>
		      </tr>
		      <tr>
		        <td>crossCorrelation</td>
		        <td>10000000</td>
		        <td>5</td>
		        <td>defaultMVL</td>
		        <td>10</td>
		        <td>162</td>
		        <td>0.758</td>
		        <td>0.727</td>
		        <td>-1607386.96</td>
		        <td>-626113.96</td>
		        <td>63874.34</td>
		        <td>753862.65</td>
		        <td>1978531.40</td>
		        <th>Good</th>
		        <th>Good</th>
		      </tr>
		      <tr>
		        <td>cubicRegression</td>
		        <td>10000000</td>
		        <td>5</td>
		        <td>defaultMVL</td>
		        <td>10</td>
		        <td>148</td>
		        <td>0.424</td>
		        <td>0.436</td>
		        <td>-5216230.18</td>
		        <td>-2161000.67</td>
		        <td>-5927.33</td>
		        <td>2149145.99</td>
		        <td>4835835.87</td>
		        <th>Good</th>
		        <th>Good</th>
		      </tr>
		      <tr>
		        <td>hyperTangent</td>
		        <td>10000000</td>
		        <td>5</td>
		        <td>defaultMVL</td>
		        <td>10</td>
		        <td>155</td>
		        <td>0.786</td>
		        <td>0.789</td>
		        <td>-0.93</td>
		        <td>0.54</td>
		        <td>1.60</td>
		        <td>2.66</td>
		        <td>3.58</td>
		        <th>Good</th>
		        <th>Good</th>
		      </tr>
		      <tr>
		        <td>elipsoid</td>
		        <td>10000000</td>
		        <td>5</td>
		        <td>defaultMVL</td>
		        <td>10</td>
		        <td>214</td>
		        <td>0.068</td>
		        <td>0.068</td>
		        <td>3982.16</td>
		        <td>8285.16</td>
		        <td>12593.57</td>
		        <td>16901.98</td>
		        <td>22634.57</td>
		        <th>Good</th>
		        <th>Good</th>
		      </tr>
		      <tr>
		        <td>hiddenModel</td>
		        <td>10000000</td>
		        <td>5</td>
		        <td>defaultMVL</td>
		        <td>0</td>
		        <td>82</td>
		        <td>0.000</td>
		        <td>0.000</td>
		        <td>-0.53</td>
		        <td>0.18</td>
		        <td>1.55</td>
		        <td>2.93</td>
		        <td>3.67</td>
		        <th>Super</th>
		        <th>Super</th>
		      </tr>
		      <tr>
		        <td>linearRegression</td>
		        <td>10000000</td>
		        <td>5</td>
		        <td>defaultMVL</td>
		        <td>10</td>
		        <td>162</td>
		        <td>0.001</td>
		        <td>0.001</td>
		        <td>-3203.86</td>
		        <td>-1459.97</td>
		        <td>-0.12</td>
		        <td>1459.72</td>
		        <td>3080.99</td>
		        <th>Good</th>
		        <th>Good</th>
		      </tr>
		      <tr>
		        <td>mixedModels</td>
		        <td>10000000</td>
		        <td>5</td>
		        <td>defaultMVL</td>
		        <td>10</td>
		        <td>166</td>
		        <td>0.689</td>
		        <td>1.084</td>
		        <td>-22596.16</td>
		        <td>-8533.47</td>
		        <td>18924.26</td>
		        <td>46381.99</td>
		        <td>79859.95</td>
		        <th>Poor</th>
		        <th>Good</th>
		      </tr>
		      <tr>
		        <td>ratioRegression</td>
		        <td>10000000</td>
		        <td>5</td>
		        <td>defaultMVL</td>
		        <td>10</td>
		        <td>181</td>
		        <td>0.235</td>
		        <td>2.067</td>
		        <td>-6.05</td>
		        <td>0.73</td>
		        <td>1.91</td>
		        <td>3.09</td>
		        <td>3.93</td>
		        <th>Poor</th>
		        <th>Fair</th>
		      </tr>
		      <tr>
		        <td>cyclicSeries</td>
		        <td>10000000</td>
		        <td>5</td>
		        <td>defaultMVL</td>
		        <td>10</td>
		        <td>220</td>
		        <td>0.143</td>
		        <td>0.088</td>
		        <td>-14803.68</td>
		        <td>-3408.59</td>
		        <td>-696.95</td>
		        <td>2014.67</td>
		        <td>7670.57</td>
		        <th>Super</th>
		        <th>Super</th>
		      </tr>
            </table>


			<p>The following are the ESM performance results for these tests executed with one million rows and twenty columns per test.
            For each test, using the defaultMVL option setting, the ESM was trained for a maximum of 10 generations with a training cutoff at .01% error rate. 
            No random noise was added, so an exact solution is theoretical possible.
            </p>

            <P><B>Performance Results 10 Generations no random noise:</B></P>
		    <table border="3" cellpadding="2" width="100%" bgcolor="#99CCCC">
			<colgroup><col class="member"><col class="description"></colgroup>
		      <tr>
		        <th>Name</th>
		        <th>Rows</th>
		        <th>Cols</th>
		        <th>Option</th>
		        <th>Gens</th>
		        <th>Minutes</th>
		        <th>Train Err</th>
		        <th>Test Err</th>
		        <th>Bot 10%</th>
		        <th>Bot 50%</th>
		        <th>Avg Y</th>
		        <th>Top 50%</th>
		        <th>Top 10%</th>
		        <th>Error</th>
		        <th>Sort</th>
		      </tr>
		      <tr>
		        <td>crossCorrelation</td>
		        <td>10000000</td>
		        <td>20</td>
		        <td>defaultMVL</td>
		        <td>10</td>
		        <td>927</td>
		        <td>0.942</td>
		        <td>0.975</td>
		        <td>-875641.41</td>
		        <td>-508022.75</td>
		        <td>63700.81</td>
		        <td>635424.39</td>
		        <td>1303488.18</td>
		        <th>Fair</th>
		        <th>Good</th>
		      </tr>
		      <tr>
		        <td>cubicRegression</td>
		        <td>10000000</td>
		        <td>20</td>
		        <td>defaultMVL</td>
		        <td>10</td>
		        <td>724</td>
		        <td>0.402</td>
		        <td>0.404</td>
		        <td>-11283276.13</td>
		        <td>-5124302.15</td>
		        <td>-63005.25</td>
		        <td>4998291.64</td>
		        <td>11652267.63</td>
		        <th>Good</th>
		        <th>Super</th>
		      </tr>
		      <tr>
		        <td>hyperTangent</td>
		        <td>10000000</td>
		        <td>20</td>
		        <td>defaultMVL</td>
		        <td>10</td>
		        <td>659</td>
		        <td>0.707</td>
		        <td>0.709</td>
		        <td>-4.73</td>
		        <td>-1.04</td>
		        <td>2.14</td>
		        <td>5.34</td>
		        <td>9.01</td>
		        <th>Fair</th>
		        <th>Super</th>
		      </tr>
		      <tr>
		        <td>elipsoid</td>
		        <td>10000000</td>
		        <td>20</td>
		        <td>defaultMVL</td>
		        <td>10</td>
		        <td>785</td>
		        <td>0.918</td>
		        <td>0.918</td>
		        <td>149846.19</td>
		        <td>163828.01</td>
		        <td>176085.25</td>
		        <td>188342.48</td>
		        <td>202543.09</td>
		        <th>Fair</th>
		        <th>Super</th>
		      </tr>
		      <tr>
		        <td>hiddenModel</td>
		        <td>10000000</td>
		        <td>20</td>
		        <td>defaultMVL</td>
		        <td>0</td>
		        <td>240</td>
		        <td>0.000</td>
		        <td>0.000</td>
		        <td>-26.10</td>
		        <td>-16.28</td>
		        <td>2.12</td>
		        <td>20.53</td>
		        <td>30.53</td>
		        <th>Super</th>
		        <th>Super</th>
		      </tr>
		      <tr>
		        <td>linearRegression</td>
		        <td>10000000</td>
		        <td>20</td>
		        <td>defaultMVL</td>
		        <td>10</td>
		        <td>716</td>
		        <td>0.006</td>
		        <td>0.006</td>
		        <td>-7392.04</td>
		        <td>-3412.65</td>
		        <td>-33.12</td>
		        <td>3346.41</td>
		        <td>7600.23</td>
		        <th>Super</th>
		        <th>Super</th>
		      </tr>
		      <tr>
		        <td>mixedModels</td>
		        <td>10000000</td>
		        <td>20</td>
		        <td>defaultMVL</td>
		        <td>10</td>
		        <td>745</td>
		        <td>0.881</td>
		        <td>2.037</td>
		        <td>93775.03</td>
		        <td>133907.69</td>
		        <td>174716.55</td>
		        <td>215525.41</td>
		        <td>258681.72</td>
		        <th>Poor</th>
		        <th>Good</th>
		      </tr>
		      <tr>
		        <td>ratioRegression</td>
		        <td>10000000</td>
		        <td>20</td>
		        <td>defaultMVL</td>
		        <td>10</td>
		        <td>708</td>
		        <td>0.507</td>
		        <td>1.088</td>
		        <td>-641.48</td>
		        <td>-74.51</td>
		        <td>-66.94</td>
		        <td>-59.36</td>
		        <td>59.00</td>
		        <th>Poor</th>
		        <th>Fair</th>
		      </tr>
		      <tr>
		        <td>cyclicSeries</td>
		        <td>10000000</td>
		        <td>20</td>
		        <td>defaultMVL</td>
		        <td>10</td>
		        <td>730</td>
		        <td>0.957</td>
		        <td>0.970</td>
		        <td>-1209.56</td>
		        <td>139.21</td>
		        <td>216.06</td>
		        <td>292.91</td>
		        <td>1203.97</td>
		        <th>Fair</th>
		        <th>Fair</th>
		      </tr>
            </table>


			<p>The following are the ESM performance results for these tests executed with one million rows and five columns per test.
            For each test, using the defaultMVL option setting, the ESM was trained for a maximum of 10 generations with a training cutoff at .99% error rate. 
            Additionally, we add 40% random noise as follows: y = (y * 0.8) + (y * (random 0.4)), so an exact solution is NOT theoretical possible.
            </p>

            <P><B>Performance Results 10 Generations with 40% random noise:</B></P>
		    <table border="3" cellpadding="2" width="100%" bgcolor="#99CCCC">
			<colgroup><col class="member"><col class="description"></colgroup>
		      <tr>
		        <th>Name</th>
		        <th>Rows</th>
		        <th>Cols</th>
		        <th>Option</th>
		        <th>Gens</th>
		        <th>Minutes</th>
		        <th>Error</th>
		        <th>Bot 10%</th>
		        <th>Bot 50%</th>
		        <th>Avg Y</th>
		        <th>Top 50%</th>
		        <th>Top 10%</th>
		        <th>Error</th>
		        <th>Sort</th>
		      </tr>
		      <tr>
		        <td>linearRegression</td>
		        <td>10000000</td>
		        <td>5</td>
		        <td>defaultMVL</td>
		        <td>10</td>
		        <td>290</td>
		        <td>0.008</td>
		        <td>-3220.49</td>
		        <td>-1466.08</td>
		        <td>-8.06</td>
		        <td>1449.94</td>
		        <td>3031.00</td>
		        <th>Good</th>
		        <th>Good</th>
		      </tr>
		      <tr>
		        <td>crossCorrelation</td>
		        <td>10000000</td>
		        <td>5</td>
		        <td>defaultMVL</td>
		        <td>10</td>
		        <td>379</td>
		        <td>0.698</td>
		        <td>-1569811.77</td>
		        <td>-613692.21</td>
                <td>62890.12</td>
		        <td>739472.46</td>
		        <td>2027089.34</td>
		        <th>Good</th>
		        <th>Good</th>
		      </tr>
		      <tr>
		        <td>cubicRegression</td>
		        <td>10000000</td>
		        <td>5</td>
		        <td>defaultMVL</td>
		        <td>10</td>
		        <td>469</td>
		        <td>0.038</td>
		        <td>-5602513.27</td>
		        <td>-2229349.39</td>
                <td>-12071.50</td>
		        <td>2205206.38</td>
		        <td>5200360.75</td>
		        <th>Good</th>
		        <th>Good</th>
		      </tr>
		      <tr>
		        <td>hyperTangent</td>
		        <td>10000000</td>
		        <td>5</td>
		        <td>defaultMVL</td>
		        <td>10</td>
		        <td>373</td>
		        <td>0.449</td>
		        <td>-1.97</td>
		        <td>-0.06</td>
		        <td>1.58</td>
		        <td>3.23</td>
		        <td>5.00</td>
		        <th>Good</th>
		        <th>Good</th>
		      </tr>
		      <tr>
		        <td>elipsoid</td>
		        <td>10000000</td>
		        <td>5</td>
		        <td>defaultMVL</td>
		        <td>10</td>
		        <td>409</td>
		        <td>0.163</td>
		        <td>4027.53</td>
		        <td>8348.29</td>
		        <td>12590.29</td>
		        <td>16832.29</td>
		        <td>22468.17</td>
		        <th>Good</th>
		        <th>Good</th>
		      </tr>
		      <tr>
		        <td>hiddenModel</td>
		        <td>10000000</td>
		        <td>5</td>
		        <td>defaultMVL</td>
		        <td>10</td>
		        <td>395</td>
		        <td>0.008</td>
		        <td>-0.54</td>
		        <td>0.18</td>
		        <td>1.55</td>
		        <td>2.92</td>
		        <td>3.65</td>
		        <th>Good</th>
		        <th>Good</th>
		      </tr>
		      <tr>
		        <td>cyclicSeries</td>
		        <td>10000000</td>
		        <td>5</td>
		        <td>defaultMVL</td>
		        <td>10</td>
		        <td>315</td>
		        <td>0.202</td>
		        <td>-14521.80</td>
		        <td>-3333.54</td>
		        <td>-668.93</td>
		        <td>1995.67</td>
		        <td>2471.85</td>
		        <th>Good</th>
		        <th>Good</th>
		      </tr>
		      <tr>
		        <td>mixedModels</td>
		        <td>10000000</td>
		        <td>5</td>
		        <td>defaultMVL</td>
		        <td>10</td>
		        <td>395</td>
		        <td>1.083</td>
		        <td>-37375.94</td>
		        <td>-15139.40</td>
		        <td>18794.82</td>
		        <td>52729.05</td>
		        <td>70491.15</td>
		        <th>Poor</th>
		        <th>Good</th>
		      </tr>
		      <tr>
		        <td>ratioRegression</td>
		        <td>10000000</td>
		        <td>5</td>
		        <td>defaultMVL</td>
		        <td>10</td>
		        <td>483</td>
		        <td>2.294</td>
		        <td>-1.86</td>
		        <td>12.05</td>
		        <td>1.91</td>
		        <td>-8.21</td>
		        <td>-12.39</td>
		        <th>Poor</th>
		        <th>Poor</th>
		      </tr>
            </table>

			<p>The following are the ESM performance results for these tests executed with one million rows and five columns per test.
            For each test, using the defaultMVL option setting, the ESM was trained for a maximum of 60 generations with a training cutoff at 00% error rate. 
            We added no random noise, so an exact solution is theoretical possible.
            </p>

            <P><B>Performance Results 60 Generations with no random noise:</B></P>
		    <table border="3" cellpadding="2" width="100%" bgcolor="#99CCCC">
			<colgroup><col class="member"><col class="description"></colgroup>
		      <tr>
		        <th>Name</th>
		        <th>Rows</th>
		        <th>Cols</th>
		        <th>Option</th>
		        <th>Gens</th>
		        <th>Minutes</th>
		        <th>Error</th>
		        <th>Bot 10%</th>
		        <th>Bot 50%</th>
		        <th>Avg Y</th>
		        <th>Top 50%</th>
		        <th>Top 10%</th>
		        <th>Error</th>
		        <th>Sort</th>
		      </tr>
		      <tr>
		        <td>ratioRegression</td>
		        <td>10000000</td>
		        <td>5</td>
		        <td>defaultMVL</td>
		        <td>60</td>
		        <td>1440</td>
		        <td>1.037</td>
		        <td>-46.42</td>
		        <td>23.07</td>
		        <td>-7.94</td>
		        <td>7.56</td>
		        <td>5.52</td>
		        <th>Poor</th>
		        <th>Poor</th>
		      </tr>
            </table>

	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Cross Correlation Function</Heading>
		<Description><![CDATA[
			<p>The Cross Correlation function, given by the following equation,
            <BR> 
            <BR> 
            &nbsp;&nbsp;&nbsp;&nbsp;<B><i>y = -9.165146942478 - (9.165146942478*x0*x0*x0) - (19.5666514757*x0*x1*x1) + (21.87460482304*x0*x1*x2);</i></B>
            <BR>
            <BR>
            is a standard measure of performance in the ESM. 
            We compared several of the default option settings to perform a
            series of tests on one thousand rows with with three columns. 
            The tests are all exact with no random noise added.
            The following are the ESM methods calls as issued for these tests.</P>

            <P><B>Actual ESM Methods Calls:</B></P>
            <ul>
               <li>(esm.setOptions regressGPSR: 00% false)(esm.selfTest crossCorrelation: 1 3 1000 500 00% checkpoint:)</li>
               <li>(esm.setOptions regressGPSR: 00% false)(esm.selfTest crossCorrelation: 1 3 4000 500 00% checkpoint:)</li>
               <li>(esm.setOptions regress2GPSR: 00% false)(esm.selfTest crossCorrelation: 1 3 1000 500 00% checkpoint:)</li>
               <li>(esm.setOptions defaultMVL: 00% false)(esm.selfTest crossCorrelation: 1 3 1000 500 00% checkpoint:)</li>
               <li>(esm.setOptions regressMVL: 00% false)(esm.selfTest crossCorrelation: 1 3 1000 500 00% checkpoint:)</li>
               <li>(esm.setOptions regress2MVL: 00% false)(esm.selfTest crossCorrelation: 1 3 1000 500 00% checkpoint:)</li>
               <li>(esm.setOptions regress3MVL: 00% false)(esm.selfTest crossCorrelation: 1 3 1000 500 00% checkpoint:)</li>
               <li>(esm.setOptions regress4MVL: 00% false)(esm.selfTest crossCorrelation: 1 3 1000 500 00% checkpoint:)</li>
            </ul>

            <P>The following are the ESM performance results for these tests.</P>

            <P><B>Performance Results:</B></P>
		    <table border="3" cellpadding="2" width="100%" bgcolor="#99CCCC">
			<colgroup><col class="member"><col class="description"></colgroup>
		      <tr>
		        <th>Rows</th>
		        <th>Columns</th>
		        <th>Option</th>
		        <th>Generations</th>
		        <th>Elapsed Minutes</th>
		        <th>Train ErrorPct</th>
		        <th>Test ErrorPct</th>
		        <th>WFF Length</th>
		        <th>WFF Source</th>
		      </tr>
		      <tr>
		        <td>1000</td>
		        <td>3</td>
		        <td>regressGPSR</td>
		        <td>500</td>
		        <td>33</td>
		        <td>.805</td>
		        <td>.817</td>
		        <td>445</td>
		        <td>regress(((sqrt(abs(exp(x0))) - (x2 / x1)) * (sqrt(abs(sqrt(abs(exp(sin(x1)))))) * (x1 + sqrt(abs(exp(x0))))))+(((2.155598583178 * (2.155598583178 * x0))*(2.155598583178 * (2.155598583178 * x0))*(2.155598583178 * (2.155598583178 * x0)))*((2.155598583178 * (2.155598583178 * x0))*(2.155598583178 * (2.155598583178 * x0))*(2.155598583178 * (2.155598583178 * x0)))*((2.155598583178 * (2.155598583178 * x0))*(2.155598583178 * (2.155598583178 * x0))*(2.155598583178 * (2.155598583178 * x0))))+((if ((sqrt(abs(exp(x0))) / (x0 * (sqrt(abs(sqrt(abs(exp(x0))))) * (x1 + sqrt(abs(exp(x0))))))) &lt;= exp(x2)) {sqrt(abs(x1))} else {x2}) + sqrt(abs(exp(x0)))));</td>
		      </tr>
		      <tr>
		        <td>1000</td>
		        <td>3</td>
		        <td>regressGPSR</td>
		        <td>4000</td>
		        <td>351</td>
		        <td>0.781830076169</td>
		        <td>0.8121874611082</td>
		        <td>814</td>
		        <td>regress(((sqrt(abs(exp(x0))) - (x0 / x1)) * (sqrt(abs(sqrt(abs(exp(sin((if (x2 &lt; x1) {(x1 - -4.319379008717)} else {avg(x1 , 1.410307832834)}))))))) * (x1 + sqrt(abs(exp(x0))))))+(((2.155598583178 * (2.155598583178 * x0))*(2.155598583178 * (2.155598583178 * x0))*(2.155598583178 * (2.155598583178 * x0)))*((2.155598583178 * (2.155598583178 * x0))*(2.155598583178 * (2.155598583178 * x0))*(2.155598583178 * (2.155598583178 * x0)))*((2.155598583178 * (2.155598583178 * x0))*(2.155598583178 * (2.155598583178 * x0))*(2.155598583178 * (2.155598583178 * x0))))+(min((if ((sqrt(abs(exp(x0))) / (x0 * (sqrt(abs(sqrt(abs(exp(x0))))) * (x1 + sqrt(abs(exp(x0))))))) &lt;= exp(x2)) {sqrt(abs(x1))} else {x2}) , (tan(((if ((sqrt(abs(exp(x0))) / (x0 * (sqrt(abs(sqrt(abs(exp(x0))))) * (x1 + sqrt(abs(exp(x0))))))) &lt;= exp(x2)) {sqrt(abs(x1))} else {x2}) + sqrt(abs(exp(x1)))))*tan(((if ((sqrt(abs(exp(x0))) / (x0 * (sqrt(abs(sqrt(abs(exp(x0))))) * (x1 + sqrt(abs(exp(x0))))))) &lt;= exp(x2)) {sqrt(abs(x1))} else {x2}) + sqrt(abs(exp(x1)))))*tan(((if ((sqrt(abs(exp(x0))) / (x0 * (sqrt(abs(sqrt(abs(exp(x0))))) * (x1 + sqrt(abs(exp(x0))))))) &lt;= exp(x2)) {sqrt(abs(x1))} else {x2}) + sqrt(abs(exp(x1))))))) + (avg(x1 , -1.24991753992) % (if (x2 &lt; x0) {-2.348585731656} else {4.934507338818}))));</td>
		      </tr>
		      <tr>
		        <td>1000</td>
		        <td>3</td>
		        <td>regress2GPSR</td>
		        <td>500</td>
		        <td>214</td>
		        <td>1.095703457052</td>
		        <td>1.105457070352</td>
		        <td>817</td>
		        <td>regress((x2 - -4.858537919425)+expt(abs(((x0*x0*x0)*(x0*x0*x0)*(x0*x0*x0))) ,abs(min(abs(sign(log(abs((tan((if (x2 > x0) {x1} else {x2})) - 2.273652137605))))) , tanh((min((if (x0 >= abs(x2)) {(sin(x0) / (if (x1 &lt; x2) {x0} else {x0}))} else {(-4.692444869851*-4.692444869851)}) , ((if (x2 > x1) {x1} else {x1}) + (x0 * -2.452897778057))) / tan((x2 + 6.874027424659)))))))+avg((if (cos((if (x2 >= -0.05725764695619) {2.73846549776} else {x2})) &lt;= (if (x1 >= x2) {2.548358603843} else {0.1167608312345})) {(((((x0*x0*x0)*(x0*x0*x0)*(x0*x0*x0)) + ((((x0*x0*x0)*(x0*x0*x0)*(x0*x0*x0)) + sqrt(abs(3.205745271566))) / sqrt(abs((if (x0 &lt;= -0.004453697825539) {2.613227834073} else {3.05898924687}))))) / tanh(tanh(exp(-2.195361979221)))) - x1)} else {(if (x1 &lt; -0.04882083603768) {x1} else {-1.101098884998})}) , tan((if (x1 &lt; -0.04882083603768) {x1} else {(sqrt(abs(exp(-1.359169090488))) - ((if (x0 >= abs(x2)) {((x2 * x1)*(x2 * x1)*(x2 * x1))} else {(-4.692444869851*-4.692444869851)}) - min((-4.692444869851*-4.692444869851) , ninteger(abs((x0 - x0))))))}))));</td>
		      </tr>
		      <tr>
		        <td>1000</td>
		        <td>3</td>
		        <td>defaultMVL</td>
		        <td>500</td>
		        <td>48</td>
		        <td>.658</td>
		        <td>.660</td>
		        <td>456</td>
		        <td>mvlregress((((x0*x0*x0) * sqrt(abs(abs(x2)))) - (((x0*x0*x0) * sqrt(abs(x1))) - abs(x2))),((((x0*x0*x0) * sqrt(abs(((-x1) + x2)))) - (ninteger(log(abs(x0)))*ninteger(log(abs(x0)))*ninteger(log(abs(x0))))) * log(abs(x2))),avg(avg(avg(abs(log(abs(abs(x1)))) , ((x0*x0*x0) * log(abs(abs((-x1)))))) , abs((-x0))) , abs(x1)));</td>
		      </tr>
		      <tr>
		        <td>1000</td>
		        <td>3</td>
		        <td>regressMVL</td>
		        <td>500</td>
		        <td>59</td>
		        <td>1.03</td>
		        <td>.963</td>
		        <td>258</td>
		        <td>mvlregress(((cos(x0) * (x0*x0)) * max(exp(x0) , (-x2))),avg(max(x1 , (x0*x0)) , (ninteger(x0) - exp(x0))),avg((sign(x2) + tan(x2)) , ((x0*x0*x0) / sign(x1))));</td>
		      </tr>
		      <tr>
		        <td>1000</td>
		        <td>3</td>
		        <td>regress2MVL</td>
		        <td>500</td>
		        <td>258</td>
		        <td>.218</td>
		        <td>.179</td>
		        <td>465</td>
		        <td>mvlregress(((x0*x0*x0) - (((ninteger((ninteger(x2) - x1)) - x1)*(ninteger((ninteger(x2) - x1)) - x1)) * (-x0))),(log(abs((avg(log(abs((x0*x0*x0))) , sign(x2))*avg(log(abs((x0*x0*x0))) , sign(x2))*avg(log(abs((x0*x0*x0))) , sign(x2))))) * (log(abs((1.0 / x2))) * avg(cos(x2) , (x0*x0)))),(ninteger(((x2*x2)*(x2*x2))) % (((x0*x0*x0)*(x0*x0*x0)*(x0*x0*x0)) + (ninteger(((x2*x2)*(x2*x2))) % (((x0*x0*x0)*(x0*x0*x0)*(x0*x0*x0)) + log(abs((x0*x0*x0))))))));</td>
		      </tr>
		      <tr>
		        <td>1000</td>
		        <td>3</td>
		        <td>regress3MVL</td>
		        <td>500</td>
		        <td>34</td>
		        <td>1.02</td>
		        <td>.892</td>
		        <td>497</td>
		        <td>mvlregress(min(min(min((sqrt(abs(x0))*sqrt(abs(x0))) , (abs(x2)*abs(x2))) , (((abs(x2)*abs(x2))*(abs(x2)*abs(x2)))*((abs(x2)*abs(x2))*(abs(x2)*abs(x2))))) , ((abs(((abs(x2)*abs(x2))*(abs(x2)*abs(x2))))*abs(((abs(x2)*abs(x2))*(abs(x2)*abs(x2)))))*(abs(((abs(x2)*abs(x2))*(abs(x2)*abs(x2))))*abs(((abs(x2)*abs(x2))*(abs(x2)*abs(x2))))))),max(min(expt(abs(abs(x0)) ,log(abs(x1))) , expt(abs(tan(x1)) ,x1)) , sqrt(abs((x0*x0*x0)))),avg(avg(((avg((x0*x0*x0) , sign(x2)) + ((1.0 / ninteger(abs(x2)))*(1.0 / ninteger(abs(x2)))*(1.0 / ninteger(abs(x2)))))*(avg((x0*x0*x0) , sign(x2)) + ((1.0 / ninteger(abs(x2)))*(1.0 / ninteger(abs(x2)))*(1.0 / ninteger(abs(x2)))))*(avg((x0*x0*x0) , sign(x2)) + ((1.0 / ninteger(abs(x2)))*(1.0 / ninteger(abs(x2)))*(1.0 / ninteger(abs(x2)))))) , abs((x0*x0))) , (x1*x1)));</td>
		      </tr>
		      <tr>
		        <td>1000</td>
		        <td>3</td>
		        <td>regress4MVL</td>
		        <td>500</td>
		        <td>44</td>
		        <td>.858</td>
		        <td>.848</td>
		        <td>437</td>
		        <td>mvlregress((((if (x1 >= x0) {-4.175317310707} else {x0})*(if (x1 >= x0) {-4.175317310707} else {x0})) - tanh(tanh((x1*x1)))),((((((ninteger(x1)*ninteger(x1)*ninteger(x1))*(ninteger(x1)*ninteger(x1)*ninteger(x1))*(ninteger(x1)*ninteger(x1)*ninteger(x1)))*((ninteger(x1)*ninteger(x1)*ninteger(x1))*(ninteger(x1)*ninteger(x1)*ninteger(x1))*(ninteger(x1)*ninteger(x1)*ninteger(x1)))*((ninteger(x1)*ninteger(x1)*ninteger(x1))*(ninteger(x1)*ninteger(x1)*ninteger(x1))*(ninteger(x1)*ninteger(x1)*ninteger(x1)))) / exp(x2)) % ((x0*x0*x0) - sin(x2))) - tanh(((x0*x0*x0)*(x0*x0*x0)))),(avg(log(abs(x1)) , sqrt(abs(sin(x2)))) + (((x0*x0*x0) + sign((exp((1.0 / tanh(2.137261851988))) + x2)))*((x0*x0*x0) + sign((exp((1.0 / tanh(2.137261851988))) + x2)))*((x0*x0*x0) + sign((exp((1.0 / tanh(2.137261851988))) + x2))))));</td>
		      </tr>
		      <tr>
		        <td>1000</td>
		        <td>5</td>
		        <td>regress2MVL</td>
		        <td>1000</td>
		        <td>405</td>
		        <td>0.9874421853553</td>
		        <td>2.280139329293</td>
		        <td>254</td>
		        <td>mvlregress(max(ninteger(x2) , abs(expt(abs(tanh(x4)) ,x0))),expt(abs(log(abs(x0))) ,sin(max(abs(x1) , x3))),expt(abs(abs(x3)) ,x0),(abs(ninteger(x0)) / (x1*x1*x1)),((1.0 / exp(x3)) + cos(x3)));</td>
		      </tr>
		      <tr>
		        <td>1000</td>
		        <td>5</td>
		        <td>regressCMVL</td>
		        <td>1000</td>
		        <td>34</td>
		        <td>0.9850127675769</td>
		        <td>1.018537978708E+041</td>
		        <td>853</td>
		        <td>mvlregress(((x4*x4*x4) - expt(abs(((sign(x3) / log(abs(x3))) - (exp(x0) / abs(x1)))) ,(sign(x0) * sign(sign(x0))))),((exp(x0) * ((1.0 / x3)*(1.0 / x3))) * ((((exp(x0) * (1.0 / x3)) * (1.0 / x3)) * (1.0 / x3))*(((exp(x0) * (1.0 / x3)) * (1.0 / x3)) * (1.0 / x3)))),((exp(x0) * ((1.0 / x3)*(1.0 / x3))) * exp(x0)),expt(abs(((sign(x3) / log(abs(x3))) * expt(abs(cos(((exp(x0) / abs(x1)) * sign(x0)))) ,ninteger(x1)))) ,expt(abs(((sign(x3) / log(abs(x3))) - (exp(log(abs(x3))) / abs(x1)))) ,(sign(x0) * sign(x0)))),(((if (x1 &lt;= x0) {x0} else {4.808727247637})*(if (x1 &lt;= x0) {x0} else {4.808727247637})*(if (x1 &lt;= x0) {x0} else {4.808727247637})) + ((exp(x0) * ((1.0 / x3)*(1.0 / x3))) * (ninteger((x4*x4*x4))*ninteger((x4*x4*x4))))));</td>
		      </tr>
		      <tr>
		        <td>1000</td>
		        <td>5</td>
		        <td>regress2CMVL</td>
		        <td>1000</td>
		        <td>35</td>
		        <td>0.978221611109</td>
		        <td>65269846.83037</td>
		        <td>1391</td>
		        <td>mvlregress(((avg(((exp(x0) / abs(x1)) - avg(sign(x2) , avg((1.0 / x1) , abs(x2)))) , (cos(x1) - sqrt(abs(x1)))) + (exp(x0) * (1.0 / x4))) - max((((log(abs(x0)) % abs(cos(x1))) / cos(cos(x1))) - ((-x4) - (exp(x0) / abs(x1)))) , log(abs(x2)))),(max((sign(x2) % tanh(x2)) , ((exp(x0) * (x4*x4)) * (exp(x2) * exp(x1)))) * sign(x2)),((((log(abs(x0)) % abs(cos(x1))) / cos(x1)) - (exp(x0) / abs(x1))) % exp(x0)),(expt(abs(((exp(x0) * log(abs(x2))) * (exp(x0) / abs(x1)))) ,((cos(x4) % tan(((1.0 / x3) - (1.0 / x2)))) * (sign(x3) / log(abs(log(abs(x3))))))) % max(exp(x4) , ((exp(x0) * (x4*x4)) * (exp(x2) * exp(x1))))),max(((abs(x1) * exp(x0)) * exp((max(log(abs(x3)) , max((sign(x3) / log(abs(log(abs(x3))))) , ((1.0 / x0) - (x1*x1)))) - expt(abs((-x1)) ,log(abs(x2)))))) , (((exp(x0) * (x4*x4)) * (x4*x4)) * (((1.0 / cos(x3)) - (1.0 / x0))*((1.0 / cos(x3)) - (1.0 / x0))))));</td>
		      </tr>
		      <tr>
		        <td>1000</td>
		        <td>5</td>
		        <td>regressSvmMVL</td>
		        <td>1</td>
		        <td>1.29</td>
		        <td>6.073144521891E-006</td>
		        <td>3.835072392581E-006</td>
		        <td>78</td>
		        <td>svmregress(x4,x3,x2,x1,x0);</td>
		      </tr>
		      <tr>
		        <td>1000</td>
		        <td>10 <i>Note: too many columns, too few rows</i></td>
		        <td>regressSvmMVL</td>
		        <td>1</td>
		        <td>1.14</td>
		        <td>1.037243384018</td>
		        <td>1.109568524034E+042</td>
		        <td>358</td>
		        <td>mvlregress((exp(x9) - tanh(x2)),(cos(x3) - (1.0 / x9)),(x7 % (-x9)),(exp(x5) % ninteger(x0)),min(exp(x1) , (-x5)),expt(abs((x7*x7*x7)) ,sqrt(abs(x2))),expt(abs(cos(x6)) ,x1),((-x8) / (1.0 / x3)),(exp(x1) / (-x5)),(abs(x4) + abs(x2)));</td>
		      </tr>
            </table>

	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Ellipsoidal Function</Heading>
		<Description><![CDATA[
			<p>The Ellipsoidal function, given by the following equation,
            <BR> 
            <BR> 
            &nbsp;&nbsp;&nbsp;&nbsp;<B><i>y = 0.0 + (1*x[0]*x[0]) + (2*x[1]*x[1]) + ... + (M*x[M-1]*x[M-1])</i></B>
            <BR>
            <BR>
            is a standard measure of performance in [1]. 
            We used the <i>defaultMVL</i> option settings to perform a
            series of tests on five hundred rows with from one to ten columns. 
            The tests are all exact with no random noise added.
            The following are the ESM methods calls as issued for these tests.</P>

            <P><B>Actual ESM Methods Calls:</B></P>
            <ul>
               <li>(esm.setOptions defaultMVL: 00% false)(esm.selfTest elipsoid: 1 01 1000 100 .00% checkpoint:)</li>
               <li>(esm.setOptions regressSMVL: 00% false)(esm.selfTest elipsoid: 1 01 1000 100 .00% checkpoint:)</li>
               <li>(esm.setOptions defaultMVL: 00% false)(esm.selfTest elipsoid: 1 03 1000 100 .00% checkpoint:)</li>
               <li>(esm.setOptions regressSMVL: 00% false)(esm.selfTest elipsoid: 1 03 1000 100 .00% checkpoint:)</li>
               <li>(esm.setOptions defaultMVL: 00% false)(esm.selfTest elipsoid: 1 05 1000 100 .00% checkpoint:)</li>
               <li>(esm.setOptions regressSMVL: 00% false)(esm.selfTest elipsoid: 1 05 1000 100 .00% checkpoint:)</li>
               <li>(esm.setOptions defaultMVL: 00% false)(esm.selfTest elipsoid: 1 10 1000 100 .00% checkpoint:)</li>
               <li>(esm.setOptions regressSMVL: 00% false)(esm.selfTest elipsoid: 1 10 1000 100 .00% checkpoint:)</li>
            </ul>

            <P>The following are the ESM performance results for these tests.</P>

            <P><B>Performance Results:</B></P>
		    <table border="3" cellpadding="2" width="100%" bgcolor="#99CCCC">
			<colgroup><col class="member"><col class="description"></colgroup>
		      <tr>
		        <th>Rows</th>
		        <th>Columns</th>
		        <th>Option</th>
		        <th>Generations</th>
		        <th>Elapsed Minutes</th>
		        <th>Train ErrorPct</th>
		        <th>Test ErrorPct</th>
		        <th>WFF Length</th>
		        <th>WFF Source</th>
		      </tr>
		      <tr>
		        <td>1000</td>
		        <td>1</td>
		        <td>defaultMVL</td>
		        <td>0</td>
		        <td>1.8</td>
		        <td>0.00</td>
		        <td>0.00</td>
		        <td>39</td>
		        <td>mvlregress(((-x0) * x0));</td>
		      </tr>
		      <tr>
		        <td>1000</td>
		        <td>1</td>
		        <td>regressSMVL</td>
		        <td>0</td>
		        <td>1.8</td>
		        <td>0.00</td>
		        <td>0.00</td>
		        <td>39</td>
		        <td>mvlregress(((-x0) * x0));</td>
		      </tr>
		      <tr>
		        <td>1000</td>
		        <td>3</td>
		        <td>defaultMVL</td>
		        <td>0</td>
		        <td>34</td>
		        <td>5.800862412751E-016</td>
		        <td>1.457022869628E-006</td>
		        <td>318</td>
		        <td>mvlregress((((x2*x2) + (x1*x1)) - ((x2*x2) + (x0*x0))),min(((x2*x2) + (x0*x0)) , ((max((x2*x2) , (x1*x1))*max((x2*x2) , (x1*x1))) + (x0*x0))),(max(ninteger(x2) , ((x0*x0)*(x0*x0))) / (x1*x1)));</td>
		      </tr>
		      <tr>
		        <td>1000</td>
		        <td>3</td>
		        <td>regressSMVL</td>
		        <td>0</td>
		        <td>36</td>
		        <td>4.240183209616E-019</td>
		        <td>6.055618236929E-019</td>
		        <td>235</td>
		        <td>mvlregress(max(exp(x2) , (if (x0 &lt; 0.9354712246726) {x0} else {x1})),((x2*x2) + ((x2*x2) + ((x2*x2) + (x0*x0)))),((x1*x1) + ninteger(cos(cos(cos(x0))))));</td>
		      </tr>
		      <tr>
		        <td>1000</td>
		        <td>5</td>
		        <td>defaultMVL</td>
		        <td>0</td>
		        <td>38</td>
		        <td>0.03731375508097</td>
		        <td>0.04863589414105</td>
		        <td>525</td>
		        <td>mvlregress((log(abs(min(3.271128758164 , -4.199347511765))) * (exp(min(3.271128758164 , -4.199347511765)) * abs((x1*x1*x1)))),(abs(abs(x3)) * abs(x3)),max(log(abs(x2)) , (log(abs(x2)) % log(abs(x2)))),avg(abs(abs(abs(x2))) , abs(avg(sqrt(abs(x3)) , abs(avg(abs(x0) , abs(x1)))))),((((1.0 / abs(x4)) - (abs(x4)*abs(x4))) - (abs(x3)*abs(x3))) + abs((1.0 / abs(x0)))));</td>
		      </tr>
		      <tr>
		        <td>1000</td>
		        <td>5</td>
		        <td>regressSMVL</td>
		        <td>0</td>
		        <td>43</td>
		        <td>2.471663352211E-018</td>
		        <td>2.789144747384E-018</td>
		        <td>486</td>
		        <td>mvlregress(avg(avg((x0*x0) , avg((x2*x2) , (x2*x2))) , (x2*x2)),(x0 + expt(abs((x3 * 2.697792111764)) ,sign(x0))),(((((abs(x4)*abs(x4)) + (abs(x1)*abs(x1))) + (abs(x1)*abs(x1)))*(((abs(x4)*abs(x4)) + (abs(x1)*abs(x1))) + (abs(x1)*abs(x1)))) + ((abs(x1)*abs(x1)) + (x3*x3))),((abs(x4)*abs(x4)) + (x3*x3)),(((abs(x4)*abs(x4)) + (abs(x1)*abs(x1))) + (x1*x1)));</td>
		      </tr>
		      <tr>
		        <td>1000</td>
		        <td>10</td>
		        <td>defaultMVL</td>
		        <td>0</td>
		        <td>63</td>
		        <td>0.002575105800314</td>
		        <td>0.002559622208024</td>
		        <td>1089</td>
		        <td>mvlregress((abs(cos(x7)) - (sign(min((abs(x2)*abs(x2)) , avg(x3 , (ninteger(x5)*ninteger(x5))))) + (x7*x7))),(abs(min(abs(x2) , abs(x2))) * abs(x2)),max(min(sqrt(abs(x2)) , avg(x9 , abs(x2))) , (x3*x3)),(if (x4 &lt; avg(cos(x7) , max(cos(cos(x7)) , avg((x1*x1) , x4)))) {(x8*x8)} else {((x8*x8) + x1)}),avg(avg(x3 , (x4*x4)) , (x4*x4)),avg(avg(x3 , (ninteger(x5)*ninteger(x5))) , avg((x4*x4) , abs((log(abs(x8)) - (abs(x9)*abs(x9)))))),avg(avg((cos(x7)*cos(x7)) , (ninteger(x5)*ninteger(x5))) , avg((x0*x0) , abs((log(abs(x8)) - (x7*x7))))),avg((((-4.784491584615 + x9) + (x6*x6)) + x0) , (abs(x9)*abs(x9))),(avg(max(cos(cos(x7)) , avg((x1*x1) , x4)) , (abs(x9)*abs(x9))) + (x6*x6)),((avg((x7*x7) , (ninteger(x5)*ninteger(x5))) + x3) + x3));</td>
		      </tr>
		      <tr>
		        <td>1000</td>
		        <td>10</td>
		        <td>regressSMVL</td>
		        <td>0</td>
		        <td>76</td>
		        <td>0.0003037471829059</td>
		        <td>0.0003600732894825</td>
		        <td>980</td>
		        <td>mvlregress((log(abs(((log(abs(x0)) - abs(x9))*(log(abs(x0)) - abs(x9))*(log(abs(x0)) - abs(x9))))) - (log(abs(((log(abs(x0)) - abs(x9))*(log(abs(x0)) - abs(x9))*(log(abs(x0)) - abs(x9))))) - (x9*x9))),max(sign(x8) , (x4*x4)),max(max(max(x8 , max(log(abs(x6)) , (x7*x7))) , x0) , x1),max(max(max((x3*x3) , x0) , x6) , x0),max(log(abs(((x5*x5*x5)*(x5*x5*x5)*(x5*x5*x5)))) , max((((x5*x5*x5)*(x5*x5*x5)*(x5*x5*x5))*((x5*x5*x5)*(x5*x5*x5)*(x5*x5*x5))) , max((((x5*x5*x5)*(x5*x5*x5)*(x5*x5*x5))*((x5*x5*x5)*(x5*x5*x5)*(x5*x5*x5))) , x4))),avg((avg(x1 , x1)*avg(x1 , x1)) , avg(avg((x5*x5) , (x6*x6)) , log(abs(x2)))),avg(avg((x5*x5) , (x6*x6)) , avg(avg((x5*x5) , (x6*x6)) , log(abs(x2)))),((1.0 / x4) + avg((x4*x4) , (x2*x2))),((((x8*x8) + avg((x4*x4) , (x2*x2))) + abs(x1)) + sin(x5)),abs((sign(sign(max(x3 , x1))) + avg((x4*x4) , (x0*x0)))));</td>
		      </tr>
            </table>

	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Cyclic Series Function</Heading>
		<Description><![CDATA[
			<p>The Cyclic Series function, given by the following equation,
            <BR> 
            <BR> 
            &nbsp;&nbsp;&nbsp;&nbsp;<B><i>y = -9.165146942478 - (9.165146942478*x0*sin(x0)) - (19.5666514757*x1*cos(x0)) + (21.87460482304*x2*tan(x0)) - (17.48124453288*x3*sin(x0)) + (38.81839452492*x4*cos(x0));</i></B>
            <BR>
            <BR>
            is a standard measure of performance in the ESM. 
            We compared several of the default option settings to perform a
            series of tests on one thousand rows with with three columns. 
            The tests are all exact with no random noise added.
            The following are the ESM methods calls as issued for these tests.</P>

            <P><B>Actual ESM Methods Calls:</B></P>
            <ul>
               <li>(esm.setOptions regressGaMVL: 00% true)(esm.selfTest cyclicSeries: 1 5 1000 200 01% checkpoint:)</li>
            </ul>

            <P>The following are the ESM performance results for these tests.</P>

            <P><B>Performance Results:</B></P>
		    <table border="3" cellpadding="2" width="100%" bgcolor="#99CCCC">
			<colgroup><col class="member"><col class="description"></colgroup>
		      <tr>
		        <th>Rows</th>
		        <th>Columns</th>
		        <th>Option</th>
		        <th>Generations</th>
		        <th>Elapsed Minutes</th>
		        <th>Train ErrorPct</th>
		        <th>Test ErrorPct</th>
		        <th>WFF Source</th>
		      </tr>
		      <tr>
		        <td>1000</td>
		        <td>5</td>
		        <td>regressGaMVL</td>
		        <td>200</td>
		        <td>17</td>
		        <td>1.013562168645</td>
		        <td>5.063307414908E+014</td>
		        <td>mvlregress(min(log(abs(x2)) , tan(x1)),expt(abs(exp(x2)) ,log(abs(x2))),expt(abs(cos(x0)) ,(-x2)),expt(abs(cos(x0)) ,ninteger(x1)));</td>
		      </tr>
		      <tr>
		        <td>1000</td>
		        <td>5</td>
		        <td>regressGPSR</td>
		        <td>200</td>
		        <td>14</td>
		        <td>3.012862873945</td>
		        <td>3.135662535118</td>
		        <td>regress(tanh(abs(x4))+((if (x3 &lt; x4) {2.455796326077} else {2.859343562168})*(if (x3 &lt; x4) {2.455796326077} else {2.859343562168}))+(max(-3.640206438313 , x3) % tan(-4.89198167607)));</td>
		      </tr>
		      <tr>
		        <td>1000</td>
		        <td>5</td>
		        <td>regressGaCMVL</td>
		        <td>200</td>
		        <td>27</td>
		        <td>0.9977733359539</td>
		        <td>5.063391381703E+014</td>
		        <td>mvlregress(min((1.0 / x0) , log(abs(x4))),expt(abs(x3) ,x2),expt(abs(cos(x0)) ,(-x2)),expt(abs(cos(x0)) ,ninteger(x1)));</td>
		      </tr>
		      <tr>
		        <td>1000</td>
		        <td>5</td>
		        <td>regress2GaCMVL</td>
		        <td>200</td>
		        <td>38</td>
		        <td>1.004924529883</td>
		        <td>5.063264120872E+014</td>
		        <td>mvlregress(min(log(abs(x2)) , tan(x1)),expt(abs(cos(x0)) ,(-x2)),expt(abs(cos(x0)) ,ninteger(x1)));</td>
		      </tr>
            </table>

	    ]]></Description>
	</Section>
	<Section>
	    <Heading>The regressGaCMVL Option Setting</Heading>
		<Description><![CDATA[
			<p>The <B>regressGaCMVL</B> option setting can be used against all of the ESM test suites to 
            provide a comparison study and a standard measure of performance of the ESM. 
            All of the tests are reported with the number of rows and columns. 
            The tests are all exact with no random noise added.
            The following are the ESM methods calls as issued for these tests.</P>

            <P><B>Actual ESM Methods Calls:</B></P>
            <ul>
               <li>(esm.setOptions regressSvmMVL: 00% false)(esm.selfTest all: 1 5 1000 200 .99% checkpoint:)</li>
               <li>(esm.setOptions regressSvmMVL: 00% false)(esm.selfTest all: 1 10 1000 200 .99% checkpoint:)</li>
               <li>(esm.setOptions regressSvmMVL: 00% false)(esm.selfTest all: 1 20 1000 200 .99% checkpoint:)</li>
            </ul>

            <P>The following are the ESM performance results for these tests.</P>

            <P><B>Performance Results:</B></P>
		    <table border="3" cellpadding="2" width="100%" bgcolor="#99CCCC">
			<colgroup><col class="member"><col class="description"></colgroup>
		      <tr><th width=200 >Test</th><td>linearRegression</td></tr>
		      <tr><th width=200 >Rows</th><td>1000</td></tr>
		      <tr><th width=200 >Columns</th><td>5</td></tr>
		      <tr><th width=200 >Training Gens</th><td>0</td></tr>
		      <tr><th width=200 >Training Time</th><td>.01 minutes</td></tr>
		      <tr><th width=200 >Training Fittness</th><td>Score=2.805047073921E-005 (ErrorPct=0.02301464075688)</td></tr> 
		      <tr><th width=200 >Testing Fittness</th><td>Score=4.33661031386E-005 (ErrorPct=0.02299119330935)</td></tr> 
		      <tr><th width=200 >Decile Act Y</th><td>-3366 -2107 -1375 -817 -265 215 756 1354 2127 3388 </td></tr> 
		      <tr><th width=200 >Decile Avg EY</th><td>-3366 -2107 -1375 -817 -265 215 756 1354 2127 3388 </td></tr> 
		      <tr><th width=200 >Decile Diff EY</th><td>3154  3823  4573  5495  6755 </td></tr> 
		      <tr><th width=200 >Target Source</th><td>y = 36.28472509997 + (36.28472509997*x0) + (30.58638330984*x1) + (13.02686266162*x2) - (29.03445701301*x3) + (35.00133469577*x4);</td></tr>
		      <tr><th width=200 >WFF Source</th><td>mvlregress(x4,x3,x2,x1,x0);</td></tr>
              <tr><th width=200 >Console Display</th>
                  <td>
                    <![CDATA[ <blockquote> <pre> 
                  
Starting test case: linearRegression
Building test data as: y = 36.28472509997 + (36.28472509997*x0) + (30.58638330984*x1) + (13.02686266162*x2) - (29.03445701301*x3) + (35.00133469577*x4);
...starting new evolutionary process.
...starting generation of constant selector WFF.
...starting regress selector for each column name.
...starting a best-of-breed MVL selector WFF.
...locating the best selector and the best regressor from this training run.

Final results of training
esm: N = [1000], M = [5], Generations = [0], WFFs = [8], Score=[2.805047073921E-005], ScoreHistory=[#(num| 2.805047073921E-005 )]
esm.myBest, Score=[2.805047073921E-005], ErrorPct=[0.02301464075688], lengthWFF=[78], WFF = mvlregress(x4,x3,x2,x1,x0);
esm.myBestRegressor, Score=[2.805047073921E-005], ErrorPct=[0.02301464075688], lengthWFF=[78], WFF = mvlregress(x4,x3,x2,x1,x0);
myBestSelectorChampions
myBestSelectorChampions[0],Score=[2.805047073921E-005,ErrorPct=[0.02301464075688],WFF= mvlregress(x4,x3,x2,x1,x0);
myBestRegressorChampions
myBestRegressorChampions[0],ErrorPct=[0.02301464075688,Score=[2.805047073921E-005],WFF= mvlregress(x4,x3,x2,x1,x0);


Show final results on training data, Score=[2.805047073921E-005], ErrorPct=[0.02301464075688]
X[0], ey=[-5843.163750627], y=[-5808.843994914], ErrorPct=[0.02178099153448]
X[50], ey=[-3060.383583548], y=[-3025.383739751], ErrorPct=[0.02221260861667]
X[100], ey=[-2544.331592509], y=[-2508.973852964], ErrorPct=[0.02243974672117]
X[150], ey=[-2122.265937244], y=[-2086.549905584], ErrorPct=[0.02266713637871]
X[200], ey=[-1739.390229865], y=[-1703.731995936], ErrorPct=[0.02263045511856]
X[250], ey=[-1418.373048652], y=[-1382.407482634], ErrorPct=[0.02282550305842]
X[300], ey=[-1090.853942556], y=[-1055.615701569], ErrorPct=[0.02236390710521]
X[350], ey=[-816.5446456864], y=[-780.8900495508], ErrorPct=[0.0226281463977]
X[400], ey=[-610.3393271481], y=[-573.9496629182], ErrorPct=[0.02309465647645]
X[450], ey=[-337.8171650406], y=[-302.0340395261], ErrorPct=[0.02270971741289]
X[500], ey=[-41.35736908715], y=[-5.178759057364], ErrorPct=[0.0229607111831]
X[550], ey=[183.4619330142], y=[220.2310103636], ErrorPct=[0.02333545055471]
X[600], ey=[410.0478792595], y=[446.020085315], ErrorPct=[0.02282971715001]
X[650], ey=[696.07861249], y=[733.0760047121], ErrorPct=[0.02348035031306]
X[700], ey=[939.2025895943], y=[976.6369766491], ErrorPct=[0.02375768855615]
X[750], ey=[1222.181265982], y=[1258.197601923], ErrorPct=[0.02285772412896]
X[800], ey=[1650.86192273], y=[1687.688121192], ErrorPct=[0.02337170239994]
X[850], ey=[2003.908581394], y=[2041.888413328], ErrorPct=[0.0241038544903]
X[900], ey=[2591.889695326], y=[2628.666017283], ErrorPct=[0.02334004833617]
X[950], ey=[3391.951723443], y=[3429.611172845], ErrorPct=[0.02390052410342]
Actual computed error on training data is ErrorPct=[0.02301464075688] versus reported ErrorPct=[0.02301464075688] while average Y is AvgY=[-0.5984121923563]
yHistory=[#(num| -3283.281808261 -2091.508320913 -1396.777278985 -805.6204340371 -304.1315504475 219.4866579832 716.1484604536 1277.013920423 2085.484803221 3577.20142864 )]
eHistory=[#(num| -3283.281808261 -2091.508320913 -1396.777278985 -805.6204340371 -304.1315504475 219.4904608229 716.1446576139 1277.013920423 2085.484803221 3577.20142864 )]
aHistory=[#(num| -3283.281808261 -2687.395064587 -2257.189136053 -1894.296960549 -1576.263878529 1575.067054144 1913.961202474 2313.233384095 2831.34311593 3577.20142864 )]
dHistory=[#(num| 3151.330932673 3808.258163023 4570.422520148 5518.738180518 6860.483236901 )]


Final testing on test data returns Score=[4.33661031386E-005], ErrorPct=[0.02299119330935]
X[0], ey=[-6518.618720943], y=[-6484.623704412], ErrorPct=[0.02155033987663]
X[50], ey=[-3271.303887069], y=[-3236.437925615], ErrorPct=[0.02210245489317]
X[100], ey=[-2628.175261211], y=[-2593.162567164], ErrorPct=[0.02219547256389]
X[150], ey=[-2123.983228316], y=[-2087.961124077], ErrorPct=[0.02283536437485]
X[200], ey=[-1711.654778528], y=[-1675.368766003], ErrorPct=[0.02300266281603]
X[250], ey=[-1446.293822763], y=[-1409.961854118], ErrorPct=[0.02303179561565]
X[300], ey=[-1118.701300059], y=[-1082.131532354], ErrorPct=[0.0231825427274]
X[350], ey=[-837.6775684122], y=[-801.0312808508], ErrorPct=[0.02323105068783]
X[400], ey=[-576.0428723192], y=[-540.2923618324], ErrorPct=[0.02266319391404]
X[450], ey=[-275.0037064372], y=[-237.6116087693], ErrorPct=[0.02370383943506]
X[500], ey=[-25.06080852636], y=[10.30082433361], ErrorPct=[0.0224166741037]
X[550], ey=[153.2989159602], y=[190.7620580547], ErrorPct=[0.02374887637566]
X[600], ey=[488.6687779334], y=[524.3325539087], ErrorPct=[0.02260821061378]
X[650], ey=[727.335100493], y=[764.1127556697], ErrorPct=[0.02331432809277]
X[700], ey=[971.7974974384], y=[1007.200600128], ErrorPct=[0.0224429629251]
X[750], ey=[1344.327010143], y=[1380.437501899], ErrorPct=[0.02289139555896]
X[800], ey=[1630.753673893], y=[1667.834312768], ErrorPct=[0.02350639747083]
X[850], ey=[2072.065773895], y=[2109.188899938], ErrorPct=[0.02353333121008]
X[900], ey=[2586.913588067], y=[2623.683696846], ErrorPct=[0.02330954423191]
X[950], ey=[3164.033562841], y=[3201.358129815], ErrorPct=[0.02366102994308]
Actual computed error on testing data is ErrorPct=[0.02299119330935], Avg Y=[-8.890305832169], AvgDev Y=[1577.470087507]
yHistory=[#(num| -3366.450229611 -2107.733923626 -1375.136018612 -817.2387459612 -265.1521579873 215.549764995 756.1077597689 1354.709844058 2127.666211191 3388.774437463 )]
eHistory=[#(num| -3366.450229611 -2107.733923626 -1375.136018612 -817.2387459612 -265.1521579873 215.549764995 756.1077597689 1354.709844058 2127.666211191 3388.774437463 )]
aHistory=[#(num| -3366.450229611 -2737.092076618 -2283.10672395 -1916.639729452 -1586.342215159 1568.561603495 1906.81456312 2290.383497571 2758.220324327 3388.774437463 )]
dHistory=[#(num| 3154.903818655 3823.454292573 4573.49022152 5495.312400945 6755.224667073 )]

esm.selfTest[linearRegression]: completed in [0.01303333333338] minutes.



                    </pre> </blockquote> ]]>
                  </td>
		      </tr>
            </table>


		    <table border="3" cellpadding="2" width="100%" bgcolor="#99CCCC">
			<colgroup><col class="member"><col class="description"></colgroup>
		      <tr>
		        <th>Rows</th>
		        <th>Cols</th>
		        <th>Test</th>
		        <th>Gens</th>
		        <th>Time</th>
		        <th>Score</th>
		        <th>Error</td>
		        <th>Problem Formula</th>
		        <th>Evolved Formula</th>
		      </tr>
		      <tr>
		        <td>1000</td>
		        <td>5</td>
		        <td>linearRegression</td>
		        <td>0</td>
		        <td>.01</td>
		        <td>.000043</td>
		        <td>0.02299</td>
		        <td>y = 36.28472509997 + (36.28472509997*x0) + (30.58638330984*x1) + (13.02686266162*x2) - (29.03445701301*x3) + (35.00133469577*x4);</td>
		        <td>mvlregress(x4,x3,x2,x1,x0);</td>
		      </tr>
		      <tr>
		        <td>1000</td>
		        <td>10</td>
		        <td>linearRegression</td>
		        <td>0</td>
		        <td>.03</td>
		        <td>.000010</td>
		        <td>0.00858</td>
		        <td>y = -13.99864194741 - (13.99864194741*x0) - (32.66789144459*x1) - (30.73171497235*x2) + (10.01135458484*x3) - (2.83159613203*x4) + (5.499486281783*x5) - (46.33521181329*x6) + (10.35455431571*x7) + (18.14155680594*x8) - (3.363675194861*x9);</td>
		        <td>mvlregress(x9,x8,x7,x6,x5,x4,x3,x2,x1,x0);</td>
		      </tr>
		      <tr>
		        <td>1000</td>
		        <td>5</td>
		        <td>hiddenModel</td>
		        <td>0</td>
		        <td>1.24</td>
		        <td>0.00000</td>
		        <td>0.00000</td>
		        <td>y = 36.28472509997 + (13.02686266162*sin(x2));</td>
		        <td>regress(sin(x2));</td>
		      </tr>
		      <tr>
		        <td>1000</td>
		        <td>10</td>
		        <td>hiddenModel</td>
		        <td>0</td>
		        <td>2.36</td>
		        <td>0.00000</td>
		        <td>0.00000</td>
		        <td>y = -13.99864194741 + (5.499486281783*sin(x5));</td>
		        <td>regress(sin(x5));</td>
		      </tr>
		      <tr>
		        <td>1000</td>
		        <td>5</td>
		        <td>elipsoid</td>
		        <td>0</td>
		        <td>2.04</td>
		        <td>0.009296</td>
		        <td>0.203166</td>
		        <td>y = 0.0 + (1.0*x0*x0) + (2.0*x1*x1) + (3.0*x2*x2) + (4.0*x3*x3) + (5.0*x4*x4);</td>
		        <td>mvlregress(sqrt(abs(x1)),max((x3*x3) , x4),max((x2*x2) , x2),avg(x0 , (x2*x2)),avg((x4*x4) , x3));</td>
		      </tr>
		      <tr>
		        <td>1000</td>
		        <td>10</td>
		        <td>elipsoid</td>
		        <td>10</td>
		        <td>7.31</td>
		        <td>0.008742</td>
		        <td>0.199586</td>
		        <td>y = 0.0 + (1.0*x0*x0) + (2.0*x1*x1) + (3.0*x2*x2) + (4.0*x3*x3) + (5.0*x4*x4) + (6.0*x5*x5) + (7.0*x6*x6) + (8.0*x7*x7) + (9.0*x8*x8) + (10.0*x9*x9);</td>
		        <td>mvlregress(max(sqrt(abs(x2)) , min(min(max(x7 , min(x6 , x7)) , (cos(x6)*cos(x6))) , x7)),max(max((x9*x9) , (ninteger(x3) / exp(x3))) , abs(x6)),max(ninteger(x0) , max(x0 , max(x1 , x7))),(max(x1 , x8) / (avg(x1 , abs(x8)) % avg((x7*x7) , x9))),avg((x8*x8) , abs(max(abs(avg((x8*x8) , avg((x8*x8) , x3))) , x1))),avg((x4*x4) , min(max(x0 , x8) , max(max(x1 , x9) , x8))),avg(avg((x7*x7) , avg((x5*x5) , x5)) , sqrt(abs(avg(abs(x6) , sqrt(abs(x4)))))),avg(avg(ninteger(x0) , ninteger(x0)) , x4),avg(avg(avg((max(x7 , avg((x5*x5) , x2))*max(x7 , avg((x5*x5) , x2))) , x6) , x4) , avg((x9*x9) , x4)),avg(abs(x6) , abs(avg(abs(x3) , abs(x4)))));</td>
		      </tr>
            </table>

	    ]]></Description>
	</Section>
	<Section>
	    <Heading>The regressSvmMVL Option Setting</Heading>
		<Description><![CDATA[
			<p>The <B>regressSvmMVL</B> option setting can be used against all of the ESM test suites to 
            provide a comparison study and a standard measure of performance of the ESM. 
            All of the tests are reported with the number of rows and columns. 
            The tests are all exact with no random noise added.
            The following are the ESM methods calls as issued for these tests.</P>

            <P><B>Actual ESM Methods Calls:</B></P>
            <ul>
               <li>(esm.setOptions regressSvmMVL: 00% false)(esm.selfTest linearRegression: 1 5 1000 500 01% checkpoint:)</li>
               <li>(esm.setOptions regressSvmMVL: 00% false)(esm.selfTest cubicRegression: 1 5 1000 500 01% checkpoint:)</li>
               <li>(esm.setOptions regressSvmMVL: 00% false)(esm.selfTest crossCorrelation: 1 5 1000 500 01% checkpoint:)</li>
               <li>(esm.setOptions regressSvmMVL: 00% false)(esm.selfTest elipsoid: 1 5 1000 500 01% checkpoint:)</li>
               <li>(esm.setOptions regressSvmMVL: 00% false)(esm.selfTest hiddenModel: 1 5 1000 500 01% checkpoint:)</li>
               <li>(esm.setOptions regressSvmMVL: 00% false)(esm.selfTest hyperTangent: 1 5 1000 500 01% checkpoint:)</li>
               <li>(esm.setOptions regressSvmMVL: 00% false)(esm.selfTest cyclicSeries: 1 5 1000 500 01% checkpoint:)</li>
               <li>(esm.setOptions regressSvmMVL: 00% false)(esm.selfTest ratioRegression: 1 5 1000 500 01% checkpoint:)</li>
               <li>(esm.setOptions regressSvmMVL: 00% false)(esm.selfTest mixedModels: 1 5 1000 500 01% checkpoint:)</li>
               <li>(esm.setOptions regressSvmMVL: 00% false)(esm.selfTest cyclicSeries: 1 3 1000 500 01% checkpoint:)</li>
               <li>(esm.setOptions regressSvmMVL: 00% false)(esm.selfTest ratioRegression: 1 3 1000 500 01% checkpoint:)</li>
               <li>(esm.setOptions regressSvmMVL: 00% false)(esm.selfTest mixedModels: 1 3 1000 500 01% checkpoint:)</li>
               <li>(esm.setOptions regressSvmMVL: 00% false)(esm.selfTest cyclicSeries: 1 5 10000 500 01% checkpoint:)</li>
               <li>(esm.setOptions regressSvmMVL: 00% false)(esm.selfTest ratioRegression: 1 5 10000 500 01% checkpoint:)</li>
               <li>(esm.setOptions regressSvmMVL: 00% false)(esm.selfTest mixedModels: 1 5 10000 500 01% checkpoint:)</li>
               <li>(esm.setOptions regressSvmMVL: 00% false)(esm.selfTest cyclicSeries: 1 5 1000000 200 01% checkpoint:)</li>
            </ul>

            <P>The following are the ESM performance results for these tests.</P>

            <P><B>Performance Results:</B></P>
		    <table border="3" cellpadding="2" width="100%" bgcolor="#99CCCC">
			<colgroup><col class="member"><col class="description"></colgroup>
		      <tr>
		        <th>Rows</th>
		        <th>Columns</th>
		        <th>Test</th>
		        <th>Generations</th>
		        <th>Elapsed Minutes</th>
		        <th>Train ErrorPct</th>
		        <th>Test ErrorPct</th>
		        <th>Target Source</th>
		        <th>WFF Source</th>
		      </tr>
		      <tr>
		        <td>1000</td>
		        <td>5</td>
		        <td>linearRegression</td>
		        <td>0</td>
		        <td>.03</td>
		        <td>0.0006142805107495</td>
		        <td>0.0005763637957966</td>
		        <td>y = -9.165146942478 - (9.165146942478*x0) - (19.5666514757*x1) + (21.87460482304*x2) - (17.48124453288*x3) + (38.81839452492*x4);</td>
		        <td>mvlregress(x4,x3,x2,x1,x0);</td>
		      </tr>
		      <tr>
		        <td>1000</td>
		        <td>5</td>
		        <td>cubicRegression</td>
		        <td>0</td>
		        <td>.04</td>
		        <td>1.65533573241E-006</td>
		        <td>2.267653076895E-006</td>
		        <td>y = -9.165146942478 - (9.165146942478*x0*x0*x0) - (19.5666514757*x1*x1*x1) + (21.87460482304*x2*x2*x2) - (17.48124453288*x3*x3*x3) + (38.81839452492*x4*x4*x4);</td>
		        <td>svmregress(x4,x3,x2,x1,x0);</td>
		      </tr>
		      <tr>
		        <td>1000</td>
		        <td>5</td>
		        <td>crossCorrelation</td>
		        <td>0</td>
		        <td>.04</td>
		        <td>6.073144521891E-006</td>
		        <td>3.835072392581E-006</td>
		        <td>y = -9.165146942478 - (9.165146942478*x0*x0*x0) - (19.5666514757*x0*x1*x1) + (21.87460482304*x0*x1*x2) - (17.48124453288*x1*x2*x3) + (38.81839452492*x2*x3*x4);</td>
		        <td>svmregress(x4,x3,x2,x1,x0);</td>
		      </tr>
		      <tr>
		        <td>1000</td>
		        <td>5</td>
		        <td>elipsoid</td>
		        <td>0</td>
		        <td>.04</td>
		        <td>3.168706321611E-005</td>
		        <td>3.218076531146E-005</td>
		        <td>y = 0.0 + (1.0*x0*x0) + (2.0*x1*x1) + (3.0*x2*x2) + (4.0*x3*x3) + (5.0*x4*x4);</td>
		        <td>svmregress(x4,x3,x2,x1,x0);</td>
		      </tr>
		      <tr>
		        <td>1000</td>
		        <td>5</td>
		        <td>hiddenModel</td>
		        <td>3</td>
		        <td>1.4</td>
		        <td>6.151124792666E-014</td>
		        <td>1.123567903671E-015</td>
		        <td>y = -9.165146942478 + (21.87460482304*sin(x2));</td>
		        <td>mvlregress(((1.0 / x4) - x2),(tanh(x0) * max(sign((abs(x0) % abs(x3))) , sin(x0))),min(sin(x2) , min(sin(x2) , sin(x2))),max(((x1*x1*x1) % cos(x3)) , sin(x0)),expt(abs(sign(x4)) ,expt(abs(abs(x3)) ,sin(x1))));</td>
		      </tr>
		      <tr>
		        <td>1000</td>
		        <td>5</td>
		        <td>hyperTangent</td>
		        <td>235</td>
		        <td>17</td>
		        <td>0.007754828862494</td>
		        <td>0.009049663931868</td>
		        <td>y = -9.165146942478 - (9.165146942478*tanh(x0*x0*x0)) - (19.5666514757*tanh(x1*x1*x1)) + (21.87460482304*tanh(x2*x2*x2)) - (17.48124453288*tanh(x3*x3*x3)) + (38.81839452492*tanh(x4*x4*x4));</td>
		        <td>mvlregress((tanh((-x2)) - (tanh(x4) - (tanh(x0) + (-x2)))),max(sign(x1) , max(sign(x1) , min(log(abs(sign(x0))) , min(sign(x1) , sign(sign(x1)))))),expt(abs(sign(log(abs(x1)))) ,expt(abs(sign(sign((-x0)))) ,expt(abs(sign(log(abs(x1)))) ,min(log(abs(sign(x1))) , tanh(x0))))),((-x2) + (x3 + x3)),(x2 + (tanh((sign(x1) + x3)) + sign(x1))));</td>
		      </tr>
		      <tr>
		        <td>1000</td>
		        <td>3</td>
		        <td>cyclicSeries</td>
		        <td>500</td>
		        <td>38</td>
		        <td>0.3071915645385</td>
		        <td>0.1568942436519</td>
		        <td>y = -9.165146942478 - (9.165146942478*x0*sin(x0)) - (19.5666514757*x1*cos(x0)) + (21.87460482304*x2*tan(x0));</td>
		        <td>mvlregress((tan(x0) - (-x1)),((-tan(cos((-tan((-x1)))))) - (((-tan(x0)) / (1.0 / x2)) - ((-x1) * cos(x0)))),avg(tan(x0) , (((-tan(x0)) / (1.0 / x2)) / (-tan(cos((-tan(x0))))))));</td>
		      </tr>
		      <tr>
		        <td>1000</td>
		        <td>5</td>
		        <td>cyclicSeries</td>
		        <td>500</td>
		        <td>49</td>
		        <td>0.9854823932357</td>
		        <td>9.998681355293E+050</td>
		        <td>y = -9.165146942478 - (9.165146942478*x0*sin(x0)) - (19.5666514757*x1*cos(x0)) + (21.87460482304*x2*tan(x0)) - (17.48124453288*x3*sin(x0)) + (38.81839452492*x4*cos(x0));</td>
		        <td>mvlregress(min((1.0 / log(abs(cos(x1)))) , sqrt(abs(((1.0 / x1) * sqrt(abs(x1)))))),min(((1.0 / tanh(x2)) / (x4*x4*x4)) , sqrt(abs(cos(x1)))),expt(abs(expt(abs((tan(x4) / cos(x1))) ,sqrt(abs(sign(x2))))) ,sqrt(abs(tan(x0)))),((1.0 / sqrt(abs(x0))) / ((x2*x2*x2)*(x2*x2*x2)*(x2*x2*x2))),avg(min((1.0 / (1.0 / x2)) , sqrt(abs(ninteger(x0)))) , expt(abs(cos(x0)) ,(-x2))));</td>
		      </tr>
		      <tr>
		        <td>10by1000(myTimeON)</td>
		        <td>5</td>
		        <td>cyclicSeries</td>
		        <td>200</td>
		        <td>50</td>
		        <td>1.074203433326</td>
		        <td>1.159588590583</td>
		        <td>y = -9.165146942478 - (9.165146942478*x0*sin(xtime)) - (19.5666514757*x1*cos(xtime)) + (21.87460482304*x2*tan(xtime)) - (17.48124453288*x3*sin(xtime)) + (38.81839452492*x4*cos(xtime));</td>
		        <td> mvlregress(((x1 - (x4*x4*x4)) * tan(x2)),((sqrt(abs(x2))*sqrt(abs(x2))*sqrt(abs(x2))) % sqrt(abs(x2))),max(sign(x2) , (tanh(x2) * x4)),max((1.0 / log(abs(x4))) , expt(abs(log(abs(x4))) ,tanh(x3))),avg(abs(x1) , sin(abs(x1))));</td>
		      </tr>
		      <tr>
		        <td>10000</td>
		        <td>5</td>
		        <td>cyclicSeries</td>
		        <td>500</td>
		        <td>139</td>
		        <td>0.9837278996768</td>
		        <td>1.407708513919</td>
		        <td>y = -9.165146942478 - (9.165146942478*x0*sin(x0)) - (19.5666514757*x1*cos(x0)) + (21.87460482304*x2*tan(x0)) - (17.48124453288*x3*sin(x0)) + (38.81839452492*x4*cos(x0));</td>
		        <td>mvlregress(((-x3) * exp(x4)),((1.0 / x1) * min(((1.0 / cos(x0))*(1.0 / cos(x0))*(1.0 / cos(x0))) , max(sign(sin(x3)) , abs(sign(sin(cos(x0))))))),((1.0 / x1) * min(((1.0 / cos(exp((1.0 / abs(x0)))))*(1.0 / cos(exp((1.0 / abs(x0)))))*(1.0 / cos(exp((1.0 / abs(x0)))))) , expt(abs((cos(x0)*cos(x0)*cos(x0))) ,abs(abs(x2))))),max(exp((1.0 / x4)) , sin(sign(x0))),(max(exp((1.0 / x4)) , sin((1.0 / x1))) / exp(x4)));</td>
		      </tr>
		      <tr>
		        <td>1000000</td>
		        <td>5</td>
		        <td>cyclicSeries</td>
		        <td>200</td>
		        <td>3012</td>
		        <td>0.9837278296768</td>
		        <td>1.407708513919</td>
		        <td>y = -9.165146942478 - (9.165146942478*x0*sin(x0)) - (19.5666514757*x1*cos(x0)) + (21.87460482304*x2*tan(x0)) - (17.48124453288*x3*sin(x0)) + (38.81839452492*x4*cos(x0));</td>
		        <td>mvlregress(((-x3) * exp(x4)),((1.0 / x1) * min(((1.0 / cos(x0))*(1.0 / cos(x0))*(1.0 / cos(x0))) , max(sign(sin(x3)) , abs(sign(sin(cos(x0))))))),((1.0 / x1) * min(((1.0 / cos(exp((1.0 / abs(x0)))))*(1.0 / cos(exp((1.0 / abs(x0)))))*(1.0 / cos(exp((1.0 / abs(x0)))))) , expt(abs((cos(x0)*cos(x0)*cos(x0))) ,abs(abs(x2))))),max(exp((1.0 / x4)) , sin(sign(x0))),(max(exp((1.0 / x4)) , sin((1.0 / x1))) / exp(x4)));</td>
		      </tr>
		      <tr>
		        <td>1000</td>
		        <td>3</td>
		        <td>ratioRegression</td>
		        <td>0</td>
		        <td>1.03</td>
		        <td>1.298961438178E-016</td>
		        <td>93.84796937406</td>
		        <td>
                  if ((x0 % 4) == 0) y = + ((9.165146942478*x0)/(19.5666514757*x1)) + ((19.5666514757*x1)/(21.87460482304*x2));
                  if ((x0 % 4) == 1) y = + ((9.165146942478*x0)%(19.5666514757*x1)) + ((19.5666514757*x1)%(21.87460482304*x2));
                  if ((x0 % 4) == 2) y = + ((9.165146942478*sin(x0))/(19.5666514757*tan(x1))) + ((19.5666514757*sin(x1))/(21.87460482304*tan(x2)));
                  if ((x0 % 4) == 3) y = - (19.5666514757* log(.000001+abs(x1))) + (21.87460482304* log(.000001+abs(x2)));
	            </td>
		        <td>mvlregress((sign(x0) - (1.0 / x0)),((-x1) * (1.0 / x2)),(exp(x1) * sqrt(abs(x1))));</td>
		      </tr>
		      <tr>
		        <td>1000</td>
		        <td>5</td>
		        <td>ratioRegression</td>
		        <td>476</td>
		        <td>47</td>
		        <td>2.399015365933E-015</td>
		        <td>1.748617462844</td>
		        <td>
                  if ((x0 % 4) == 0) y = + ((9.165146942478*x0)/(19.5666514757*x1)) + ((19.5666514757*x1)/(21.87460482304*x2)) + ((21.87460482304*x2)/(17.48124453288*x3)) + ((17.48124453288*x3)/(38.81839452492*x4));
                  if ((x0 % 4) == 1) y = + ((9.165146942478*x0)%(19.5666514757*x1)) + ((19.5666514757*x1)%(21.87460482304*x2)) + ((21.87460482304*x2)%(17.48124453288*x3)) + ((17.48124453288*x3)%(38.81839452492*x4));
                  if ((x0 % 4) == 2) y = + ((9.165146942478*sin(x0))/(19.5666514757*tan(x1))) + ((19.5666514757*sin(x1))/(21.87460482304*tan(x2))) + ((21.87460482304*sin(x2))/(17.48124453288*tan(x3))) + ((17.48124453288*sin(x3))/(38.81839452492*tan(x4)));
                  if ((x0 % 4) == 3) y = - (19.5666514757* log(.000001+abs(x1))) + (21.87460482304* log(.000001+abs(x2))) - (17.48124453288* log(.000001+abs(x3))) + (38.81839452492* log(.000001+abs(x4)));             
                </td>
		        <td>mvlregress(min((1.0 / x2) , (1.0 / x2)),((1.0 / x4) / (1.0 / x3)),((((-(1.0 / abs(x1))) + (-x1)) + (abs(abs((x0*x0))) + (1.0 / abs(x1)))) / (1.0 / log(abs(exp(x2))))),avg(((1.0 / x3) / (-(1.0 / log(abs(exp(x2)))))) , tan(x0)),((x0*x0) + ((-x1) * (1.0 / x2))));</td>
		      </tr>
		      <tr>
		        <td>10000</td>
		        <td>5</td>
		        <td>ratioRegression</td>
		        <td>500</td>
		        <td>124</td>
		        <td>0.2584533130733</td>
		        <td>1.260190723419</td>
		        <td>
                  if ((x0 % 4) == 0) y = + ((9.165146942478*x0)/(19.5666514757*x1)) + ((19.5666514757*x1)/(21.87460482304*x2)) + ((21.87460482304*x2)/(17.48124453288*x3)) + ((17.48124453288*x3)/(38.81839452492*x4));
                  if ((x0 % 4) == 1) y = + ((9.165146942478*x0)%(19.5666514757*x1)) + ((19.5666514757*x1)%(21.87460482304*x2)) + ((21.87460482304*x2)%(17.48124453288*x3)) + ((17.48124453288*x3)%(38.81839452492*x4));
                  if ((x0 % 4) == 2) y = + ((9.165146942478*sin(x0))/(19.5666514757*tan(x1))) + ((19.5666514757*sin(x1))/(21.87460482304*tan(x2))) + ((21.87460482304*sin(x2))/(17.48124453288*tan(x3))) + ((17.48124453288*sin(x3))/(38.81839452492*tan(x4)));
                  if ((x0 % 4) == 3) y = - (19.5666514757* log(.000001+abs(x1))) + (21.87460482304* log(.000001+abs(x2))) - (17.48124453288* log(.000001+abs(x3))) + (38.81839452492* log(.000001+abs(x4)));             
                </td>
		        <td>mvlregress(min(sign(min(min((-x4) , sign((-x4))) , (1.0 / (1.0 / x3)))) , (log(abs(sign((1.0 / x3)))) / cos(cos(x1)))),avg(((1.0 / x3) * (1.0 / x1)) , (1.0 / x3)),avg((1.0 / x3) , ((1.0 / x3) / (1.0 / x2))),avg((1.0 / x3) , avg((1.0 / x3) , ((1.0 / x2) / ((1.0 / x1) / log(abs(x4)))))),avg(((1.0 / x2) / ((1.0 / x1) / log(abs(sign(x4))))) , ((1.0 / x3) * (1.0 / x1))));</td>
		      </tr>
		      <tr>
		        <td>1000</td>
		        <td>3</td>
		        <td>mixedModels</td>
		        <td>427</td>
		        <td>52</td>
		        <td>8.336917024588E-005</td>
		        <td>0.998412605213</td>
		        <td>
                  if ((x0 % 4) == 0) y = - (9.165146942478*log(.000001+abs(x0))) - (19.5666514757*log(.000001+abs(x1))) + (21.87460482304*log(.000001+abs(x2)));
                  if ((x0 % 4) == 1) y = - (9.165146942478*x0*x0) - (19.5666514757*x1*x1) + (21.87460482304*x2*x2);
                  if ((x0 % 4) == 2) y = - (9.165146942478*sin(x0)) - (19.5666514757*sin(x1)) + (21.87460482304*sin(x2));
                  if ((x0 % 4) == 3) y = - (9.165146942478*x0) - (19.5666514757*x1) + (21.87460482304*x2);
                </td>
		        <td>svmregress(((log(abs(x0)) - (log(abs(x2)) - log(abs(x1)))) - (log(abs(x2)) - log(abs(x2)))),(log(abs(x2)) - log(abs(x0))),(log(abs(x0)) - (log(abs(x1)) - log(abs(x1)))));</td>
		      </tr>
		      <tr>
		        <td>1000</td>
		        <td>5</td>
		        <td>mixedModels</td>
		        <td>500</td>
		        <td>50</td>
		        <td>0.4864255161109</td>
		        <td>1.009994583082</td>
		        <td>
                  if ((x0 % 4) == 0) y = - (9.165146942478*log(.000001+abs(x0))) - (19.5666514757*log(.000001+abs(x1))) + (21.87460482304*log(.000001+abs(x2))) - (17.48124453288*log(.000001+abs(x3))) + (38.81839452492*log(.000001+abs(x4)));
                  if ((x0 % 4) == 1) y = - (9.165146942478*x0*x0) - (19.5666514757*x1*x1) + (21.87460482304*x2*x2) - (17.48124453288*x3*x3) + (38.81839452492*x4*x4);
                  if ((x0 % 4) == 2) y = - (9.165146942478*sin(x0)) - (19.5666514757*sin(x1)) + (21.87460482304*sin(x2)) - (17.48124453288*sin(x3)) + (38.81839452492*sin(x4));
                  if ((x0 % 4) == 3) y = - (9.165146942478*x0) - (19.5666514757*x1) + (21.87460482304*x2) - (17.48124453288*x3) + (38.81839452492*x4);
                </td>
		        <td>mvlregress((avg(cos(x3) , abs(x2)) - ((log(abs(x1)) * sqrt(abs(x4))) + abs(x3))),(log(abs(max(abs((log(abs(max(abs(x4) , sin(abs(x4))))) * sqrt(abs(x4)))) , sin(abs(x4))))) * sqrt(abs(x4))),min(ninteger(x0) , min(ninteger(ninteger(x0)) , min(min(ninteger(x0) , cos(x3)) , cos(x3)))),max(sign(x4) , abs(avg(abs(x4) , tanh(max(sign(x4) , avg(abs(x4) , tanh(x1))))))),(log(abs(x0)) + (x3*x3)));</td>
		      </tr>
		      <tr>
		        <td>10000</td>
		        <td>5</td>
		        <td>mixedModels</td>
		        <td>500</td>
		        <td>134</td>
		        <td>0.07680159990685</td>
		        <td>1.021738837223</td>
		        <td>
                  if ((x0 % 4) == 0) y = - (9.165146942478*log(.000001+abs(x0))) - (19.5666514757*log(.000001+abs(x1))) + (21.87460482304*log(.000001+abs(x2))) - (17.48124453288*log(.000001+abs(x3))) + (38.81839452492*log(.000001+abs(x4)));
                  if ((x0 % 4) == 1) y = - (9.165146942478*x0*x0) - (19.5666514757*x1*x1) + (21.87460482304*x2*x2) - (17.48124453288*x3*x3) + (38.81839452492*x4*x4);
                  if ((x0 % 4) == 2) y = - (9.165146942478*sin(x0)) - (19.5666514757*sin(x1)) + (21.87460482304*sin(x2)) - (17.48124453288*sin(x3)) + (38.81839452492*sin(x4));
                  if ((x0 % 4) == 3) y = - (9.165146942478*x0) - (19.5666514757*x1) + (21.87460482304*x2) - (17.48124453288*x3) + (38.81839452492*x4);
                </td>
		        <td>mvlregress((log(abs(((1.0 / ninteger(x0)) / ninteger(x3)))) * (sign(abs(x0)) % exp(avg(log(abs(abs(abs(x0)))) , log(abs(x2)))))),(avg(log(abs(x4)) , log(abs((x1*x1)))) * (sign(abs(abs(log(abs(sqrt(abs(x3))))))) % exp(avg(log(abs(abs(abs(x0)))) , log(abs(x2)))))),avg(log(abs(x4)) , log(abs(x4))),avg(log(abs(x2)) , abs(avg(log(abs(x4)) , log(abs(x2))))),avg(avg(log(abs(x3)) , log(abs((x1*x1)))) , abs(avg(log(abs(((1.0 / ninteger(x0)) / ninteger(x3)))) , log(abs(abs(log(abs((x1*x1))))))))));</td>
		      </tr>
            </table>

	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Linear Regression</Heading>
		<Description><![CDATA[
			<p>We ran the self test known as linearRegression in this SelfTest log.
            The training and test data sets were constructed from a simple linear polynomial model on from 1 to 10 variables (not including xtime).
            The training set was 10,000 elements distributed over twenty time periods. The testing data set was 500 elements in time period 21. 
            After the polynomial model was computed, up to forty percent random noise was added to the final y value.
            Learning was very quick with training set least squares error very good and order preservation nearly perfect.
            The results are shown below.
            </p>

            <h3>1 Independent Variable</h3>
            <p>This linear regression problem had one independent variable and was solved in the first generation by a Linear Regression Selector Lambda.
            The average percent error on the training data was ~0%, and the selector score was 100%.
            On the testing data, the average percent error was ~0%, and the selector score was also 100%.
            The training time was .08 minutes. 
            </p>

			<blockquote>
		    <pre>

Starting test case: linearRegression
Building test data as: y = -9.165146942478 + (21.87460482304*x1);
...starting generation of constant selector WFFs.
...starting regress selector for each column.
...locating the best selector and the best regressor from this training run.

Final results of training
esm: N = [500], M = [1], Generations = [0], WFFs = [8], Score=[1.0], ScoreHistory=[#(num| 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 )]

Show final results on training data.
X[0], ey=[-1100.622914395], y=[-1100.622914395], ErrorPct=[8.263454175607E-016]
X[500], ey=[-977.9218407407], y=[-977.9218407407], ErrorPct=[8.13774507223E-016]
X[1000], ey=[-875.6608545148], y=[-875.6608545148], ErrorPct=[9.088083131137E-016]
X[1500], ey=[-766.1196418103], y=[-766.1196418103], ErrorPct=[7.419652983507E-016]
X[2000], ey=[-650.1570965152], y=[-650.1570965152], ErrorPct=[8.743028287391E-016]
X[2500], ey=[-537.6886979103], y=[-537.6886979103], ErrorPct=[6.343085032108E-016]
X[3000], ey=[-433.7621621211], y=[-433.7621621211], ErrorPct=[7.862846115877E-016]
X[3500], ey=[-318.5181072361], y=[-318.5181072361], ErrorPct=[8.923106343005E-016]
X[4000], ey=[-204.0563073157], y=[-204.0563073157], ErrorPct=[6.964183024844E-016]
X[4500], ey=[-81.87104300442], y=[-81.87104300442], ErrorPct=[5.207282401826E-016]
X[5000], ey=[19.23916975494], y=[19.23916975494], ErrorPct=[2.215925358976E-015]
X[5500], ey=[121.2593130926], y=[121.2593130926], ErrorPct=[1.054745315431E-015]
X[6000], ey=[239.5186478294], y=[239.5186478294], ErrorPct=[9.49294251215E-016]
X[6500], ey=[345.3966668729], y=[345.3966668729], ErrorPct=[1.152020184874E-015]
X[7000], ey=[466.4614943253], y=[466.4614943253], ErrorPct=[9.748872231013E-016]
X[7500], ey=[574.3343407973], y=[574.3343407973], ErrorPct=[7.917815784012E-016]
X[8000], ey=[687.4609309323], y=[687.4609309323], ErrorPct=[8.26860353849E-016]
X[8500], ey=[801.5735242539], y=[801.5735242539], ErrorPct=[8.509774907606E-016]
X[9000], ey=[897.1039443362], y=[897.1039443362], ErrorPct=[8.870854587983E-016]
X[9500], ey=[995.8838319791], y=[995.8838319791], ErrorPct=[7.990970819054E-016]
Actual computed error on training data is ErrorPct=[8.579416468462E-016] versus reported ErrorPct=[8.579416468462E-016]


Final testing on test data returns Score=[1.0]
X[0], ey=[-1100.622914395], y=[-1100.622914395], ErrorPct=[8.263454175607E-016]
X[25], ey=[-978.7067741528], y=[-978.7067741528], ErrorPct=[8.131218512718E-016]
X[50], ey=[-894.1805536148], y=[-894.1805536148], ErrorPct=[7.628448455652E-016]
X[75], ey=[-808.9760061026], y=[-808.9760061026], ErrorPct=[8.431906770832E-016]
X[100], ey=[-684.9129330526], y=[-684.9129330526], ErrorPct=[6.639491370964E-016]
X[125], ey=[-554.8222816813], y=[-554.8222816813], ErrorPct=[8.19627051582E-016]
X[150], ey=[-449.3678348325], y=[-449.3678348325], ErrorPct=[6.3248206096E-016]
X[175], ey=[-320.8667085755], y=[-320.8667085755], ErrorPct=[7.086234544327E-016]
X[200], ey=[-213.269343612], y=[-213.269343612], ErrorPct=[5.330669462201E-016]
X[225], ey=[-96.70193594824], y=[-96.70193594824], ErrorPct=[5.878208983451E-016]
X[250], ey=[-2.690871633651], y=[-2.690871633651], ErrorPct=[8.58184337871E-015]
X[275], ey=[132.1365256035], y=[132.1365256035], ErrorPct=[1.075467562833E-015]
X[300], ey=[227.2941351483], y=[227.2941351483], ErrorPct=[1.000349944335E-015]
X[325], ey=[347.1982497606], y=[347.1982497606], ErrorPct=[9.823220981097E-016]
X[350], ey=[467.502979815], y=[467.502979815], ErrorPct=[8.511259803801E-016]
X[375], ey=[571.1005103104], y=[571.1005103104], ErrorPct=[9.953312566628E-016]
X[400], ey=[677.4217598584], y=[677.4217598584], ErrorPct=[6.712913251879E-016]
X[425], ey=[762.409743828], y=[762.409743828], ErrorPct=[8.946908560019E-016]
X[450], ey=[855.3449210621], y=[855.3449210621], ErrorPct=[9.303940953588E-016]
X[475], ey=[970.9557282972], y=[970.9557282972], ErrorPct=[8.196129245222E-016]
Actual computed error on testing data is ErrorPct=[8.537718700755E-016]

esm.selfTest: completed in [0.0823] minutes.

            </pre>
			</blockquote>

            <h3>10 Independent Variables</h3>
            <p>This linear regression problem had ten independent variables and was solved in the first generation by a Support Vector Regression Selector Lambda.
            The average percent error on the training data was ~0%, and the selector score was 100%.
            On the testing data, the average percent error was ~0%, and the selector score was also 100%.
            The training time was 1.2 minutes. 
            </p>

			<blockquote>
		    <pre>

Starting test case: linearRegression
Building test data as: y = -9.165146942478 + (21.87460482304*x1) - (17.48124453288*x2) + (38.81839452492*x3) - (38.63656433142*x4) + (13.18212824804*x5) - (2.045229508597*x6) + (45.1292360492*x7) + (26.03603502163*x8) + (21.98935009253*x9) + (39.25956682671*x10);
...starting generation of constant selector WFFs.
...starting regress selector for each column.
...starting a simple SVM selector WFF.
...locating the best selector and the best regressor from this training run.

Final results of training
esm: N = [500], M = [10], Generations = [0], WFFs = [27], Score=[0.9999998952854], ScoreHistory=[#(num| 0.9999994794294 0.9999998881055 0.9999999272587 1.0 1.0 0.9999997613158 1.0 0.9999999080865 1.0 1.0 0.9999999078672 0.9999997336707 1.0 0.9999999892104 0.9999999507207 0.9999999275923 0.9999998924631 0.9999999651269 0.999999740334 0.9999998345264 )]

Show final results on training data.
X[0], ey=[-7755.472103702], y=[-7758.617785004], ErrorPct=[0.0004054435196709]
X[500], ey=[-4474.374391617], y=[-4467.612161579], ErrorPct=[0.001513611699827]
X[1000], ey=[-3346.291629968], y=[-3343.463462794], ErrorPct=[0.0008458794913414]
X[1500], ey=[-2787.713079039], y=[-2787.821100673], ErrorPct=[3.874769192525E-005]
X[2000], ey=[-2273.435258231], y=[-2267.618622062], ErrorPct=[0.002565085729984]
X[2500], ey=[-1781.14314751], y=[-1783.293333814], ErrorPct=[0.001205738990653]
X[3000], ey=[-1374.177281401], y=[-1376.269808461], ErrorPct=[0.001520433745732]
X[3500], ey=[-1015.405301084], y=[-1010.797807356], ErrorPct=[0.004558274359622]
X[4000], ey=[-627.4462464587], y=[-625.855780544], ErrorPct=[0.002541265838145]
X[4500], ey=[-274.2398365724], y=[-270.7514723928], ErrorPct=[0.01288400815989]
X[5000], ey=[72.90922284451], y=[74.84424407819], ErrorPct=[0.02585397524578]
X[5500], ey=[476.3819451045], y=[471.6547763325], ErrorPct=[0.01002251860738]
X[6000], ey=[849.454282321], y=[843.8480499106], ErrorPct=[0.006643651556678]
X[6500], ey=[1280.320924571], y=[1279.627448872], ErrorPct=[0.0005419356232263]
X[7000], ey=[1639.428671247], y=[1644.844841415], ErrorPct=[0.003292815243906]
X[7500], ey=[2065.872186807], y=[2064.539562798], ErrorPct=[0.0006454824276692]
X[8000], ey=[2548.658477857], y=[2551.792933803], ErrorPct=[0.001228334754293]
X[8500], ey=[3094.130197363], y=[3097.744360288], ErrorPct=[0.001166707934827]
X[9000], ey=[3728.398070621], y=[3734.538936269], ErrorPct=[0.001644343720128]
X[9500], ey=[4904.205764074], y=[4900.295426089], ErrorPct=[0.0007979800490971]
Actual computed error on training data is ErrorPct=[0.004199650754758] versus reported ErrorPct=[0.004199650754758]


Final testing on test data returns Score=[0.9999998952854]
X[0], ey=[-7383.734757145], y=[-7381.048038815], ErrorPct=[0.0003640022819898]
X[25], ey=[-4575.742427123], y=[-4568.567784514], ErrorPct=[0.001570435844867]
X[50], ey=[-3314.786445915], y=[-3307.995916656], ErrorPct=[0.002052762285691]
X[75], ey=[-2727.919603418], y=[-2720.371869638], ErrorPct=[0.002774522800884]
X[100], ey=[-2201.829334101], y=[-2197.372135978], ErrorPct=[0.002028422064043]
X[125], ey=[-1835.257352889], y=[-1827.050688309], ErrorPct=[0.004491755281807]
X[150], ey=[-1335.176052456], y=[-1335.054875426], ErrorPct=[9.076558004382E-005]
X[175], ey=[-1028.087544438], y=[-1024.367427432], ErrorPct=[0.003631623679638]
X[200], ey=[-747.621079155], y=[-739.6539884774], ErrorPct=[0.01077137526694]
X[225], ey=[-279.4986178372], y=[-271.8264626968], ErrorPct=[0.02822446006283]
X[250], ey=[142.4609159913], y=[144.7322138525], ErrorPct=[0.01569310522366]
X[275], ey=[489.3009718991], y=[491.4578248953], ErrorPct=[0.004388683803576]
X[300], ey=[824.703327615], y=[832.5314612748], ErrorPct=[0.009402808210722]
X[325], ey=[1157.847273215], y=[1158.527485922], ErrorPct=[0.0005871355797024]
X[350], ey=[1530.179624996], y=[1535.624165652], ErrorPct=[0.003545490347641]
X[375], ey=[2061.754787376], y=[2064.539562798], ErrorPct=[0.001348860284111]
X[400], ey=[2549.682695724], y=[2555.107495271], ErrorPct=[0.002123119891005]
X[425], ey=[3083.260348754], y=[3084.191902813], ErrorPct=[0.000302041535919]
X[450], ey=[3602.910079815], y=[3603.603187558], ErrorPct=[0.0001923374209181]
X[475], ey=[4711.256775183], y=[4716.293725843], ErrorPct=[0.001067989178232]
Actual computed error on testing data is ErrorPct=[0.007552823914835]

esm.selfTest: completed in [1.292716666667] minutes.

            </pre>
			</blockquote>

	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Cubic Regression</Heading>
		<Description><![CDATA[
			<p>We ran the self test known as cubicRegression in this SelfTest log.
            The training and test data sets were constructed from a cubic polynomial model on 10 variables (not including xtime).
            The training set was 10,000 elements distributed over twenty time periods. The testing data set was 500 elements in time period 21. 
            After the polynomial model was computed, up to forty percent random noise was added to the final y value.
            Learning was very quick with training set least squares error very good and order preservation nearly perfect.
            The results are shown below.
            </p>

            <h3>1000 Training Examples</h3>
            <p>This cubic regression problem had ten independent variables and was solved in the first generation by a Support Vector Regression Selector Lambda.
            The average percent error on the training data was 185%, and the selector score was 90%.
            On the testing data, the average percent error was 402%, and the selector score was also 90%.
            The training time was 1.2 minutes. 
            </p>

			<blockquote>
		    <pre>

Starting test case: cubicRegression
Building test data as: y = -9.165146942478 + (21.87460482304*x1*x1*x1) - (17.48124453288*x2*x2*x2) + (38.81839452492*x3*x3*x3) - (38.63656433142*x4*x4*x4) + (13.18212824804*x5*x5*x5) - (2.045229508597*x6*x6*x6) + (45.1292360492*x7*x7*x7) + (26.03603502163*x8*x8*x8) + (21.98935009253*x9*x9*x9) + (39.25956682671*x10*x10*x10);
...starting generation of constant selector WFFs.
...starting regress selector for each column.
...starting a simple SVM selector WFF.
...locating the best selector and the best regressor from this training run.

Final results of training
esm: N = [500], M = [10], Generations = [0], WFFs = [27], Score=[0.9624595337713], ScoreHistory=[#(num| 0.9626133997241 0.9614386395896 0.9617583848667 0.9634478813472 0.9607579778166 0.960129748756 0.9652223008648 0.9611321314246 0.9616496737299 0.9616360195769 0.9626758374936 0.9615892631018 0.9628325719447 0.963578717838 0.9609716094134 0.9638966457252 0.9632583921888 0.962554254991 0.9659688920523 0.9620996967779 )]

Show final results on training data.
X[0], ey=[-10738027.55205], y=[-12689112.60833], ErrorPct=[0.1537605596631]
X[500], ey=[-3634629.741068], y=[-6972996.265278], ErrorPct=[0.478756390683]
X[1000], ey=[-3608841.829266], y=[-5582558.586546], ErrorPct=[0.3535505676621]
X[1500], ey=[-3528236.987998], y=[-4451324.007402], ErrorPct=[0.2073735854476]
X[2000], ey=[-4417881.233603], y=[-3513032.627206], ErrorPct=[0.2575690870019]
X[2500], ey=[-3488236.686056], y=[-2758416.604204], ErrorPct=[0.26457935351]
X[3000], ey=[524209.807062], y=[-2142524.262906], ErrorPct=[1.24466925119]
X[3500], ey=[-1425188.564291], y=[-1404079.181062], ErrorPct=[0.01503432535219]
X[4000], ey=[-2089868.033668], y=[-909719.0032918], ErrorPct=[1.297267646499]
X[4500], ey=[-1568183.871674], y=[-434057.5971096], ErrorPct=[2.612847424204]
X[5000], ey=[716098.8918349], y=[215466.4214243], ErrorPct=[2.323482550558]
X[5500], ey=[3329758.674043], y=[822098.1425177], ErrorPct=[3.050317719786]
X[6000], ey=[1712615.745767], y=[1450310.292918], ErrorPct=[0.1808616088088]
X[6500], ey=[3941095.137277], y=[1953189.575078], ErrorPct=[1.017773997754]
X[7000], ey=[3749136.678147], y=[2660095.144577], ErrorPct=[0.4093994667031]
X[7500], ey=[3528866.856054], y=[3443352.465078], ErrorPct=[0.02483463190131]
X[8000], ey=[4990357.623486], y=[4273764.542653], ErrorPct=[0.1676725691555]
X[8500], ey=[3728523.559345], y=[5157561.009586], ErrorPct=[0.2770762086158]
X[9000], ey=[5544382.037212], y=[6424539.679378], ErrorPct=[0.1369993316393]
X[9500], ey=[7819366.090833], y=[7989997.405712], ErrorPct=[0.0213556158049]
Actual computed error on training data is ErrorPct=[1.855473332292] versus reported ErrorPct=[1.855473332292]


Final testing on test data returns Score=[0.9624595337713]
X[0], ey=[-10421217.04314], y=[-12242495.16574], ErrorPct=[0.1487669055975]
X[25], ey=[-3551880.981322], y=[-6972996.265278], ErrorPct=[0.4906234212387]
X[50], ey=[-7414060.241013], y=[-5686040.364381], ErrorPct=[0.3039056647324]
X[75], ey=[-3858987.804062], y=[-4502379.364917], ErrorPct=[0.1429003441754]
X[100], ey=[-3603157.934828], y=[-3492747.882022], ErrorPct=[0.0316112289049]
X[125], ey=[-2550274.989886], y=[-2768299.025298], ErrorPct=[0.07875740063465]
X[150], ey=[-763874.1104257], y=[-2179788.735419], ErrorPct=[0.6495650711403]
X[175], ey=[-2126984.815349], y=[-1469005.273047], ErrorPct=[0.447908223595]
X[200], ey=[2434996.044713], y=[-1036507.126356], ErrorPct=[3.349232323441]
X[225], ey=[1243892.763984], y=[-509542.0411575], ErrorPct=[3.441197513671]
X[250], ey=[413918.7149167], y=[-474.5817220763], ErrorPct=[873.1758459339]
X[275], ey=[1763687.028657], y=[696465.9411651], ErrorPct=[1.532337799185]
X[300], ey=[1879705.1708], y=[1464422.683059], ErrorPct=[0.2835810265333]
X[325], ey=[4103382.305378], y=[1997723.123778], ErrorPct=[1.054029538196]
X[350], ey=[2690195.16785], y=[2480321.009882], ErrorPct=[0.08461572398538]
X[375], ey=[1091521.614159], y=[3287359.626662], ErrorPct=[0.6679640385839]
X[400], ey=[6269098.521779], y=[4097497.475216], ErrorPct=[0.529982278134]
X[425], ey=[4607628.080926], y=[5070911.922915], ErrorPct=[0.09136105083891]
X[450], ey=[4245226.618161], y=[5980304.586028], ErrorPct=[0.2901320397493]
X[475], ey=[8647884.77089], y=[7651679.936871], ErrorPct=[0.1301942635131]
Actual computed error on testing data is ErrorPct=[2.857782914171]

esm.selfTest: completed in [1.29245] minutes.

            </pre>
			</blockquote>

	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Hidden Model</Heading>
		<Description><![CDATA[
			<p>We ran the self test known as hiddenModel in this SelfTest log.
            The training and test data sets were constructed from a linear regression model on 1 variable hidden within 10 variables (not including xtime).
            The training set was 10,000 elements distributed over twenty time periods. The testing data set was 500 elements in time period 21. 
            After the polynomial model was computed, up to forty percent random noise was added to the final y value.
            Learning was very quick with training set least squares error very good and order preservation nearly perfect.
            The results are shown below.
            </p>

            <h3>10 Independent Variables</h3>
            <p>This hidden model problem had ten independent variables and was solved in the first generation by a Linear Regression Selector Lambda.
            The average percent error on the training data was ~0%, and the selector score was 100%.
            On the testing data, the average percent error was ~0%, and the selector score was also 100%.
            The training time was 2.1 minutes. 
            </p>

			<blockquote>
		    <pre>

Starting test case: hiddenModel
Building test data as: y = -9.165146942478 + (13.18212824804*sin(x5));
...starting generation of constant selector WFFs.
...starting regress selector for each column.
...starting a simple SVM selector WFF.
...starting generation of root selector WFFs.
...locating the best selector and the best regressor from this training run.

Final results of training
esm: N = [500], M = [10], Generations = [0], WFFs = [325], Score=[1.0], ScoreHistory=[#(num| 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 )]

Show final results on training data.
X[0], ey=[-22.34713937702], y=[-22.34713937702], ErrorPct=[2.066719907421E-015]
X[500], ey=[-22.23268437175], y=[-22.23268437175], ErrorPct=[1.917562604351E-015]
X[1000], ey=[-21.80191774634], y=[-21.80191774634], ErrorPct=[2.118404369825E-015]
X[1500], ey=[-20.91931664459], y=[-20.91931664459], ErrorPct=[2.037952045466E-015]
X[2000], ey=[-19.86363907462], y=[-19.86363907462], ErrorPct=[2.146261517613E-015]
X[2500], ey=[-18.33298570199], y=[-18.33298570199], ErrorPct=[2.131668627362E-015]
X[3000], ey=[-16.71480652025], y=[-16.71480652025], ErrorPct=[2.12548896363E-015]
X[3500], ey=[-14.8435172737], y=[-14.8435172737], ErrorPct=[2.034427940021E-015]
X[4000], ey=[-13.08450673226], y=[-13.08450673226], ErrorPct=[2.036404821079E-015]
X[4500], ey=[-11.47383529233], y=[-11.47383529233], ErrorPct=[2.16745274078E-015]
X[5000], ey=[-9.504464736572], y=[-9.504464736572], ErrorPct=[2.055868034127E-015]
X[5500], ey=[-7.697183620316], y=[-7.697183620316], ErrorPct=[2.077020939504E-015]
X[6000], ey=[-5.408479465884], y=[-5.408479465884], ErrorPct=[2.299074620554E-015]
X[6500], ey=[-3.331448721395], y=[-3.331448721395], ErrorPct=[2.39943833623E-015]
X[7000], ey=[-1.607070376992], y=[-1.607070376992], ErrorPct=[2.763346373675E-015]
X[7500], ey=[-0.06273221033439], y=[-0.06273221033439], ErrorPct=[0.0]
X[8000], ey=[1.324704161861], y=[1.324704161861], ErrorPct=[1.340946069728E-015]
X[8500], ey=[2.565264575535], y=[2.565264575535], ErrorPct=[2.077396058491E-015]
X[9000], ey=[3.383496451446], y=[3.383496451446], ErrorPct=[1.575018799243E-015]
X[9500], ey=[3.902495902795], y=[3.902495902795], ErrorPct=[1.820739222945E-015]
Actual computed error on training data is ErrorPct=[2.179545952627E-015] versus reported ErrorPct=[2.179545952624E-015]


Final testing on test data returns Score=[1.0]
X[0], ey=[3.656280482863], y=[-14130.82057838], ErrorPct=[1.000258745093]
X[25], ey=[3.576418219893], y=[-8423.338542127], ErrorPct=[1.000424584409]
X[50], ey=[3.012877252078], y=[-6683.472633956], ErrorPct=[1.00045079518]
X[75], ey=[-22.26725724601], y=[-4946.50047313], ErrorPct=[0.9954983816605]
X[100], ey=[-17.80460681437], y=[-4014.136918831], ErrorPct=[0.9955645242864]
X[125], ey=[-7.665206430643], y=[-3142.218580129], ErrorPct=[0.9975605750411]
X[150], ey=[-19.33165050183], y=[-2438.191438348], ErrorPct=[0.992071315567]
X[175], ey=[-21.32656879903], y=[-1865.945882325], ErrorPct=[0.9885706391589]
X[200], ey=[3.004574770358], y=[-1280.499991293], ErrorPct=[1.00234640749]
X[225], ey=[3.676432318054], y=[-605.4326797663], ErrorPct=[1.006072404812]
X[250], ey=[-16.49356144996], y=[30.89556852886], ErrorPct=[1.533848776227]
X[275], ey=[3.326483812403], y=[726.4655616223], ErrorPct=[0.9954210027452]
X[300], ey=[1.504946569266], y=[1421.028928923], ErrorPct=[0.9989409458607]
X[325], ey=[-4.27068615444], y=[2022.907261926], ErrorPct=[1.0021111626]
X[350], ey=[-21.54905375565], y=[2904.073915692], ErrorPct=[1.007420284187]
X[375], ey=[3.831612084195], y=[3729.465923294], ErrorPct=[0.9989726110486]
X[400], ey=[-21.87872994679], y=[4738.599770458], ErrorPct=[1.004617129744]
X[425], ey=[-18.32288327488], y=[5621.61726765], ErrorPct=[1.003259361568]
X[450], ey=[3.434573905321], y=[6671.118094573], ErrorPct=[0.9994851576817]
X[475], ey=[-13.05858845954], y=[8273.813528861], ErrorPct=[1.001578303453]
Actual computed error on testing data is ErrorPct=[0.9952989880657]

esm.selfTest: completed in [2.1638] minutes.

            </pre>
			</blockquote>

	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Cycle Dependent Regression (No Random Noise)</Heading>
		<Description><![CDATA[
			<p>We ran the self test known as cyclicSeries in this SelfTest log.
            This training set was 10,000 elements distributed over 20 time periods. The testing data set was 500 elements in time period 21. 
            The cyclic series regression problem had ten independent variables and was trained for one hundred generations.
            The average percent error on the training data was 888%, and the selector score was 99%.
            On the testing data, the average percent error was 106%, and the selector score was 97%. 
            The training time was 29 minutes. 
            </p>

			<blockquote>
		    <pre>

Starting test case: cyclicSeries
Building test data as: y = -9.165146942478 + (21.87460482304*x1*tan(xtime)) - (17.48124453288*x2*sin(xtime)) + (38.81839452492*x3*cos(xtime)) - (38.63656433142*x4*tan(xtime)) + (13.18212824804*x5*sin(xtime)) - (2.045229508597*x6*cos(xtime)) + (45.1292360492*x7*tan(xtime)) + (26.03603502163*x8*sin(xtime)) + (21.98935009253*x9*cos(xtime)) + (39.25956682671*x10*tan(xtime));
...starting generation of constant selector WFFs.
...starting regress selector for each column.
...starting a simple SVM selector WFF.
...starting generation of root selector WFFs.
...starting a best-of-breed SVM selector WFF.
...locating the best selector and the best regressor from this training run.

Final results of training
esm: N = [500], M = [10], Generations = [21], WFFs = [26266], Score=[0.9911268261292], ScoreHistory=[#(num| 0.9911268261292 )]

Show final results on training data.
X[0], ey=[-1363774.693723], y=[-1375362.762508], ErrorPct=[0.008425463521875]
X[500], ey=[-17349.50930215], y=[-13448.78913555], ErrorPct=[0.2900424809464]
X[1000], ey=[-8905.237990786], y=[-5545.307800046], ErrorPct=[0.6059050844234]
X[1500], ey=[-4128.692628519], y=[-3308.049582962], ErrorPct=[0.2480745904727]
X[2000], ey=[-4852.835019633], y=[-2398.31127248], ErrorPct=[1.023438356529]
X[2500], ey=[191.1819910936], y=[-1866.148891692], ErrorPct=[1.102447340587]
X[3000], ey=[-157.7516005295], y=[-1468.963024363], ErrorPct=[0.8926102305414]
X[3500], ey=[-581.0716045963], y=[-1070.256775494], ErrorPct=[0.4570727157245]
X[4000], ey=[-1587.91893128], y=[-704.6452138374], ErrorPct=[1.253501336697]
X[4500], ey=[-643.1456133688], y=[-378.5644419159], ErrorPct=[0.6989065589835]
X[5000], ey=[1089.920866978], y=[-63.37984131596], ErrorPct=[18.19664871901]
X[5500], ey=[-1456.525418802], y=[256.6557957738], ErrorPct=[6.675014719268]
X[6000], ey=[-841.6051292026], y=[579.4262635109], ErrorPct=[2.452480120772]
X[6500], ey=[-1001.238158853], y=[923.938265989], ErrorPct=[2.083663482409]
X[7000], ey=[1148.725681029], y=[1300.541524679], ErrorPct=[0.1167327922787]
X[7500], ey=[6309.000498483], y=[1702.345261327], ErrorPct=[2.70606400582]
X[8000], ey=[-1000.619044082], y=[2230.046548377], ErrorPct=[1.448698725509]
X[8500], ey=[1999.432204919], y=[3062.420135702], ErrorPct=[0.3471071517558]
X[9000], ey=[7716.825031478], y=[5189.293726794], ErrorPct=[0.4870665330879]
X[9500], ey=[10778.69885257], y=[12268.54402301], ErrorPct=[0.1214361840859]
Actual computed error on training data is ErrorPct=[8.833855823745] versus reported ErrorPct=[8.833855823719]


Final testing on test data returns Score=[0.9749594370726]
X[0], ey=[-15591.64032099], y=[-14130.82057838], ErrorPct=[0.103378266995]
X[25], ey=[-8520.636864502], y=[-8423.338542127], ErrorPct=[0.01155104023043]
X[50], ey=[-9095.743534773], y=[-6683.472633956], ErrorPct=[0.3609307665242]
X[75], ey=[-6443.779712418], y=[-4946.50047313], ErrorPct=[0.3026946519912]
X[100], ey=[-4178.110794209], y=[-4014.136918831], ErrorPct=[0.04084909874609]
X[125], ey=[-3770.899447551], y=[-3142.218580129], ErrorPct=[0.2000754725968]
X[150], ey=[-967.321935149], y=[-2438.191438348], ErrorPct=[0.603262516661]
X[175], ey=[-1666.406533345], y=[-1865.945882325], ErrorPct=[0.106937372016]
X[200], ey=[112.3667088861], y=[-1280.499991293], ErrorPct=[1.087752213706]
X[225], ey=[-1807.693487039], y=[-605.4326797663], ErrorPct=[1.98578776378]
X[250], ey=[-413.012118513], y=[30.89556852886], ErrorPct=[14.36800512757]
X[275], ey=[2171.774097773], y=[726.4655616223], ErrorPct=[1.989507297391]
X[300], ey=[1730.522612779], y=[1421.028928923], ErrorPct=[0.2177954843545]
X[325], ey=[856.3197786302], y=[2022.907261926], ErrorPct=[0.5766885636592]
X[350], ey=[2584.825912967], y=[2904.073915692], ErrorPct=[0.1099310871531]
X[375], ey=[3195.224088673], y=[3729.465923294], ErrorPct=[0.1432488848562]
X[400], ey=[5907.175852199], y=[4738.599770458], ErrorPct=[0.2466078880572]
X[425], ey=[6214.081364008], y=[5621.61726765], ErrorPct=[0.1053903295352]
X[450], ey=[5727.516960004], y=[6671.118094573], ErrorPct=[0.1414457248683]
X[475], ey=[10006.54502602], y=[8273.813528861], ErrorPct=[0.2094235615919]
Actual computed error on testing data is ErrorPct=[1.068852801293]

esm.selfTest: completed in [29.19818333333] minutes.

            </pre>
			</blockquote>
		]]></Description>
	</Section>
	<Section>
	    <Heading>Cycle Dependent Regression (20% Random Noise - Ten Variables)</Heading>
		<Description><![CDATA[
			<p>We ran the self test known as cyclicSeries in this SelfTest log.
            This training set was 10,000 elements distributed over 20 time periods. The testing data set was 500 elements in time period 21.
            The cyclic series regression problem had ten independent variables and was trained for one hundred generations.
            The average percent error on the training data was 311%, and the selector score was 95%.
            On the testing data, the average percent error was 349%, and the selector score was also 94%. 
            The training time was 40 minutes. 
            </p>

			<blockquote>
		    <pre>

(esm.verboseON)(esm.selfTest cyclicSeries: 20 10 500 100 100% checkpoint:)

Starting test case: cyclicSeries
Building test data as: y = -9.165146942478 - (9.165146942478*x0*sin(x0)) - (19.5666514757*x1*cos(x0)) + (21.87460482304*x2*tan(x0)) - (17.48124453288*x3*sin(x0)) + (38.81839452492*x4*cos(x0)) - (38.63656433142*x5*tan(x0)) + (13.18212824804*x6*sin(x0)) - (2.045229508597*x7*cos(x0)) + (45.1292360492*x8*tan(x0)) + (26.03603502163*x9*sin(x0));
Additionally, we add random error as: y = (y * 0.8) + (random 0.4);
...starting new evolutionary process.
...starting a best-of-breed MVL selector WFF.
...starting generation of random selector WFFs.

Final results of training
esm: N = [500], M = [10], Generations = [100], WFFs = [2392], Score=[0.9536374356331], ScoreHistory=[#(num| 0.9536374356331 )]

Show final results on training data, Score=[0.9536374356331]
X[0], ey=[-1021777.925635], y=[-1024937.036537], ErrorPct=[0.003082248752647]
X[500], ey=[-4483.476129939], y=[-9773.816980605], ErrorPct=[0.5412768482532]
X[1000], ey=[-3528.588934626], y=[-4734.189061328], ErrorPct=[0.2546582130717]
X[1500], ey=[-4947.563815254], y=[-2741.356998936], ErrorPct=[0.8047863949038]
X[2000], ey=[820.6485870431], y=[-2014.765351952], ErrorPct=[1.407317202595]
X[2500], ey=[4444.802166782], y=[-1551.93614668], ErrorPct=[3.864036755823]
X[3000], ey=[-17.23712673685], y=[-1214.215685637], ErrorPct=[0.9858039004596]
X[3500], ey=[1765.250322921], y=[-852.3491808088], ErrorPct=[3.071041261806]
X[4000], ey=[281.0756905509], y=[-546.3941850847], ErrorPct=[1.51441925669]
X[4500], ey=[698.4894660466], y=[-166.432262292], ErrorPct=[5.196839341289]
X[5000], ey=[52.81240447232], y=[202.0334096688], ErrorPct=[0.7385956879167]
X[5500], ey=[444.3919904601], y=[475.5120027448], ErrorPct=[0.0654452718439]
X[6000], ey=[-449.8137394823], y=[835.4455784947], ErrorPct=[1.538411778171]
X[6500], ey=[499.2260933349], y=[1127.0717718], ErrorPct=[0.5570591812998]
X[7000], ey=[1001.029993127], y=[1395.835859137], ErrorPct=[0.282845481742]
X[7500], ey=[266.7345621041], y=[1750.870197658], ErrorPct=[0.8476560041625]
X[8000], ey=[2414.841192803], y=[2253.533779632], ErrorPct=[0.07157976269522]
X[8500], ey=[5151.2139101], y=[3165.207691911], ErrorPct=[0.627448942218]
X[9000], ey=[5267.140712911], y=[5084.541523732], ErrorPct=[0.03591261637391]
X[9500], ey=[-3697.275512855], y=[12510.20723974], ErrorPct=[1.295540708639]
Actual computed error on training data is ErrorPct=[3.111358108758] versus reported ErrorPct=[3.111358108756]


Final testing on test data returns Score=[0.9489645992764]
X[0], ey=[-128544.2771458], y=[-124201.1295381], ErrorPct=[0.03496866432546]
X[25], ey=[-5087.455928916], y=[-11708.6792366], ErrorPct=[0.5654970277936]
X[50], ey=[-3116.375102743], y=[-4577.939792496], ErrorPct=[0.3192625407937]
X[75], ey=[167.6665939178], y=[-2851.651130517], ErrorPct=[1.058796320533]
X[100], ey=[-4008.334085871], y=[-2034.338308002], ErrorPct=[0.9703380062715]
X[125], ey=[930.2975756272], y=[-1502.105850924], ErrorPct=[1.619328907517]
X[150], ey=[40.80119930855], y=[-1024.530167446], ErrorPct=[1.0398243025]
X[175], ey=[-500.9908245318], y=[-787.9442378846], ErrorPct=[0.3641798487203]
X[200], ey=[-168.7968856838], y=[-526.9949650224], ErrorPct=[0.6796992440399]
X[225], ey=[1827.793344069], y=[-297.930618511], ErrorPct=[7.134963076988]
X[250], ey=[-633.6144786958], y=[87.58875452875], ErrorPct=[8.233970640465]
X[275], ey=[770.7458687625], y=[449.1817587149], ErrorPct=[0.7158886214963]
X[300], ey=[-574.2131520707], y=[877.0752604536], ErrorPct=[1.654690854892]
X[325], ey=[1536.681621044], y=[1143.703149587], ErrorPct=[0.3436018092626]
X[350], ey=[910.3026959833], y=[1543.207097808], ErrorPct=[0.4101227908577]
X[375], ey=[-269.7042664736], y=[1993.748514877], ErrorPct=[1.135274967962]
X[400], ey=[3182.258289575], y=[2842.154821905], ErrorPct=[0.1196639483005]
X[425], ey=[1777.112127462], y=[3750.151177889], ErrorPct=[0.5261225366219]
X[450], ey=[1445.283352716], y=[6025.534239412], ErrorPct=[0.7601402140805]
X[475], ey=[9236.020924487], y=[9229.935060505], ErrorPct=[0.000659361516909]
Actual computed error on testing data is ErrorPct=[3.497904871389]

esm.selfTest: completed in [40.02631666667] minutes.

            </pre>
			</blockquote>

	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Cycle Dependent Regression (20% Random Noise - Ten Variables)</Heading>
		<Description><![CDATA[
			<p>We ran the self test known as cyclicSeries in this SelfTest log for an extended training run.
            This training set was 10,000 elements distributed over 20 time periods. The testing data set was 500 elements in time period 21.
            The cyclic series regression problem had ten independent variables and was trained for one thousand generations.
            The average percent error on the training data was 233%, and the selector score was 98%.
            On the testing data, the average percent error was 123%, and the selector score was also 98%. 
            The training time was 406 minutes. 
            </p>

			<blockquote>
		    <pre>

(esm.verboseON)(esm.selfTest cyclicSeries: 20 10 500 1000 100% checkpoint:)

Starting test case: cyclicSeries
Building test data as: y = -9.165146942478 - (9.165146942478*x0*sin(x0)) - (19.5666514757*x1*cos(x0)) + (21.87460482304*x2*tan(x0)) - (17.48124453288*x3*sin(x0)) + (38.81839452492*x4*cos(x0)) - (38.63656433142*x5*tan(x0)) + (13.18212824804*x6*sin(x0)) - (2.045229508597*x7*cos(x0)) + (45.1292360492*x8*tan(x0)) + (26.03603502163*x9*sin(x0));
Additionally, we add random error as: y = (y * 0.8) + (random 0.4);
...starting new evolutionary process.
...starting a best-of-breed MVL selector WFF.
...starting generation of random selector WFFs.

Final results of training
esm: N = [500], M = [10], Generations = [1000], WFFs = [3658], Score=[0.9868305026246], ScoreHistory=[#(num| 0.9868305026246 )]

Show final results on training data, Score=[0.9868305026246]
X[0], ey=[-1030387.885584], y=[-1024937.036537], ErrorPct=[0.005318228195902]
X[500], ey=[-9951.968398611], y=[-9773.816980605], ErrorPct=[0.01822741497603]
X[1000], ey=[-5026.077277652], y=[-4734.189061328], ErrorPct=[0.06165537804754]
X[1500], ey=[-2389.50105742], y=[-2741.356998936], ErrorPct=[0.1283510107048]
X[2000], ey=[152.5281842689], y=[-2014.765351952], ErrorPct=[1.075705185282]
X[2500], ey=[-2426.423652351], y=[-1551.93614668], ErrorPct=[0.5634816274761]
X[3000], ey=[-321.4679325519], y=[-1214.215685637], ErrorPct=[0.7352464340936]
X[3500], ey=[-1750.625794543], y=[-852.3491808088], ErrorPct=[1.053883354334]
X[4000], ey=[-770.2560062024], y=[-546.3941850847], ErrorPct=[0.4097075467284]
X[4500], ey=[4.037444566292], y=[-166.432262292], ErrorPct=[1.024258785591]
X[5000], ey=[-41.75577288999], y=[202.0334096688], ErrorPct=[1.206677563669]
X[5500], ey=[376.8685300024], y=[475.5120027448], ErrorPct=[0.207446861852]
X[6000], ey=[-122.2949251977], y=[835.4455784947], ErrorPct=[1.146382874415]
X[6500], ey=[-175.2483823423], y=[1127.0717718], ErrorPct=[1.155489993386]
X[7000], ey=[391.5756677349], y=[1395.835859137], ErrorPct=[0.7194686859693]
X[7500], ey=[28.98547329412], y=[1750.870197658], ErrorPct=[0.9834451044213]
X[8000], ey=[1897.957254799], y=[2253.533779632], ErrorPct=[0.1577861969705]
X[8500], ey=[2791.831698307], y=[3165.207691911], ErrorPct=[0.117962557262]
X[9000], ey=[7701.353302604], y=[5084.541523732], ErrorPct=[0.5146603222057]
X[9500], ey=[10309.16392174], y=[12510.20723974], ErrorPct=[0.1759397966655]
Actual computed error on training data is ErrorPct=[2.335759377277] versus reported ErrorPct=[2.335759377275]


Final testing on test data returns Score=[0.9806423241556]
X[0], ey=[-115451.2602401], y=[-124201.1295381], ErrorPct=[0.07044919261671]
X[25], ey=[-9838.523613893], y=[-11708.6792366], ErrorPct=[0.1597238753338]
X[50], ey=[-4194.544848452], y=[-4577.939792496], ErrorPct=[0.08374835874245]
X[75], ey=[-66.62102060797], y=[-2851.651130517], ErrorPct=[0.9766377380827]
X[100], ey=[-1455.414907781], y=[-2034.338308002], ErrorPct=[0.2845757748078]
X[125], ey=[887.2675655723], y=[-1502.105850924], ErrorPct=[1.590682450925]
X[150], ey=[42.27356990154], y=[-1024.530167446], ErrorPct=[1.041261420351]
X[175], ey=[-3.768977131112], y=[-787.9442378846], ErrorPct=[0.9952166956113]
X[200], ey=[177.543440904], y=[-526.9949650224], ErrorPct=[1.336897793504]
X[225], ey=[1293.830523561], y=[-297.930618511], ErrorPct=[5.342724255825]
X[250], ey=[-258.6390532574], y=[87.58875452875], ErrorPct=[3.952879677865]
X[275], ey=[601.9472850416], y=[449.1817587149], ErrorPct=[0.3400973511564]
X[300], ey=[3.474482138168], y=[877.0752604536], ErrorPct=[0.9960385587249]
X[325], ey=[-542.693525946], y=[1143.703149587], ErrorPct=[1.474505579653]
X[350], ey=[802.4738362764], y=[1543.207097808], ErrorPct=[0.479996017763]
X[375], ey=[-212.1186601163], y=[1993.748514877], ErrorPct=[1.106391883697]
X[400], ey=[2069.361573499], y=[2842.154821905], ErrorPct=[0.2719039942688]
X[425], ey=[3500.670810997], y=[3750.151177889], ErrorPct=[0.06652541592516]
X[450], ey=[5784.617586656], y=[6025.534239412], ErrorPct=[0.03998262115578]
X[475], ey=[12627.53580548], y=[9229.935060505], ErrorPct=[0.3681066792678]
Actual computed error on testing data is ErrorPct=[1.239902944162]

esm.selfTest: completed in [406.1893166667] minutes.
true

            </pre>
			</blockquote>

	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Cycle Dependent Regression (20% Random Noise - One Variable)</Heading>
		<Description><![CDATA[
			<p>We ran the self test known as cyclicSeries in this SelfTest log.
            This training set was 10,000 elements distributed over 20 time periods. The testing data set was 500 elements in time period 21.
            The cyclic series regression problem had one independent variable and was trained for one hundred generations.
            The average percent error on the training data was 10.89%, and the selector score was 100%.
            On the testing data, the average percent error was 20.8%, and the selector score was also 100%. 
            The training time was 16 minutes. 
            </p>

			<blockquote>
		    <pre>

(esm.verboseON)(esm.selfTest cyclicSeries: 20 1 500 100 95% checkpoint:)

Starting test case: cyclicSeries
Building test data as: y = -9.165146942478 - (9.165146942478*x0*sin(x0));
Additionally, we add random error as: y = (y * 0.8) + (random 0.4);
...starting new evolutionary process.
...starting generation of random selector WFFs.
...locating the best selector and the best regressor from this training run.

Final results of training
esm: N = [500], M = [1], Generations = [0], WFFs = [793], Score=[1.006816272303], ScoreHistory=[#(num| 0.9979056891189 )]

Show final results on training data, Score=[1.006816272303]
X[0], ey=[-427.6182084951], y=[-506.9782643317], ErrorPct=[0.1565354205889]
X[500], ey=[-307.7902799242], y=[-325.4068558196], ErrorPct=[0.05413707664845]
X[1000], ey=[-255.527758913], y=[-257.9804685534], ErrorPct=[0.00950734625057]
X[1500], ey=[-242.8483250453], y=[-200.9154043552], ErrorPct=[0.2087093362736]
X[2000], ey=[-194.3474723665], y=[-157.7609266026], ErrorPct=[0.2319113265358]
X[2500], ey=[-107.5728164173], y=[-122.026327275], ErrorPct=[0.1184458401759]
X[3000], ey=[-107.1230185854], y=[-90.07507300321], ErrorPct=[0.1892637442728]
X[3500], ey=[-50.67575314912], y=[-60.00700279038], ErrorPct=[0.1555026781433]
X[4000], ey=[-30.55491873375], y=[-32.38969431089], ErrorPct=[0.05664689390186]
X[4500], ey=[-24.38133847832], y=[-21.19309281874], ErrorPct=[0.1504379604642]
X[5000], ey=[-12.19907631577], y=[-10.91480044338], ErrorPct=[0.1176637061809]
X[5500], ey=[5.327362752015], y=[5.601717365505], ErrorPct=[0.0489768754096]
X[6000], ey=[25.98236699818], y=[26.9483102306], ErrorPct=[0.0358442968837]
X[6500], ey=[44.62551819287], y=[47.50131773271], ErrorPct=[0.06054146868145]
X[7000], ey=[92.92167218496], y=[81.40759008087], ErrorPct=[0.1414374518722]
X[7500], ey=[129.9745064425], y=[121.9276990494], ErrorPct=[0.06599654923171]
X[8000], ey=[165.2761425368], y=[164.526875016], ErrorPct=[0.004554073738647]
X[8500], ey=[228.5039765556], y=[205.417055358], ErrorPct=[0.1123904787626]
X[9000], ey=[312.6812709789], y=[253.5713976278], ErrorPct=[0.233109388141]
X[9500], ey=[350.8422730084], y=[324.1865089111], ErrorPct=[0.08222354528808]
Actual computed error on training data is ErrorPct=[0.1089416815593] versus reported ErrorPct=[0.1089416815592]


Final testing on test data returns Score=[1.0]
X[0], ey=[-429.5762926448], y=[-504.468951844], ErrorPct=[0.1484584114155]
X[25], ey=[-311.6964465494], y=[-315.9856984548], ErrorPct=[0.01357419632079]
X[50], ey=[-253.5554224639], y=[-245.3901859345], ErrorPct=[0.03327450320898]
X[75], ey=[-191.726382307], y=[-190.8939319332], ErrorPct=[0.004360800604633]
X[100], ey=[-137.2710292195], y=[-153.5216225652], ErrorPct=[0.1058521469105]
X[125], ey=[-134.4310125008], y=[-121.6553376754], ErrorPct=[0.1050153250119]
X[150], ey=[-81.07204762472], y=[-72.02273336397], ErrorPct=[0.1256452489107]
X[175], ey=[-44.14497401221], y=[-52.3375131604], ErrorPct=[0.1565328318729]
X[200], ey=[-30.48183145157], y=[-25.38470140429], ErrorPct=[0.2007953517395]
X[225], ey=[-9.611858391581], y=[-11.14855738027], ErrorPct=[0.137838371035]
X[250], ey=[0.6697740410197], y=[0.4641264142951], ErrorPct=[0.4430853758602]
X[275], ey=[17.30123909837], y=[18.74961578631], ErrorPct=[0.07724833961618]
X[300], ey=[35.39259112192], y=[34.90209278805], ErrorPct=[0.01405355079573]
X[325], ey=[82.11114041793], y=[66.28020100262], ErrorPct=[0.2388486935137]
X[350], ey=[82.73637869504], y=[90.90025647488], ErrorPct=[0.08981138333861]
X[375], ey=[148.5845659846], y=[137.0874559316], ErrorPct=[0.08386697364005]
X[400], ey=[171.514087292], y=[183.5839612851], ErrorPct=[0.06574579777345]
X[425], ey=[242.9632608865], y=[231.0925715838], ErrorPct=[0.05136768015242]
X[450], ey=[314.7003771462], y=[284.7935027532], ErrorPct=[0.1050124883608]
X[475], ey=[342.3524388714], y=[349.2146878608], ErrorPct=[0.0196505165101]
Actual computed error on testing data is ErrorPct=[0.2080051678402]

esm.selfTest: completed in [1.2362] minutes.

            </pre>
			</blockquote>

	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Mixed Model Regression (20% Random Noise - Ten Variables)</Heading>
		<Description><![CDATA[
			<p>We ran the self test known asmixedModels in this SelfTest log for an extended training run.
            This training set was 10,000 elements distributed over 20 time periods. The testing data set was 500 elements in time period 21.
            The mixed models regression problem had ten independent variables and was trained for one thousand generations.
            The average percent error on the training data was 50536%, and the selector score was 80%.
            On the testing data, the average percent error was 3248%, and the selector score was 00%. 
            The training time was 424 minutes. 
            </p>

			<blockquote>
		    <pre>

(esm.verboseON)(esm.selfTest mixedModels: 20 10 500 1000 100% checkpoint:)

Starting test case: mixedModels
Building test data as:
if ((xtime % 4) == 0) y = - (9.165146942478*x0) - (19.5666514757*x1) + (21.87460482304*x2) - (17.48124453288*x3) + (38.81839452492*x4) - (38.63656433142*x5) + (13.18212824804*x6) - (2.045229508597*x7) + (45.1292360492*x8) + (26.03603502163*x9);
if ((xtime % 4) == 1) y = - (9.165146942478*x0*x0) - (19.5666514757*x1*x1) + (21.87460482304*x2*x2) - (17.48124453288*x3*x3) + (38.81839452492*x4*x4) - (38.63656433142*x5*x5) + (13.18212824804*x6*x6) - (2.045229508597*x7*x7) + (45.1292360492*x8*x8) + (26.03603502163*x9*x9);
if ((xtime % 4) == 2) y = - (9.165146942478* sin(x0)) - (19.5666514757* sin(x1)) + (21.87460482304* sin(x2)) - (17.48124453288* sin(x3)) + (38.81839452492* sin(x4)) - (38.63656433142* sin(x5)) + (13.18212824804* sin(x6)) - (2.045229508597* sin(x7)) + (45.1292360492* sin(x8)) + (26.03603502163* sin(x9));
if ((xtime % 4) == 3) y = - (9.165146942478* log(.000001+abs(x0))) - (19.5666514757* log(.000001+abs(x1))) + (21.87460482304* log(.000001+abs(x2))) - (17.48124453288* log(.000001+abs(x3))) + (38.81839452492* log(.000001+abs(x4))) - (38.63656433142* log(.000001+abs(x5))) + (13.18212824804* log(.000001+abs(x6))) - (2.045229508597* log(.000001+abs(x7))) + (45.1292360492* log(.000001+abs(x8))) + (26.03603502163* log(.000001+abs(x9)));
Additionally, we add random error as: y = (y * 0.8) + (random 0.4);
...starting new evolutionary process.
...starting a best-of-breed MVL selector WFF.
...starting generation of random selector WFFs.

Final results of training
esm: N = [500], M = [10], Generations = [1000], WFFs = [402], Score=[0.8035448886348], ScoreHistory=[#(num| 0.8035448886348 )]

Show final results on training data, Score=[0.8035448886348]
X[0], ey=[-32597.88243325], y=[-147284.662848], ErrorPct=[0.7786742909756]
X[500], ey=[-1046.763113004], y=[-6570.500014129], ErrorPct=[0.8406874498511]
X[1000], ey=[39166.4907362], y=[-2276.962399079], ErrorPct=[18.20120224736]
X[1500], ey=[-2505.245595347], y=[-771.5552339419], ErrorPct=[2.247007453437]
X[2000], ey=[-5621.604017633], y=[-80.86076042705], ErrorPct=[68.5220275934]
X[2500], ey=[14237.91684146], y=[-37.38012611882], ErrorPct=[381.8953665966]
X[3000], ey=[17374.89436088], y=[-6.710495068673], ErrorPct=[2590.212000466]
X[3500], ey=[7879.270880261], y=[20.78444779664], ErrorPct=[378.0945497976]
X[4000], ey=[5858.201014608], y=[46.45071253512], ErrorPct=[125.1164941265]
X[4500], ey=[-17132.00711641], y=[84.39001125974], ErrorPct=[204.0098925296]
X[5000], ey=[24047.05128093], y=[126.7475591102], ErrorPct=[188.7239792998]
X[5500], ey=[-1775.21180529], y=[165.7180737949], ErrorPct=[11.71224015968]
X[6000], ey=[-7254.115537802], y=[203.7209962901], ErrorPct=[36.60808983808]
X[6500], ey=[24473.39276444], y=[254.0801951847], ErrorPct=[95.32152851049]
X[7000], ey=[37201.6380334], y=[459.9116701141], ErrorPct=[79.88865852038]
X[7500], ey=[121.7070494704], y=[1817.01090678], ErrorPct=[0.9330179862893]
X[8000], ey=[67612.90428292], y=[4005.337457948], ErrorPct=[15.88070106272]
X[8500], ey=[14889.96003386], y=[30149.4859814], ErrorPct=[0.5061288924447]
X[9000], ey=[9769.747919464], y=[62347.96279989], ErrorPct=[0.8433028525596]
X[9500], ey=[16489.2905917], y=[102730.4162725], ErrorPct=[0.8394896936077]
Actual computed error on training data is ErrorPct=[505.3691049917] versus reported ErrorPct=[505.3691049881]


Final testing on test data returns Score=[0.0]
X[0], ey=[4812.89290625], y=[-8488.049098751], ErrorPct=[1.567019918271]
X[25], ey=[38847.67623433], y=[-3830.006284664], ErrorPct=[11.14297976217]
X[50], ey=[7414.348054674], y=[-3040.41073662], ErrorPct=[3.438600800008]
X[75], ey=[22649.36134591], y=[-2365.440306462], ErrorPct=[10.57511431764]
X[100], ey=[43758.85519317], y=[-1891.00487813], ErrorPct=[24.14053004265]
X[125], ey=[7001.101979928], y=[-1622.202418647], ErrorPct=[5.315800481771]
X[150], ey=[15166.65300329], y=[-1322.378704244], ErrorPct=[12.46922054523]
X[175], ey=[7939.249311458], y=[-1017.610017436], ErrorPct=[8.801858448157]
X[200], ey=[2256.418268778], y=[-565.7443713525], ErrorPct=[4.988406041731]
X[225], ey=[14882.42517168], y=[-134.4781470489], ErrorPct=[111.6679821092]
X[250], ey=[4370.096295289], y=[126.4518341456], ErrorPct=[33.55937452246]
X[275], ey=[1760.025838243], y=[409.6319071121], ErrorPct=[3.29660338388]
X[300], ey=[-4741.132012577], y=[783.7084577302], ErrorPct=[7.049611900716]
X[325], ey=[15802.1857114], y=[1107.199990813], ErrorPct=[13.27220542136]
X[350], ey=[3258.27407803], y=[1655.621227236], ErrorPct=[0.9680069477419]
X[375], ey=[13919.77508376], y=[1946.057743713], ErrorPct=[6.152806810965]
X[400], ey=[8323.170088229], y=[2409.684335566], ErrorPct=[2.454049962223]
X[425], ey=[13025.6547548], y=[2844.257418189], ErrorPct=[3.579632867089]
X[450], ey=[5549.721532291], y=[3467.732911693], ErrorPct=[0.6003889785102]
X[475], ey=[29825.52360295], y=[4302.293724524], ErrorPct=[5.932470331567]
Actual computed error on testing data is ErrorPct=[32.48176037513]

esm.selfTest: completed in [424.675] minutes.
true

            </pre>
			</blockquote>

	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Mixed Model Regression (20% Random Noise - Ten Variables)</Heading>
		<Description><![CDATA[
			<p>We ran the self test known asmixedModels in this SelfTest log for an extended training run.
            This training set was 10,000 elements distributed over 20 time periods. The testing data set was 500 elements in time period 21.
            The mixed models regression problem had ten independent variables and was trained for two hundred generations.
            The average percent error on the training data was 79338%, and the selector score was 69%.
            On the testing data, the average percent error was 3533%, and the selector score was 00%. 
            The training time was 424 minutes. 
            </p>

			<blockquote>
		    <pre>

        ;; Begin User Defined Learning Options
        (myBestAverage 1)  	            ;; The count of best selector champions to average in creating the final myBest selector Lambda.
        (Integer:myBGMMaximum 0003)     ;; The maximum number of chromosomes to allowed in any one BGM regression.
        (Number:myCrossColPct 0.00)  	;; The probability of cross over in each column island during each generation step.
        (Number:myCrossRegPct 0.00)  	;; The probability of cross over in the best-regressor island during each generation step.
        (Number:myCrossSelPct 1.00)  	;; The probability of cross over in the best-of-breed island during each generation step.
        (Integer:myFRMMaximum 1000)     ;; The maximum number of chromosomes to allowed in any one FRM regression.
        (Integer:myGreedyDepth 00)    	;; The depth of greedy search exploration of each chosen column island  during each generation step. 
        (Integer:myGreedyGEN  000)    	;; The number of greedy search WFFs the system will grow in the best-of-breed island  during each generation step. 
        (myGreedyWidth narrow)    	    ;; The width of each greedy search WFF the system will grow in the best-of-breed island (base, full, narrow). 
        (myGreedyRule MVL:)          	;; The rule used for greedy search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
        (Integer:myGrowColINIT 00)   	;; The number of random WFFs the system will grow in each column island during the initialization step.
        (Integer:myGrowColGEN 00)    	;; The number of random WFFs the system will grow in each column island during each generation step. 
        (myGrowColRule REG:)         	;; The rule used for random WFFs grown in each column island (BGM, FRM, MVL, ENN, REG, or SVM). 
        (Integer:myGrowSelINIT 5000)   	;; The number of random WFFs the system will grow in the best-of-breed island during the initialization step.
        (Integer:myGrowSelGEN 00)    	;; The number of random WFFs the system will grow in the best-of-breed island during each generation step. 
        (myGrowSelRule REG:)         	;; The rule used for random WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
        (myGrowWFFStyle root:)       	;; The style used for random WFFs grown in the best-of-breed island (full, or root). 
        (Integer:myMaxColWFFLen 05)  	;; The maximum length of a single WFF for each column.
        (Number:myMigratePct 0.00)   	;; The probability of Column island mutation during each generation step.
        (Number:myMutateColPct 0.00) 	;; The probability of mutation in each column island during each generation step.
        (Number:myMutateRegPct 0.00) 	;; The probability mutation in the best-regressor island during each generation step.
        (Number:myMutateSelPct 1.00) 	;; The probability mutation in the best-of-breed island during each generation step.
        (Integer:myMVLMaximum 1000)     ;; The maximum number of chromosomes to allowed in any one MVL regression.
        (Integer:myENNMaximum 0003)     ;; The maximum number of chromosomes to allowed in any one ENN regression.
        (myRandomError .40)          	;; The amount of random error to add to each selfTest test case (0.00 thru .50).
        (myREGMultiple true)            ;; Support multiple columns when growing REG regression expressions.
        (Integer:myREGMaximum 1000)     ;; The maximum number of chromosomes to allowed in any one REG regression.
        (Integer:myRestartGap 00010) 	;; The count of generations to allow, without measurable improvement, before initiating a new evolutionary process.
        (Integer:myRootINIT 00)  	 	;; The number of root WFFs the system will grow in each column island during the initialization step.
        (Integer:myRootGEN 00)       	;; The number of root WFFs the system will grow in each column island during each generation step. 
        (myRootRule REG:)            	;; The rule used for root WFFs grown in each column island (BGM, FRM, MVL, ENN, REG, or SVM). 
        (mySamplingON  true)   	     	;; Support initial scoring on a small sample set before scoring on the full training data set.
        (myScoreStyle sequence:)        ;; The style of scoring the sequencing of the training data (errorpct, sequence, or combined).
        (mySeedDefault 2407987.0)    	;; The default seed number used to initialize the random number genenerator at the start of each training run.
        (mySequenceFocus 005)        	;; The focus of scoring the sequencing of the training data (05, 10, 15, 20, 25, 30, 35, 40, 45, 50, or 100).
        (mySequenceStyle high:)         ;; The style of scoring the sequencing of the training data (average, low, high, or contrast).
        (Integer:mySurvivors  25)     	;; The count of surviving selector WFFs in each generation.
        (mySurvivorStyle equalOFF:)  	;; The option allowing surviving selector WFFs to have equal scores (equalON, or equalOFF).
        (mySvmKernelID cube:)       	;; The kernelID to use for all SVM learning (see svmRegress: all, binary, bipolar, composite, cosine, cube, exp, linear, log, poly, quart, sigmoid, sine, square, tan, tanh).
        (Integer:mySVMMaximum 1000)     ;; The maximum number of chromosomes to allowed in any one SVM regression.
        (mySvmMaxGen 1)             	;; The maximum number of training generations to use for all SVM learning (see svmRegress).
        (mySvmMaxLayers 1)             	;; The maximum number of training layers to use for all SVM learning (see svmRegress).
        (mySvmModelCount 1)             ;; The maximum number of training models to use for all SVM learning (see svmRegress).
        (Integer:myTournamentSize 05)	;; The size of the myBestSelectorChampions queue which initiates a tournament-of-champions when the next evolutionary process begins.
        (myTimeON  true)  			 	;; Support separate sequence scoring of each individual time period.
        (myUseBGM false)             	;; Grow BGM selector candidates during the initialization step and during each generation step.
        (myUseFRM false)             	;; Grow FRM selector candidates during the initialization step and during each generation step.
        (myUseMVL false)             	;; Grow MVL selector candidates during the initialization step and during each generation step.
        (myUseENN false)             	;; Grow ENN selector candidates during the initialization step and during each generation step.
        (myUseREG  true)             	;; Grow REG selector candidates during the initialization step and during each generation step.
        (myUseSVM false)             	;; Grow SVM selector candidates during the initialization step and during each generation step.
        (myVerboseSW false)		     	;; The switch controlling verbose mode during testing, maintennance, and development.
        ;; End User Defined Learning Options

(esm.verboseON)(esm.selfTest mixedModels: 260 30 500 25 00% checkpoint:)

Starting test case: mixedModels
Building test data as:
if ((xtime % 4) == 0) y = - (19.5666514757*x1) + (21.87460482304*x2) - (17.48124453288*x3) + (38.81839452492*x4) - (38.63656433142*x5) + (13.18212824804*x6) - (2.045229508597*x7) + (45.1292360492*x8) + (26.03603502163*x9) + (21.98935009253*x10) + (39.25956682671*x11) - (36.9527398792*x12) + (8.601886956836*x13) + (7.533313498536*x14) - (23.03633770636*x15) - (41.11091670539*x16) + (46.07594342939*x17) - (4.158230358161*x18) + (35.61243398897*x19) - (0.8539648928299*x20) + (48.46608517312*x21) + (21.92818922469*x22) - (5.51333900824*x23) - (3.155879615049*x24) - (35.40602155255*x25) + (21.05316767957*x26) + (48.63345254591*x27) + (18.09100857528*x28) - (45.97902827701*x29) + (18.31189165581*x30);
if ((xtime % 4) == 1) y = - (19.5666514757*x1*x1) + (21.87460482304*x2*x2) - (17.48124453288*x3*x3) + (38.81839452492*x4*x4) - (38.63656433142*x5*x5) + (13.18212824804*x6*x6) - (2.045229508597*x7*x7) + (45.1292360492*x8*x8) + (26.03603502163*x9*x9) + (21.98935009253*x10*x10) + (39.25956682671*x11*x11) - (36.9527398792*x12*x12) + (8.601886956836*x13*x13) + (7.533313498536*x14*x14) - (23.03633770636*x15*x15) - (41.11091670539*x16*x16) + (46.07594342939*x17*x17) - (4.158230358161*x18*x18) + (35.61243398897*x19*x19) - (0.8539648928299*x20*x20) + (48.46608517312*x21*x21) + (21.92818922469*x22*x22) - (5.51333900824*x23*x23) - (3.155879615049*x24*x24) - (35.40602155255*x25*x25) + (21.05316767957*x26*x26) + (48.63345254591*x27*x27) + (18.09100857528*x28*x28) - (45.97902827701*x29*x29) + (18.31189165581*x30*x30);
if ((xtime % 4) == 2) y = - (19.5666514757*sin(x1)) + (21.87460482304*sin(x2)) - (17.48124453288*sin(x3)) + (38.81839452492*sin(x4)) - (38.63656433142*sin(x5)) + (13.18212824804*sin(x6)) - (2.045229508597*sin(x7)) + (45.1292360492*sin(x8)) + (26.03603502163*sin(x9)) + (21.98935009253*sin(x10)) + (39.25956682671*sin(x11)) - (36.9527398792*sin(x12)) + (8.601886956836*sin(x13)) + (7.533313498536*sin(x14)) - (23.03633770636*sin(x15)) - (41.11091670539*sin(x16)) + (46.07594342939*sin(x17)) - (4.158230358161*sin(x18)) + (35.61243398897*sin(x19)) - (0.8539648928299*sin(x20)) + (48.46608517312*sin(x21)) + (21.92818922469*sin(x22)) - (5.51333900824*sin(x23)) - (3.155879615049*sin(x24)) - (35.40602155255*sin(x25)) + (21.05316767957*sin(x26)) + (48.63345254591*sin(x27)) + (18.09100857528*sin(x28)) - (45.97902827701*sin(x29)) + (18.31189165581*sin(x30));
if ((xtime % 4) == 3) y = - (19.5666514757*log(.000001+abs(x1))) + (21.87460482304*log(.000001+abs(x2))) - (17.48124453288*log(.000001+abs(x3))) + (38.81839452492*log(.000001+abs(x4))) - (38.63656433142*log(.000001+abs(x5))) + (13.18212824804*log(.000001+abs(x6))) - (2.045229508597*log(.000001+abs(x7))) + (45.1292360492*log(.000001+abs(x8))) + (26.03603502163*log(.000001+abs(x9))) + (21.98935009253*log(.000001+abs(x10))) + (39.25956682671*log(.000001+abs(x11))) - (36.9527398792*log(.000001+abs(x12))) + (8.601886956836*log(.000001+abs(x13))) + (7.533313498536*log(.000001+abs(x14))) - (23.03633770636*log(.000001+abs(x15))) - (41.11091670539*log(.000001+abs(x16))) + (46.07594342939*log(.000001+abs(x17))) - (4.158230358161*log(.000001+abs(x18))) + (35.61243398897*log(.000001+abs(x19))) - (0.8539648928299*log(.000001+abs(x20))) + (48.46608517312*log(.000001+abs(x21))) + (21.92818922469*log(.000001+abs(x22))) - (5.51333900824*log(.000001+abs(x23))) - (3.155879615049*log(.000001+abs(x24))) - (35.40602155255*log(.000001+abs(x25))) + (21.05316767957*log(.000001+abs(x26))) + (48.63345254591*log(.000001+abs(x27))) + (18.09100857528*log(.000001+abs(x28))) - (45.97902827701*log(.000001+abs(x29))) + (18.31189165581*log(.000001+abs(x30)));
Additionally, we add random error as: y = (y * 0.8) + (random 0.4);

Final results of training
esm: N = [500], M = [31], Generations = [25], WFFs = [3829], Score=[0.4014102032497], ScoreHistory=[#(num| 0.4314788707252 0.370830565833 0.5295548213807 0.3451624716074 0.4245445237928 0.340761449759 0.5175313435104 0.355543825195 0.4290839836546 0.3105922068026 0.5218991986507 0.2968193440175 0.3490550282891 0.3814960523474 0.5104515767008 0.3109523184407 0.4313117120513 0.3443044136048 0.5668039806836 0.3841316700523 0.4243050469755 0.2765376224936 0.4831745940098 0.3117904631927 0.3918253679707 0.3144697867339 0.5380055269564 0.3682886420199 0.4266606264549 0.3358316316095 0.5132966054544 0.3381703489507 0.3950420787284 0.3018453711844 0.5056426714079 0.3817591844732 0.5050711888544 0.3116186006794 0.595857617804 0.2995290542275 0.3625880776079 0.3540208219479 0.5600797426684 0.3263173721829 0.4064148604007 0.2837620240312 0.5762523918192 0.3556091663523 0.451788531232 0.2757394616948 0.4822318699195 0.3371976627996 0.4311446784735 0.3166378852358 0.5751618248858 0.3421843929752 0.4122873281423 0.3505610125923 0.5436604420031 0.333169694755 0.3556937548836 0.2978145598364 0.5152109006675 0.373531773994 0.5205788467954 0.2776995784607 0.5650951356193 0.2981484142706 0.3725328029706 0.3886330639769 0.5371652335046 0.3227352016664 0.4439135957985 0.3608044706109 0.5749654404464 0.3726671204379 0.4620401955774 0.2769618881544 0.4964868043753 0.3258344995575 0.4823246774442 0.3060655836617 0.5618348856339 0.3494428853556 0.4092578675161 0.3755983291156 0.5527130312327 0.3250545168439 0.3867426218476 0.3073466517031 0.5290661037633 0.4080520343218 0.4810850908941 0.286278721833 0.4982343136662 0.3119917339389 0.4047181747002 0.3470698040036 0.5195644413011 0.3124605079275 0.4320490305069 0.3252519657162 0.543547718997 0.3594731592158 0.3885670042547 0.3151804709757 0.5074877194542 0.3164815746723 0.4533149220085 0.292666228445 0.5261264396323 0.352882957115 0.4249043798564 0.3468062962237 0.5183515635114 0.3241220108847 0.4081926674651 0.3053849438537 0.5614489491272 0.4023472977057 0.4717718742258 0.2991586665644 0.5141703123601 0.3145213570247 0.3744232833164 0.3457247002533 0.506433626938 0.3178178525966 0.4272790136613 0.3585490925221 0.5675081193945 0.379352349927 0.3973305955673 0.3035211656092 0.5011545699324 0.3394168868888 0.4810214730109 0.2967597603466 0.5103934066082 0.3433985615239 0.4134789017764 0.3588417387975 0.507051280275 0.3345258530082 0.4069214944099 0.3279194874627 0.5541044343287 0.3786283060802 0.4176623447525 0.2883566586139 0.4981261106465 0.309672228571 0.3735069754608 0.3137466683324 0.5706462874675 0.3177954647703 0.4291697807343 0.3439528891504 0.5929245079231 0.3943021015065 0.4050304336118 0.328563465219 0.5055439903962 0.3583973251459 0.4257309348618 0.3184227430329 0.5317569580203 0.3587046743092 0.3998586316313 0.3755735494453 0.4995475884492 0.317587458776 0.453865856167 0.3233113796393 0.5430173741853 0.4216228543847 0.422032646461 0.2766485873816 0.4915655759963 0.2778163010047 0.3653227228014 0.3266492733405 0.5145711257676 0.29680026712 0.4293098813007 0.3185692748384 0.5948029658241 0.3731014824681 0.3776213775237 0.3453259575388 0.5052523349203 0.4330747442372 0.4466365615599 0.3190427538287 0.5301682437947 0.3239009331685 0.3902836960459 0.33513329047 0.5414153331693 0.3118163622999 0.4310590395454 0.2939959842038 0.5568470185001 0.3836087091312 0.3720524499737 0.2732039002871 0.484850897882 0.2595313800206 0.3876494510467 0.3078321471122 0.5326263126294 0.3320138624043 0.4303199410314 0.3176866394394 0.5883776340682 0.3447882384584 0.3701259800116 0.3088621378253 0.5070643916968 0.3577590605366 0.4970578083207 0.3112806023384 0.578261865986 0.345726480088 0.3544236525196 0.3699619344759 0.5214777261026 0.3201776246431 0.4541731568738 0.3326795592414 0.5656231554794 0.3823393800701 0.4549581869223 0.2775062166551 0.4849322125012 0.3178523848134 0.4148305815001 0.3086796690786 0.5365461209695 0.3646331826142 0.4234514321185 0.308776379774 0.5745796292992 0.338116298431 0.4079348218563 0.2760824635071 0.5057054928388 0.3947963090068 0.4856336377755 0.2979272722732 0.5430188880362 0.3382135868254 0.3589938903 0.3540349375879 0.5110186470176 0.3211552515249 0.4463167141724 0.3106002126643 0.5743125386278 0.3534470611767 )]

Show final results on training data, Score=[0.4014102032497]
X[0], ey=[44451.97049223], y=[-220272.9889479], ErrorPct=[1.201804001047]
X[6500], ey=[44451.97049223], y=[-5202.136617654], ErrorPct=[9.544944848504]
X[13000], ey=[44451.97049223], y=[-1904.040746597], ErrorPct=[24.34612353841]
X[19500], ey=[44451.97049223], y=[-185.5385371954], ErrorPct=[240.5834911936]
X[26000], ey=[44451.97049223], y=[-86.32421757094], ErrorPct=[515.942060792]
X[32500], ey=[44451.97049223], y=[-18.77296532424], ErrorPct=[2368.871549565]
X[39000], ey=[44451.97049223], y=[36.87332396086], ErrorPct=[1204.53195962]
X[45500], ey=[44451.97049223], y=[101.4782237056], ErrorPct=[437.0444283416]
X[52000], ey=[44451.97049223], y=[252.811269997], ErrorPct=[174.8306522204]
X[58500], ey=[44451.97049223], y=[451.5633009914], ErrorPct=[97.44017526367]
X[65000], ey=[44451.97049223], y=[544.3752388899], ErrorPct=[80.65685600044]
X[71500], ey=[44451.97049223], y=[634.1011087847], ErrorPct=[69.10233837539]
X[78000], ey=[44451.97049223], y=[741.3660810574], ErrorPct=[58.9595417541]
X[84500], ey=[44451.97049223], y=[944.6313245087], ErrorPct=[46.05748088055]
X[91000], ey=[44451.97049223], y=[2966.628745906], ErrorPct=[13.98400180797]
X[97500], ey=[44451.97049223], y=[6915.682377919], ErrorPct=[5.427705620802]
X[104000], ey=[44451.97049223], y=[67716.05118204], ErrorPct=[0.3435534158255]
X[110500], ey=[44451.97049223], y=[139756.885234], ErrorPct=[0.68193359191]
X[117000], ey=[44451.97049223], y=[206353.6011967], ErrorPct=[0.7845835001936]
X[123500], ey=[45917.90175831], y=[287132.8061244], ErrorPct=[0.8400813115781]
Actual computed error on training data is ErrorPct=[733.1831084353] versus reported ErrorPct=[736.0152916494] while average Y is AvgY=[44467.53993857]
yHistory=[#(num| -14697.97808232 -490.3771358847 -21.55606906054 116.2166289993 435.7528068228 638.2474396991 1322.921629548 15195.26245748 137496.8512217 304680.0584888 )]
eHistory=[#(num| 51894.6907767 42034.2375117 40855.38917786 40722.03645727 44832.89505535 42161.24532025 40471.50705644 41988.51410296 43859.91543311 55854.96849413 )]
aHistory=[#(num| 51894.6907767 46964.4641442 44928.10582209 43876.58848088 44067.84979578 44867.23008138 45543.72627166 47234.46601007 49857.44196362 55854.96849413 )]
dHistory=[#(num| 799.3802856023 1667.137790777 2306.36018798 2892.97781942 3960.277717427 )]


Final testing on test data returns Score=[0.472430987482]
X[0], ey=[44451.97049223], y=[-14517.77378233], ErrorPct=[4.061899927544]
X[25], ey=[44451.97049223], y=[-7622.368238458], ErrorPct=[6.831779455099]
X[50], ey=[44455.29761921], y=[-5654.441918486], ErrorPct=[8.862013309195]
X[75], ey=[44451.97049223], y=[-4750.136542517], ErrorPct=[10.35804057302]
X[100], ey=[44451.97049223], y=[-4028.168825865], ErrorPct=[12.03527990356]
X[125], ey=[44451.97049223], y=[-3344.333520915], ErrorPct=[14.29172769828]
X[150], ey=[44451.97049223], y=[-2423.739559073], ErrorPct=[19.34024217901]
X[175], ey=[44451.97049223], y=[-1776.27287899], ErrorPct=[26.02541755719]
X[200], ey=[44451.97049223], y=[-1241.743366626], ErrorPct=[36.79803338352]
X[225], ey=[44451.97049223], y=[-666.3796905339], ErrorPct=[67.70667057188]
X[250], ey=[44451.97049223], y=[-144.9999524571], ErrorPct=[307.5654142569]
X[275], ey=[44451.97049223], y=[400.3511060442], ErrorPct=[110.032465806]
X[300], ey=[44451.97049223], y=[821.0298006443], ErrorPct=[53.14172598528]
X[325], ey=[44451.97049223], y=[1624.025058851], ErrorPct=[26.37148066156]
X[350], ey=[44451.97049223], y=[2229.439285945], ErrorPct=[18.93863245008]
X[375], ey=[44451.97049223], y=[3171.958676678], ErrorPct=[13.01404464033]
X[400], ey=[44451.97049223], y=[4194.169328732], ErrorPct=[9.598515941575]
X[425], ey=[44451.97049223], y=[5536.621426978], ErrorPct=[7.028717707814]
X[450], ey=[44451.97049223], y=[7069.391097242], ErrorPct=[5.287948973368]
X[475], ey=[44451.97049223], y=[9050.354515307], ErrorPct=[3.911627540894]
Actual computed error on testing data is ErrorPct=[57.78003618958], Avg Y=[167.1804860875]
yHistory=[#(num| -8000.584457878 -4778.813377477 -3251.59536317 -1795.697077344 -676.9018915225 342.7062307246 1540.356059729 3218.72270074 5626.136995946 9447.475041127 )]
eHistory=[#(num| 733.9390631426 1172.973545901 -798.5704823483 535.9608211486 798.1202766635 -1493.752219877 -155.7082847917 493.3005453239 -255.3236731795 640.8652688918 )]
aHistory=[#(num| 733.9390631426 953.4563045218 369.4473755651 411.075736961 488.4846449015 -154.1236727266 180.7834640611 292.9473803454 192.7707978561 640.8652688918 )]
dHistory=[#(num| -642.6083176281 -230.2922728999 -76.49999521972 -760.6855066657 -93.07379425079 )]

esm.selfTest: completed in [137.8567833333] minutes.
true


            </pre>
			</blockquote>

	    ]]></Description>
	</Section>

	<Section>
	    <Heading>Mixed Model Regression (run parameter comparison)</Heading>
		<Description><![CDATA[
			<p>We ran the self test known asmixedModels in this SelfTest log for an extended training run.
            This training set was 10,000 elements distributed over 20 time periods. The testing data set was 500 elements in time period 21.
            The mixed models regression problem had ten independent variables and was trained for two hundred generations.
            The average percent error on the training data was 79338%, and the selector score was 69%.
            On the testing data, the average percent error was 3533%, and the selector score was 00%. 
            The training time was 424 minutes. 
            </p>

			<blockquote>
		    <pre>

        ;; Begin User Defined Learning Options
        (myBestAverage 3)  	            ;; The count of best selector champions to average in creating the final myBest selector Lambda.
        (Integer:myBGMMaximum 0003)     ;; The maximum number of chromosomes to allowed in any one BGM regression.
        (Number:myCrossColPct 0.00)  	;; The probability of cross over in each column island during each generation step.
        (Number:myCrossRegPct 0.00)  	;; The probability of cross over in the best-regressor island during each generation step.
        (Number:myCrossSelPct 1.00)  	;; The probability of cross over in the best-of-breed island during each generation step.
        (Integer:myFRMMaximum 1000)     ;; The maximum number of chromosomes to allowed in any one FRM regression.
        (Integer:myGreedyDepth 00)    	;; The depth of greedy search exploration of each chosen column island  during each generation step. 
        (Integer:myGreedyGEN  000)    	;; The number of greedy search WFFs the system will grow in the best-of-breed island  during each generation step. 
        (myGreedyWidth narrow)    	    ;; The width of each greedy search WFF the system will grow in the best-of-breed island (base, full, narrow). 
        (myGreedyRule MVL:)          	;; The rule used for greedy search WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
        (Integer:myGrowColINIT 00)   	;; The number of random WFFs the system will grow in each column island during the initialization step.
        (Integer:myGrowColGEN 00)    	;; The number of random WFFs the system will grow in each column island during each generation step. 
        (myGrowColRule REG:)         	;; The rule used for random WFFs grown in each column island (BGM, FRM, MVL, ENN, REG, or SVM). 
        (Integer:myGrowSelINIT 5000)   	;; The number of random WFFs the system will grow in the best-of-breed island during the initialization step.
        (Integer:myGrowSelGEN 00)    	;; The number of random WFFs the system will grow in the best-of-breed island during each generation step. 
        (myGrowSelRule REG:)         	;; The rule used for random WFFs grown in the best-of-breed island (BGM, FRM, MVL, ENN, REG, or SVM). 
        (myGrowWFFStyle root:)       	;; The style used for random WFFs grown in the best-of-breed island (full, or root). 
        (Integer:myMaxColWFFLen 05)  	;; The maximum length of a single WFF for each column.
        (Number:myMigratePct 0.00)   	;; The probability of Column island mutation during each generation step.
        (Number:myMutateColPct 0.00) 	;; The probability of mutation in each column island during each generation step.
        (Number:myMutateRegPct 0.00) 	;; The probability mutation in the best-regressor island during each generation step.
        (Number:myMutateSelPct 1.00) 	;; The probability mutation in the best-of-breed island during each generation step.
        (Integer:myMVLMaximum 1000)     ;; The maximum number of chromosomes to allowed in any one MVL regression.
        (Integer:myENNMaximum 0003)     ;; The maximum number of chromosomes to allowed in any one ENN regression.
        (myRandomError .40)          	;; The amount of random error to add to each selfTest test case (0.00 thru .50).
        (myREGMultiple true)            ;; Support multiple columns when growing REG regression expressions.
        (Integer:myREGMaximum 1000)     ;; The maximum number of chromosomes to allowed in any one REG regression.
        (Integer:myRestartGap 00010) 	;; The count of generations to allow, without measurable improvement, before initiating a new evolutionary process.
        (Integer:myRootINIT 00)  	 	;; The number of root WFFs the system will grow in each column island during the initialization step.
        (Integer:myRootGEN 00)       	;; The number of root WFFs the system will grow in each column island during each generation step. 
        (myRootRule REG:)            	;; The rule used for root WFFs grown in each column island (BGM, FRM, MVL, ENN, REG, or SVM). 
        (mySamplingON  true)   	     	;; Support initial scoring on a small sample set before scoring on the full training data set.
        (myScoreStyle sequence:)        ;; The style of scoring the sequencing of the training data (errorpct, sequence, or combined).
        (mySeedDefault 2407987.0)    	;; The default seed number used to initialize the random number genenerator at the start of each training run.
        (mySequenceFocus 005)        	;; The focus of scoring the sequencing of the training data (05, 10, 15, 20, 25, 30, 35, 40, 45, 50, or 100).
        (mySequenceStyle high:)         ;; The style of scoring the sequencing of the training data (average, low, high, or contrast).
        (Integer:mySurvivors  25)     	;; The count of surviving selector WFFs in each generation.
        (mySurvivorStyle equalOFF:)  	;; The option allowing surviving selector WFFs to have equal scores (equalON, or equalOFF).
        (mySvmKernelID cube:)       	;; The kernelID to use for all SVM learning (see svmRegress: all, binary, bipolar, composite, cosine, cube, exp, linear, log, poly, quart, sigmoid, sine, square, tan, tanh).
        (Integer:mySVMMaximum 1000)     ;; The maximum number of chromosomes to allowed in any one SVM regression.
        (mySvmMaxGen 1)             	;; The maximum number of training generations to use for all SVM learning (see svmRegress).
        (mySvmMaxLayers 1)             	;; The maximum number of training layers to use for all SVM learning (see svmRegress).
        (mySvmModelCount 1)             ;; The maximum number of training models to use for all SVM learning (see svmRegress).
        (Integer:myTournamentSize 05)	;; The size of the myBestSelectorChampions queue which initiates a tournament-of-champions when the next evolutionary process begins.
        (myTimeON  true)  			 	;; Support separate sequence scoring of each individual time period.
        (myUseBGM false)             	;; Grow BGM selector candidates during the initialization step and during each generation step.
        (myUseFRM false)             	;; Grow FRM selector candidates during the initialization step and during each generation step.
        (myUseMVL false)             	;; Grow MVL selector candidates during the initialization step and during each generation step.
        (myUseENN false)             	;; Grow ENN selector candidates during the initialization step and during each generation step.
        (myUseREG  true)             	;; Grow REG selector candidates during the initialization step and during each generation step.
        (myUseSVM false)             	;; Grow SVM selector candidates during the initialization step and during each generation step.
        (myVerboseSW false)		     	;; The switch controlling verbose mode during testing, maintennance, and development.
        ;; End User Defined Learning Options

***MixedModels with pop size of 5000, and 100 generations
(setq esm.myGrowSelINIT 5000)(esm.verboseON)(esm.selfTest mixedModels: 20 10 500 100 00% checkpoint:)

Starting test case: mixedModels
Building test data as:
if ((xtime % 4) == 0) y = - (19.5666514757*x1) + (21.87460482304*x2) - (17.48124453288*x3) + (38.81839452492*x4) - (38.63656433142*x5) + (13.18212824804*x6) - (2.045229508597*x7) + (45.1292360492*x8) + (26.03603502163*x9) + (21.98935009253*x10);
if ((xtime % 4) == 1) y = - (19.5666514757*x1*x1) + (21.87460482304*x2*x2) - (17.48124453288*x3*x3) + (38.81839452492*x4*x4) - (38.63656433142*x5*x5) + (13.18212824804*x6*x6) - (2.045229508597*x7*x7) + (45.1292360492*x8*x8) + (26.03603502163*x9*x9) + (21.98935009253*x10*x10);
if ((xtime % 4) == 2) y = - (19.5666514757*sin(x1)) + (21.87460482304*sin(x2)) - (17.48124453288*sin(x3)) + (38.81839452492*sin(x4)) - (38.63656433142*sin(x5)) + (13.18212824804*sin(x6)) - (2.045229508597*sin(x7)) + (45.1292360492*sin(x8)) + (26.03603502163*sin(x9)) + (21.98935009253*sin(x10));
if ((xtime % 4) == 3) y = - (19.5666514757*log(.000001+abs(x1))) + (21.87460482304*log(.000001+abs(x2))) - (17.48124453288*log(.000001+abs(x3))) + (38.81839452492*log(.000001+abs(x4))) - (38.63656433142*log(.000001+abs(x5))) + (13.18212824804*log(.000001+abs(x6))) - (2.045229508597*log(.000001+abs(x7))) + (45.1292360492*log(.000001+abs(x8))) + (26.03603502163*log(.000001+abs(x9))) + (21.98935009253*log(.000001+abs(x10)));
Additionally, we add random error as: y = (y * 0.8) + (random 0.4);

Final results of training
esm: N = [500], M = [11], Generations = [100], WFFs = [1809], Score=[0.3275879981541], ScoreHistory=[#void]

Final testing on test data returns Score=[0.308659408276]
Actual computed error on testing data is ErrorPct=[119.753714189], Avg Y=[-141.1903455357]
yHistory=[#(num| -4691.535215325 -2795.313902086 -1773.158565375 -1037.766299339 -495.2014939888 85.82106338726 732.5554099929 1478.674833022 2604.292012692 4479.728701661 )]
eHistory=[#(num| 1654.046922833 464.5885406671 -270.1604971771 -1072.570517571 -1406.065867575 -470.8927025826 -818.6333478567 -1209.652338245 80.71531320669 1636.721038942 )]
aHistory=[#(num| 1654.046922833 1059.31773175 616.1583221076 193.9761121879 -126.0322837646 -156.3484073069 -77.71233348801 169.2613379682 858.7181760746 1636.721038942 )]
dHistory=[#(num| -30.31612354238 -271.688445676 -446.8969841394 -200.5995556753 -17.32588389025 )]

esm.selfTest: completed in [41.58985] minutes.
true

***MixedModels with pop size of 1000, and 400 generations
(setq esm.myGrowSelINIT 1000)(esm.verboseON)(esm.selfTest mixedModels: 20 10 500 400 00% checkpoint:)

Starting test case: mixedModels
Building test data as:
if ((xtime % 4) == 0) y = - (19.5666514757*x1) + (21.87460482304*x2) - (17.48124453288*x3) + (38.81839452492*x4) - (38.63656433142*x5) + (13.18212824804*x6) - (2.045229508597*x7) + (45.1292360492*x8) + (26.03603502163*x9) + (21.98935009253*x10);
if ((xtime % 4) == 1) y = - (19.5666514757*x1*x1) + (21.87460482304*x2*x2) - (17.48124453288*x3*x3) + (38.81839452492*x4*x4) - (38.63656433142*x5*x5) + (13.18212824804*x6*x6) - (2.045229508597*x7*x7) + (45.1292360492*x8*x8) + (26.03603502163*x9*x9) + (21.98935009253*x10*x10);
if ((xtime % 4) == 2) y = - (19.5666514757*sin(x1)) + (21.87460482304*sin(x2)) - (17.48124453288*sin(x3)) + (38.81839452492*sin(x4)) - (38.63656433142*sin(x5)) + (13.18212824804*sin(x6)) - (2.045229508597*sin(x7)) + (45.1292360492*sin(x8)) + (26.03603502163*sin(x9)) + (21.98935009253*sin(x10));
if ((xtime % 4) == 3) y = - (19.5666514757*log(.000001+abs(x1))) + (21.87460482304*log(.000001+abs(x2))) - (17.48124453288*log(.000001+abs(x3))) + (38.81839452492*log(.000001+abs(x4))) - (38.63656433142*log(.000001+abs(x5))) + (13.18212824804*log(.000001+abs(x6))) - (2.045229508597*log(.000001+abs(x7))) + (45.1292360492*log(.000001+abs(x8))) + (26.03603502163*log(.000001+abs(x9))) + (21.98935009253*log(.000001+abs(x10)));
Additionally, we add random error as: y = (y * 0.8) + (random 0.4);

Final results of training
esm: N = [500], M = [11], Generations = [400], WFFs = [1430], Score=[0.3125165028855], ScoreHistory=[#void]

Final testing on test data returns Score=[0.2938626212464]
Actual computed error on testing data is ErrorPct=[119.9161473847], Avg Y=[-141.1903455357]
yHistory=[#(num| -4691.535215325 -2795.313902086 -1773.158565375 -1037.766299339 -495.2014939888 85.82106338726 732.5554099929 1478.674833022 2604.292012692 4479.728701661 )]
eHistory=[#(num| 1475.360382573 787.9226397162 -295.0090995577 -1183.962480502 -1153.836654106 -470.8349219526 -1039.889389597 -1069.459058002 -362.8949387362 1900.700064807 )]
aHistory=[#(num| 1475.360382573 1131.641511145 656.0913075771 196.0778605573 -73.90504237532 -208.4756486962 -142.8858303821 156.115356023 768.9025630355 1900.700064807 )]
dHistory=[#(num| -134.5706063209 -338.9636909394 -499.9759515541 -362.738948109 425.3396822344 )]

esm.selfTest: completed in [111.5481833333] minutes.
true

***MixedModels with pop size of 10000, and 100 generations
(setq esm.myGrowSelINIT 10000)(esm.verboseON)(esm.selfTest mixedModels: 20 10 500 100 00% checkpoint:)

Starting test case: mixedModels
Building test data as:
if ((xtime % 4) == 0) y = - (19.5666514757*x1) + (21.87460482304*x2) - (17.48124453288*x3) + (38.81839452492*x4) - (38.63656433142*x5) + (13.18212824804*x6) - (2.045229508597*x7) + (45.1292360492*x8) + (26.03603502163*x9) + (21.98935009253*x10);
if ((xtime % 4) == 1) y = - (19.5666514757*x1*x1) + (21.87460482304*x2*x2) - (17.48124453288*x3*x3) + (38.81839452492*x4*x4) - (38.63656433142*x5*x5) + (13.18212824804*x6*x6) - (2.045229508597*x7*x7) + (45.1292360492*x8*x8) + (26.03603502163*x9*x9) + (21.98935009253*x10*x10);
if ((xtime % 4) == 2) y = - (19.5666514757*sin(x1)) + (21.87460482304*sin(x2)) - (17.48124453288*sin(x3)) + (38.81839452492*sin(x4)) - (38.63656433142*sin(x5)) + (13.18212824804*sin(x6)) - (2.045229508597*sin(x7)) + (45.1292360492*sin(x8)) + (26.03603502163*sin(x9)) + (21.98935009253*sin(x10));
if ((xtime % 4) == 3) y = - (19.5666514757*log(.000001+abs(x1))) + (21.87460482304*log(.000001+abs(x2))) - (17.48124453288*log(.000001+abs(x3))) + (38.81839452492*log(.000001+abs(x4))) - (38.63656433142*log(.000001+abs(x5))) + (13.18212824804*log(.000001+abs(x6))) - (2.045229508597*log(.000001+abs(x7))) + (45.1292360492*log(.000001+abs(x8))) + (26.03603502163*log(.000001+abs(x9))) + (21.98935009253*log(.000001+abs(x10)));
Additionally, we add random error as: y = (y * 0.8) + (random 0.4);

Final results of training
esm: N = [500], M = [11], Generations = [100], WFFs = [2531], Score=[0.3213088237771], ScoreHistory=[#void]

Final testing on test data returns Score=[0.3185339114028]
Actual computed error on testing data is ErrorPct=[120.0871767379], Avg Y=[-141.1903455357]
yHistory=[#(num| -4691.535215325 -2795.313902086 -1773.158565375 -1037.766299339 -495.2014939888 85.82106338726 732.5554099929 1478.674833022 2604.292012692 4479.728701661 )]
eHistory=[#(num| 546.8936251201 -144.6559102388 -975.0962673922 -119.8486679541 -887.6513490177 -257.157985214 -672.4295914858 -211.8205766707 352.916686806 956.9465806897 )]
aHistory=[#(num| 546.8936251201 201.1188574406 -190.952850837 -173.1768051162 -316.0717138965 33.69102282503 106.4032748348 366.014230275 654.9316337478 956.9465806897 )]
dHistory=[#(num| 349.7627367216 279.580079951 556.967081112 453.8127763072 410.0529555696 )]

esm.selfTest: completed in [41.78828333333] minutes.
true

***MixedModels with pop size of 500, and 200 generations
(setq esm.myGrowSelINIT 500)(esm.verboseON)(esm.selfTest mixedModels: 20 10 500 200 00% checkpoint:)

Starting test case: mixedModels
Building test data as:
if ((xtime % 4) == 0) y = - (19.5666514757*x1) + (21.87460482304*x2) - (17.48124453288*x3) + (38.81839452492*x4) - (38.63656433142*x5) + (13.18212824804*x6) - (2.045229508597*x7) + (45.1292360492*x8) + (26.03603502163*x9) + (21.98935009253*x10);
if ((xtime % 4) == 1) y = - (19.5666514757*x1*x1) + (21.87460482304*x2*x2) - (17.48124453288*x3*x3) + (38.81839452492*x4*x4) - (38.63656433142*x5*x5) + (13.18212824804*x6*x6) - (2.045229508597*x7*x7) + (45.1292360492*x8*x8) + (26.03603502163*x9*x9) + (21.98935009253*x10*x10);
if ((xtime % 4) == 2) y = - (19.5666514757*sin(x1)) + (21.87460482304*sin(x2)) - (17.48124453288*sin(x3)) + (38.81839452492*sin(x4)) - (38.63656433142*sin(x5)) + (13.18212824804*sin(x6)) - (2.045229508597*sin(x7)) + (45.1292360492*sin(x8)) + (26.03603502163*sin(x9)) + (21.98935009253*sin(x10));
if ((xtime % 4) == 3) y = - (19.5666514757*log(.000001+abs(x1))) + (21.87460482304*log(.000001+abs(x2))) - (17.48124453288*log(.000001+abs(x3))) + (38.81839452492*log(.000001+abs(x4))) - (38.63656433142*log(.000001+abs(x5))) + (13.18212824804*log(.000001+abs(x6))) - (2.045229508597*log(.000001+abs(x7))) + (45.1292360492*log(.000001+abs(x8))) + (26.03603502163*log(.000001+abs(x9))) + (21.98935009253*log(.000001+abs(x10)));
Additionally, we add random error as: y = (y * 0.8) + (random 0.4);

Final results of training
esm: N = [500], M = [11], Generations = [400], WFFs = [1430], Score=[0.3125165028855], ScoreHistory=[#void]

Final testing on test data returns Score=[0.2938626212464]
Actual computed error on testing data is ErrorPct=[119.9161473847], Avg Y=[-141.1903455357]
yHistory=[#(num| -4691.535215325 -2795.313902086 -1773.158565375 -1037.766299339 -495.2014939888 85.82106338726 732.5554099929 1478.674833022 2604.292012692 4479.728701661 )]
eHistory=[#(num| 1475.360382573 787.9226397162 -295.0090995577 -1183.962480502 -1153.836654106 -470.8349219526 -1039.889389597 -1069.459058002 -362.8949387362 1900.700064807 )]
aHistory=[#(num| 1475.360382573 1131.641511145 656.0913075771 196.0778605573 -73.90504237532 -208.4756486962 -142.8858303821 156.115356023 768.9025630355 1900.700064807 )]
dHistory=[#(num| -134.5706063209 -338.9636909394 -499.9759515541 -362.738948109 425.3396822344 )]

esm.selfTest: completed in [115.4716166667] minutes.
true

***MixedModels with pop size of 500, and 400 generations
(setq esm.myBestAverage 1)(setq esm.myGrowSelINIT 5000)(esm.verboseON)(esm.selfTest mixedModels: 20 10 500 00 00% restart:)

Starting test case: mixedModels
Building test data as:
if ((xtime % 4) == 0) y = - (19.5666514757*x1) + (21.87460482304*x2) - (17.48124453288*x3) + (38.81839452492*x4) - (38.63656433142*x5) + (13.18212824804*x6) - (2.045229508597*x7) + (45.1292360492*x8) + (26.03603502163*x9) + (21.98935009253*x10);
if ((xtime % 4) == 1) y = - (19.5666514757*x1*x1) + (21.87460482304*x2*x2) - (17.48124453288*x3*x3) + (38.81839452492*x4*x4) - (38.63656433142*x5*x5) + (13.18212824804*x6*x6) - (2.045229508597*x7*x7) + (45.1292360492*x8*x8) + (26.03603502163*x9*x9) + (21.98935009253*x10*x10);
if ((xtime % 4) == 2) y = - (19.5666514757*sin(x1)) + (21.87460482304*sin(x2)) - (17.48124453288*sin(x3)) + (38.81839452492*sin(x4)) - (38.63656433142*sin(x5)) + (13.18212824804*sin(x6)) - (2.045229508597*sin(x7)) + (45.1292360492*sin(x8)) + (26.03603502163*sin(x9)) + (21.98935009253*sin(x10));
if ((xtime % 4) == 3) y = - (19.5666514757*log(.000001+abs(x1))) + (21.87460482304*log(.000001+abs(x2))) - (17.48124453288*log(.000001+abs(x3))) + (38.81839452492*log(.000001+abs(x4))) - (38.63656433142*log(.000001+abs(x5))) + (13.18212824804*log(.000001+abs(x6))) - (2.045229508597*log(.000001+abs(x7))) + (45.1292360492*log(.000001+abs(x8))) + (26.03603502163*log(.000001+abs(x9))) + (21.98935009253*log(.000001+abs(x10)));
Additionally, we add random error as: y = (y * 0.8) + (random 0.4);
...locating the best selector and the best regressor from this training run.

Final results of training
esm: N = [500], M = [11], Generations = [400], WFFs = [1430], Score=[0.3117256108914], ScoreHistory=[#(num| 0.250142227357 0.1964027356312 0.5665627938412 0.3382466414492 0.209203953871 0.2335213165134 0.4858299669171 0.2766134956018 0.2538766705944 0.258936338793 0.5079217716361 0.3118152615314 0.2167300036121 0.1722409649235 0.5181139526711 0.3070378418319 0.2330286567987 0.2729992082218 0.4644234279636 0.2620678610953 )]

Final testing on test data returns Score=[0.3010878174698]
Actual computed error on testing data is ErrorPct=[39.93423195005], Avg Y=[-141.1903455357]
yHistory=[#(num| -4691.535215325 -2795.313902086 -1773.158565375 -1037.766299339 -495.2014939888 85.82106338726 732.5554099929 1478.674833022 2604.292012692 4479.728701661 )]
eHistory=[#(num| 2140.173244019 10.91508697633 -208.1379210388 -1144.480713873 -1075.121278043 -114.146936064 -825.7358207427 -1028.940512751 -633.4196222147 1466.991018376 )]
aHistory=[#(num| 2140.173244019 1075.544165497 647.6501366521 199.6174240207 -55.33031639209 -227.0503746794 -255.2762343333 -65.12303886347 416.7856980805 1466.991018376 )]
dHistory=[#(num| -171.7200582873 -454.893658354 -712.7731755155 -658.758467417 -673.182225643 )]

esm.selfTest: completed in [115.4716166667] minutes.
true

***MixedModels with pop size of 500, and 400 generations
(setq esm.myBestAverage 5)(setq esm.myGrowSelINIT 500)(esm.verboseON)(esm.selfTest mixedModels: 20 10 500 400 00% checkpoint:)

Starting test case: mixedModels
Building test data as:
if ((xtime % 4) == 0) y = - (19.5666514757*x1) + (21.87460482304*x2) - (17.48124453288*x3) + (38.81839452492*x4) - (38.63656433142*x5) + (13.18212824804*x6) - (2.045229508597*x7) + (45.1292360492*x8) + (26.03603502163*x9) + (21.98935009253*x10);
if ((xtime % 4) == 1) y = - (19.5666514757*x1*x1) + (21.87460482304*x2*x2) - (17.48124453288*x3*x3) + (38.81839452492*x4*x4) - (38.63656433142*x5*x5) + (13.18212824804*x6*x6) - (2.045229508597*x7*x7) + (45.1292360492*x8*x8) + (26.03603502163*x9*x9) + (21.98935009253*x10*x10);
if ((xtime % 4) == 2) y = - (19.5666514757*sin(x1)) + (21.87460482304*sin(x2)) - (17.48124453288*sin(x3)) + (38.81839452492*sin(x4)) - (38.63656433142*sin(x5)) + (13.18212824804*sin(x6)) - (2.045229508597*sin(x7)) + (45.1292360492*sin(x8)) + (26.03603502163*sin(x9)) + (21.98935009253*sin(x10));
if ((xtime % 4) == 3) y = - (19.5666514757*log(.000001+abs(x1))) + (21.87460482304*log(.000001+abs(x2))) - (17.48124453288*log(.000001+abs(x3))) + (38.81839452492*log(.000001+abs(x4))) - (38.63656433142*log(.000001+abs(x5))) + (13.18212824804*log(.000001+abs(x6))) - (2.045229508597*log(.000001+abs(x7))) + (45.1292360492*log(.000001+abs(x8))) + (26.03603502163*log(.000001+abs(x9))) + (21.98935009253*log(.000001+abs(x10)));
Additionally, we add random error as: y = (y * 0.8) + (random 0.4);
...locating the best selector and the best regressor from this training run.

Final results of training
esm: N = [500], M = [11], Generations = [400], WFFs = [1430], Score=[0.3153549290754], ScoreHistory=[#void]

Final testing on test data returns Score=[0.2844427938482]
Actual computed error on testing data is ErrorPct=[199.8234189266], Avg Y=[-141.1903455357]
yHistory=[#(num| -4691.535215325 -2795.313902086 -1773.158565375 -1037.766299339 -495.2014939888 85.82106338726 732.5554099929 1478.674833022 2604.292012692 4479.728701661 )]
eHistory=[#(num| 1407.339677461 865.5432271094 -382.8226492183 -954.7260867214 -1196.17695472 -668.6228226791 -1093.482046327 -1226.194445397 -147.5185777 1984.757222836 )]
aHistory=[#(num| 1407.339677461 1136.441452285 630.0200851174 233.8335421577 -52.1685572179 -230.2121338536 -120.6094616472 203.6813999128 918.619322568 1984.757222836 )]
dHistory=[#(num| -178.0435766357 -354.4430038049 -426.3386852046 -217.8221297173 577.4175453748 )]

esm.selfTest: completed in [115.30755] minutes.
true

***MixedModels with pop size of 500, and 400 generations
(setq esm.myBestAverage 100)(setq esm.myGrowSelINIT 500)(esm.verboseON)(esm.selfTest mixedModels: 20 10 500 400 00% checkpoint:)

Starting test case: mixedModels
Building test data as:
if ((xtime % 4) == 0) y = - (19.5666514757*x1) + (21.87460482304*x2) - (17.48124453288*x3) + (38.81839452492*x4) - (38.63656433142*x5) + (13.18212824804*x6) - (2.045229508597*x7) + (45.1292360492*x8) + (26.03603502163*x9) + (21.98935009253*x10);
if ((xtime % 4) == 1) y = - (19.5666514757*x1*x1) + (21.87460482304*x2*x2) - (17.48124453288*x3*x3) + (38.81839452492*x4*x4) - (38.63656433142*x5*x5) + (13.18212824804*x6*x6) - (2.045229508597*x7*x7) + (45.1292360492*x8*x8) + (26.03603502163*x9*x9) + (21.98935009253*x10*x10);
if ((xtime % 4) == 2) y = - (19.5666514757*sin(x1)) + (21.87460482304*sin(x2)) - (17.48124453288*sin(x3)) + (38.81839452492*sin(x4)) - (38.63656433142*sin(x5)) + (13.18212824804*sin(x6)) - (2.045229508597*sin(x7)) + (45.1292360492*sin(x8)) + (26.03603502163*sin(x9)) + (21.98935009253*sin(x10));
if ((xtime % 4) == 3) y = - (19.5666514757*log(.000001+abs(x1))) + (21.87460482304*log(.000001+abs(x2))) - (17.48124453288*log(.000001+abs(x3))) + (38.81839452492*log(.000001+abs(x4))) - (38.63656433142*log(.000001+abs(x5))) + (13.18212824804*log(.000001+abs(x6))) - (2.045229508597*log(.000001+abs(x7))) + (45.1292360492*log(.000001+abs(x8))) + (26.03603502163*log(.000001+abs(x9))) + (21.98935009253*log(.000001+abs(x10)));
Additionally, we add random error as: y = (y * 0.8) + (random 0.4);
...locating the best selector and the best regressor from this training run.

Final results of training
esm: N = [500], M = [11], Generations = [400], WFFs = [1430], Score=[0.3281711314848], ScoreHistory=[#void]

Final testing on test data returns Score=[0.382084157061]
Actual computed error on testing data is ErrorPct=[719.41230309], Avg Y=[-141.1903455357]
yHistory=[#(num| -4691.535215325 -2795.313902086 -1773.158565375 -1037.766299339 -495.2014939888 85.82106338726 732.5554099929 1478.674833022 2604.292012692 4479.728701661 )]
eHistory=[#(num| 799.4650206441 374.1809965654 -420.055956874 -662.5139725485 -759.8706810837 -705.3853014574 -1139.709493466 -230.6558417 261.8158786566 1070.825895906 )]
aHistory=[#(num| 799.4650206441 586.8230086048 251.1966867785 22.76902194676 -133.7589186593 -148.6217724122 -9.430890150843 367.3286442874 666.3208872811 1070.825895906 )]
dHistory=[#(num| -14.86285375282 -32.1999120976 116.1319575089 79.49787867636 271.3608752616 )]

esm.selfTest: completed in [115.39895] minutes.
true

            </pre>
			</blockquote>

	    ]]></Description>
	</Section>

</Essay>




































;;**EXPORTKEY**:esm:%%Essay_Selector_Introduction
;#text#
<?xml version="1.0" encoding="UTF-8"?>
<Essay>
	<KnowledgeBase>
	    <Title>Selector Introduction</Title>
		<Topic>Selector</Topic>
		<SubTopic>Overview</SubTopic>
		<HumanKeywords>Selector Programming Artificial-Intelligence</HumanKeywords>
	</KnowledgeBase>
	<Section>
	    <Heading>Introduction</Heading>
		<Description><![CDATA[
             <p>The Evolutionary Sequencing Machine evolves a population of well-formed-formulas (WFFs) in a problem-oriented grammar known as <b>Selector</b>.
             Each WFF is known as a <i>selector</i>, and is trained to operate on the data from a single time step.
             After training the selector will be given the <i>testing data</i> for the testing time step, 
             and the selector will be expected to select the "best" elements from the testing time period.
             </p>

             <p>The Selector grammar is defined, within the ESM Lambda, in esm:selector:%DECLARATION, which is a feature-based grammar specification understood by the ParseLib.
             The esm.selector child Lambda is a "Selector" parser, generated from esm:selector:%DECLARATION by ParseLib.
             The Selector parser translates ASCII text strings into Selector WFFs which are annotated s-expressions.
             Each WFF s-expression, in the population, may be annotated with grammar notes taken from the rules defined in esm:selector:%DECLARATION.
             </p> 

             <p>The Selector language is a dialect of JavaScript, to make learning and reading easy by the current generation of programmers.
             Each Selector WFF must have the following enclosing syntax to remain gramatically correct.
             </p> 

             <pre>         
             function (XT) {                        // Select the best Vectors from the set, XT, of Vectors.


             }
             </pre>

             <p>While a complete layout of Selector is inappropriate right here (the full Selector language is documented later in this reference material), 
             the Selector language does allow the full range of logical and arithmetic operators as well as sorting, filtering, and regression operators against the input data.
             The input data is always for <u>a single time step</u> with only the fitness function allowed to compare each selector's performance across all training time periods.
             The argument, <b>XT</b> is an Lambda containing the set of Number Vectors for a single time step as well as offering a number of operators to aid in selecting the best vectors. 
             The expression <b>XT.allRows</b> represents all of the Number Vectors in the current time step.
             The expression <b>XT.selectedRows</b> represents all of the Number Vectors, in the current time step, currently selected.
             Each of the Number vectors in <b>XT.allRows</b> contain M+1 elements (<i>known as <b>xtime</b>, and <b>x1</b> thru <b>Xm</b> to selector Lambdas</i>).
             The argument <b>XT</b>, when first passed to a Selector Lambda, is always in a state where XT.allRows == XT.selectedRows. 
             The task of the Selector Lambda is to perform selection operations on XT such that, when completed, XT.selectedRows contains the "best" vectors found in XT.allRows.
             Some examples of simple Selector Lambdas are as follows:
             </p> 

             <pre>
             regress (x10 / sin(x12));          // Train a linear regression for the model "(x10 / sin(x12))" and score based upon the trained model.
             </pre>

             <pre>
             svmregress (x10,cos(x12)/log(x3)); // Train a support vector regression for the pseudo variables (x10 and cos(x12)/log(x3) and score based upon the trained model.
             </pre>
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Lambda Objects</Heading>
		<Description><![CDATA[
			<p><b>AIS</b> &quot;Lambdas&quot; are a unique type of objects designed to act as the <b><u>building
		    blocks</u></b><u></u> for intelligent, adaptive, systems. Lambdas contain
		    more than just binary machine code (Analytic Information Server supports many
		    built-in functions, which are primarily binary machine code, but these are
		    not Lambdas). Lambdas are something more than just functions. Lambdas are
		    building block objects, which contain the necessary structure to provide
		    some rudimentary autonomy. Lambdas can contain other child Lambdas and can
		    give birth to other child Lambdas. Lambdas can publish their preferred style
		    of interface. Lambdas have an abstract threshold (like a cell membrane) which
		    makes the Lambda aware of any mutative or referential access attempt from the
		    outside. Lambdas may run on native machine code or they may be emulated by a
		    virtual machine. There may be a different virtual machine for each Lambda.
		    Lambdas contain their persistent and temporary knowledge variables. Lambdas
		    contain the original source code used to compile them. Lambdas can be
		    generated from multiple languages. The Evolutionary Sequencing Machine comes with a
		    built-in <b>Selector</b> compiler, which produces AIS Lambdas.</p>
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Lambda Properties</Heading>
		<Description><![CDATA[
			<p>The Lambda object stores Lambda behavior and knowledge in a standard
		    building block format (regardless of the original source language). The
		    Analytic Information Server Lambda object contains the following eight
		    properties:</p>
			
		    <table border="3" cellpadding="2" width="100%" bgcolor="#99CCCC">
			<colgroup><col class="member"><col class="description"></colgroup>
		      <tr>
		        <td>Av:</td>
		        <td>
		          <p>The arguments Structure object containing the Lambda's arguments.<br><br></p>
		        </td>
		      </tr>
		      <tr">
		        <td>In:</td>
		        <td>
		          <p>The <b>faces</b>: Structure object containing the Lambda's published
		          interface styles.</p>
		        </td>
		      </tr>
		      <tr>
		        <td>Pc:</td>
		        <td>
		          <p>The Pcode Vector object containing the Lambda's virtual machine
		          codes.</p>
		        </td>
		      </tr>
		      <tr>
		        <td>Pv:</td>
		        <td>
		          <p>The <b>pvars</b>: Structure object containing the Lambda's
		          persistent variables.</p>
		        </td>
		      </tr>
		      <tr>
		        <td>Cv:</td>
		        <td>
		          <p>The <b>cvars</b>: Structure object containing the Lambda's
		          persistent class variables</p>
		        </td>
		      </tr>
		      <tr>
		        <td>Nc:</td>
		        <td>
		          <p>The Native Code Vector object containing the Lambda's native machine
		          code.</p>
		        </td>
		      </tr>
		      <tr>
		        <td>Sc:</td>
		        <td>
		          <p>The Source Code Vector containing the original language source for
		          debugger display.</p>
		        </td>
		      </tr>
		      <tr>
		        <td>Tv:</td>
		        <td>
		          <p>The <b>vars</b>: Structure object containing the Lambda's temporary
		          frame variables.</p>
		        </td>
		      </tr>
		      <tr>
		        <td>Vm:</td>
		        <td>
		          <p>The Virtual Machine emulator function (each Lambda may run on a
		          separate virtual machine).</p>
		        </td>
		      </tr>
		    </table>
		    <p>&nbsp;</p>
		    <p>An Lambda is <i>First Class Object</i>. A First Class object in Lambda
		    Information Server is any object that is fully exposed, i.e., all of the
		    Structures are visible and modifiable by the programmer. All Lambdas have the
		    following data structures: source code tokens (<b>Sc</b>), pcode tokens (<b>Pc</b>),
		    argument variables (<b>Av</b>), persistent variables (<b>Pv</b>)<b>, </b>persistent
		    class variables (<b>Cv</b>)<b>, </b>temporary variables (<b>Tv</b>),
		    interfaces (<b>In</b>), native code (<b>Nc</b>), and the virtual machine
		    emulator (<b>Vm</b>). All Lambda structures can viewed and modified by the
		    programmer:</p>
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Selector Lambdas</Heading>
		<Description><![CDATA[
			<p>The principal activity of the Selector Lambda is to reduce sets to smaller
		    sets which have a higher <i>score</i> than the original set. This
		    process is called <i>selection</i>. For instance, in the stock market, we start
		    with a set of all possible stocks and we wish to select a few stocks to
		    purchase. If the few stocks we purchase have a higher <i>score</i> (percent
		    profit) than the average of all stocks, then we are happy. The act of
		    selecting a few stocks to purchase reduces the original set of all stocks
		    down to the set of those we wish to purchase. This process is called
		    selection.</p>
		    <p>Other examples of selection include reviewing a set of all United
		    States households to select only those households which are to receive this
		    month's promotional mailing. Reviewing a set of possible oil deposit sites
		    to select only those sites where we wish to drill. There are many other
		    examples of selection.</p>
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Selector Data Types</Heading>
		<Description><![CDATA[
		   <p>The <b>Selector</b> Programmer has access to all of the types in the
		    Analytic Information Server environment. These are the same as the Lisp data
		    types. The Lisp data types are divided into three categories: Native Data
		    Types (also known as Immediate types), Objects (heap objects) and
		    Repositories. The Native (immediate) types can be entirely contained within
		    the immediate data of a single Virtual Machine Container. The Objects (heap
		    objects) types are too large to be contained within a single Virtual Machine
		    Container and require extra memory must be managed by the heap manager.
		    Without exception, all of the Object types are identified by an object id.
		    The object id identifies a block of memory, managed by the Analytic Information
		    Server memory manager, in which the Object's data is stored. (see Object
		    Identifier Notation).</p>
		    <p>Virtual Machine Containers are of fixed length and come in different
		    sizes. Small data items are stored in immediate mode, and may be moved to
		    the heap if the data becomes too large to store immediately.</p>
		    <p>The Heap contains memory resident data, which is of variable length or is
		    too large to fit in small fixed containers. The Analytic Information Server
		    object Heap manager supports automated object resizing, garbage collection,
		    and anti-fragmentation algorithms so that the user may concentrate on the
		    analysis and modeling of data rather than on memory management.</p>
		    <p>Repositories (databases) contain persistent data of all sorts. Lambda
		    Information Server supports repositories with multiple database volumes and
		    multiple database schema's including General Object Repositories, Text
		    Repositories, and Lambda Repositories.</p>
		    <p>The generic Analytic Information Server data type is known to Selector as <b>obj</b>.
		    No type identification, such as <i>var n;</i>, will cause Selector to
		    treat the variable, <i>n</i>, as being of type <b>obj</b>, that is to say
		    any possible Analytic Information Server data type.</p>
		    <p>The Selector compiler also supports strong typing of declared variables
		    <b>obj</b>. Providing a type identification, such as <i>var int n;</i>, will
		    cause Selector to treat the variable, <i>n</i>, as being of type <b>int</b>,
		    that is to say it will be managed as an Analytic Information Server type <b>Integer</b>.</p>
		    <p>The following is a list of Selector strong data types together with the
		    Analytic Information Server types which they represent.</p>
		    <table border="3" cellpadding="2" width="100%" bgcolor="#99CCCC">
			<colgroup><col><col class="italic"><col><col class="italic"></colgroup>
		      <tr align="top">
		        <th>obj</th>
		        <td>Object</td>
		        <th>bool</th>
		        <td>Boolean</td>
		        <th>char</th>
		        <td>Character</td>
		        <th>int</th>
		        <td>Integer</td>
		        <th>float</th>
		        <td>Number</td>
		      </tr>
		      <tr align="top">
		        <th>text</th>
		        <td>Text</td>
		        <th>string</th>
		        <td>String</td>
		        <th>symbol</th>
		        <td>Symbol</td>
		        <th>bytvec</th>
		        <td>ByteVector</td>
		        <th>fltvec</th>
		        <td>FloatVector</td>
		      </tr>
		      <tr align="top">
		        <th>stc</th>
		        <td>Structure</td>
		        <th>dir</th>
		        <td>Directory</td>
		        <th>dic</th>
		        <td>Dictionary</td>
		        <th>matrix</th>
		        <td>Matrix</td>
		        <th>nummat</th>
		        <td>NumMatrix</td>
		      </tr>
		      <tr align="top">
		        <th>vec</th>
		        <td>Vector</td>
		        <th>bitvec</th>
		        <td>BitVector</td>
		        <th>numvec</th>
		        <td>NumVector</td>
		        <th>intvec</th>
		        <td>IntVector</td>
		        <th>objvec</th>
		        <td>ObjVector</td>
		      </tr>
		    </table>
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Strong Typing</Heading>
		<Description><![CDATA[
			<p>The <b>Selector</b> Programmer has access to compile time strong typing
		    variable declarations. Strongly typed variables are compiled with Lambda
		    Information Server's strong typed virtual machine instructions. Strongly
		    typed variables operate faster, at run time; but, are more prone to
		    programmer error as there is little or no run time type checking performed.</p>
		    <p>The programmer can even cast an arbitrary Selector expression to a
		    valid type. The casting will alert the Selector compiler to treat the
		    result of the cast expression as specified. This will direct the Selector
		    compiler to use Analytic Information Server's strong typed virtual machine
		    instructions with the cast expression. Warning: casting does not introduce
		    any run time type checking.</p>
		    <p>The following Selector code sample illustrates the actions of the
		    Selector compiler when strong typing variable declarations and type casts
		    are encounteres.</p>
		    <p><b>The Selector source code for foo</b></p>
		    <blockquote>
			  <pre>
		      // A test of strong typing, including expression type casting, in Selector.
		      function foo(int i) {
		      var char c1, string name=new('String',&quot;Hello There&quot;);
		      c1 +=name[((int)length(name))-i];
		      }</pre>
		    </blockquote>
		    <p><b>The compiled code for foo</b></p>
		    <table border="3" cellpadding="2" width="100%" bgcolor="#99CCCC">
		      <tr>
		        <td><u>Virtual Machine Instructions for</u>: <b>foo</b></td>
		      </tr>
		      <tr>
		        <td>0000: push &quot;String,&quot;Hello There&quot;</td>
		      </tr>
		      <tr>
		        <td>0007: call 2,new,vars:(name)</td>
		      </tr>
		      <tr>
		        <td>0011: push vars:(name)</td>
		      </tr>
		      <tr>
		        <td>0013: call 1,length,vars:(__T4)</td>
		      </tr>
		      <tr>
		        <td>0017: isub args:(i),vars:(__T4),vars:(__T3)</td>
		      </tr>
		      <tr>
		        <td>0021: refstring vars:(__T3),vars:(name),vars:(__T2)</td>
		      </tr>
		      <tr>
		        <td>0025: cadd vars:(__T2),vars:(c1),vars:(c1)</td>
		      </tr>
		      <tr>
		        <td>0029: return vars:(c1)</td>
		      </tr>
		    </table>
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>White Space</Heading>
		<Description><![CDATA[
			<p>The Selector compiler uses white space to separate each of its symbols
		    and operators. The Selector white space characters include all the
		    standard 8-bit ASCII control characters (less than 32 decimal), and the
		    blank character (32 decimal).</p>
		    <b>LF, CR, TAB</b> ..control chars..
		    <p>space</p>
		    <p>The Selector compiler ignores whitespace</p>
		    <blockquote>
		      <pre>a = 1 + 2;  // This is a valid statement
		       b=1+2;  // This is also a valid statement</pre>
		    </blockquote>
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Special Characters</Heading>
		<Description><![CDATA[
			<p>Selector uses the standard 8-bit ASCII character set. Some of the
		    Selector special characters serve to group a set of characters as a single
		    unit (e.g. double quotes group characters to form a string constant). The
		    remainder of the special characters serve to separate tokens (e.g. comma or
		    blank) or prefix a constant (e.g. $ # ).</p>
		    <p>The following are the Selector special characters.</p>
		    <table border="3" cellpadding="2" width="50%" style="font-weight: bold" bgcolor="#99CCCC">
		      <tr align="top">
		        <td>\</td>
		        <td>|</td>
		        <td>(</td>
		        <td>)</td>
		        <td>[</td>
		        <td>]</td>
		        <td>{</td>
		        <td>}</td>
		        <td>#</td>
		        <td>@</td>
		      </tr>
		      <tr align="top">
		        <td>'</td>
		        <td>'</td>
		        <td>,</td>
		        <td>&quot;</td>
		        <td>:</td>
		        <td>;</td>
		        <td>$</td>
		        <td>%</td>
		        <td>.</td>
		        <td>&nbsp;</td>
		      </tr>
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Naming Conventions for Variables</Heading>
		<Description><![CDATA[
			<p>Selector variable names are composed of case-sensitive alphanumeric
		    characters. No spaces are allowed in a variable name but the underscore (_)
		    character may be embedded to separate multi-word names . Another convention
		    to make multiple word names more readable its to use start the first word
		    with a lowercase letter and begin the first letter of each succeeding word
		    with an uppercase letter.</p>
		    <p>For example</p>
		    <blockquote>
		      <pre>myVariable</pre>
		      <pre>sum</pre>
		      <pre>namesOfStudents</pre>
		     </blockquote>
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Constants</Heading>
		<Description><![CDATA[
			<p>Selector is a dynamically typed language. The type of a variable is
		    unknown until runtime when data is stored into it. The follow table contains
		    the constant forms recognized by the Selector compiler. For more detail on
		    the data types listed below, see <b>Analytic Information Server Programmer's
		    Guide.</b></p>
		    <table bgcolor="#99CCCC" border="3" cellpadding="2" width="50%">
		      <tr align="top">
		        <td><b>Type</b></td>
		        <td><b>Constant Form</b></td>
		      </tr>
		      <tr align="top">
		        <td><b>Void</b></td>
		        <td><b>void</b> or <b>nil</b></td>
		      </tr>
		      <tr align="top">
		        <td><b>Boolean</b></td>
		        <td><b>true</b> or <b>false</b></td>
		      </tr>
		      <tr align="top">
		        <td><b>Date</b></td>
		        <td><b>#Mar,2,1987</b> or <b>#Jun,1,200BC</b></td>
		      </tr>
		      <tr align="top">
		        <td><b>Integer</b></td>
		        <td><b>12 </b>or<b> -2345</b></td>
		      </tr>
		      <tr align="top">
		        <td><b>Number</b></td>
		        <td><b>12.9 </b>or<b> 0.123456</b></td>
		      </tr>
		      <tr align="top">
		        <td><b>Object</b></td>
		        <td><b>#&lt;Vector 1273&gt;</b></td>
		      </tr>
		      <tr align="top">
		        <td><b>String</b></td>
		        <td><b>&quot;Hello World&quot;</b></td>
		      </tr>
		      <tr align="top">
		        <td><b>Symbol</b></td>
		        <td><b>'Hello'</b></td>
		      </tr>
		    </table>
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Comments</Heading>
		<Description><![CDATA[
			<p>Because the Selector compiler tries to evaluate all of the words in a
		    script, it is useful to have text, which is to be ignored by the compiler.
		    This ignored text, called a comment, allows you to include information,
		    which may be useful to understanding the Selector statements. There are
		    two types of comments: single line and multi-line.</p>
		    <p>A single line comment tells the compiler to ignore all the characters up
		    to the end-of-line (eol). A single line comment must begin with the
		    characters: //</p>
		    <p>For Example:</p>
		    <blockquote>
		      <pre>// This is a comment</pre>
		    </blockquote>
		    <p>A multi-line comment tells the compiler to ignore all the characters
		    embedded in between the delimiters: <b>/*</b> and */</p>
		    <p>For Example:</p>
		    <blockquote>
		      <pre>/*  Humpty Dumpty 
		
		            sat on a wall  */</pre>
		    </blockquote>
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Global Variables</Heading>
		<Description><![CDATA[
			<p>Selector variables have automatic global declaration. Referencing a
		    symbol, which has not already been declared, automatically causes it to be
		    declared as a global variable. This feature has been added to make
		    Selector user-friendlier and to make Selector consistent with other
		    Analytic Information Server languages.</p>
		    <p>The following Selector expressions are equivalent (The assumption is
		    made that X has not already been referenced).</p>
		    <blockquote>
		      <pre>X = 23</pre>
		    </blockquote>
		    <p>is equivalent to:</p>
		    <blockquote>
		      <pre>var X = 23</pre>
		    </blockquote>
		    <p>Selector global variables are valid during the whole life of the
		    current workspace (see the <b>_globals</b> global symbol table variable).
		    Selector global variables are referenced by specifying the symbol. In
		    addition to user defined globals, Selector global variables include all of
		    the built-in functions such as <b>+ - * upperCase, sin, cos, date, etc.</b></p>
		    <p>The Analytic Information Server Selector dialect is specified as
		    case-sensitive (most dialects of Selector are case-insensitive). Therefore</p>
		    <blockquote>
		      <pre>Var</pre>
		    </blockquote>
		    <p>is NOT equivalent to:</p>
		    <blockquote>
		      <pre>var</pre>
		    </blockquote>
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Function Calls</Heading>
		<Description><![CDATA[
			<p>Any user-defined Selector Lambda, and Lisp Lambda, and any Lambda
		    Information Server function may be called from Selector. The syntax is
		    simple the function name followed by parenthesis, (). If the function
		    requires arguments, they must be supplied in between the parenthesis and
		    multiple arguments should be separated by a comma. The parenthesis are
		    mandatory even if no arguments are supplied. All Selector functions
		    receive arguments by value. After function invocation, one and only one
		    result value is returned.</p>
		    <p>For example</p>
		    <blockquote>
		      <pre>
                   mod(10, 2);     //Returns 0
		
                   today();        //Returns 729855
              </pre>
		    </blockquote>
	    ]]></Description>
	</Section>
</Essay>




































;;**EXPORTKEY**:esm:%%Essay_Selector_Statements
;#text#
<?xml version="1.0" encoding="UTF-8"?>
<Essay>
	<KnowledgeBase>
	    <Title>Selector Statements</Title>
		<Topic>Selector</Topic>
		<SubTopic>Overview</SubTopic>
		<HumanKeywords>Selector Programming Artificial-Intelligence</HumanKeywords>
	</KnowledgeBase>
	<Section>
	    <Heading>Statements Overview</Heading>
		<Description><![CDATA[
			<p>This chapter is a reference guide to the Selector statements. These
		    statements contain the basic control flow operators for the Selector
		    language. All of the basic building blocks of Selector are contained in
		    its statements. Selector is case sensitive.</p>

		    <p>All Selector statements are lexical structures, which are understood
		    only by the Selector parser. They are not understood by the system outside
		    of the Selector parser, and they are not available nor are they understood
		    by Lisp. All Selector statements have meaning only at parse time.</p>

		    <p>All Selector special forms are structures, which are understood only by
		    the Selector compiler. Unlike Function objects, the Selector special
		    forms are invalid outside of compilation, at run time.</p>

		    <p>Selector supports user defined global symbols, allowing the user to
		    assign any valid Selector object to the newly created global symbol. User
		    defined global symbols behave just like their built-in cousins; however,
		    Selector even allows the user to redefine any of the built-in Selector
		    special form symbols.</p>
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Binary Arithmetic Operators</Heading>
		<Description><![CDATA[
			<h3>Overview</h3>
		    <p>Selector supports the basic binary and unary arithmetic operations:
		    addition (+), subtraction (-), multiplication (*), division (/), protected division (#), and modulus
		    (%).</p>
		    <h3>Type</h3>
		    <p><b>Statement</b></p>
		    <h3>Syntax</h3>
		    <p><b>operand1 binaryOperator operand2</b></p>
		    <h3>Arguments</h3>
		    <hr>
		    <table border="3" cellpadding="2" width="100%" bgcolor="#99CCCC">
		      <tr>
		        <th align="CENTER" width="20%">Arguments</th>
		        <th align="LEFT" width="80%">Explanation</th>
		      </tr>
		      <tr>
		        <th align="CENTER" width="20%"><b style="mso-bidi-font-weight: normal">Operand1</b><b style="mso-bidi-font-weight: normal"><o:p>
		          </o:p>
		          </b></th>
		        <td align="LEFT" width="80%">The<span style="mso-spacerun: yes"> </span>first
		          operand.</td>
		      </tr>
		      <tr>
		        <th align="CENTER" width="20%"><b>binaryOperator</b><b style="mso-bidi-font-weight: normal"><o:p>
		          </o:p>
		          </b></th>
		        <td align="LEFT" width="80%">The<span style="mso-spacerun: yes"> binary
		          operator. </span>Must be one of: &nbsp; <b>+, -, *, /, #, %.</b></td>
		      </tr>
		      <tr>
		        <th align="CENTER" width="20%"><b style="mso-bidi-font-weight: normal">Operand2</b><b style="mso-bidi-font-weight: normal"><o:p>
		          </o:p>
		          </b></th>
		        <td align="LEFT" width="80%">The<span style="mso-spacerun: yes"> second
		          operand</span>.</td>
		      </tr>
		      <tr>
		        <th align="CENTER" width="20%">RETURN</th>
		        <td align="LEFT" width="80%">The result of the binary operation.</td>
		      </tr>
		    </table>
		    <hr>
		    <h3><u>When To Use</u></h3>
		    <p>Use the <b style="mso-bidi-font-weight: normal">binary operator</b>
		    statement to perform basic arithmetic operations on two numeric values, or
		    to concatenate two string values.</p>
		    <h3><u>Example1</u></h3>
		    <p class="MsoBodyText">These simple examples demonstrates the effect of the
		    various binary operators.</p>
		    <p class="MsoBodyText"><b>a = 1 + 20;
		    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //Returns
		    21<br>
		    <br>
		    b = 30 - 5;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
		    //Returns 25<br>
		    <br>
		    c = 2 * 2.1;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
		    //Returns 4.2<br>
		    <br>
		    d = 100 / 10;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
		    //Returns 10<br>
		    <br>
		    e = 10 % 4;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
		    //Returns 2&nbsp; (the remainder)</b><b style="mso-bidi-font-weight: normal"><u><o:p>
		    </o:p>
		    </u></b></p>
		    <h3><u>Notes &amp; Hints</u></h3>
		    <p>The <b style="mso-bidi-font-weight: normal">binary operator</b> statement
		    performs basic arithmetic operations on two numeric values, or concatenates
		    two string values.</p>
			<p>On a divsion by zero, the protected divide operator returns the highest possible number in the system if 
			the sign of the numerator is negative or the lowest possible number in the system if the sign of the numerator is positive.</p>
            
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Unary Arithmetic Operators</Heading>
		<Description><![CDATA[
			<h3>Overview</h3>
		    <p>Selector supports the basic unary arithmetic operations: increment
		    (++), decrement (--), and negation (-).<br>
		    <p><b>Syntax</b>: unaryOperator operand1</p>
		    <h3>Type</h3>
		    <p><b>Statement</b></p>
		    <h3>Syntax</h3>
		    <p><b>unaryOperator operand1</b></p>
		    <p><b>operand1</b> <b>incrementOperator</b></p>
		    <p><b>operand1</b> <b>decrementOperator</b></p>
		    <h3>Arguments</h3>
		    <hr>
		    <table border="3" cellpadding="2" width="100%" bgcolor="#99CCCC">
		      <tr>
		        <th align="CENTER" width="20%">Arguments</th>
		        <th align="LEFT" width="80%">Explanation</th>
		      </tr>
		      <tr>
		        <th align="CENTER" width="20%"><b>unaryOperator</b><b style="mso-bidi-font-weight: normal"><o:p>
		          </o:p>
		          </b></th>
		        <td align="LEFT" width="80%">The<span style="mso-spacerun: yes"> unary
		          operator. </span>Must be one of: &nbsp; <b>++, --, -.</b></td>
		      </tr>
		      <tr>
		        <th align="CENTER" width="20%"><b style="mso-bidi-font-weight: normal">Operand<o:p>1
		          </o:p>
		          </b></th>
		        <td align="LEFT" width="80%">The<span style="mso-spacerun: yes"> first
		          operand</span>.</td>
		      </tr>
		      <tr>
		        <th align="CENTER" width="20%">RETURN</th>
		        <td align="LEFT" width="80%">The result of the unary operation.</td>
		      </tr>
		    </table>
		    <hr>
		    <h3><u>When To Use</u></h3>
		    <p>Use the <b style="mso-bidi-font-weight: normal">unary operator</b>
		    statement to perform basic arithmetic operations on a single numeric value.</p>
		    <h3><u>Example1</u></h3>
		    <p class="MsoBodyText">These simple examples demonstrates the effect of the
		    various unary operators.</p>
		    <p class="MsoBodyText"><b>var a = 100;<br>
		    <br>
		    var b&nbsp; = 100;<br>
		    <br>
		    var c = 10;<br>
		    <br>
		    var d = 10;<br>
		    <br>
		    ++a; &nbsp;&nbsp;&nbsp; //Returns 101<br>
		    <br>
		    --b;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //Returns 99<br>
		    <br>
		    c++;&nbsp;&nbsp;&nbsp; //Returns 11<br>
		    <br>
		    d--;&nbsp;&nbsp;&nbsp;&nbsp; //Returns 9<br>
		    <br>
		    -a;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //Returns -101</b></p>
		    <h3><u>Notes &amp; Hints</u></h3>
		    <p>The <b style="mso-bidi-font-weight: normal">unary operator</b> statement
		    performs basic arithmetic operations on a single numeric value.</p>
            
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Assigment Operators</Heading>
		<Description><![CDATA[
			<h3>Overview</h3>
		    <p>Selector assigns a value to a variable with the <b>=</b> (equal)
		    operator. Another class of assignment operators perform math operations on
		    the variable and saves the result in the same variable. They are <b>+=, -=,
		    *=, /=and %=</b>. The <b>+=</b> operator is shorthand for a=a + exp.</p>
		    <h3>Type</h3>
		    <p><b>Statement</b></p>
		    <h3>Syntax</h3>
		    <p><b>variable assignmentOperator expression</b></p>
		    <h3>Arguments</h3>
		    <hr>
		    <table border="3" cellpadding="2" width="100%" bgcolor="#99CCCC">
		      <tr>
		        <th align="CENTER" width="20%">Arguments</th>
		        <th align="LEFT" width="80%">Explanation</th>
		      </tr>
		      <tr>
		        <th align="CENTER" width="20%">variable <b style="mso-bidi-font-weight: normal"></o:p>
		          </b></th>
		        <td align="LEFT" width="80%">The<span style="mso-spacerun: yes">
		          variable to be assigned a new value.</span></td>
		      </tr>
		      <tr>
		        <th align="CENTER" width="20%"><b>assignmentOperator</b><b style="mso-bidi-font-weight: normal">
		          </o:p>
		          </b></th>
		        <td align="LEFT" width="80%">The assignment Operator. Must be one of:&nbsp;
		          <b>=, +=, -=, *=, /=, %=.</b></td>
		      </tr>
		      <tr>
		        <th align="CENTER" width="20%"><b style="mso-bidi-font-weight: normal">expression</b><b style="mso-bidi-font-weight: normal">
		          </o:p>
		          </b></th>
		        <td align="LEFT" width="80%">If the <b>assignmentOperator</b> is (=),
		          the expression may return any value. If the <b>assignmentOperator</b>
		          is (+=, -=, *=, /=, or %=), then the expression must return a numeric
		          value.</td>
		      </tr>
		      <tr>
		        <th align="CENTER" width="20%">RETURN</th>
		        <td align="LEFT" width="80%">If the <b>assignmentOperator</b> is (=),
		          the variable is set to the value of expression. If the <b>assignmentOperator</b>
		          is (+=, -=, *=, /=, or %=), then the arithmetic operation is performed
		          on the original variable and the expression.</td>
		      </tr>
		    </table>
		    <hr>
		    <h3><u>When To Use</u></h3>
		    <p>Use the <b style="mso-bidi-font-weight: normal">assignment</b><b style="mso-bidi-font-weight: normal">
		    operator</b> statement to assign a new value to a Selector variable.</p>
		    <h3><u>Example1</u></h3>
		    <p class="MsoBodyText">This simple example demonstrates the effect of the
		    simple assignment operator.</p>
		    <p class="MsoBodyText"><b>a = 3.14;
		    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //Returns 3.14</b></p>
		    <h3><u>Example2</u></h3>
		    <p class="MsoBodyText">This examples demonstrate the effects of the
		    arithmetic assignment operators.</p>
		    <p class="MsoBodyText"><b>var a = 100;<br>
		    <br>
		    var b&nbsp; = 100;<br>
		    <br>
		    var c = 10;<br>
		    <br>
		    var d = 10;<br>
		    <br>
		    a += 10;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //Returns 110<br>
		    <br>
		    b -= 10;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //Returns 90<br>
		    <br>
		    c *= 10;&nbsp;&nbsp;&nbsp;&nbsp; //Returns 100</b></p>
		    <h3><u>Notes &amp; Hints</u></h3>
		    <p>The <b style="mso-bidi-font-weight: normal">assignment</b><b style="mso-bidi-font-weight: normal">
		    operator</b> statement assigns a new value to a Selector variable.</p>
            
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>bgmregress</Heading>
		<Description><![CDATA[
			<p>The <b>bgmregress</b> command trains one of our proprietary basis grid regression Lambdas, on the specified numeric expression list.
            The bgmregress statement should be the ONLY statement in the Selector program, and returns the trained basis grid regression Lambda. 
            <br>
            <h3>Syntax</h3>		    
		    <p>bgmregress (numericExpression1,numericExpression2,numericExpression3,numericExpressionN);</p>
		    <table bgcolor="#99CCCC" class=members_table border="3" cellpadding="2" >
			<colgroup><col class="member"><col class="description"></colgroup>
		      <tr>
		        <td valign="top" width="31%"><b>bgmregress</b></td>
		        <td valign="top" width="69%">Mandatory keyword</td>
		      </tr>
		      <tr>
		        <td valign="top" width="31%"><b>numericExpression1</b></td>
		        <td valign="top" width="69%">(Mandatory)Numeric expression.</td>
		      </tr>
		      <tr>
		        <td valign="top" width="31%"><b>numericExpression2</b></td>
		        <td valign="top" width="69%">(Optional)Numeric expression.</td>
		      </tr>
		      <tr>
		        <td valign="top" width="31%"><b>numericExpression3</b></td>
		        <td valign="top" width="69%">(Optional)Numeric expression.</td>
		      </tr>
		      <tr>
		        <td valign="top" width="31%"><b>numericExpression4</b></td>
		        <td valign="top" width="69%">(Optional)Numeric expression.</td>
		      </tr>
		    </table>
		    <h3>Example1</h3>
		    <pre>bgmregress(exp(x4),x10);</pre>
		    <p>This will train a basis grid regression Lambda on the numeric expressions: exp(x4) and x10.</p>
            <p><font color=red><B>Note:</b></font> 
            Inside the <i>numericExpressions</i>, the name <b>XT</b> refers to the ESM row manager Lambda input argument.
            The name <b>xv</b> refers to the currently focused record.
            The names <b>xtime</b>, <b>y</b>, and <b>x1</b> thru <b>xm</b>, refer to elements within the currently focused record.</p>
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>child</Heading>
		<Description><![CDATA[
			<h3>Overview</h3>
		    <p>The <b>child</b> function declaration creates a new Lambda object and
		    assigns it to the specified persistent variable name <b>childName</b> of the
		    parent Lambda <b>parent</b>. The <b>child</b> function declaration always
		    returns the newly created object identifier of the new Lambda. The child
		    Lambda is invoked using the <b>parent.childName()</b> syntax form.</p>
		    <h3>Type</h3>
		    <p><b>Statement</b></p>
		    <h3>Syntax</h3>
		    <p><b>child parent childName ([type] arg..) { cvardec pvardec vardec exp }</b></p>
		    <h3>Arguments</h3>
		    <hr>
		    <table border="3" cellpadding="2" width="100%" bgcolor="#99CCCC">
		      <tr>
		        <th align="CENTER" width="20%">Arguments</th>
		        <th align="LEFT" width="80%">Explanation</th>
		      </tr>
		      <tr>
		        <td align="center"><b>child</b></td>
		        <td>
		          <p>Mandatory keyword.</p>
		        </td>
		      </tr>
		      <tr>
		        <td align="center"><b>parent</b></td>
		        <td>
		          <p>Name of the parent class</p>
		        </td>
		      </tr>
		      <tr>
		        <td align="center"><b>childName</b></td>
		        <td>
		          <p>The name of the child Lambda (function)</p>
		        </td>
		      </tr>
		      <tr>
		        <td align="center"><b>([type] arg...)</b></td>
		        <td>
		          <p>A list of type hints (optional) and arguments separated by commas.
		          If there are no arguments the parenthesis are still required. <b>Note</b>:
		          The optional type hints can be one of the following: <b>char</b>,<b>
		          bool</b>,<b> int</b>,<b> float</b>,<b> </b>or<b> obj</b></p>
		        </td>
		      </tr>
		      <tr>
		        <td align="center"><b>cvardec</b></td>
		        <td>
		          <p>Optional declaration. If present, must be followed by a persistent
		          class variable declaration. See <b>cvar</b>.</p>
		        </td>
		      </tr>
		      <tr>
		        <td align="center"><b>pvardec</b></td>
		        <td>
		          <p>Optional declaration. If present, must be followed by a persistent
		          variable declaration. See <b>pvar</b>.</p>
		        </td>
		      </tr>
		      <tr>
		        <td align="center"><b>vardec</b></td>
		        <td>
		          <p>Optional declaration. If present, must be followed by a local
		          variable declaration. See <b>var</b>.</p>
		        </td>
		      </tr>
		      <tr>
		        <td align="center"><b>exp</b></td>
		        <td>
		          <p>The Selector statements that form the Lambda</p>
		        </td>
		      </tr>
		      <tr>
		        <th align="CENTER" width="20%">RETURN</th>
		        <td align="LEFT" width="80%">An Lambda object.</td>
		      </tr>
		    </table>
		    <hr>
		    <h3><u>When To Use</u></h3>
		    <p>Use the <b style="mso-bidi-font-weight: normal">child</b> statement to
		    create a new Selector Lambda with a child relationship to a parent Lambda.</p>
		    <h3><u>Example1</u></h3>
		    <p class="MsoBodyText">This simple example demonstrates the effect of the
		    simple assignment operator.</p>
		    <blockquote>
		      <pre><b>function  foo(x)  {
		
		               pvar y;
		
		               y = x
		
		               return (y);
		
		               }
		
		
		
		               child foo sum (x)  {
		
		               var temp ;
		
		               temp = x +  y;
		
		               return ( temp );
		
		    }</b></pre>
		    </blockquote>
		    <p>We can invoke the parent and the child Lambda as follows:</p>
		    <blockquote>
		      <pre><b>foo(10);                          //Returns 10
		
		              foo.sum(3);                       //Returns 13
		
		              compareEQ(foo.Cv, foo.sum.Cv); &nbsp;&nbsp; //Returns true
		
		              compareEQ(foo.Pv, foo.sum.Pv); &nbsp;&nbsp;&nbsp;//Returns true</b></pre>
		    </blockquote>
		    <h3><u>Notes &amp; Hints</u></h3>
		    <p>The <b style="mso-bidi-font-weight: normal">child</b> statement creates a
		    new Selector Lambda with a child relationship to a parent Lambda. A Child
		    Lambda inherits <b>pvar</b> and <b>cvar</b> structures from its parent Lambda.</p>
           
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>class</Heading>
		<Description><![CDATA[
			<h3>Overview</h3>
		    <p>The <b>class</b> statement declares a class and names all of its members.
		    The <b>class</b> statement can also use to sub-class, i.e., create a class
		    that inherits from another class and extend it.</p>
		    <h3>Type</h3>
		    <p><b>Statement</b></p>
		    <h3>Syntax</h3>
		    <p><b>class className {member1 ; member2 ... }</b></p>
		    <p><b>class className extends parentClass {member1 ; member2 ... }</b></p>
		    <h3>Arguments</h3>
		    <hr>
		    <table border="3" cellpadding="2" width="100%" bgcolor="#99CCCC">
		      <tr>
		        <th align="CENTER" width="20%">Arguments</th>
		        <th align="LEFT" width="80%">Explanation</th>
		      </tr>
		      <tr>
		        <td align="center"><b>class</b></td>
		        <td>
		          <p>Mandatory Keyword</p>
		        </td>
		      </tr>
		      <tr>
		        <td align="center"><b>className</b></td>
		        <td>
		          <p>The name of this class</p>
		        </td>
		      </tr>
		      <tr>
		        <td align="center"><b>extends</b></td>
		        <td>
		          <p>Optional Keyword. If present, it must be followed by an existing
		          parent class.</p>
		        </td>
		      </tr>
		      <tr>
		        <td align="center"><b>{member1; member2; ...}</b></td>
		        <td>
		          <p>The names of the class members, where each name is separated by a
		          semicolon</p>
		        </td>
		      </tr>
		      <tr>
		        <th align="CENTER" width="20%">RETURN</th>
		        <td align="LEFT" width="80%">An class name.</td>
		      </tr>
		    </table>
		    <hr>
		    <h3><u>When To Use</u></h3>
		    <p>Use the <b style="mso-bidi-font-weight: normal">class</b> statement to
		    create a new class or to extend and existing class through in heritance.</p>
		    <h3><u>Example1</u></h3>
		    <p class="MsoBodyText">This simple example demonstrates the effect of the
		    simple assignment operator.</p>
		    <blockquote>
		      <pre><b>class employee {Name; Address; Salary }
		
		              emp = new ('employee' , 'Name' ,&quot;Tom Jones&quot; , 'Address',  &quot;200 Main Street, Small Town,  USA&quot; )
		
		              display(emp.Name)</b></pre>
		    </blockquote>
		    <h3><u>Example2</u></h3>
		    <p class="MsoBodyText">This simple example demonstrates the effect of the
		    simple assignment operator.</p>
		    <blockquote>
		      <pre><b>class employee {Name; Address; Salary }
		
		              class manager extends 'employee' {Perks }
		
		              emp = new ('employee' , 'Name' ,&quot;Tom Jones&quot; , 'Address',  &quot;200 Main Street, Small Town,  USA&quot; )
		
		              mgr = new('manager', 'Name', &quot;John Smith&quot;, 'Perks', &quot;Special Parking Space&quot;)</b></pre>
		    </blockquote>
		    <h3><u>Notes &amp; Hints</u></h3>
		    <p>The <b style="mso-bidi-font-weight: normal">class</b> statement creates a
		    new class or to extend and existing class through in heritance.</p>
             
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Comparison Operators</Heading>
		<Description><![CDATA[
			<p>The comparison operators are <b>==(equal) , !=(not equal) ,&lt; (less
		    than) , &lt;=(less than or equal) , &gt; (greater than), &gt;=(greater than
		    or equal)</b> . The resulting value is a Boolean value of <b>true</b> if the
		    relational operator results in a true comparison otherwise the resulting
		    value is <b>false. </b>Selector performs type checking and type
		    conversions, therefore the operands need not be the same type.</p>
            <h3>Syntax</h3>		    
            <p>(obj1 operator obj2)</p>
		    <table bgcolor="#99CCCC" class=members_table border="3" cellpadding="2" >
			<colgroup><col class="member"><col class="description"></colgroup>
		      <tr align="top">
		        <td><b>obj1</b></td>
		        <td>
		          <p>First object to be compared</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>operator</b></td>
		        <td>
		          <p>Must be <b>==(equal), !=(not equal) ,&lt; (less than) , &lt;=(less
		          than or equal) , &gt; (greater than), &gt;=(greater than or equal)</b>
		          .</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>obj2</b></td>
		        <td>
		          <p>Second object to be compared</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>Returns</b></td>
		        <td>
		          <p>Returns true if the comparison is true, false if the comparison is
		          false</p>
		        </td>
		      </tr>
		    </table>
		    <p>Example1</p>
		    <pre>
                 var a = true; //Returns True
		
                 (a == true ); //Returns  True  
		
                 ( 2 &lt; 3);  //Returns  True
		
                 ( 2 &gt; 3);  //Returns  False
            </pre>
             
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Conditional Expression</Heading>
		<Description><![CDATA[
			<p>A conditional expression is a shorthand version of the if-else statement.
		    Besides having a shorter syntax, conditional expression always return a
		    value, unlike the if statement.</p>
            <h3>Syntax</h3>		    
            <p>condition ? val1 : val2</p>
		    <table bgcolor="#99CCCC" class=members_table border="3" cellpadding="2" >
			<colgroup><col class="member"><col class="description"></colgroup>
		      <tr align="top">
		        <td><b>condition</b></td>
		        <td>
		          <p>A Boolean expression</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>?</b></td>
		        <td>
		          <p>Mandatory operator</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>val1</b></td>
		        <td>
		          <p>The value to be returned if condition is true.</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>:</b></td>
		        <td>
		          <p>Mandatory operator</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>val2</b></td>
		        <td>
		          <p>The value to be returned if condition is false.</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>Returns</b></td>
		        <td>
		          <p>If the condition is true, val1 is returned, otherwise val2 is
		          returned</p>
		        </td>
		      </tr>
		    </table>
		    <p>Example1</p>
		    <pre>a = 1;
		
		         X = (a &gt;= 1) ? 1 : -1  ); // X will be set to 1</pre>
           
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>cvar</Heading>
		<Description><![CDATA[
			<p>The <b>cvar</b> statement declares a persistent class variable within an
		    Lambda. This means that the persistent class variable will have a scope for
		    the life of the Lambda object. The class variable is created and initialized
		    when the Lambda is instantiated and re-initialized each time the Lambda is
		    invoked. Although Selector is a dynamically typed language, the cvar
		    allows the programmer to optionally provide hints to the compiler by
		    specifying a type for variable declarations. The supported types are: char,
		    bool, int, float, and obj. Specifying a type of obj is equivalent to
		    specifying no type information.<br>
            <h3>Syntax</h3>		    
		    <p>cvar varName<br>
		    cvar varName=init<br>
		    cvar type varName<br>
		    cvar type varName=init</p>
		    <table bgcolor="#99CCCC" class=members_table border="3" cellpadding="2" >
			<colgroup><col class="member"><col class="description"></colgroup>
		      <tr align="top">
		        <td><b>cvar</b></td>
		        <td>
		          <p>Mandatory keyword</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>type</b></td>
		        <td>obj char bool int float obj text string symbol bitvec fltvec pcdvec
		          stc dir dic matrix nummat vec bitvec numvec intvec objvec</td>
		      </tr>
		      <tr align="top">
		        <td><b>cvarName</b></td>
		        <td>
		          <p>The name of the persistent class variable</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>=init</b></td>
		        <td>
		          <p>Optional argument. If present, the compiler will assign the <b>cvarName</b>
		          to the value in <b>init</b>.</p>
		        </td>
		      </tr>
		    </table>
		    <b>Note</b>: If type hints are supplied, the programmer must assure that no
		    other types are saved in the variable. <b>Note</b>: Multiple variable
		    declarations may be separated by commas as in: <b>cvar x, int y, z;</b>.
		    <p>When To Use</p>
		    <p>Unlike a temporary variable <b>(var)</b> that is declared inside a
		    function and which is only &quot;alive&quot; while the function is currently
		    active, a <b>cvar </b>(persistent variable) is accessible, as long the Lambda
		    object instance is active. The contents of the entire <b>cvar</b> structure
		    is in <b>LambdaName.Cv</b>, where <b>LambdaName</b> is the name of the
		    instance of the Lambda. The contents of a single <b>cvar </b>variable is <b>LambdaName.Cv.memberName;</b></p>
		    <p>A persistent class variable <b>(cvar)</b> is analogous to a static
		    variable in a C or C++ programming environment. It is created and
		    initialized when the Lambda is instantiated and it is persistent, meaning the
		    memory is not released as long as the Lambda is in memory. However, the <b>cvar</b>
		    differs from the <b>pvar</b> in that it is always re-initialized on entry to
		    the Lambda that encapsulates it.</p>
		    <p>Example1</p>
		    <pre>function squareIt(x)  {
		
		    cvar float xx;
		
		    var y;
		
		    xx = x * x;
		
		    y = xx;
		
		    return (xx);
		
		    }
		
		
		
		    var result, int;
		
		    result = squareIt(10);
		
		    writeln(result); //displays &quot;100&quot;
		
		    writeln(squareIt.Cv.xx); //displays &quot;100&quot;		
		
		    writeln(squareIt.y); //error !Cannot find name: y in the PV structure of Lambda squareIt!</pre>
		    <p>Example2</p>
		    <pre>function foo  (x)  {
		
		         cvar z;
		
		         z = x;
		
		         return(z);
		
		         }
		
		
		
		         var z;
		
		         result = foo(7);
		
		         display(result);  //displays &quot;7&quot;
		
		         display(z);  //displays #void  (accesses the contents of the global variable)
		
		         display(foo.Cv.z);  //displays &quot;7&quot;</pre>
		    <p>Notes and Hints</p>
		    <p>The scope of variables in nested Lambdas depends upon the following
		    compilation rules: Nested Lambdas do not share either their arguments or
		    their temporary variables. The argument and the temporary variables are
		    always limited in scope to the immediate Lambda object. A nested Lambda shares
		    the persistent variables <b>(pvar)</b> and the persistent class variables <b>(cvar)</b>
		    of its parents. When a variable reference is encountered, the Selector
		    compiler first searches the local Lambda's temporary variables<b> (var)</b>,
		    then the persistent variable structure <b>(pvar)</b>, then the persistent
		    class structure <b>(cvar)</b>, and finally the global variables.</p>
		    <p>Also note: Any variable that is used without explicit declaration is
		    automatically declared as a global variable. (This is true even if it is
		    used inside a function definition).</p>
            
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>ennregress</Heading>
		<Description><![CDATA[
			<p>The <b>ennregress</b> command trains one of our evolutionary neural net regression Lambdas, on the specified numeric weight list.
            The ennregress statement should be the ONLY statement in the Selector program, and returns the trained neural net regression Lambda. 
            <br>
            <h3>Syntax</h3>		    
		    <p>ennregress (weightVector);</p>
		    <table bgcolor="#99CCCC" class=members_table border="3" cellpadding="2" >
			<colgroup><col class="member"><col class="description"></colgroup>
		      <tr>
		        <td valign="top" width="31%"><b>ennregress</b></td>
		        <td valign="top" width="69%">Mandatory keyword</td>
		      </tr>
		      <tr>
		        <td valign="top" width="31%"><b>weightVector</b></td>
		        <td valign="top" width="69%">Vector of hidden layer weights.</td>
		      </tr>
		    </table>
		    <h3>Example1</h3>
		    <pre>ennregress(#(obj| #(obj| #(num| 34.56 -23.5))));</pre>
		    <p>This will train a neural net regression Lambda on the numeric hidden layer weight vector.</p>
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>for</Heading>
		<Description><![CDATA[
			<p>The <b>for</b> statement repeats a set of Selector statements until the
		    specified condition is false.</p>
            <h3>Syntax</h3>		    
		    <p>for ( initexp; cond; increxp ) { exp ... }</p>
		    <table bgcolor="#99CCCC" class=members_table border="3" cellpadding="2" >
			<colgroup><col class="member"><col class="description"></colgroup>
		      <tr align="top">
		        <td><b>for</b></td>
		        <td>
		          <p>Mandatory keyword</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>Initexp;</b></td>
		        <td>
		          <p>Initializer expression. Usually resets an index to be used to
		          determine loop control</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>cond;</b></td>
		        <td>
		          <p>An expression that returns a Boolean value. Usually tests the loop
		          index to see if has reached a max number.</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>Increxp</b></td>
		        <td>
		          <p>Increment expression. Usually increments (or decrements) the loop
		          index.</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>{expr...}</b></td>
		        <td>
		          <p>A set of Selector statements</p>
		        </td>
		      </tr>
		    </table>
		    <p>Example1</p>
		    <pre>var int a;
		
		         for (a = 1; a &lt; 10; ++a ) { display(&quot; &quot;, a)}</pre>
		    <p>This will display the following line in the console screen:</p>
		    <pre>1  2  3  4  5  6  7  8  9  </pre>
            
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>friend</Heading>
		<Description><![CDATA[
			<p>The <b>friend </b>statement creates a new Lambda object and assigns it to
		    the specified persistent variable name <b>friendName</b> of the parent Lambda
		    <b>parent</b>. The <b>friend </b>function<b> </b>declaration always returns
		    the newly created Lambda object. The new friend Lambda is invoked by using the
		    <b>parent.friendName ()</p>
            <h3>Syntax</h3>		    
		    <p>friend parent friendName ([type] arg..) { cvardec pvardec vardec exp}</p>
		    <table bgcolor="#99CCCC" class=members_table border="3" cellpadding="2" >
			<colgroup><col class="member"><col class="description"></colgroup>
		      <tr align="top">
		        <td><b>friend</b></td>
		        <td>
		          <p>Mandatory keyword.</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>parent</b></td>
		        <td>
		          <p>Name of the parent class</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>friendName</b></td>
		        <td>
		          <p>The name of the friend Lambda (function)</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>([type] arg...)</b></td>
		        <td>
		          <p>A list of type hints (optional) and arguments separated by commas.
		          If there are no arguments the parenthesis are still required. <b>Note</b>:
		          The optional type hints can be one of the following: <b>char</b>,<b>
		          bool</b>,<b> int</b>,<b> float</b>,<b> </b>or<b> obj</b></p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>cvardec</b></td>
		        <td>
		          <p>Optional declaration. If present, must be followed by a persistent
		          class variable declaration. See <b>cvar</b>.</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>pvardec</b></td>
		        <td>
		          <p>Optional declaration. If present, must be followed by a persistent
		          variable declaration. See <b>pvar</b>.</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>vardec</b></td>
		        <td>
		          <p>Optional declaration. If present, must be followed by a local
		          variable declaration. See <b>var</b>.</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>exp</b></td>
		        <td>
		          <p>The Selector statements that form the Lambda</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>Returns</b></td>
		        <td>
		          <p>Lambda object identifier</p>
		        </td>
		      </tr>
		    </table>
		    <p>Example1</p>
		    <b>function foo(x) {<br>
		    pvar y=5, d;<br>
		    d=x<br>
		    return (d);<br>
		    }<br>
		    <br>
		    friend foo sum (x) {<br>
		    pvar d=20 ;<br>
		    var temp ;<br>
		    temp=x + d + y;<br>
		    return ( temp );<br>
		    }</b>
		    <p>We can invoke the parent and friend Lambdas as follows:</p>
		    <pre>foo(10); //Returns 10
		
	           	 foo.sum(3); //Returns 28
		
		         compareEQ(foo.Pv,  foo.sum.Cv); //Returns true
		
		         compareEQ(foo.Pv,  foo.sum.Pv); //Returns false</pre>
		    <p>Notes and Hints</p>
		    <p>A friend Lambda has a different <b>pvar</b> structure than its parent
		    Lambda, but the <b>cvar</b> structure of a friend Lambda is the <b>pvar</b>
		    structure of its parent Lambda.</p>
            
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>frmregress</Heading>
		<Description><![CDATA[
			<p>The <b>frmregress</b> command trains a multivariate factor regression on the specified numeric expression list.
            The frmregress statement should be the ONLY statement in the Selector program, and returns the trained
            multivariate factor regression Lambda. 
            <br>
            <h3>Syntax</h3>		    
		    <p>frmregress(numericExpression1,numericExpression2,,...,numericExpressionN);</p>
		    <table bgcolor="#99CCCC" class=members_table border="3" cellpadding="2" >
			<colgroup><col class="member"><col class="description"></colgroup>
		      <tr>
		        <td valign="top" width="31%"><b>frmregress</b></td>
		        <td valign="top" width="69%">Mandatory keyword</td>
		      </tr>
		      <tr>
		        <td valign="top" width="31%"><b>numericExpression1</b></td>
		        <td valign="top" width="69%">Numeric expression.</td>
		      </tr>
		      <tr>
		        <td valign="top" width="31%"><b>numericExpressionN</b></td>
		        <td valign="top" width="69%">Numeric expression.</td>
		      </tr>
		    </table>
		    <h3>Example1</h3>
		    <pre>frmregress(exp(x4),x10);</pre>
		    <p>This will train a multivariate factor regression model on the numeric expressions: exp(x4) and x10.</p>
            <p><font color=red><B>Note:</b></font> 
            Inside the <i>numericExpressions</i>, the name <b>XT</b> refers to the ESM row manager Lambda input argument.
            The name <b>xv</b> refers to the currently focused record.
            The names <b>xtime</b>, <b>y</b>, and <b>x1</b> thru <b>xm</b>, refer to elements within the currently focused record.</p>
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>function</Heading>
		<Description><![CDATA[
			<p>A Selector function compiles into a first class Lambda object. It has
		    the same footprint and is treated like any Lambda Object created by any
		    compiler in the LambdaClient development environment.</p>
            <h3>Syntax</h3>		    
		    <p>function name ([type] arg..) { cvardec pvardec vardec exp }<br>
		    function ([type] arg..) { cvardec pvardec vardec exp }</p>
		    <table bgcolor="#99CCCC" class=members_table border="3" cellpadding="2" >
			<colgroup><col class="member"><col class="description"></colgroup>
		      <tr align="top">
		        <td><b>function</b></td>
		        <td>
		          <p>Mandatory keyword.</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>name</b></td>
		        <td>
		          <p>(Optional) The name of the Lambda (function)</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>([type] arg...)</b></td>
		        <td>
		          <p>A list of type hints (optional) and arguments separated by commas.
		          If there are no arguments the parenthesis are still required. <b>Note</b>:
		          The optional type hints can be one of the following: <b>char</b>,<b>
		          bool</b>,<b> int</b>,<b> float</b>,<b> </b>or<b> obj</b></p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>cvardec</b></td>
		        <td>
		          <p>Optional Argument. If present, must be followed by a persistent
		          variable declaration. See <b>cvar</b>.</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>pvardec</b></td>
		        <td>
		          <p>Optional Argument. If present, must be followed by a persistent
		          variable declaration. See <b>pvar</b>.</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>vardec</b></td>
		        <td>
		          <p>Optional Argument. If present, must be followed by a local variable
		          declaration. See <b>var</b>.</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>exp</b></td>
		        <td>
		          <p>The Selector statements that form the Lambda</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>Returns</b></td>
		        <td>
		          <p>Lambda object identifier</p>
		        </td>
		      </tr>
		    </table>
		    <p>Note: A function with no name will generate an unnamed function.</p>
		    <p>When To Use</p>
		    <p>The <b>function </b>statement defines the name of an Lambda, its
		    arguments, and the statements that comprise the Lambda.</p>
		    <p>Example1</p>
		    <pre>function foo (x) {
		
		         writeln( &quot;The value of x is &quot; , x);
		
		         }</pre>
		    <p>To invoke:</p>
		    <pre>foo(29);</pre>
		    <p>Example2</p>
		    <pre>function sayHello  ( ) {
		
		         writeln( &quot;Hello World&quot;);
		
		          }</pre>
		    <p>To invoke:</p>
		    <pre>sayHello();</pre>
             
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Function Invocation</Heading>
		<Description><![CDATA[
				<p>Selector functions may be called by name or by Object Identifier. The
			    function to be invoked must always have a set of parenthesis following the
			    function name. If the function requires arguments, the arguments are passed
			    as a comma separated list inside the parenthesis.</p>
                <h3>Syntax</h3>		    
			     name ()<br>
			     name (arg ...)<br>
			     Lambda.child ()<br>
			     Lambda.friend ( arg ...)<br>
			     class.method ()<br>
			     class.method ( arg ...)</p>
			    <table bgcolor="#99CCCC" class=members_table border="3" cellpadding="2" >
				<colgroup><col class="member"><col class="description"></colgroup>
				  <tr align="top">
			        <td><b>name</b></td>
			        <td>
			          <p>The name of the function to be invoked.</p>
			        </td>
			      </tr>
			      <tr align="top">
			        <td><b>class.method</b></td>
			        <td>
			          <p>Alternate form of function name. Must be a method of a specified
			          class</p>
			        </td>
			      </tr>
			      <tr align="top">
			        <td><b>(arg...)</b></td>
			        <td>
			          <p>A list of arguments separated by commas. If there are no arguments
			          the parenthesis are still required.</p>
			        </td>
			      </tr>
			      <tr align="top">
			        <td><b>Returns</b></td>
			        <td>
			          <p>Lambda object identifier</p>
			        </td>
			      </tr>
			    </table>
			    <p>Example1</p>
			    <pre>foo (x) {
			
			         display( &quot;The value of x is &quot; , x);
			
			          }</pre>
			    <p>To invoke:</p>
			    <pre>foo (1998);</pre>
			    <p>Example2</p>
			    <pre>function  foo(x) {
			
			         pvar y;
			
			         y = x;
			
			         return (y);
			
			         }
			
			
			
		           	child foo sum (x) {
			
			        var temp;
			
			        temp = x +  y;
			
			        return ( temp );
			
			        }</pre>
			    <p>We can invoke the parent and the child Lambda as follows:</p>
			    <pre>foo(10);  //Returns 10
			
			         foo.sum(3);  //Returns 13</pre>
                 
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>if</Heading>
		<Description><![CDATA[
			<p>The <b>if </b>statement selects one of two expressions to evaluate based
		    upon the value of a <b>{test}</b> expression. If the <b>{cond}</b> evaluates
		    to True, then the expressions in the <b>{thenClause}</b> is evaluated.</p>
		    <p>If the <b>else</b> keyword is present and the <b>{test} </b>evaluates to <b>false</b>,
		    then the expressions in the <b>{elseClause}</b> is evaluated.</p>
		    <p>The <b>{test}</b> and <b>{thenClause}</b> expressions are mandatory. The <b>{else}</b>
		    expression is optional. The braces <b>{} </b>are also mandatory.</p>
		    <p>The <b>{elseClause} </b>may also be an <b>if </b>statement therefore
		    creating a limitless number of conditions and conditional paths.</p>
            <h3>Syntax</h3>		    
		    <p>if (test) { trueClause... }<br>
		     if (test) { trueClause... } else { elseClause... }<br>
		     if (test) { trueClause... } else if (test2) { exp ... }</p>
		    <table bgcolor="#99CCCC" class=members_table border="3" cellpadding="2" >
			<colgroup><col class="member"><col class="description"></colgroup>
		      <tr align="top">
		        <td><b>test</b></td>
		        <td>
		          <p>An expression that returns a Boolean value. Must be enclosed in
		          parenthesis.</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>thenClause</b></td>
		        <td>
		          <p>The expression to be executed if the cond clause returned True</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>else</b></td>
		        <td>
		          <p>Optional Keyword</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>elseClause</b></td>
		        <td>
		          <p>Optional expression. If the else keyword is present the elseClause
		          must be present. The expression to be executed if the cond clause
		          returned False</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>else if</b></td>
		        <td>
		          <p>If present, must be a valid if statement.</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>Returns</b></td>
		        <td>
		          <p>Returns the value of the expression evaluated. If no else clause is
		          specified, the returned value is the result of the condition
		          expression.</p>
		        </td>
		      </tr>
		    </table>
		    <p>When To Use</p>
		    <p>The <b>if </b>statement is used whenever it is necessary to perform
		    actions based on a condition</p>
		    <p>Example1</p>
		    <b>j=1</b>
		    <pre>if ( j &lt; 10 ) { k = 100;} else { k = -100;} //sets k to 100</pre>
		    <p>Example2</p>
		    <pre>	j = 2;
		
	  				if ( j &lt; 10 )  { k = 100;} 
		
					else if ( j == 0)  {k = 10;}
		
					else if ( j == 1 ) {k = 100;} 
		
					else if ( j == 2 ) {k = 1000;} //sets k to 1000</pre>
            
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Logical Operators</Heading>
		<Description><![CDATA[
				<p>The logical operators are <b>&amp;&amp; (and) , || (or) , </b>and<b> !
			    (not) </b>. The resulting value from a logical operation is a Boolean value.
			    The <b>&amp;&amp;</b> operator returns <b>true</b> if <i><u>both</u></i><u></u>
			    operands are <b>true</b>, the <b>|| </b>operator<b> </b>returns <b>true</b>
			    if <i><u>one</u></i><u></u> or <i><u>both</u></i><u></u> operands are <b>true</b>,
			    and the <b>!</b> operator returns <b>true</b> if the operand is <b>false</b>,
			    otherwise the <b>!</b> operator returns <b>false</b>. Selector performs
			    type checking and type conversions, therefore if the operands are not
			    Boolean, the operands are converted to Boolean.</p>
                <h3>Syntax</h3>		    
			    <p> (obj1 &amp;&amp; obj2 )<br>
			    (obj1 || obj2 )</p>
		        <table bgcolor="#99CCCC" class=members_table border="3" cellpadding="2" >
			    <colgroup><col class="member"><col class="description"></colgroup>
			      <tr align="top">
			        <td><b>obj1</b></td>
			        <td>
			          <p>First object to be compared</p>
			        </td>
			      </tr>
			      <tr align="top">
			        <td><b>operator</b></td>
			        <td>
			          <p>Must be <b>&amp;&amp; (and) , || (or)</b></p>
			        </td>
			      </tr>
			      <tr align="top">
			        <td><b>obj2</b></td>
			        <td>
			          <p>Second object to be compared</p>
			        </td>
			      </tr>
			      <tr align="top">
			        <td><b>Returns</b></td>
			        <td>
			          <p>The <b>&amp;&amp;</b> operator returns <b>true</b> if <i><u>both</u></i><u></u>
			          operands are <b>true</b>, otherwise it returns <b>false</b>. The <b>||
			          </b>operator<b> </b>returns <b>true</b> if <i><u>one</u></i><u></u> or
			          <i><u>both</u></i><u></u> operands are <b>true</b> otherwise it
			          returns <b>false</b>.</p>
			        </td>
			      </tr>
			    </table>
			    <p><br>
                <h3>Syntax</h3>		    
			    <p>(!obj1)</p>
		        <table bgcolor="#99CCCC" class=members_table border="3" cellpadding="2" >
			    <colgroup><col class="member"><col class="description"></colgroup>
			      <tr align="top">
			        <td><b>!</b></td>
			        <td>
			          <p>Logical not operator<b></b></p>
			        </td>
			      </tr>
			      <tr align="top">
			        <td><b>obj1</b></td>
			        <td>
			          <p>Object to be negated</p>
			        </td>
			      </tr>
			      <tr align="top">
			        <td><b>Returns</b></td>
			        <td>
			          <p>Returns <b>true</b> if the operand is <b>false</b>, otherwise the <b>!</b>
			          operator returns <b>false</b>.</p>
			        </td>
			      </tr>
			    </table>
			    <p>Example1</p>
			    <pre>	var a = true, k;
			
						if ( 2 &lt; 3) &amp;&amp; (a == true) {k = 1;} else {k = 2;} // sets k to 1   
			
						if ( 2 &gt; 3) || (a == true) {k = 1;} else {k = 2;} // sets k to 1   
			
						if ( 2 &gt; 3) &amp;&amp; (!a) {k = 1;} else {k = 2;}// sets k to 2</pre>
              
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>method</Heading>
		<Description><![CDATA[
			<p>The <b>method</b> statement defines a method to Selector class.</p>
            <h3>Syntax</h3>		    
		    <p>method className methodName([type] arg...) { expr }</p>
		    <table bgcolor="#99CCCC" class=members_table border="3" cellpadding="2" >
			<colgroup><col class="member"><col class="description"></colgroup>
		      <tr align="top">
		        <td><b>method</b></td>
		        <td>
		          <p>Mandatory keyword</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>className</b></td>
		        <td>
		          <p>Name of the parent class of the method</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>methodName</b></td>
		        <td>
		          <p>Name of this method.</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>([type] arg...)</b></td>
		        <td>
		          <p>A list of type hints (optional) and arguments separated by commas.
		          If there are no arguments the parenthesis are still required. <b>Note</b>:
		          The optional type hints can be one of the following: <b>char</b>,<b>
		          bool</b>,<b> int</b>,<b> float</b>,<b> </b>or<b> obj</b></p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>{expr}</b></td>
		        <td>
		          <p>A set of Selector statements</p>
		        </td>
		      </tr>
		    </table>
		    <p>Example1</p>
		    <pre>	class employee {Name; Address; Salary }
		
					method employee updateSalary( this, newSalary) {
		
					this.Salary = newSalary;
		
					}
		
					emp = new ('employee', 'Name', &quot;Tom Jones&quot;,  'Address',  &quot;200 Main Street, Small Town,  USA&quot; )
		
					emp.updateSalary(45000);
		
					writeln (emp.Name, &quot;'s new salary is &quot;, emp.Salary);</pre>
		    <b>Note: </b>In a method declaration the receiving object (this) is
		    declared, but is not passed in the function invocation.
		    <pre>emp.updateSalary(45000); // This is correct
		
				emp.updateSalary(emp,45000); // This is incorrect</pre>
             
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>mvlregress</Heading>
		<Description><![CDATA[
			<p>The <b>mvlregress</b> command trains a multivariate linear regression on the specified numeric expression list.
            The mvlregress statement should be the ONLY statement in the Selector program, and returns the trained
            multivariate linear regression Lambda. 
            <br>
            <h3>Syntax</h3>		    
		    <p>mvlregress(numericExpression1,numericExpression2,,...,numericExpressionN);</p>
		    <table bgcolor="#99CCCC" class=members_table border="3" cellpadding="2" >
			<colgroup><col class="member"><col class="description"></colgroup>
		      <tr>
		        <td valign="top" width="31%"><b>mvlregress</b></td>
		        <td valign="top" width="69%">Mandatory keyword</td>
		      </tr>
		      <tr>
		        <td valign="top" width="31%"><b>numericExpression1</b></td>
		        <td valign="top" width="69%">Numeric expression.</td>
		      </tr>
		      <tr>
		        <td valign="top" width="31%"><b>numericExpressionN</b></td>
		        <td valign="top" width="69%">Numeric expression.</td>
		      </tr>
		    </table>
		    <h3>Example1</h3>
		    <pre>mvlregress(exp(x4),x10);</pre>
		    <p>This will train a multivariate linear regression model on the numeric expressions: exp(x4) and x10.</p>
            <p><font color=red><B>Note:</b></font> 
            Inside the <i>numericExpressions</i>, the name <b>XT</b> refers to the ESM row manager Lambda input argument.
            The name <b>xv</b> refers to the currently focused record.
            The names <b>xtime</b>, <b>y</b>, and <b>x1</b> thru <b>xm</b>, refer to elements within the currently focused record.</p>
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>new</Heading>
		<Description><![CDATA[
					<p>The <b>new</b> function creates an instance of an object. The Lambda
		    Information Server definition of an object is very specific. It is a memory
		    resident container to store data, which is of variable length or is too
		    large to fit in small fixed containers. The Analytic Information Server object
		    Heap manager supports automated object resizing, garbage collection, and
		    anti-fragmentation algorithms so that the user may concentrate on the
		    analysis and modeling of data rather than on memory management.</p>
		    <p>The Selector compiler supports all of the Analytic Information Server
		    objects. Each object is covered in detail in its own chapter in the <b>Lambda
		    Information Server Programmer's Guide.</b></p>
            <h3>Syntax</h3>		    
		    <p>new ('objectType' )<br>
		
   new ('String' , value)<br>
		
   new ('Symbol' , value)<br>
		
   new  ('Vector' , vecSubType,  size)<br>
		
   new  ('Vector',  vecSubType,  size,  value)<br>
		
   new  ('Structure',  key,  value)<br>
		
   new  ('Structure',  key,  value ... . cdr )<br>
		
   new  ('Dictionary',  key,  value)<br>
		
   new  ('Directory',  key,  value)<br>
		
   new  ('ObjectRepository',  filename,  'key',  code, 'buffer', count)</p>
		    <table bgcolor="#99CCCC" class=members_table border="3" cellpadding="2" >
			<colgroup><col class="member"><col class="description"></colgroup>
		      <tr align="top">
		        <td><b>new</b></td>
		        <td>
		          <p>Mandatory keyword</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>objectType</b></td>
		        <td>
		          <p>Must be 'String', 'Symbol', 'Vector', 'Structure' , 'Dictionary',
		          'Directory', 'ObjectRepository'</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>value</b></td>
		        <td>
		          <p>Optional Argument. If present, must be a constant of type
		          objectType.</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>vecSubType</b></td>
		        <td>
		          <p>Optional argument, but if present must only be used with objectType,
		          Vector. Valid vecSubtypes are</p>
		          <p>'normal'</p>
		          <p>'bit'</p>
		          <p>'integer'</p>
		          <p>'float'</p>
		          <p>'number'</p>
		          <p>'small'</p>
		          <p>'object'</p>
		          <p>'pcode'.</p>
		          <p>If omitted, the default is normal vector.</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>size</b></td>
		        <td>
		          <p>Optional argument, but if present must only be used with objectType,
		          Vector. It must be an integer representing the number of items the
		          Vector will store.</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>key, value</b></td>
		        <td>
		          <p>Optional arguments to Structure, Dictionary, and Directory, but
		          must appear in pairs. There may be an arbitrary number of key, value
		          pairs.</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>. cdr</b></td>
		        <td>
		          <p>Optional Argument to Structure only. If present, it must be a
		          constant that will be assigned to the cdr (tail) of a structure.</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>filename</b></td>
		        <td>
		          <p>If objectType is ObjectRepository, filename is a mandatory field
		          which names the database archive file to be associated with the
		          ObjectRepository. If no such file exists, a new database archive file
		          will be created.</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>'clear'</b></td>
		        <td>
		          <p>If objectType is ObjectRepository, 'clear' is an optional keyword.
		          If present, the database archive file will be cleared immediately
		          before any further processing. If no such file exists, a new database
		          archive file will be created.</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>'key', code</b></td>
		        <td>
		          <p>If objectType is ObjectRepository, 'key' is an optional keyword. If
		          present and it must followed by a number. See ObjectRepository in the
		          Programmer's Guide for more detail.</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>'buffer',count</b></td>
		        <td>
		          <p>If objectType is ObjectRepository, 'buffer' is an optional keyword.
		          If the key word 'buffer' is present, the numeric buffered object count
		          must follow. See ObjectRepository in the Programmer's Guide for more
		          detail.</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>Returns</b></td>
		        <td>
		          <p>The <b>new</b> function will return an object identifier for the
		          type specified in objectType .</p>
		        </td>
		      </tr>
		    </table>
		    <p>Example1</p>
		    <table bgcolor="#99CCCC" class=members_table border="3" cellpadding="2" >
			<colgroup><col class="member"><col class="description"></colgroup>
		      <tr align="top">
		        <td><b>Selector Syntax</b></td>
		        <td><b>Lisp Syntax</b></td>
		      </tr>
		      <tr align="top">
		        <td><b>new ('String', &quot;John&quot;)</b></td>
		        <td><b>(new String: &quot;John&quot;)</b></td>
		      </tr>
		      <tr align="top">
		        <td><b>new ('Symbol' , &quot;Donald&quot;)</b></td>
		        <td><b>(new Symbol: &quot;Donald&quot;)</b></td>
		      </tr>
		      <tr align="top">
		        <td><b>new ('Vector', 3, 11, 12, 99)</b></td>
		        <td><b>(new Vector: 3 11 12 99)</b></td>
		      </tr>
		      <tr align="top">
		        <td><b>new ('Vector' , 'bit', 5, 1 )</b></td>
		        <td><b>(new Vector: bit: 5 1)</b></td>
		      </tr>
		      <tr align="top">
		        <td><b>new ('Structure', 'X', 22, 'Y', 34, ' .', 3)</b></td>
		        <td><b>(new Structure: X: 22 Y: 34 . 3)</b></td>
		      </tr>
		      <tr align="top">
		        <td><b>new ('Dictionary' , 'Name', &quot;John Doe&quot;, 'Age', 30)</b></td>
		        <td><b>(new Dictionary: Name: &quot;John Doe&quot; Age: 30)</b></td>
		      </tr>
		      <tr align="top">
		        <td><b>new ('Directory' , 1, &quot;New Year's Day&quot;, 2,
		          &quot;Valentine's Day&quot; )</b></td>
		        <td><b>(new Directory: 1 &quot;New Year's Day&quot; 2 &quot;Valentine's
		          Day&quot; )</b></td>
		      </tr>
		      <tr align="top">
		        <td><b>new ( 'ObjectRepository', &quot;myarchive.odb&quot;)</b></td>
		        <td><b>(new ObjectRepository: &quot;myarchive.odb&quot;)</b></td>
		      </tr>
		    </table>
            
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>orphan</Heading>
		<Description><![CDATA[
			<p>The <b>orphan </b>statement creates a new Lambda object and assigns it to
		    the specified persistent variable name <b>orphanName</b> of the parent Lambda
		    <b>parent</b>. The <b>orphan </b>function<b> </b>declaration always returns
		    the newly created Lambda object. The new orphan Lambda is invoked by using the
		    <b>parent.orphanName ()</b> syntax form.</p>
            <h3>Syntax</h3>		    
		    <p>orphan parent orphanName ([type] arg..) { cvardec pvardec vardec exp}</p>
		    <table bgcolor="#99CCCC" class=members_table border="3" cellpadding="2" >
			<colgroup><col class="member"><col class="description"></colgroup>
		      <tr align="top">
		        <td><b>orphan</b></td>
		        <td>
		          <p>Mandatory keyword.</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>parent</b></td>
		        <td>
		          <p>Name of the parent class</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>orphanName</b></td>
		        <td>
		          <p>The name of the orphan Lambda (function)</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>([type] arg...)</b></td>
		        <td>
		          <p>A list of type hints (optional) and arguments separated by commas.
		          If there are no arguments the parenthesis are still required. <b>Note</b>:
		          The optional type hints can be one of the following: <b>char</b>,<b>
		          bool</b>,<b> int</b>,<b> float</b>,<b> </b>or<b> obj</b></p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>cvardec</b></td>
		        <td>
		          <p>Optional declaration. If present, must be followed by a persistent
		          class variable declaration. See <b>cvar</b>.</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>pvardec</b></td>
		        <td>
		          <p>Optional declaration. If present, must be followed by a persistent
		          variable declaration. See <b>pvar</b>.</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>vardec</b></td>
		        <td>
		          <p>Optional declaration. If present, must be followed by a local
		          variable declaration. See <b>var</b>.</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>exp</b></td>
		        <td>
		          <p>The Selector statements that form the Lambda</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>Returns</b></td>
		        <td>
		          <p>Lambda object identifier</p>
		        </td>
		      </tr>
		    </table>
		    <p>Example1</p>
		    <pre>  	 function  foo(x) {
		
		   			 pvar y = 5; pvar d =30;
		
		   			 var temp ;
		
		   			 temp = x + d + y;
		
		   			 return ( temp );
		
		   			 }
		
		
		
					orphan foo  sum (x) {
		
		   		    pvar d = 20 ; 
		
		  		    cvar e = 100 ; 
		
		   			var temp ;
		
		   		    temp = x + d + y;
		
		   		   return ( temp );
		
		    		}</pre>
		    <p>We can invoke the parent and orphan Lambdas as follows:</p>
		    <pre> 	y = 10; //declare a global variable
		
				 	foo(10); //Returns 45
		
					foo.sum(10); //Returns 40 (uses the value of global  y)
		
					compareEQ(foo.Cv,  foo.sum.Cv); //Returns false
		
					compareEQ(foo.Pv,  foo.sum.Pv); //Returns false</pre>
		    <p>Notes and Hints</p>
		    <p>An orphan<b> </b>Lambda is a member of its parent's <b>pvar</b> structure.
		    Therefore, the parent knows about the orphan. However, the orphan Lambda has
		    a different <b>pvar</b> structure than its parent Lambda, and a different <b>cvar</b>
		    structure than its parent Lambda. Therefore, an orphan Lambda knows nothing of
		    its parent Lambda.</p>
             
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>pvar</Heading>
		<Description><![CDATA[
		  <p>The <b>pvar</b> statement declares a persistent variable within an Lambda.
		    This means that the persistent class variable will have a scope for the life
		    of the Lambda object. The persistent variable is created and initialized when
		    the Lambda is instantiated and <u>not</u> re-initialized each time the Lambda
		    is invoked. Although Selector is a dynamically typed language, the pvar
		    allows the programmer to optionally provide hints to the compiler by
		    specifying a type for variable declarations. The supported types are: char,
		    bool, int, float, and obj. Specifying a type of obj is equivalent to
		    specifying no type information.</p>
		    <b>Note</b>: If type hints are supplied, the programmer must assure that no
		    other types are saved in the variable.
            <h3>Syntax</h3>		    
		    <p>pvar varName<br> 
		
   pvar varName = init<br>
		
   pvar type varName<br> 
		
   pvar type varName = init</p>
		    <table bgcolor="#99CCCC" class=members_table border="3" cellpadding="2" >
			<colgroup><col class="member"><col class="description"></colgroup>
		      <tr align="top">
		        <td><b>pvar</b></td>
		        <td>
		          <p>Mandatory keyword</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>type</b></td>
		        <td>obj char bool int float obj text string symbol bitvec fltvec pcdvec
		          stc dir dic matrix nummat vec bitvec numvec intvec objvec</td>
		      </tr>
		      <tr align="top">
		        <td><b>pvarName</b></td>
		        <td>
		          <p>The name of the persistent variable</p>
		        </td>
		      </tr>
		      <tr align="top">
		        <td><b>=init</b></td>
		        <td>
		          <p>Optional argument. If present, the compiler will assign the <b>pvarName</b>
		          to the value in <b>init</b>.</p>
		        </td>
		      </tr>
		    </table>
		    <p>When To Use</p>
		    <p>Unlike a <b>var </b>statement that is declared inside a function, the
		    variables are only &quot;alive&quot; while the function is currently active.
		    A <b>pvar </b>(persistent variable) is accessible, as long the Lambda object
		    instance is active. The contents of the <b>pvar</b> structure is in <b>LambdaName.Pv</b>,
		    where <b>LambdaName</b> is the name of the instance of the Lambda.</p>
		    <p>A persistent variable <b>(pvar)</b> is analogous to a static variable in
		    a C or C++ programming environment. . It created and initialized when the
		    Lambda is instantiated.</p>
		    <p>Unlike temporary variables <b>(var),</b> which are always allocated and
		    initialized upon entry to a function and released upon exit, a <b>pvar</b>
		    is initialized once and the memory allocated is fixed until the application
		    ends.</p>
		    <p>Example1</p>
		    <pre>	function squareIt  (x)  {
		
		    		pvar xx;
		
		    		var y;
		
		   			xx = x * x;
		
		    		y = xx;
		
		    		return (xx);
		
		    		}
		
		
		
					var result;
		
					result = squareIt (10);
		
					writeln(result);  //displays &quot;100&quot;
		
					writeln(squareIt.xx);  //displays &quot;100&quot;
		
					writeln(squareIt.y); //error !Cannot find name: y in the PV structure of Lambda squareIt!</pre>
		    <p>Notes and Hints</p>
		    <p>The scope of variables in nested Lambdas depends upon the following
		    compilation rules: Nested Lambdas do not share either their arguments or
		    their temporary variables. The argument and the temporary variables are
		    always limited in scope to the immediate Lambda object. A nested Lambda shares
		    the persistent variables <b>(pvar)</b> and the persistent class variables <b>(cvar)</b>
		    of its parents. When a variable reference is encountered, the Selector
		    compiler first searches the local Lambda's temporary variables<b> (var)</b>,
		    then the persistent variable structure <b>(pvar)</b>, then the persistent
		    class structure <b>(cvar)</b>, and finally the global variables.</p>
		    <p>Also note: Any variable that is used without explicit declaration is
		    automatically declared as a global variable. (This is true even if it is
		    used inside a function definition).</p>
            
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>reg</Heading>
		<Description><![CDATA[
			   <p>The <b>reg</b> statement declares a register variable inside a
		        Selector program. If a <b>reg</b> statement appears outside a
		        function definition, the variable declaration generates an error
		        message.</p>
		        <p>The <b>reg</b> statement declares a register variable within an
		        Lambda. This means that the register variable will have a scope for this
		        invocation of the Lambda object. The register variable is created and
		        initialized each time the Lambda is invoked. Although Selector is a
		        dynamically typed language, the reg allows the programmer to optionally
		        provide hints to the compiler by specifying a type for register
		        declarations. The supported types are: int and float.
		        Specifying a type of int is equivalent to specifying no type
		        information.</p>
		        <b>Note</b>: If type hints are supplied, the programmer must assure that
		        no other types are saved in the variable.</p>
                <h3>Syntax</h3>		    
  		        <p>reg varName<br>
		         reg varName=init|<br>
		         reg type varName<br>
		         reg type varName=init</p>
		        <table bgcolor="#99CCCC" class=members_table border="3" cellpadding="2" >
			    <colgroup><col class="member"><col class="description"></colgroup>
		          <tr align="top">
		            <td><b>reg</b></td>
		            <td>
		              <p>Mandatory keyword</p>
		            </td>
		          </tr>
		          <tr align="top">
		            <td><b>type</b></td>
		            <td>int float</td>
		          </tr>
		          <tr align="top">
		            <td><b>varName</b></td>
		            <td>
		              <p>The name of the register variable</p>
		            </td>
		          </tr>
		          <tr align="top">
		            <td><b>=init</b></td>
		            <td>
		              <p>Optional argument. If present, the compiler will assign the <b>varName</b>
		              to the value in <b>init</b>.</p>
		            </td>
		          </tr>
		        </table>
		        <p>When To Use</p>
		        <p>It is necessary to explicitly declare register variables inside a function
		        using the <b>reg</b> statement. The register variable structure, <b>LambdaName.Rv,
		        </b>(where <b>LambdaName</b> is the name of the instance of the Lambda) is
		        accessible.</p>
		        <p>Register variables are always allocated and initialized upon entry
		        to a function and released upon exit.</p>
		        <p>Example1</p>
		        <pre>	function zoo  (x)  {
		
		    			reg int m;
		
		    			m = x;
		
		    			writeln(m);
		
		    			return (m);
		
		    			}
		
		
		
						var m = 99;
		
						result = zoo(7); //displays &quot;7&quot;  (accesses the contents of the local variable)
		
						writeln(m);  //displays &quot;99&quot;  (accesses the contents of the global variable)
		
						writeln(zoo.m); //error--!Cannot find name: m in the PV structure of Lambda zoo!</pre>
            
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>regress</Heading>
		<Description><![CDATA[
			<p>The <b>regress</b> command trains a simple linear regression on the specified numeric expression.
            The regress statement should be the ONLY statement in the Selector program, and returns the trained
            simple linear regression Lambda. 
            <br>
            <h3>Syntax</h3>		    
		    <p>regress numericExpression;</p>
		    <table bgcolor="#99CCCC" class=members_table border="3" cellpadding="2" >
			<colgroup><col class="member"><col class="description"></colgroup>
		      <tr>
		        <td valign="top" width="31%"><b>regress</b></td>
		        <td valign="top" width="69%">Mandatory keyword</td>
		      </tr>
		      <tr>
		        <td valign="top" width="31%"><b>numericExpression</b></td>
		        <td valign="top" width="69%">Numeric expression.</td>
		      </tr>
		    </table>
		    <h3>Example1</h3>
		    <pre>regress exp(x4)*x10;</pre>
		    <p>This will train a simple linear regression model on the numeric expression: exp(x4)*x10.</p>
            <p><font color=red><B>Note:</b></font> 
            Inside the <i>numericExpression</i>, the name <b>XT</b> refers to the ESM row manager Lambda input argument.
            The name <b>xv</b> refers to the currently focused record.
            The names <b>xtime</b>, <b>y</b>, and <b>x1</b> thru <b>xm</b>, refer to elements within the currently focused record.</p>
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>return</Heading>
		<Description><![CDATA[
		<p>The <b>return</b> statement exits the current function passes a
        single value to the caller.</p>
        <h3>Syntax</h3>		    
        <p>return ( value )</p>
		    <table bgcolor="#99CCCC" class=members_table border="3" cellpadding="2" >
			<colgroup><col class="member"><col class="description"></colgroup>
          <tr align="top">
            <td><b>value</b></td>
            <td>
              <p>Any expression that returns a value. May be embedded in
              parenthesis</p>
            </td>
          </tr>
          <tr align="top">
            <td><b>Returns</b></td>
            <td>
              <p>Immediately evaluates the expression, returns the value, and
              resume execution of the caller function</p>
            </td>
          </tr>
        </table>
        <p>When To Use</p>
        <p>The <b>return</b> statement is used to leave the current function and
        return to the caller with a return value.</p>
        <p>Example1</p>
        <pre>	
                function foo  (x)  {

                var m;

                m = x * 2;

                return (m); // the value of m is returned to the caller

                }



                result = foo(7); //result will receive the value of 14
        </pre>
         
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>score</Heading>
		<Description><![CDATA[
			<p>The <b>score</b> statement performs a set of scoring operations on the current row manager.
		    The score statement must be followed by a single command. The score
		    statement supports commands for averaging and totalling rows from the
		    The score statement does not alter the current record view.
		    </p>
            <h3>Syntax</h3>		    
		    <p>score command;</p>
		    <table bgcolor="#99CCCC" class=members_table border="3" cellpadding="2" >
			<colgroup><col class="member"><col class="description"></colgroup>
		      <tr>
		        <td valign="top" width="31%"><b>score</b></td>
		        <td valign="top" width="69%">Keyword (optional)</td>
		      </tr>
		      <tr>
		        <td valign="top" width="31%"><b>command</b></td>
		        <td valign="top" width="69%">score statement command (mandatory).</td>
		      </tr>
		    </table>
            <h3>Syntax</h3>		    
		    <p>score expression;</p>
		    <table bgcolor="#99CCCC" class=members_table border="3" cellpadding="2" >
			<colgroup><col class="member"><col class="description"></colgroup>
		      <tr>
		        <td valign="top" width="31%"><b>score</b></td>
		        <td valign="top" width="69%">Keyword (mandatory)</td>
		      </tr>
		      <tr>
		        <td valign="top" width="31%"><b>expression</b></td>
		        <td valign="top" width="69%">Any valid javaFilter expression including
		          imbedded score statement commands (mandatory).</td>
		      </tr>
		    </table>
            <h3>Syntax</h3>		    
		    <p>score variable=command;</p>
		    <table bgcolor="#99CCCC" class=members_table border="3" cellpadding="2" >
			<colgroup><col class="member"><col class="description"></colgroup>
		      <tr>
		        <td valign="top" width="31%"><b>score</b></td>
		        <td valign="top" width="69%">Keyword (optional)</td>
		      </tr>
		      <tr>
		        <td valign="top" width="31%"><b>variable</b></td>
		        <td valign="top" width="69%">A variable name to receive the finished
		          score.</td>
		      </tr>
		      <tr>
		        <td valign="top" width="31%"><b>=</b></td>
		        <td valign="top" width="69%">The javaFilter assignment operator.</td>
		      </tr>
		      <tr>
		        <td valign="top" width="31%"><b>command</b></td>
		        <td valign="top" width="69%">score statement command (mandatory).</td>
		      </tr>
		    </table>
		    <h3>Notes and Hints</h3>
		    <p><b>Note1</b>: The <b>score</b> statement generates code which assumes
		    that the variable named, <b>XT</b>, contains the ESM row manager
		    which is to be operated upon.</p>
		    <p><b>Note2</b>: The <b>score</b> statement may be used as a stand alone
		    statement in a global declaration or inside an existing function. A stand
		    alone select statement is automatically enclosed in the following universal
		    parse tree lambda:</p>
		    <p>(lambda (XT) (begin ...score statement block...))</p>
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Score Command: average</Heading>
		<Description><![CDATA[
			<p>The <b>average</b> command averages a numeric expression over each row in
		    the current row manager.</p>
            <h3>Syntax</h3>		    
		    <p>average expression</p>
		    <table bgcolor="#99CCCC" class=members_table border="3" cellpadding="2" >
			<colgroup><col class="member"><col class="description"></colgroup>
		      <tr>
		        <td valign="top" width="31%"><b>average</b></td>
		        <td valign="top" width="69%">Mandatory keyword</td>
		      </tr>
		      <tr>
		        <td valign="top" width="31%"><b>expression</b></td>
		        <td valign="top" width="69%">Numeric expression.</td>
		      </tr>
		    </table>
		    <h3>Example1</h3>
		    <p>score average x3;</p>
		    <p>This will return the average of x3 for the current record view.</p>
            <p><font color=red><B>Note:</b></font> 
            Inside the <i>score expression</i>, the name <b>XT</b> refers to the ESM row manager Lambda input argument.
            The name <b>xv</b> refers to the currently focused record.
            The names <b>xtime</b>, <b>y</b>, and <b>x1</b> thru <b>xm</b>, refer to elements within the currently focused record.</p>
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Score Command: averageForAll</Heading>
		<Description><![CDATA[
			<p>The <b>averageForAll</b> command averages a numeric expression over ALL
		    rows in the current row manager.</p>
            <h3>Syntax</h3>		    
		    <p>averageForAll expression</p>
		    <table bgcolor="#99CCCC" class=members_table border="3" cellpadding="2" >
			<colgroup><col class="member"><col class="description"></colgroup>
		      <tr>
		        <td valign="top" width="31%"><b>averageForAll</b></td>
		        <td valign="top" width="69%">Mandatory keyword</td>
		      </tr>
		      <tr>
		        <td valign="top" width="31%"><b>expression</b></td>
		        <td valign="top" width="69%">Numeric expression.</td>
		      </tr>
		    </table>
		    <h3>Example1</h3>
		    <p>score averageForAll x10;</p>
		    <p>This will return the average of x10 for all rows in the current row manager.</p>
            <p><font color=red><B>Note:</b></font> 
            Inside the <i>score expression</i>, the name <b>XT</b> refers to the ESM row manager Lambda input argument.
            The name <b>xv</b> refers to the currently focused record.
            The names <b>xtime</b>, <b>y</b>, and <b>x1</b> thru <b>xm</b>, refer to elements within the currently focused record.</p>
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Score Command: extract</Heading>
		<Description><![CDATA[
			<p>The <b>extract</b> command extracts single or multiple columns of data from the current record view.
		    </p>
            <h3>Syntax</h3>		    
		    <p>extract extractExpression</p>
		    <table bgcolor="#99CCCC" class=members_table border="3" cellpadding="2" >
			<colgroup><col class="member"><col class="description"></colgroup>
		      <tr>
		        <td valign="top" width="31%"><b>extract</b></td>
		        <td valign="top" width="69%">Mandatory keyword</td>
		      </tr>
		      <tr>
		        <td valign="top" width="31%"><b>extractExpression</b></td>
		        <td valign="top" width="69%">Single or multiple column extraction expression.</td>
		      </tr>
		    </table>
		    <p>extract : spineVectorType : extractExpression</p>
		    <table bgcolor="#99CCCC" class=members_table border="3" cellpadding="2" >
			<colgroup><col class="member"><col class="description"></colgroup>
		      <tr>
		        <td valign="top" width="31%"><b>extract</b></td>
		        <td valign="top" width="69%">Mandatory keyword</td>
		      </tr>
		      <tr>
		        <td valign="top" width="31%"><b>spineVectorType</b></td>
		        <td valign="top" width="69%">The type of the spine vector for this extraction: Integer, Number, Object, or Word (defaults to Object:).</td>
		      </tr>
		      <tr>
		        <td valign="top" width="31%"><b>extractExpression</b></td>
		        <td valign="top" width="69%">Single or multiple column extraction expression.</td>
		      </tr>
		    </table>
		    <h3>Example1</h3>
		    <p>score extract new('Vector','Number',2,(x1/x2),sin(x4));</p>
		    <p>This will extract two columns of data, as specified, from all rows in the current record view.
            The results will be returned as an Object Vector (the missing <i>spineVectorType</i> argument defaults to Object) 
            with exactly as many entries as there are rows in the current record view.</p>
		    <h3>Example2</h3>
		    <p>score extract : Number : sin(x4);</p>
		    <p>This will extract one columns of data, as specified, from all rows in the current record view.
            The results will be returned as a Number Vector with exactly as many entries as there are rows in the current record view.</p>
            <p><font color=red><B>Note:</b></font> 
            Inside the <i>extractExpression</i>, the name <b>XT</b> refers to the ESM row manager Lambda input argument.
            The name <b>xv</b> refers to the currently focused record.
            The names <b>xtime</b>, <b>y</b>, and <b>x1</b> thru <b>xm</b>, refer to elements within the currently focused record.</p>
                        
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Score Command: total</Heading>
		<Description><![CDATA[
			  <p>The <b>total</b> command totals a numeric expression over each row in
		      the current record view.</p>
              <h3>Syntax</h3>		    
		      <p>total expression<b></b></p>
		    <table bgcolor="#99CCCC" class=members_table border="3" cellpadding="2" >
			<colgroup><col class="member"><col class="description"></colgroup>
		        <tr>
		          <td valign="top" width="31%"><b>total</b></td>
		          <td valign="top" width="69%">Mandatory keyword</td>
		        </tr>
		        <tr>
		          <td valign="top" width="31%"><b>expression</b></td>
		          <td valign="top" width="69%">Numeric expression.</td>
		        </tr>
		      </table>
		      <h3>Example1</h3>
		      <p>score total x5;</p>
		      <p>This will return the total of x5 for the current record view.</p>
            <p><font color=red><B>Note:</b></font> 
            Inside the <i>score expression</i>, the name <b>XT</b> refers to the ESM row manager Lambda input argument.
            The name <b>xv</b> refers to the currently focused record.
            The names <b>xtime</b>, <b>y</b>, and <b>x1</b> thru <b>xm</b>, refer to elements within the currently focused record.</p>
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Score Command: totalForAll</Heading>
		<Description><![CDATA[
	    <p>The <b>totalForAll</b> command totals a numeric expression over ALL
        rows in the current row manager.<br>
        <h3>Syntax</h3>		    
        <p>totalForAll expression</p>
		    <table bgcolor="#99CCCC" class=members_table border="3" cellpadding="2" >
			<colgroup><col class="member"><col class="description"></colgroup>
          <tr>
            <td valign="top" width="31%"><b>totalForAll</b></td>
            <td valign="top" width="69%">Mandatory keyword</td>
          </tr>
          <tr>
            <td valign="top" width="31%"><b>expression</b></td>
            <td valign="top" width="69%">Numeric expression.</td>
          </tr>
        </table>
        <h3>Example1</h3>
        <p>score totalForAll (x4 / x2);</p>
        <p>This will return the total of (x4 / x2) for all records in the current row manager.</p>
            <p><font color=red><B>Note:</b></font> 
            Inside the <i>score expression</i>, the name <b>XT</b> refers to the ESM row manager Lambda input argument.
            The name <b>xv</b> refers to the currently focused record.
            The names <b>xtime</b>, <b>y</b>, and <b>x1</b> thru <b>xm</b>, refer to elements within the currently focused record.</p>
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Select</Heading>
		<Description><![CDATA[
			<p>The <b>select</b> statement performs a set of selection operations on the <b>XT</b> argument
            (the row manager Lambda containing the set of Number Vectors for the current time period).   
            The select statement may be followed by a single command or by
		    multiple commands. The select statement supports commands for sorting and
		    selecting rows from the current record view (the currently selected rows). 
            The select command only alters the current record view.
            </p>
            <h3>Syntax</h3>		    
		    <p>select command1; ... commandN;</p>
		    <table bgcolor="#99CCCC" class=members_table border="3" cellpadding="2" >
			<colgroup><col class="member"><col class="description"></colgroup>
		      <tr>
		        <td valign="top" width="31%"><b>select</b></td>
		        <td valign="top" width="69%">Keyword (optional)</td>
		      </tr>
		      <tr>
		        <td valign="top" width="31%"><b>Command1;</b></td>
		        <td valign="top" width="69%">select statement command (mandatory).</td>
		      </tr>
		      <tr>
		        <td valign="top" width="31%"><b>CommandN;</b></td>
		        <td valign="top" width="69%">select statement command (optional).</td>
		      </tr>
		    </table>
		    <h3>Notes and Hints</h3>
		    <p><b>Note1</b>: All the commands of a single <b>select</b> statement are
		    grouped together into a single block. This is true regardless of whether or
		    not the <u>select</u> keyword is present, and regardless of whether or not
		    the multiple select commands are enclosed in braces.</p>
		    <p><b>Note2</b>: The <b>select</b> statement generates code which assumes
		    that the variable named, <b>XT</b>, contains the ESM row manager Lambda
		    which is to be operated upon.</p>
		    <p><b>Note3</b>: The <b>select</b> statement may be used as a stand alone
		    statement in a global declaration or inside an existing function. A stand
		    alone select statement is automatically enclosed in the following universal
		    parse tree lambda:
            <br>
		    (lambda (XT) (begin ...select statement block...) XT)</p>
		    <p><b>Note4</b>: The <b>select</b> statement always automatically converts
		    any element name symbol into a row index reference.</p>
           
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Select Command: all</Heading>
		<Description><![CDATA[
			<p>The <b>all</b> command performs a selection operation on the current recordview.
		    </p>
            <h3>Syntax</h3>		    
		    <p>all selectExpression</p>
		    <table bgcolor="#99CCCC" class=members_table border="3" cellpadding="2" >
			<colgroup><col class="member"><col class="description"></colgroup>
		      <tr>
		        <td valign="top" width="31%"><b>all</b></td>
		        <td valign="top" width="69%">Mandatory keyword</td>
		      </tr>
		      <tr>
		        <td valign="top" width="31%"><b>selectExpression</b></td>
		        <td valign="top" width="69%">Select expression.</td>
		      </tr>
		    </table>
		    <h3>Example1</h3>
		    <p>select all x4 &gt; 10000;</p>
		    <p>This will delete all rows from the current record view where x4 is not
		    greater than 10000.</p>
		    <h3>Example2</h3>
		    <p>select all (x4/sin(x2)) &gt; .54;</p>
		    <p>This will delete all rows from the current record view where (x4/sin(x2)) is not
		    greater than .54.</p>
            <p><font color=red><B>Note:</b></font> 
            Inside the <i>selectExpression</i>, the name <b>XT</b> refers to the ESM row manager Lambda input argument.
            The name <b>xv</b> refers to the currently focused record.
            The names <b>xtime</b>, <b>y</b>, and <b>x1</b> thru <b>xm</b>, refer to elements within the currently focused record.</p>
                        
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Select Command: bottom</Heading>
		<Description><![CDATA[
			<p>The <b>bottom</b> command performs a sort on the current row manager,
		    followed by a deletion operation on the current row manager.</p>
            <h3>Syntax</h3>		    
		    <p>bottom sortExpression cutoff</p>
		    <table bgcolor="#99CCCC" class=members_table border="3" cellpadding="2" >
			<colgroup><col class="member"><col class="description"></colgroup>
		      <tr>
		        <td valign="top" width="31%"><b>bottom</b></td>
		        <td valign="top" width="69%">Mandatory keyword</td>
		      </tr>
		      <tr>
		        <td valign="top" width="31%"><b>sortExpression</b></td>
		        <td valign="top" width="69%">Sort expression.</td>
		      </tr>
		      <tr>
		        <td valign="top" width="31%"><b>cutoff</b></td>
		        <td valign="top" width="69%">Cut off absolute number or percent.</td>
		      </tr>
		    </table>
		    <h3>Example1</h3>
		    <p>select bottom (x10 * x1) 100;</p>
		    <p>This will delete all rows from the current record view which are not in
		    the top 100 rows of all (x10 * x1).</p>
            <p><font color=red><B>Note:</b></font> 
            Inside the <i>selectExpression</i>, the name <b>XT</b> refers to the ESM row manager Lambda input argument.
            The name <b>xv</b> refers to the currently focused record.
            The names <b>xtime</b>, <b>y</b>, and <b>x1</b> thru <b>xm</b>, refer to elements within the currently focused record.</p>
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Select Command: highest</Heading>
		<Description><![CDATA[
			<p>The <b>highest</b> command performs a sort on the current row manager,
		    followed by a deletion operation on the current row manager.<br>
            <h3>Syntax</h3>		    
		    <p>highest sortExpression cutoff</p>
		    <table bgcolor="#99CCCC" class=members_table border="3" cellpadding="2" >
			<colgroup><col class="member"><col class="description"></colgroup>
		      <tr>
		        <td valign="top" width="31%"><b>highest</b></td>
		        <td valign="top" width="69%">Mandatory keyword</td>
		      </tr>
		      <tr>
		        <td valign="top" width="31%"><b>sortExpression</b></td>
		        <td valign="top" width="69%">Sort expression.</td>
		      </tr>
		      <tr>
		        <td valign="top" width="31%"><b>cutoff</b></td>
		        <td valign="top" width="69%">Cut off absolute number or percent.</td>
		      </tr>
		    </table>
		    <h3>Example1</h3>
		    <pre>select highest x4 10%;</pre>
		    <p>This will delete all rows from the current record view which are not in
		    the top 10% of all x4.</p>
            <p><font color=red><B>Note:</b></font> 
            Inside the <i>selectExpression</i>, the name <b>XT</b> refers to the ESM row manager Lambda input argument.
            The name <b>xv</b> refers to the currently focused record.
            The names <b>xtime</b>, <b>y</b>, and <b>x1</b> thru <b>xm</b>, refer to elements within the currently focused record.</p>
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Select Command: restore</Heading>
		<Description><![CDATA[
			<p>The <b>restore</b> command restores the backup view of the current table
		    cursor.</p>
            <h3>Syntax</h3>		    
		    <p>restore</p>
		    <table bgcolor="#99CCCC" class=members_table border="3" cellpadding="2" >
			<colgroup><col class="member"><col class="description"></colgroup>
		      <tr>
		        <td valign="top" width="31%"><b>restore</b></td>
		        <td valign="top" width="69%">Mandatory keyword</td>
		      </tr>
		    </table>
		    <h3>Example1</h3>
		    <p>select restore;</p>
		    <p>This will restore the all rows view of the current row manager.</p>            
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Select Command: run</Heading>
		<Description><![CDATA[
			<p>The <b>run</b> command performs a select on the current row manager.<br>
            <h3>Syntax</h3>		    
		    <p>run functionCall</p>
		    <table bgcolor="#99CCCC" class=members_table border="3" cellpadding="2" >
			<colgroup><col class="member"><col class="description"></colgroup>
		      <tr>
		        <td valign="top" width="31%"><b>run</b></td>
		        <td valign="top" width="69%">Mandatory keyword</td>
		      </tr>
		      <tr>
		        <td valign="top" width="31%"><b>functionCall</b></td>
		        <td valign="top" width="69%">A Selector function call expression to a
		          specified select Lambda.</td>
		      </tr>
		    </table>
		    <h3>Example1</h3>
		    <pre>select run &quot;mySelector&quot;;</pre>
		    <p>This will run the &quot;mySelector&quot; Selector Lambda, in the current
		    project, on the current record view.</p>
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Select Command: slice</Heading>
		<Description><![CDATA[
			<p>The <b>slice</b> command performs an ascending sort on the current row
		    manager, followed by a tile operation on the current row manager.<br>
            <h3>Syntax</h3>		    
		    <p>slice sortExpression tileIndex of tileCount</p>
		    <table bgcolor="#99CCCC" class=members_table border="3" cellpadding="2" >
			<colgroup><col class="member"><col class="description"></colgroup>
		      <tr>
		        <td valign="top" width="31%"><b>slice</b></td>
		        <td valign="top" width="69%">Mandatory keyword</td>
		      </tr>
		      <tr>
		        <td valign="top" width="31%"><b>sortExpression</b></td>
		        <td valign="top" width="69%">Sort expression.</td>
		      </tr>
		      <tr>
		        <td valign="top" width="31%"><b>tileIndex</b></td>
		        <td valign="top" width="69%">The index of the tile to show.</td>
		      </tr>
		      <tr>
		        <td valign="top" width="31%"><b>of</b></td>
		        <td valign="top" width="69%">Mandatory keyword.</td>
		      </tr>
		      <tr>
		        <td valign="top" width="31%"><b>tileCount</b></td>
		        <td valign="top" width="69%">The number of tiles.</td>
		      </tr>
		    </table>
		    <h3>Example1</h3>
		    <p>select slice (x21 * x2) 42 of 100;</p>
		    <p>This will retain all rows from the current record view which are in the 42
		    percentile of (x21 * x2).</p>
            <p><font color=red><B>Note:</b></font> 
            Inside the <i>selectExpression</i>, the name <b>XT</b> refers to the ESM row manager Lambda input argument.
            The name <b>xv</b> refers to the currently focused record.
            The names <b>xtime</b>, <b>y</b>, and <b>x1</b> thru <b>xm</b>, refer to elements within the currently focused record.</p>
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Select Command: sort</Heading>
		<Description><![CDATA[
			<p>The <b>sort</b> command performs a sort on the current row manager.<br>
            <h3>Syntax</h3>		    
		    <p>sort [direction] sortExpression</p>
		    <table bgcolor="#99CCCC" class=members_table border="3" cellpadding="2" >
			<colgroup><col class="member"><col class="description"></colgroup>
		      <tr>
		        <td valign="top" width="31%"><b>sort</b></td>
		        <td valign="top" width="69%">Mandatory keyword</td>
		      </tr>
		      <tr>
		        <td valign="top" width="31%"><b>direction</b></td>
		        <td valign="top" width="69%">Optional Sort direction: <b>up</b> or <b>down</b>
		          (optional). If sort direction is omitted, the direction is assumed to
		          be up.</td>
		      </tr>
		      <tr>
		        <td valign="top" width="31%"><b>sortExpression</b></td>
		        <td valign="top" width="69%">Mandatory Sort expression.</td>
		      </tr>
		    </table>
		    <h3>Example1</h3>
		    <p>select sort down x3;</p>
		    <p>This will sort all rows in the current record view in descending order by
		    x3.</p>	
            <p><font color=red><B>Note:</b></font> 
            Inside the <i>selectExpression</i>, the name <b>XT</b> refers to the ESM row manager Lambda input argument.
            The name <b>xv</b> refers to the currently focused record.
            The names <b>xtime</b>, <b>y</b>, and <b>x1</b> thru <b>xm</b>, refer to elements within the currently focused record.</p>
            
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Select Command: top</Heading>
		<Description><![CDATA[
			<p>The <b>top</b> command performs a sort on the current row manager,
		    followed by a deletion operation on the current row manager.<br>
            <h3>Syntax</h3>		    
		    <p>top sortExpression cutoff</p>
		    <table bgcolor="#99CCCC" class=members_table border="3" cellpadding="2" >
			<colgroup><col class="member"><col class="description"></colgroup>
		      <tr>
		        <td valign="top" width="31%"><b>top</b></td>
		        <td valign="top" width="69%">Mandatory keyword</td>
		      </tr>
		      <tr>
		        <td valign="top" width="31%"><b>sortExpression</b></td>
		        <td valign="top" width="69%">Sort expression.</td>
		      </tr>
		      <tr>
		        <td valign="top" width="31%"><b>cutoff</b></td>
		        <td valign="top" width="69%">Cut off absolute number or percent.</td>
		      </tr>
		    </table>
		    <h3>Example1</h3>
		    <pre>select top x4 10%;</pre>
		    <p>This will delete all rows from the current record view which are not in
		    the top 10% of all x4.</p>
            <p><font color=red><B>Note:</b></font> 
            Inside the <i>selectExpression</i>, the name <b>XT</b> refers to the ESM row manager Lambda input argument.
            The name <b>xv</b> refers to the currently focused record.
            The names <b>xtime</b>, <b>y</b>, and <b>x1</b> thru <b>xm</b>, refer to elements within the currently focused record.</p>
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>Statement Blocks</Heading>
		<Description><![CDATA[
 			<p>An arbitrary number of Selector statements may be grouped logically
        by embedding the statements in braces <b>{ exp</b> ...<b>}</b>. A
        statement block such as this may be placed in any position where a
        single statement is required. Multiple statements in Selector may be
        written on a single line as long as a semicolon terminates each
        statement.</p>
            <h3>Syntax</h3>		    
            <p>{exp ...}</p>
		    <table bgcolor="#99CCCC" class=members_table border="3" cellpadding="2" >
			<colgroup><col class="member"><col class="description"></colgroup>
          <tr align="top">
            <td><b>exp ...</b></td>
            <td>
              <p>Any number of expressions to be evaluated</p>
            </td>
          </tr>
        </table>
        <p>When To Use</p>
        <p>A statement block may be placed in any position where a single
        statement is required.</p>
        <p>Example1</p>
        <pre>	j = 1;

				if ( j &lt; 10 ) { k = 100; writeln(k);} else { k = -100; writeln(k);} </pre>
         
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>svmregress</Heading>
		<Description><![CDATA[
			<p>The <b>svmregress</b> command trains a support vector regression, with a cubic kernel, on the specified numeric expression list.
            The svmregress statement should be the ONLY statement in the Selector program, and returns the trained
            support vector regression Lambda. 
            <br>
            <h3>Syntax</h3>		    
		    <p>svmregress;</p>
		    <table bgcolor="#99CCCC" class=members_table border="3" cellpadding="2" >
			<colgroup><col class="member"><col class="description"></colgroup>
		      <tr>
		        <td valign="top" width="31%"><b>svmregress</b></td>
		        <td valign="top" width="69%">Mandatory keyword</td>
		      </tr>
		    </table>
            This form of the svmregress statement is equivalent to regressing on a list of all column names.
            <br>
		    <p>svmregress(numericExpression1,numericExpression2,,...,numericExpressionN);</p>
		    <table bgcolor="#99CCCC" class=members_table border="3" cellpadding="2" >
			<colgroup><col class="member"><col class="description"></colgroup>
		      <tr>
		        <td valign="top" width="31%"><b>svmregress</b></td>
		        <td valign="top" width="69%">Mandatory keyword</td>
		      </tr>
		      <tr>
		        <td valign="top" width="31%"><b>numericExpression1</b></td>
		        <td valign="top" width="69%">Numeric expression.</td>
		      </tr>
		      <tr>
		        <td valign="top" width="31%"><b>numericExpressionN</b></td>
		        <td valign="top" width="69%">Numeric expression.</td>
		      </tr>
		    </table>
		    <h3>Example1</h3>
		    <pre>svmregress(exp(x4),x10);</pre>
		    <p>This will train a support vector regression model, with a cubic kernel, on the numeric expressions: exp(x4) and x10.</p>
            <p><font color=red><B>Note:</b></font> 
            Inside the <i>numericExpressions</i>, the name <b>XT</b> refers to the ESM row manager Lambda input argument.
            The name <b>xv</b> refers to the currently focused record.
            The names <b>xtime</b>, <b>y</b>, and <b>x1</b> thru <b>xm</b>, refer to elements within the currently focused record.</p>
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>var</Heading>
		<Description><![CDATA[
		<p>The <b>var</b> statement declares a temporary variable inside a
        Selector program. The location of the <b>var</b> statement determines
        the scope the variable. If a <b>var</b> statement appears inside a
        function definition, the variable is considered local to that function
        only. If a <b>var</b> statement appears outside a function definition,
        the variable has global scope.</p>
        <p>The <b>var</b> statement declares a temporary variable within an
        Lambda. This means that the temporary variable will have a scope for this
        invocation of the Lambda object. The temporary variable is created and
        initialized each time the Lambda is invoked. Although Selector is a
        dynamically typed language, the var allows the programmer to optionally
        provide hints to the compiler by specifying a type for variable
        declarations. The supported types are: char, bool, int, float, and obj.
        Specifying a type of obj is equivalent to specifying no type
        information.</p>
        <b>Note</b>: If type hints are supplied, the programmer must assure that
        no other types are saved in the variable.<b><u><br>
        <h3>Syntax</h3>		    
        <p>var varName<br>
        var varName=init|<br>
        var type varName<br>
        var type varName=init</p>
		    <table bgcolor="#99CCCC" class=members_table border="3" cellpadding="2" >
			<colgroup><col class="member"><col class="description"></colgroup>
          <tr align="top">
            <td><b>var</b></td>
            <td>
              <p>Mandatory keyword</p>
            </td>
          </tr>
          <tr align="top">
            <td><b>type</b></td>
            <td>obj char bool int float obj text string symbol bitvec fltvec
              pcdvec stc dir dic matrix nummat vec bitvec numvec intvec objvec</td>
          </tr>
          <tr align="top">
            <td><b>varName</b></td>
            <td>
              <p>The name of the temporary variable</p>
            </td>
          </tr>
          <tr align="top">
            <td><b>=init</b></td>
            <td>
              <p>Optional argument. If present, the compiler will assign the <b>varName</b>
              to the value in <b>init</b>.</p>
            </td>
          </tr>
        </table>
        <p>When To Use</p>
        <p>Since global variables are automatically declared in the LambdaClient
        environment, it is unnecessary declare global variables. However, it is
        necessary to explicitly declare temporary variables inside a function
        using the <b>var</b> statement. The temporary variable structure, <b>LambdaName.Tv,
        </b>(where <b>LambdaName</b> is the name of the instance of the Lambda) is
        accessible, as long the Lambda object instance is active.</p>
        <p>Temporary variables are always allocated and initialized upon entry
        to a function and released upon exit.</p>
        <p>Example1</p>
        <pre>	function zoo  (x)  {

    			var m;

    			m = x;

    			writeln(m);

    			return (m);

    			}



				var m = 99;

				result = zoo(7); //displays &quot;7&quot;  (accesses the contents of the local variable)

				writeln(m);  //displays &quot;99&quot;  (accesses the contents of the global variable)

				writeln(zoo.m); //error--!Cannot find name: m in the PV structure of Lambda zoo!</pre>
        <p>Notes and Hints</p>
        <p>The scope of variables in nested Lambdas depends upon the following
        compilation rules: Nested Lambdas do not share either their arguments or
        their temporary variables. The argument and the temporary variables are
        always limited in scope to the immediate Lambda object. A nested Lambda
        shares the persistent variables <b>(pvar)</b> and the persistent class
        variables <b>(cvar)</b> of its parents. When a variable reference is
        encountered, the Selector compiler first searches the local Lambda's
        temporary variables<b> (var)</b>, then the persistent variable structure
        <b>(pvar)</b>, then the persistent class structure <b>(cvar)</b>, and
        finally the global variables.</p>
        <p>Also note: Any variable that is used without explicit declaration is
        automatically declared as a global variable. (This is true even if it is
        used inside a function definition).</p>
        
	    ]]></Description>
	</Section>
	<Section>
	    <Heading>while</Heading>
		<Description><![CDATA[
		<p>The <b>while</b> statement repeats a set of Selector statements
        until its condition is false.<b><u><br>
        <h3>Syntax</h3>		    
        <p>while (test) { expr }</p>
		    <table bgcolor="#99CCCC" class=members_table border="3" cellpadding="2" >
			<colgroup><col class="member"><col class="description"></colgroup>
          <tr align="top">
            <td><b>while</b></td>
            <td>
              <p>Mandatory keyword</p>
            </td>
          </tr>
          <tr align="top">
            <td><b>(test)</b></td>
            <td>
              <p>An expression that returns a Boolean value</p>
            </td>
          </tr>
          <tr align="top">
            <td><b>{expr}</b></td>
            <td>
              <p>A set of Selector statements</p>
            </td>
          </tr>
        </table>
        <p>Example1</p>
        <pre>	var int a = 1;

				while (a &lt; 10)   { display (&quot; &quot;, a)  ++a }		</pre>
        <p>This will display the following line in the console screen:</p>
        <pre>1  2  3  4  5  6  7  8  9  </pre>
        
	    ]]></Description>
	</Section>

</Essay>








;;**EXPORTKEY**:esm:bgmRegress
(deforphan esm:bgmRegress(X Y)
;; *******************************************************************
;; name:     bgmRegress
;; 
;; summary:  Trains a relational regression Lambda, using a basis grid 
;;           algorithm and returns the trained regression Lambda.
;;
;;           Basis grid machines are regression machines which learn
;;           and make regression estimates on X vector arrays such as:
;;
;;               X:  An N by M+1 array representing the original observations
;;                    in the form of:    x x x ...  y
;;                                       x x x ...  y
;;                                           ... 
;;                                       x x x ...  y
;;
;;           The basis grid machine constructs a basis grid, of the
;;           X training data. The grid contains Bayesian equal probabalistic
;;           basis views of all possible N-way cross correlations of the 
;;           independent columns in X. Each row in the basis grid contains
;;           from 81 to 125 entries representing the average Y values for each
;;           basis of the specified N-way cross correlation of independent
;;           variables.
;;
;;           The basis grid machine is designed to provide either 1-way,
;;           2-way, 3-way, or 4-way column cross correlations. The BGM automatically
;;           selects the largest number of columns to cross correlate which it can
;;           fit into available resources. The basis grid size is determined
;;           by the number of independent columns, M, and the number of columns
;;           chosen for cross correlation, C. The number of rows in the basis
;;           grid is always equal to expt(M,C). 
;;            
;;           The BGM may handle up to four columns in the X training data set. 
;;           Additional columns are allowed; but, they are ignored by the learning algorithm.
;;           Each column, of the training data, is converted into a sigmoid rank 
;;           indicating its relationship with the other elements in the column. The
;;           elements in the columns are then converted into a unique number in a one,
;;           two, three, or four digit basis numbering system. Example bases and the learning
;;           grid sizes for one, two, three, or four columns are shown below.
;;
;;                Columns		Basis		GridSize
;;
;;                   1           100           100 
;;                   2            10           100 
;;                   3             5           125 
;;                   4             3            81 
;;
;;           The BGM is a ranked learning algorithm, therefore each x variable is
;;           first converted to a sigmoid range before learning. Since, relative ranking
;;           not absolute value, is important, the BGM should be rescaled before estimates
;;           are made on a new data set. 
;;
;;           The basis grid machine constructs a basis grid of the ranked
;;           XY training data. Each row in each of the basis grids contains basis
;;           models for estimating the Y value.
;;
;;
;; Parms:    X:         The N by M vector array representing the original observations
;;                      in the form of:    x x ... x
;;                                         x x ... x
;;                                             ... 
;;                                         x x ... x
;;           Y   		The N vector of dependent variables.
;;
;; Return:   Rf:     A new basis grid machine Lambda ready for regression.
;; *******************************************************************
    pvars:(;; Public variables
           Integer:myC                  ;; The number of columns to cross correlate for each basis grid row (must be either 1, 2 ,3, or 4).
           NumVector:myColumnHighs		;; The vector of high values in each of the training columns (independent variables). 
           NumVector:myColumnLows		;; The vector of low values in each of the training columns (independent variables). 
           NumVector:myColumnRanges		;; The vector of value ranges in each of the training columns (independent variables). 
           myGrid					    ;; The basis training grid for this machine (multiple views each with its own basis vector).
           Number:myGridBasis           ;; The basis number for the basis grid's unique index numbering system.
           myGridTrainingCounts			;; The basis training grid counts for this machine (multiple views each with its own basis counting vector).
           Integer:myGridPositions      ;; The number of learning positions in each row of the basis training grid, always equal to expt(myGridBasis,myC).
           Integer:myGridViews          ;; The number of rows in the basis training grid, always equal to expt(M,myC), one view for each possible set of myC columns.
           Integer:myM					;; The number of elements in each training example (independent variables). 
           Integer:myN					;; The number of training examples. 
           Number:myW1                  ;; The first weight coefficient vector for the BGM regression model.
           Number:myW2                  ;; The second weight coefficient vector for the BGM regression model.
           Number:myW3                  ;; The third weight coefficient vector for the BGM regression model.
           Number:myW4                  ;; The fourth weight coefficient vector for the BGM regression model.
           myX                          ;; The vector of training examples (independent variables).
           NumVector:myY                ;; The vector of training example scores (dependent variable).
           (Strategy BGM)               ;; The regression strategy used by this Lambda.
           ;; Public child methods
           clear			       		;; Clear the current basis grid machine.
           bgmLambda			       		;; Return an Lambda ready to compute the bgm output for specified input vector.
           train		       	     	;; Train the basis grid machine on the specified training examples.
           rescale                      ;; Reset the basis grid machines column highs, lows, and ranges on the specified training examples.
           run           			    ;; Run the basis grid machine on the specified testing data.
           ;; Private maintenance child methods
           selfTest                		;; The self test method for this Lambda. 
           ) ;; end of persistent variables
    ;; ***************************
    ;; Define Public Child Lambdas.
    ;; ***************************
    ;; Clear the current basis grid machine.
    (defun clear()
       (setq myC 0)
       (setq myGrid	#void)
       (setq myGridBasis 0.0)
       (setq myGridTrainingCounts	#void)
       (setq myGridPositions 0)
       (setq myGridViews 0)
       (setq myColumnHighs #void) 
       (setq myColumnLows #void) 
       (setq myColumnRanges #void) 
       (setq myM 0)
       (setq myN 0)
       (setq myW1 0.0)
       (setq myW2 0.0)
       (setq myW3 0.0)
       (setq myW4 0.0)
       (setq myX #void)
       (setq myY #void)
       true) ; end clear
    ; Return an Lambda ready to compute the bgm output for specified input points.
    (defun bgmLambda()
       regs:(n m NW)
       vars:(NumVector:hW Vector:hKX ObjVector:hWX)
       vars:(Lambda HRecord)
       (setq Lambda (eval 
        {(lambda(NumVector:xv) 
         pvars:(Strategy Integer:myC NumVector:myColumnHighs NumVector:myColumnLows NumVector:myColumnRanges myGrid Number:myGridBasis Integer:myGridPositions Integer:myGridViews Integer:myM Number:myW1 Number:myW2 Number:myW3 Number:myW4) 
	     (defun rescale(X)
	        regs:(k K m M n N)
	        regs:(Number:ey Number:y)
	        vars:(result NumVector:x)
		    (if (or (<> (isVector X) true) (<> (isVector X[0]) true)) (error "bgmRegress.rescale: X argument must be a Vector Array of rank 2"))
		    (setq M (length X[0]))
		    (setq N (length X))
		    (if (<> M myM) (error "bgmRegress.rescale: number of columns must match training data set."))
	        (setq myColumnHighs (new Vector: Number: myM)) 
	        (setq myColumnLows (new Vector: Number: myM)) 
	        (setq myColumnRanges (new Vector: Number: myM)) 
	        (vmregRunInHardware start:)
	        (setq x X[0])
	        (loop for m from 0 until M do (setq myColumnHighs[m] x[m]) (setq myColumnLows[m] x[m]))
	        (loop for n from 1 until N do
	           (setq x X[n])
	           (loop for m from 0 until M do
	              (if (< myColumnHighs[m] x[m]) (setq myColumnHighs[m] x[m]))
	              (if (> myColumnLows[m] x[m]) (setq myColumnLows[m] x[m]))
	              )
	           )
	        (loop for m from 0 until M do (setq myColumnRanges[m] (* (+ (- myColumnHighs[m] myColumnLows[m]) .000000000001) 1.000000000001)))
	        (vmregRunInHardware stop:)
	        true)
         (defun run(X)
             regs:(k K m M n N)
             regs:(m1 m2 m3 m4 Number:x1 Number:x2 Number:x3 Number:x4)
             regs:(Number:ey Number:y)
             regs:(Number:w Number:x Number:xs Number:xlow Number:xrange)
             vars:(NumVector:rowGridScores  NumVector:rowGridCounts NumVector:xv  NumVector:EY)
             ;; Initialize the untrained BGM model.
             (if (or (<> (isVector X) true) (<> (isVector X[0]) true)) (error "bgmRegress.run: X argument must be a Vector Array of rank 2"))
             (setq myN (length X))
             (if (<> (length X[0]) myM) (error "bgmRegress.run: number of X argument columns does not match training data"))
             (setq myX X)
             (setq N myN)
             (setq M myM)
             ;; Rescale the testing data into the sigmoid domain.
             (rescale X)
             ;; Use the basis grid to accumulate estimates for the Y values.
             ;; Note1: The basis grid is yet another vector array
             ;;        whose number vector rows contain exactly 100 
             ;;        elements which represent the basis views
             ;;        of the data.
             ;; Note2: Although the number of basis grid columns
             ;;        defaults to 100, the value of the myGridPositions variable
             ;;        may be altered to reflect a non-basis size.
             (setq EY (new Vector: Number: N))
             (setq w (/ 1.0 myGridViews))
             (cond
              ;; Manage case where basis grid is one column deep.
              ((= myC 1)
               (begin
                 (vmregRunInHardware start:)
                 (loop for m from 0 until M do
                   (setq rowGridScores myGrid[m]) 
                   ;; For each column, load row scores where: xs = x[m]
                   (loop for n from 0 until N do
                      (setq xs 0.0)
                      (setq xv X[n]) 
                      (setq x xv[m]) 
                      (setq xlow myColumnLows[m]) 
                      (setq xrange myColumnRanges[m]) 
                      (setq x (/ (- x xlow) xrange))
                      (*= x myGridBasis)
                      (-= x (fraction x))
                      (*= x myW1)
                      (setq nn (integer x))
                      (setq ey (* w rowGridScores[nn]))
                      (setq EY[n] (+ EY[n] ey))
                      ) ; end N loop
                   ) ; end M loop
                 (vmregRunInHardware stop:)
               )) ; end one column case
              ;; Manage case where basis grid is two columns deep.
              ((= myC 2)
               (begin
                 (vmregRunInHardware start:)
                 (setq m 0)
                 (loop for m1 from 0 until M do
                    (loop for m2 from 0 until M do
                       (setq rowGridScores myGrid[m]) 
                       ;; For each possible two column set, load row scores where: xs = ninteger(10.0*x[m1]) + x[m2]
                       (loop for n from 0 until N do
                          (setq xs 0.0)
                          (setq xv X[n]) 
                          (setq x1 xv[m1]) 
                          (setq xlow myColumnLows[m1]) 
                          (setq xrange myColumnRanges[m1]) 
                          (setq x1 (/ (- x1 xlow) xrange))
                          (setq x2 xv[m2]) 
                          (setq xlow myColumnLows[m2]) 
                          (setq xrange myColumnRanges[m2]) 
                          (setq x2 (/ (- x2 xlow) xrange))
                          (*= x1 myGridBasis)
                          (-= x1 (fraction x1))
                          (*= x1 myW1)
                          (+= xs x1)
                          (*= x2 myGridBasis)
                          (-= x2 (fraction x2))
                          (*= x2 myW2)
                          (+= xs x2)
                          (setq nn (integer xs))
                          (setq ey (* w rowGridScores[nn]))
                          (setq EY[n] (+ EY[n] ey))
                          ) ; end N loop
                       (++ m)
                       ) ; end m2 loop
                   ) ; end m1 loop
                 (vmregRunInHardware stop:)
               )) ; end two columns case
              ;; Manage case where basis grid is three columns deep.
              ((= myC 3)
               (begin
                 (vmregRunInHardware start:)
                 (setq m 0)
                 (loop for m1 from 0 until M do
                    (loop for m2 from 0 until M do
                       (loop for m3 from 0 until M do
                           (setq rowGridScores myGrid[m])
                           ;; For each possible three column set, load row scores where: xs = (10.0*ninteger(5.0*x[m1])) + ninteger(5.0*x[m2]) + x[m3]
                           (loop for n from 0 until N do
                              (setq xs 0.0)
                              (setq xv X[n]) 
                              (setq x1 xv[m1]) 
                              (setq xlow myColumnLows[m1]) 
                              (setq xrange myColumnRanges[m1]) 
                              (setq x1 (/ (- x1 xlow) xrange))
                              (setq x2 xv[m2]) 
                              (setq xlow myColumnLows[m2]) 
                              (setq xrange myColumnRanges[m2]) 
                              (setq x2 (/ (- x2 xlow) xrange))
                              (setq x3 xv[m3]) 
                              (setq xlow myColumnLows[m3]) 
                              (setq xrange myColumnRanges[m3]) 
                              (setq x3 (/ (- x3 xlow) xrange))
                              (*= x1 myGridBasis)
                              (-= x1 (fraction x1))
                              (*= x1 myW1)
                              (+= xs x1)
                              (*= x2 myGridBasis)
                              (-= x2 (fraction x2))
                              (*= x2 myW2)
                              (+= xs x2)
                              (*= x3 myGridBasis)
                              (-= x3 (fraction x3))
                              (*= x3 myW3)
                              (+= xs x3)
                              (setq nn (integer xs))
                              (setq ey (* w rowGridScores[nn]))
                              (setq EY[n] (+ EY[n] ey))
                              ) ; end N loop
                           (++ m)
                           ) ; end m3 loop
                       ) ; end m2 loop
                   ) ; end m1 loop
                 (vmregRunInHardware stop:)
               )) ; end three columns case
              ;; Manage case where basis grid is four columns deep.
              ((= myC 4)
               (begin
                 (vmregRunInHardware start:)
                 (setq m 0)
                 (loop for m1 from 0 until M do
                    (loop for m2 from 0 until M do
                       (loop for m3 from 0 until M do
                           (loop for m4 from 0 until M do
                               (setq rowGridScores myGrid[m])
                               ;; For each possible four column set, load row scores where: xs = (100.0*ninteger(3.0*x[m1])) + (10.0*ninteger(3.0*x[m2])) + ninteger(3.0*x[m3]) + x[m4]
                               (loop for n from 0 until N do
                                  (setq xs 0.0)
                                  (setq xv X[n]) 
                                  (setq x1 xv[m1]) 
                                  (setq xlow myColumnLows[m1]) 
                                  (setq xrange myColumnRanges[m1]) 
                                  (setq x1 (/ (- x1 xlow) xrange))
                                  (setq x2 xv[m2]) 
                                  (setq xlow myColumnLows[m2]) 
                                  (setq xrange myColumnRanges[m2]) 
                                  (setq x2 (/ (- x2 xlow) xrange))
                                  (setq x3 xv[m3]) 
                                  (setq xlow myColumnLows[m3]) 
                                  (setq xrange myColumnRanges[m3]) 
                                  (setq x3 (/ (- x3 xlow) xrange))
                                  (setq x4 xv[m4]) 
                                  (setq xlow myColumnLows[m4]) 
                                  (setq xrange myColumnRanges[m4]) 
                                  (setq x4 (/ (- x4 xlow) xrange))
                                  (*= x1 myGridBasis)
                                  (-= x1 (fraction x1))
                                  (*= x1 myW1)
                                  (+= xs x1)
                                  (*= x2 myGridBasis)
                                  (-= x2 (fraction x2))
                                  (*= x2 myW2)
                                  (+= xs x2)
                                  (*= x3 myGridBasis)
                                  (-= x3 (fraction x3))
                                  (*= x3 myW3)
                                  (+= xs x3)
                                  (*= x4 myGridBasis)
                                  (-= x4 (fraction x4))
                                  (*= x4 myW4)
                                  (+= xs x4)
                                  (setq nn (integer xs))
                                  (setq ey (* w rowGridScores[nn]))
                                  (setq EY[n] (+ EY[n] ey))
                                  ) ; end N loop
                               (++ m)
                               ) ; end m4 loop
                           ) ; end m3 loop
                       ) ; end m2 loop
                   ) ; end m1 loop
                 (vmregRunInHardware stop:)
               )) ; end four columns case
              ) ; end cond
             ;; Return the vector of estimated Y values.
             (setq myGridTrainingCounts #void)
             (setq myX #void)
             (setq myY #void)
             EY) ; end run
         ;; Begin main logic
         regs:(k K m M n N)
         regs:(m1 m2 m3 m4 Number:x1 Number:x2 Number:x3 Number:x4)
         regs:(Number:ey Number:y)
         regs:(Number:w Number:x Number:xs Number:xlow Number:xrange)
         vars:(NumVector:rowGridScores  NumVector:rowGridCounts NumVector:xv  Number:EY)
         ;; Initialize the untrained BGM model.
         (if (<> (length xv) myM) (error "bgmRegress: number of xv argument columns does not match training data"))
         (setq M myM)
         (setq EY 0.0)
         (setq w (/ 1.0 myGridViews))
         (cond
          ;; Manage case where basis grid is one column deep.
          ((= myC 1)
           (begin
             (vmregRunInHardware start:)
             (loop for m from 0 until M do
               (setq rowGridScores myGrid[m]) 
               (setq ey 0.0)
               (setq xs 0.0)
               (setq x xv[m]) 
               (setq xlow myColumnLows[m]) 
               (setq xrange myColumnRanges[m]) 
               (setq x (/ (- x xlow) xrange))
               (*= x myGridBasis)
               (-= x (fraction x))
               (*= x myW1)
               (setq nn (integer x))
               (setq ey (* w rowGridScores[nn]))
               (+= EY ey)
               ) ; end M loop
             (vmregRunInHardware stop:)
           )) ; end one column case
          ;; Manage case where basis grid is two columns deep.
          ((= myC 2)
           (begin
             (vmregRunInHardware start:)
             (setq m 0)
             (loop for m1 from 0 until M do
                (loop for m2 from 0 until M do
                   (setq rowGridScores myGrid[m]) 
                   (setq ey 0.0)
                   (setq xs 0.0)
                   (setq x1 xv[m1]) 
                   (setq xlow myColumnLows[m1]) 
                   (setq xrange myColumnRanges[m1]) 
                   (setq x1 (/ (- x1 xlow) xrange))
                   (setq x2 xv[m2]) 
                   (setq xlow myColumnLows[m2]) 
                   (setq xrange myColumnRanges[m2]) 
                   (setq x2 (/ (- x2 xlow) xrange))
                   (*= x1 myGridBasis)
                   (-= x1 (fraction x1))
                   (*= x1 myW1)
                   (+= xs x1)
                   (*= x2 myGridBasis)
                   (-= x2 (fraction x2))
                   (*= x2 myW2)
                   (+= xs x2)
                   (setq nn (integer xs))
                   (setq ey (* w rowGridScores[nn]))
                   (+= EY ey)
                   (++ m)
                   ) ; end m2 loop
               ) ; end m1 loop
             (vmregRunInHardware stop:)
           )) ; end two columns case
          ;; Manage case where basis grid is three columns deep.
          ((= myC 3)
           (begin
             (vmregRunInHardware start:)
             (setq m 0)
             (loop for m1 from 0 until M do
                (loop for m2 from 0 until M do
                   (loop for m3 from 0 until M do
                      (setq rowGridScores myGrid[m]) 
                      (setq ey 0.0)
                      (setq xs 0.0)
                      (setq x1 xv[m1]) 
                      (setq xlow myColumnLows[m1]) 
                      (setq xrange myColumnRanges[m1]) 
                      (setq x1 (/ (- x1 xlow) xrange))
                      (setq x2 xv[m2]) 
                      (setq xlow myColumnLows[m2]) 
                      (setq xrange myColumnRanges[m2]) 
                      (setq x2 (/ (- x2 xlow) xrange))
                      (setq x3 xv[m3]) 
                      (setq xlow myColumnLows[m3]) 
                      (setq xrange myColumnRanges[m3]) 
                      (setq x3 (/ (- x3 xlow) xrange))
                      (*= x1 myGridBasis)
                      (-= x1 (fraction x1))
                      (*= x1 myW1)
                      (+= xs x1)
                      (*= x2 myGridBasis)
                      (-= x2 (fraction x2))
                      (*= x2 myW2)
                      (+= xs x2)
                      (*= x3 myGridBasis)
                      (-= x3 (fraction x3))
                      (*= x3 myW3)
                      (+= xs x3)
                      (setq nn (integer xs))
                      (setq ey (* w rowGridScores[nn]))
                      (+= EY ey)
                      (++ m)
                      ) ; end m3 loop
                   ) ; end m2 loop
               ) ; end m1 loop
             (vmregRunInHardware stop:)
           )) ; end three columns case
          ;; Manage case where basis grid is four columns deep.
          ((= myC 4)
           (begin
             (vmregRunInHardware start:)
             (setq m 0)
             (loop for m1 from 0 until M do
                (loop for m2 from 0 until M do
                   (loop for m3 from 0 until M do
                       (loop for m4 from 0 until M do
                          (setq rowGridScores myGrid[m]) 
                          (setq ey 0.0)
                          (setq xs 0.0)
                          (setq x1 xv[m1]) 
                          (setq xlow myColumnLows[m1]) 
                          (setq xrange myColumnRanges[m1]) 
                          (setq x1 (/ (- x1 xlow) xrange))
                          (setq x2 xv[m2]) 
                          (setq xlow myColumnLows[m2]) 
                          (setq xrange myColumnRanges[m2]) 
                          (setq x2 (/ (- x2 xlow) xrange))
                          (setq x3 xv[m3]) 
                          (setq xlow myColumnLows[m3]) 
                          (setq xrange myColumnRanges[m3]) 
                          (setq x3 (/ (- x3 xlow) xrange))
                          (setq x4 xv[m4]) 
                          (setq xlow myColumnLows[m4]) 
                          (setq xrange myColumnRanges[m4]) 
                          (setq x4 (/ (- x4 xlow) xrange))
                          (*= x1 myGridBasis)
                          (-= x1 (fraction x1))
                          (*= x1 myW1)
                          (+= xs x1)
                          (*= x2 myGridBasis)
                          (-= x2 (fraction x2))
                          (*= x2 myW2)
                          (+= xs x2)
                          (*= x3 myGridBasis)
                          (-= x3 (fraction x3))
                          (*= x3 myW3)
                          (+= xs x3)
                          (*= x4 myGridBasis)
                          (-= x4 (fraction x4))
                          (*= x4 myW4)
                          (+= xs x4)
                          (setq nn (integer xs))
                          (setq ey (* w rowGridScores[nn]))
                          (+= EY ey)
                          (++ m)
                          ) ; end m4 loop
                       ) ; end m3 loop
                   ) ; end m2 loop
               ) ; end m1 loop
             (vmregRunInHardware stop:)
           )) ; end four columns case
          ) ; end cond
         ;; Return the estimated Y value.
         EY)
        }))
       ;; Move all important knowledge structures into the trained regression Lambda.
       (setq Lambda.Strategy BGM:)
       (setq Lambda.myC myC)
       (setq Lambda.myColumnHighs myColumnHighs)
       (setq Lambda.myColumnLows myColumnLows)
       (setq Lambda.myColumnRanges myColumnRanges)
       (setq Lambda.myGrid myGrid)
       (setq Lambda.myGridBasis myGridBasis)
       (setq Lambda.myGridPositions myGridPositions)
       (setq Lambda.myGridViews myGridViews)
       (setq Lambda.myM myM)
       (setq Lambda.myW1 myW1)
       (setq Lambda.myW2 myW2)
       (setq Lambda.myW3 myW3)
       (setq Lambda.myW4 myW4)
       Lambda) ; end bgmLambda
    ;; Reset the bgm machines column highs, lows, and ranges on the specified training examples.
    ;; Note: This happens when the BGM has been trained on one data set and is to be tested on another data set.
    ;;       The BGM is a ranked learning machine, so rescaling may be necessary for each test data set.
    (defun rescale(X)
        regs:(k K m M n N)
        vars:(result NumVector:xv)
	    ;; Initialize the untrained BGM model.
	    (if (or (<> (isVector X) true) (<> (isVector X[0]) true)) (error "bgmRegress.rescale: X argument must be a Vector Array of rank 2"))
	    (setq M (length X[0]))
	    (setq N (length X))
	    (if (<> M myM) (error "bgmRegress.rescale: number of columns must match training data set."))
		;; Initialize the persistent variables before proceeding with training.
        (setq myColumnHighs (new Vector: Number: myM)) 
        (setq myColumnLows (new Vector: Number: myM)) 
        (setq myColumnRanges (new Vector: Number: myM)) 
		;; Compute the BGM column highs, lows, and ranges.
        (vmregRunInHardware start:)
        (setq xv X[0])
        (loop for m from 0 until M do (setq myColumnHighs[m] xv[m]) (setq myColumnLows[m] xv[m]))
        (loop for n from 1 until N do
           (setq xv X[n])
           (loop for m from 0 until M do
              (if (< myColumnHighs[m] xv[m]) (setq myColumnHighs[m] xv[m]))
              (if (> myColumnLows[m] xv[m]) (setq myColumnLows[m] xv[m]))
              ) ; end M loop
           ) ; end N loop
        (loop for m from 0 until M do (setq myColumnRanges[m] (* (+ (- myColumnHighs[m] myColumnLows[m]) .000000000001) 1.000000000001)))
        (vmregRunInHardware stop:)
        true) ; end rescale
    ;; Run the basis grid machine on the specified testing data.
	;; Parms:    X:         The N by M vector array representing the testing data
	;;                      in the form of:    x x ... x
	;;                                         x x ... x
	;;                                             ... 
	;;                                         x x ... x
    ;;
    ;; Return:   EY:        The vector of Y estimates for the testing data.
    (defun run(X)
        regs:(k K m M n N)
        regs:(m1 m2 m3 m4 Number:x1 Number:x2 Number:x3 Number:x4)
        regs:(Number:ey Number:y)
        regs:(Number:w Number:x Number:xs Number:xlow Number:xrange)
        vars:(NumVector:rowGridScores  NumVector:rowGridCounts NumVector:xv  NumVector:EY)
	    ;; Initialize the untrained BGM model.
	    (if (or (<> (isVector X) true) (<> (isVector X[0]) true)) (error "bgmRegress.run: X argument must be a Vector Array of rank 2"))
	    (setq myN (length X))
	    (if (<> (length X[0]) myM) (error "bgmRegress.run: number of X argument columns does not match training data"))
	    (setq myX X)
        (setq N myN)
        (setq M myM)
	    ;; Rescale the testing data into the sigmoid domain.
        (rescale X)
        ;; Use the basis grid to accumulate estimates for the Y values.
        ;; Note1: The basis grid is yet another vector array
        ;;        whose number vector rows contain exactly 100 
        ;;        elements which represent the basis views
        ;;        of the data.
        ;; Note2: Although the number of basis grid columns
        ;;        defaults to 100, the value of the myGridPositions variable
        ;;        may be altered to reflect a non-basis size.
        (setq EY (new Vector: Number: N))
        (setq w (/ 1.0 myGridViews))
        (cond
         ;; Manage case where basis grid is one column deep.
         ((= myC 1)
          (begin
            (vmregRunInHardware start:)
            (loop for m from 0 until M do
              (setq rowGridScores myGrid[m]) 
              ;; For each column, load row scores where: xs = x[m]
              (loop for n from 0 until N do
                 (setq xs 0.0)
                 (setq xv X[n]) 
                 (setq x xv[m]) 
                 (setq xlow myColumnLows[m]) 
                 (setq xrange myColumnRanges[m]) 
                 (setq x (/ (- x xlow) xrange))
                 (*= x myGridBasis)
                 (-= x (fraction x))
                 (*= x myW1)
                 (setq nn (integer x))
                 (setq ey (* w rowGridScores[nn]))
                 (setq EY[n] (+ EY[n] ey))
                 ) ; end N loop
              ) ; end M loop
            (vmregRunInHardware stop:)
          )) ; end one column case
         ;; Manage case where basis grid is two columns deep.
         ((= myC 2)
          (begin
            (vmregRunInHardware start:)
            (setq m 0)
            (loop for m1 from 0 until M do
               (loop for m2 from 0 until M do
                  (setq rowGridScores myGrid[m]) 
                  ;; For each possible two column set, load row scores where: xs = ninteger(10.0*x[m1]) + x[m2]
                  (loop for n from 0 until N do
                     (setq xs 0.0)
                     (setq xv X[n]) 
                     (setq x1 xv[m1]) 
                     (setq xlow myColumnLows[m1]) 
                     (setq xrange myColumnRanges[m1]) 
                     (setq x1 (/ (- x1 xlow) xrange))
                     (setq x2 xv[m2]) 
                     (setq xlow myColumnLows[m2]) 
                     (setq xrange myColumnRanges[m2]) 
                     (setq x2 (/ (- x2 xlow) xrange))
                     (*= x1 myGridBasis)
                     (-= x1 (fraction x1))
                     (*= x1 myW1)
                     (+= xs x1)
                     (*= x2 myGridBasis)
                     (-= x2 (fraction x2))
                     (*= x2 myW2)
                     (+= xs x2)
                     (setq nn (integer xs))
                     (setq ey (* w rowGridScores[nn]))
                     (setq EY[n] (+ EY[n] ey))
                     ) ; end N loop
                  (++ m)
                  ) ; end m2 loop
              ) ; end m1 loop
            (vmregRunInHardware stop:)
          )) ; end two columns case
         ;; Manage case where basis grid is three columns deep.
         ((= myC 3)
          (begin
            (vmregRunInHardware start:)
            (setq m 0)
            (loop for m1 from 0 until M do
               (loop for m2 from 0 until M do
                  (loop for m3 from 0 until M do
                      (setq rowGridScores myGrid[m])
                      ;; For each possible three column set, load row scores where: xs = (10.0*ninteger(5.0*x[m1])) + ninteger(5.0*x[m2]) + x[m3]
                      (loop for n from 0 until N do
                         (setq xs 0.0)
                         (setq xv X[n]) 
                         (setq x1 xv[m1]) 
                         (setq xlow myColumnLows[m1]) 
                         (setq xrange myColumnRanges[m1]) 
                         (setq x1 (/ (- x1 xlow) xrange))
                         (setq x2 xv[m2]) 
                         (setq xlow myColumnLows[m2]) 
                         (setq xrange myColumnRanges[m2]) 
                         (setq x2 (/ (- x2 xlow) xrange))
                         (setq x3 xv[m3]) 
                         (setq xlow myColumnLows[m3]) 
                         (setq xrange myColumnRanges[m3]) 
                         (setq x3 (/ (- x3 xlow) xrange))
                         (*= x1 myGridBasis)
                         (-= x1 (fraction x1))
                         (*= x1 myW1)
                         (+= xs x1)
                         (*= x2 myGridBasis)
                         (-= x2 (fraction x2))
                         (*= x2 myW2)
                         (+= xs x2)
                         (*= x3 myGridBasis)
                         (-= x3 (fraction x3))
                         (*= x3 myW3)
                         (+= xs x3)
                         (setq nn (integer xs))
                         (setq ey (* w rowGridScores[nn]))
                         (setq EY[n] (+ EY[n] ey))
                         ) ; end N loop
                      (++ m)
                      ) ; end m3 loop
                  ) ; end m2 loop
              ) ; end m1 loop
            (vmregRunInHardware stop:)
          )) ; end three columns case
         ;; Manage case where basis grid is four columns deep.
         ((= myC 4)
          (begin
            (vmregRunInHardware start:)
            (setq m 0)
            (loop for m1 from 0 until M do
               (loop for m2 from 0 until M do
                  (loop for m3 from 0 until M do
                      (loop for m4 from 0 until M do
                          (setq rowGridScores myGrid[m])
                          ;; For each possible four column set, load row scores where: xs = (100.0*ninteger(3.0*x[m1])) + (10.0*ninteger(3.0*x[m2])) + ninteger(3.0*x[m3]) + x[m4]
                          (loop for n from 0 until N do
                             (setq xs 0.0)
                             (setq xv X[n]) 
                             (setq x1 xv[m1]) 
                             (setq xlow myColumnLows[m1]) 
                             (setq xrange myColumnRanges[m1]) 
                             (setq x1 (/ (- x1 xlow) xrange))
                             (setq x2 xv[m2]) 
                             (setq xlow myColumnLows[m2]) 
                             (setq xrange myColumnRanges[m2]) 
                             (setq x2 (/ (- x2 xlow) xrange))
                             (setq x3 xv[m3]) 
                             (setq xlow myColumnLows[m3]) 
                             (setq xrange myColumnRanges[m3]) 
                             (setq x3 (/ (- x3 xlow) xrange))
                             (setq x4 xv[m4]) 
                             (setq xlow myColumnLows[m4]) 
                             (setq xrange myColumnRanges[m4]) 
                             (setq x4 (/ (- x4 xlow) xrange))
                             (*= x1 myGridBasis)
                             (-= x1 (fraction x1))
                             (*= x1 myW1)
                             (+= xs x1)
                             (*= x2 myGridBasis)
                             (-= x2 (fraction x2))
                             (*= x2 myW2)
                             (+= xs x2)
                             (*= x3 myGridBasis)
                             (-= x3 (fraction x3))
                             (*= x3 myW3)
                             (+= xs x3)
                             (*= x4 myGridBasis)
                             (-= x4 (fraction x4))
                             (*= x4 myW4)
                             (+= xs x4)
                             (setq nn (integer xs))
                             (setq ey (* w rowGridScores[nn]))
                             (setq EY[n] (+ EY[n] ey))
                             ) ; end N loop
                          (++ m)
                          ) ; end m4 loop
                      ) ; end m3 loop
                  ) ; end m2 loop
              ) ; end m1 loop
            (vmregRunInHardware stop:)
          )) ; end four columns case
         ) ; end cond
		;; Return the vector of estimated Y values.
	    (setq myX #void)
	    (setq myY #void)
        EY) ; end run
    ;; Train the bgm machine on the specified training data.
	;; Parms:    x:         The N by M vector array representing the training examples
	;;                      in the form of:    x x ... x
	;;                                         x x ... x
	;;                                             ... 
	;;                                         x x ... x
	;;           y   		The N vector of dependent variables used for training.
    ;;           gridSize	(Optional)The number of examples in each ideal basis training grid slot (default is 5).
    ;;
    ;; Return:   result:    A new basis grid machine Lambda ready for regression.
    (defun train(X NumVector:Y ...)
        regs:(k K m M n nn N gridSize NG)
        regs:(m1 m2 m3 m4 Number:x1 Number:x2 Number:x3 Number:x4)
        regs:(Number:ey Number:y Number:NG)
        regs:(Number:w Number:x Number:xs Number:xlow Number:xrange)
        vars:(NumVector:rowGridScores  NumVector:rowGridCounts NumVector:xv)
        vars:(result)
	    ;; Clear basis grid machine for retraining.
	    (clear)
        (if (>= (argCount) 3) (setq gridSize (integer (argFetch 2))) (setq gridSize 5))        
	    ;; Initialize the untrained BGM model.
	    (if (or (<> (isVector X) true) (<> (isVector X[0]) true)) (error "bgmRegress.train: X argument must be a Vector Array of rank 2"))
	    (setq myM (length X[0]))
	    (setq myN (length X))
	    (if (or (<> (isVector Y) true) (<> (length Y) myN)) (error "bgmRegress.train: Y argument must be a Vector of length the same as X"))
	    (setq myX X)
	    (setq myY Y)
		;; Initialize the persistent variables before proceeding with training.
        (setq myColumnHighs (new Vector: Number: myM)) 
        (setq myColumnLows (new Vector: Number: myM)) 
        (setq myColumnRanges (new Vector: Number: myM)) 
	    ;; Rescale the training data into the sigmoid domain.
		;; Note: Compute the column highs, lows, and ranges.
        (vmregRunInHardware start:)
        (setq N myN)
        (setq M myM)
        (setq xv myX[0])
        (loop for m from 0 until M do (setq myColumnHighs[m] xv[m]) (setq myColumnLows[m] xv[m]))
        (loop for n from 1 until N do
           (setq xv myX[n])
           (loop for m from 0 until M do
              (if (< myColumnHighs[m] xv[m]) (setq myColumnHighs[m] xv[m]))
              (if (> myColumnLows[m] xv[m]) (setq myColumnLows[m] xv[m]))
              ) ; end M loop
           ) ; end N loop
        (loop for m from 0 until M do (setq myColumnRanges[m] (* (+ (- myColumnHighs[m] myColumnLows[m]) .000000000001) 1.000000000001)))
        (vmregRunInHardware stop:)
        ;; Build the basis grid for this training data.
        ;; Note1: The basis grid is yet another vector array
        ;;        whose number vector rows contain exactly 100 
        ;;        elements which represent the basis views
        ;;        of the data.
        ;; Note2: Although the number of basis grid columns
        ;;        defaults to 100, the value of the myGridPositions variable
        ;;        may be altered to reflect a non-basis size.
        ;; Note3: The heurism used to set the value of myC, executed
        ;;        below, attempts to keep the maximum number of
        ;;        basis grid rows at or below 4096.
        (cond
          ((<= M 1)  (setq myC 1))
          ((<= M 2)  (setq myC 2))
          ((<= M 3)  (setq myC 3))
          ((<= M 8)  (setq myC 4))
          ((<= M 16) (setq myC 3))
          ((<= M 64) (setq myC 2))
          (else      (setq myC 1))
          ) ; end cond
        (setq NG (/ N gridSize))
        ComputeIdealBasis::
        (if (< NG 10) (error "esm.bgmRegress.train: not enough examples for training"))
        (setq myGridBasis (expt NG (/ 1.0 myC)))
        (if (and (< myGridBasis 2.0) (< myC 2)) (error "esm.bgmRegress.train: not enough examples for training"))
        (if (< myGridBasis 2.0) (begin (-- myC) (goto ComputeIdealBasis:)))
        (setq myGridBasis (- myGridBasis (fraction myGridBasis)))
        (setq myGridPositions (integer (expt myGridBasis myC)))
        (setq myGridViews (integer (expt M myC)))
        (cond
          ((<= myC 1)  (begin (setq myW1 1.0)))
          ((<= myC 2)  (begin (setq myW1 myGridBasis) (setq myW2 1.0)))
          ((<= myC 3)  (begin (setq myW1  (* myGridBasis myGridBasis)) (setq myW2 myGridBasis) (setq myW3 1.0)))
          (else        (begin (setq myW1  (* myGridBasis myGridBasis myGridBasis)) (setq myW2 (* myGridBasis myGridBasis)) (setq myW3 myGridBasis) (setq myW3 1.0)))
          ) ; end cond
        (setq w (/ N myGridPositions myGridViews))
        (if (= myGrid #void) (setq myGrid (new Vector: Object: myGridViews)))
        (if (= myGridTrainingCounts #void) (setq myGridTrainingCounts (new Vector: Object: myGridViews)))
        (cond
         ;; Manage case where basis grid is one column deep.
         ((= myC 1)
          (begin
            (vmregRunInHardware start:)
            (loop for m from 0 until M do
              (if (= myGrid[m] #void) (setq myGrid[m] (new Vector: Number: myGridPositions)))
              (if (= myGridTrainingCounts[m] #void) (setq myGridTrainingCounts[m] (new Vector: Number: myGridPositions)))
              (setq rowGridScores myGrid[m]) 
              (setq rowGridCounts myGridTrainingCounts[m]) 
              ;; For each column, load row scores where: xs = x[m]
              (loop for n from 0 until N do
                 (setq y Y[n]) 
                 (setq xv X[n]) 
                 (setq x xv[m]) 
                 (setq xlow myColumnLows[m]) 
                 (setq xrange myColumnRanges[m]) 
                 (setq x (/ (- x xlow) xrange))
                 (*= x myGridBasis)
                 (-= x (fraction x))
                 (*= x myW1)
                 (setq nn (integer x))
                 (setq rowGridScores[nn] (+ rowGridScores[nn] y))
                 (setq rowGridCounts[nn] (+ rowGridCounts[nn] 1.0))
                 ) ; end accumulate basis grid Y values and counts loop
		      ;; Compute the basis grid average Y values.
              (loop for nn from 0 until myGridPositions do
                 (if (> rowGridCounts[nn] 0.0) (setq rowGridScores[nn] (setq ey (/ rowGridScores[nn] rowGridCounts[nn]))))
                 ) ; end average basis grid Y values loop
              ) ; end M loop
            (vmregRunInHardware stop:)
          )) ; end one column case
         ;; Manage case where basis grid is two columns deep.
         ((= myC 2)
          (begin
            (vmregRunInHardware start:)
            (setq m 0)
            (loop for m1 from 0 until M do
               (loop for m2 from 0 until M do
                  (if (= myGrid[m] #void) (setq myGrid[m] (new Vector: Number: myGridPositions)))
                  (if (= myGridTrainingCounts[m] #void) (setq myGridTrainingCounts[m] (new Vector: Number: myGridPositions)))
                  (setq rowGridScores myGrid[m]) 
                  (setq rowGridCounts myGridTrainingCounts[m]) 
                  ;; For each possible two column set, load row scores where: xs = ninteger(10.0*x[m1]) + x[m2]
                  (loop for n from 0 until N do
                     (setq y Y[n]) 
                     (setq xv X[n]) 
	                 (setq x1 xv[m1]) 
	                 (setq xlow myColumnLows[m1]) 
	                 (setq xrange myColumnRanges[m1]) 
	                 (setq x1 (/ (- x1 xlow) xrange))
	                 (setq x2 xv[m2]) 
	                 (setq xlow myColumnLows[m2]) 
	                 (setq xrange myColumnRanges[m2]) 
	                 (setq x2 (/ (- x2 xlow) xrange))
                     (setq xs 0.0)
                     (*= x1 myGridBasis)
                     (-= x1 (fraction x1))
                     (*= x1 myW1)
                     (+= xs x1)
                     (*= x2 myGridBasis)
                     (-= x2 (fraction x2))
                     (*= x2 myW2)
                     (+= xs x2)
                     (setq nn (integer xs))
                     (setq rowGridScores[nn] (+ rowGridScores[nn] y))
                     (setq rowGridCounts[nn] (+ rowGridCounts[nn] 1.0))
                     ) ; end accumulate basis grid Y values and counts loop
		          ;; Compute the basis grid average Y values.
                  (loop for nn from 0 until myGridPositions do
                     (if (> rowGridCounts[nn] 0.0) (setq rowGridScores[nn] (setq ey (/ rowGridScores[nn] rowGridCounts[nn]))))
                     ) ; end average basis grid Y values loop
                  (++ m)
                  ) ; end m2 loop
              ) ; end m1 loop
            (vmregRunInHardware stop:)
          )) ; end two columns case
         ;; Manage case where basis grid is three columns deep.
         ((= myC 3)
          (begin
            (vmregRunInHardware start:)
            (setq m 0)
            (loop for m1 from 0 until M do
               (loop for m2 from 0 until M do
                  (loop for m3 from 0 until M do
                      (if (= myGrid[m] #void) (setq myGrid[m] (new Vector: Number: myGridPositions)))
                      (if (= myGridTrainingCounts[m] #void) (setq myGridTrainingCounts[m] (new Vector: Number: myGridPositions)))
                      (setq rowGridScores myGrid[m]) 
                      (setq rowGridCounts myGridTrainingCounts[m]) 
                      ;; For each possible three column set, load row scores where: xs = (10.0*ninteger(5.0*x[m1])) + ninteger(5.0*x[m2]) + x[m3]
                      (loop for n from 0 until N do
                         (setq y Y[n]) 
                         (setq xv X[n]) 
	                     (setq x1 xv[m1]) 
	                     (setq xlow myColumnLows[m1]) 
	                     (setq xrange myColumnRanges[m1]) 
	                     (setq x1 (/ (- x1 xlow) xrange))
	                     (setq x2 xv[m2]) 
	                     (setq xlow myColumnLows[m2]) 
	                     (setq xrange myColumnRanges[m2]) 
	                     (setq x2 (/ (- x2 xlow) xrange))
	                     (setq x3 xv[m3]) 
	                     (setq xlow myColumnLows[m3]) 
	                     (setq xrange myColumnRanges[m3]) 
	                     (setq x3 (/ (- x3 xlow) xrange))
                         (setq xs 0.0)
                         (*= x1 myGridBasis)
                         (-= x1 (fraction x1))
                         (*= x1 myW1)
                         (+= xs x1)
                         (*= x2 myGridBasis)
                         (-= x2 (fraction x2))
                         (*= x2 myW2)
                         (*= x3 myGridBasis)
                         (-= x3 (fraction x3))
                         (*= x3 myW3)
                         (+= xs x3)
                         (setq nn (integer xs))
                         (setq rowGridScores[nn] (+ rowGridScores[nn] y))
                         (setq rowGridCounts[nn] (+ rowGridCounts[nn] 1.0))
                         ) ; end accumulate basis grid Y values and counts loop
		              ;; Compute the basis grid average Y values.
                      (loop for nn from 0 until myGridPositions do
                         (if (> rowGridCounts[nn] 0.0) (setq rowGridScores[nn] (setq ey (/ rowGridScores[nn] rowGridCounts[nn]))))
                         ) ; end average basis grid Y values loop
                      (++ m)
                      ) ; end m3 loop
                  ) ; end m2 loop
              ) ; end m1 loop
            (vmregRunInHardware stop:)
          )) ; end three columns case
         ;; Manage case where basis grid is four columns deep.
         ((= myC 4)
          (begin
            (vmregRunInHardware start:)
            (setq m 0)
            (loop for m1 from 0 until M do
               (loop for m2 from 0 until M do
                  (loop for m3 from 0 until M do
                      (loop for m4 from 0 until M do
                          (if (= myGrid[m] #void) (setq myGrid[m] (new Vector: Number: myGridPositions)))
                          (if (= myGridTrainingCounts[m] #void) (setq myGridTrainingCounts[m] (new Vector: Number: myGridPositions)))
                          (setq rowGridScores myGrid[m]) 
                          (setq rowGridCounts myGridTrainingCounts[m]) 
                          ;; For each possible four column set, load row scores where: xs = (100.0*ninteger(3.0*x[m1])) + (10.0*ninteger(3.0*x[m2])) + ninteger(3.0*x[m3]) + x[m4]
                          (loop for n from 0 until N do
                             (setq y Y[n]) 
                             (setq xv X[n]) 
	                         (setq x1 xv[m1]) 
	                         (setq xlow myColumnLows[m1]) 
	                         (setq xrange myColumnRanges[m1]) 
	                         (setq x1 (/ (- x1 xlow) xrange))
	                         (setq x2 xv[m2]) 
	                         (setq xlow myColumnLows[m2]) 
	                         (setq xrange myColumnRanges[m2]) 
	                         (setq x2 (/ (- x2 xlow) xrange))
	                         (setq x3 xv[m3]) 
	                         (setq xlow myColumnLows[m3]) 
	                         (setq xrange myColumnRanges[m3]) 
	                         (setq x3 (/ (- x3 xlow) xrange))
	                         (setq x4 xv[m4]) 
	                         (setq xlow myColumnLows[m4]) 
	                         (setq xrange myColumnRanges[m4]) 
	                         (setq x4 (/ (- x4 xlow) xrange))
                             (setq xs 0.0)
                             (*= x1 myGridBasis)
                             (-= x1 (fraction x1))
                             (*= x1 myW1)
                             (+= xs x1)
                             (*= x2 myGridBasis)
                             (-= x2 (fraction x2))
                             (*= x2 myW2)
                             (*= x3 myGridBasis)
                             (-= x3 (fraction x3))
                             (*= x3 myW3)
                             (*= x4 myGridBasis)
                             (-= x4 (fraction x4))
                             (*= x4 myW4)
                             (+= xs x4)
                             (setq nn (integer xs))
                             (setq rowGridScores[nn] (+ rowGridScores[nn] y))
                             (setq rowGridCounts[nn] (+ rowGridCounts[nn] 1.0))
                             ) ; end accumulate basis grid Y values and counts loop
		                  ;; Compute the basis grid average Y values.
                          (loop for nn from 0 until myGridPositions do
                             (if (> rowGridCounts[nn] 0.0) (setq rowGridScores[nn] (setq ey (/ rowGridScores[nn] rowGridCounts[nn]))))
                             ) ; end average basis grid Y values loop
                          (++ m)
                          ) ; end m4 loop
                      ) ; end m3 loop
                  ) ; end m2 loop
              ) ; end m1 loop
            (vmregRunInHardware stop:)
          )) ; end four columns case
         ) ; end cond
		;; Return a basis grid regression Lambda.
	    (setq myGridTrainingCounts #void)
	    (setq myX #void)
	    (setq myY #void)
        (setq result (bgmLambda))
        result) ; end train
    ;; ****************************************
    ;; Define Private Maintenance Child Lambdas.
    ;; ****************************************
    ;; The self test method for this Lambda.
    (defun selfTest(Test Ns Ms Gs)
       vars:(k m n g G y ey C c X Y Yv avgY avgTopEy topEyCnt
             Lambda err Net pct sortedY EY 
             eyGrid eyGridCount eyGridSize errScore 
             startTime endTime startTimeT endTimeT
             (checkResults true)
             (tol 0.0) (errStop 0.01) (Cs 1.0)
             ) ; end temporary variables
       (clear)
       (setq startTimeT (getTickCount 0))
       (setq srandom.seed 8192.0)
       (setq eyGridSize Gs)
       ;; Select the requested test case
       ;; Test Case srandom 
       (if (or (= Test all:) (= Test linear:))
           (begin
		       ;; Create a test polynomial regression where y = C[0]*X[0] + C[1]*X[1] + C[2]*X[2] + C[3]*X[3] + C[4]*X[4] ...
		       (setq M Ms)
		       (setq N Ns)
		       (setq eyGrid (new Vector: Number: eyGridSize))
		       (setq eyGridCount (new Vector: Number: eyGridSize))
		       (setq X (new Vector: object: N))
		       (setq Y (new Vector: Number: N))
		       (setq C (new Vector: Number: M))
		       (loop for m from 0 until M do
		          (setq C[m] (- (srandom 10.0) 5.0))
		          ) ; end C loop
		       (loop for n from 0 until N do
		          (setq X[n] (new Vector: Number: M))
		          (setq y 0.0)
		          (loop for m from 0 until M do 
		          	 (setq X[n][m] (- (srandom 100.0) 50.0))
		             (setq y (+ y (* X[n][m] C[m])))
		             ) ; end M loop
		          (setq Y[n] (+ (* y .8) (* y (srandom .4))))
		          ) ; end N loop
		       ;; Train on the training data.
		       (writeln _eol "Starting test case: linear")
		       (setq Lambda (setq Lambda (esm.bgmRegress.train X Y )))
		       (writeln "bgmRegress: N = [" Ns "], M = [" Ms "]")
		       ;; Score on the test case.
		       (loop for n from 0 until N do
		          (setq X[n] (new Vector: Number: M))
		          (setq y 0.0)
		          (loop for m from 0 until M do 
		          	 (setq X[n][m] (- (srandom 100.0) 50.0))
		             (setq y (+ y (* X[n][m] C[m])))
		             ) ; end M loop
		          (setq Y[n] (+ (* y .8) (* y (srandom .4))))
		          ) ; end N loop
               (setq EY (Lambda.run X))
               (setq err 0.0)
               (setq errScore 0.0)
               (setq avgTopEy 0.0)
               (setq topEyCnt 0)
               (setq avgY (avg Y))
               (setq G (/ N eyGridSize))
               (setq sortedY (|Gv:sort| Y < true)) 
               (loop for n from 0 until N do
                  (setq k sortedY[n])                  
                  (setq ey EY[k])
                  (setq y Y[k])
                  (setq g (divi n G))
                  (setq eyGrid[g] (+ eyGrid[g] ey))
                  (setq eyGridCount[g] (+ eyGridCount[g] 1.0))
                  (setq pct (/ (abs (- ey y)) (+ (abs ey) .000000001)))
                  (+= err pct)
                  (if (= (modi n (divi N 10)) 0) (writeln "[" n "] ey=[" ey "] y=[" y "] err=[" (- ey y) "] err%=[" pct "]"))
                  ) ; end N loop
               (/= err N)
               (loop for g from 0 until eyGridSize do (setq eyGrid[g] (/ eyGrid[g] eyGridCount[g]))) 
               (loop for g from 1 until eyGridSize do (if (<= eyGrid[g] eyGrid[(- g 1)]) (++ errScore)))
               (setq errScore (/ (- eyGridSize errScore 1) (- eyGridSize 1)))
               (writeln "bgmRegress: err=[" err "], avgY=[" avgY "], errScore=[" errScore "], eyGrid=[" (string eyGrid true) "]")
          )) ; end Test Case linear
       ;; Test Case mixedRandom 
       (if (or (= Test all:) (= Test mixed:))
           (begin
		       ;; Create a test polynomial linear model where y = C[0]*X[0] + C[1]*X[1] + C[2]*X[2] ...
		       ;; Create a test polynomial square model where y = C[0]*X[0]*X[0] + C[1]*X[1]*X[1] + C[2]*X[2]*X[2] ...
		       ;; Create a test polynomial sin model where y = C[0]*sin(X[0]) + C[1]*sin(X[1]) + C[2]*sin(X[2]) ...
		       ;; Create a test polynomial log model where y = C[0]*log(abs(X[0])+.000001) + C[1]*log(abs(X[1])+.000001) + C[1]*log(abs(X[2])+.000001) ...
               ;; These four models are mixed together and random noise is added.
		       (setq M Ms)
		       (setq N Ns)
		       (setq eyGrid (new Vector: Number: eyGridSize))
		       (setq eyGridCount (new Vector: Number: eyGridSize))
		       (setq X (new Vector: object: N))
		       (setq Y (new Vector: Number: N))
		       (setq C (new Vector: Number: M))
		       (loop for m from 0 until M do
		          (setq C[m] (- (srandom 10.0) 5.0))
		          ) ; end C loop
		       (loop for n from 0 until N do
		          (setq X[n] (new Vector: Number: M))
		          (setq y 0.0)
                  (setq k (modi n 4)) 
 		          (setq X[n][1] (number k))
		          (loop for m from 1 until M do 
		          	 (setq X[n][m] (- (srandom 100.0) 50.0))
		             (setq y (+ y (* X[n][m] C[m])))
                     (cond
                        ;; Linear model
                        ((= k 0) (setq y (+ y (* X[n][m] C[m]))))
                        ;; Square model
                        ((= k 1) (setq y (+ y (* X[n][m] X[n][m] C[m]))))
                        ;; Sine model
                        ((= k 2) (setq y (+ y (* (|Gv:sin| X[n][m]) C[m]))))
                        ;; Log model
                        (else (setq y (+ y (* (|Gv:log| (+ .000001 (|Gv:abs| X[n][m]))) C[m]))))
                        ) ; end cond
		             ) ; end M loop
		          (setq Y[n] (+ (* y .8) (* y (srandom .4))))
		          ) ; end N loop
		       ;; Train on the training data.
		       (writeln _eol "Starting test case: mixed")
		       (setq Lambda (setq Lambda (esm.bgmRegress.train X Y )))
		       (writeln "bgmRegress: N = [" Ns "], M = [" Ms "]")
		       ;; Score on the test case.
		       (loop for n from 0 until N do
		          (setq X[n] (new Vector: Number: M))
		          (setq y 0.0)
                  (setq k (modi n 4)) 
 		          (setq X[n][1] (number k))
		          (loop for m from 1 until M do 
		          	 (setq X[n][m] (- (srandom 100.0) 50.0))
                     (cond
                        ;; Linear model
                        ((= k 0) (setq y (+ y (* X[n][m] C[m]))))
                        ;; Square model
                        ((= k 1) (setq y (+ y (* X[n][m] X[n][m] C[m]))))
                        ;; Sine model
                        ((= k 2) (setq y (+ y (* (|Gv:sin| X[n][m]) C[m]))))
                        ;; Log model
                        (else (setq y (+ y (* (|Gv:log| (+ .000001 (|Gv:abs| X[n][m]))) C[m]))))
                        ) ; end cond
		             ) ; end M loop
		          (setq Y[n] (+ (* y .8) (* y (srandom .4))))
		          ) ; end N loop
               (setq EY (Lambda.run X))
               (setq err 0.0)
               (setq errScore 0.0)
               (setq avgTopEy 0.0)
               (setq topEyCnt 0)
               (setq avgY (avg Y))
               (setq G (/ N eyGridSize))
               (setq sortedY (|Gv:sort| Y < true)) 
               (loop for n from 0 until N do
                  (setq k sortedY[n])                  
                  (setq ey EY[k])
                  (setq y Y[k])
                  (setq g (divi n G))
                  (setq eyGrid[g] (+ eyGrid[g] ey))
                  (setq eyGridCount[g] (+ eyGridCount[g] 1.0))
                  (setq pct (/ (abs (- ey y)) (+ (abs ey) .000000001)))
                  (+= err pct)
                  (if (= (modi n (divi N 10)) 0) (writeln "[" n "] ey=[" ey "] y=[" y "] err=[" (- ey y) "] err%=[" pct "]"))
                  ) ; end N loop
               (/= err N)
               (loop for g from 0 until eyGridSize do (setq eyGrid[g] (/ eyGrid[g] eyGridCount[g]))) 
               (loop for g from 1 until eyGridSize do (if (<= eyGrid[g] eyGrid[(- g 1)]) (++ errScore)))
               (setq errScore (/ (- eyGridSize errScore 1) (- eyGridSize 1)))
               (writeln "bgmRegress: err=[" err "], avgY=[" avgY "], errScore=[" errScore "], eyGrid=[" (string eyGrid true) "]")
          )) ; end Test Case mixed
       ;; Test Case sigmoid 
       (if (or (= Test all:) (= Test sigmoid:))
           (begin
		       ;; Create a test polynomial regression where y = C[0]*X[0] + C[1]*X[1] + C[2]*X[2] + C[3]*X[3] + C[4]*X[4] ...
		       ;; Note1: The inputs, X, are restricted to the sigmoid domain.
		       (setq M Ms)
		       (setq N Ns)
		       (setq eyGrid (new Vector: Number: eyGridSize))
		       (setq eyGridCount (new Vector: Number: eyGridSize))
		       (setq X (new Vector: object: N))
		       (setq Y (new Vector: Number: N))
		       (setq C (new Vector: Number: M))
		       (loop for m from 0 until M do
		          (setq C[m] (- (srandom 1.0) .50))
		          ) ; end C loop
		       (loop for n from 0 until N do
		          (setq X[n] (new Vector: Number: M))
		          (setq y 0.0)
		          (loop for m from 0 until M do 
		          	 (setq X[n][m] (srandom 1.0))
		             (setq y (+ y (* X[n][m] C[m])))
		             ) ; end M loop
		          (setq Y[n] (+ (* y .8) (* y (srandom .4))))
		          ) ; end N loop
		       ;; Train on the training data.
		       (writeln _eol "Starting test case: sigmoid")
		       (setq Lambda (setq Lambda (esm.bgmRegress.train X Y )))
		       (writeln "bgmRegress: N = [" Ns "], M = [" Ms "]")
		       ;; Score on the test case.
		       (loop for n from 0 until N do
		          (setq X[n] (new Vector: Number: M))
		          (setq y 0.0)
		          (loop for m from 0 until M do 
		          	 (setq X[n][m] (srandom 1.0))
		             (setq y (+ y (* X[n][m] C[m])))
		             ) ; end M loop
		          (setq Y[n] (+ (* y .8) (* y (srandom .4))))
		          ) ; end N loop
               (setq EY (Lambda.run X))
               (setq err 0.0)
               (setq errScore 0.0)
               (setq avgTopEy 0.0)
               (setq topEyCnt 0)
               (setq avgY (avg Y))
               (setq G (/ N eyGridSize))
               (setq sortedY (|Gv:sort| Y < true)) 
               (loop for n from 0 until N do
                  (setq k sortedY[n])                  
                  (setq ey EY[k])
                  (setq y Y[k])
                  (setq g (divi n G))
                  (setq eyGrid[g] (+ eyGrid[g] ey))
                  (setq eyGridCount[g] (+ eyGridCount[g] 1.0))
                  (setq pct (/ (abs (- ey y)) (+ (abs ey) .000000001)))
                  (+= err pct)
                  (if (= (modi n (divi N 10)) 0) (writeln "[" n "] ey=[" ey "] y=[" y "] err=[" (- ey y) "] err%=[" pct "]"))
                  ) ; end N loop
               (/= err N)
               (loop for g from 0 until eyGridSize do (setq eyGrid[g] (/ eyGrid[g] eyGridCount[g]))) 
               (loop for g from 1 until eyGridSize do (if (<= eyGrid[g] eyGrid[(- g 1)]) (++ errScore)))
               (setq errScore (/ (- eyGridSize errScore 1) (- eyGridSize 1)))
               (writeln "bgmRegress: err=[" err "], avgY=[" avgY "], errScore=[" errScore "], eyGrid=[" (string eyGrid true) "]")
          )) ; end Test Case sigmoid
       ;; Test Case square 
       (if (or (= Test all:) (= Test square:))
           (begin
		       ;; Create a test polynomial regression where y = C[0]*X[0] + C[1]*(X[1]**2) + C[2]*X[2] + C[3]*(X[3]**2) ...
		       (setq M Ms)
		       (setq N Ns)
		       (setq eyGrid (new Vector: Number: eyGridSize))
		       (setq eyGridCount (new Vector: Number: eyGridSize))
		       (setq X (new Vector: object: N))
		       (setq Y (new Vector: Number: N))
		       (setq C (new Vector: Number: M))
		       (loop for m from 0 until M do
		          (setq C[m] (- (srandom 10.0) 5.0))
		          ) ; end C loop
		       (loop for n from 0 until N do
		          (setq X[n] (new Vector: Number: M))
		          (setq y 0.0)
		          (loop for m from 0 until M do 
		          	 (setq X[n][m] (- (srandom 100.0) 50.0))
                     (setq y (+ y (* X[n][m] X[n][m] C[m])))
		             ) ; end M loop
		          (setq Y[n] (+ (* y .8) (* y (srandom .4))))
		          ) ; end N loop
		       ;; Train on the training data.
		       (writeln _eol "Starting test case: square")
		       (setq Lambda (setq Lambda (esm.bgmRegress.train X Y )))
		       (writeln "bgmRegress: N = [" Ns "], M = [" Ms "]")
		       ;; Score on the test case.
		       (loop for n from 0 until N do
		          (setq X[n] (new Vector: Number: M))
		          (setq y 0.0)
                  (setq k (modi n 4)) 
		          (loop for m from 0 until M do 
		          	 (setq X[n][m] (- (srandom 100.0) 50.0))
                     (setq y (+ y (* X[n][m] X[n][m] C[m])))
		             ) ; end M loop
		          (setq Y[n] (+ (* y .8) (* y (srandom .4))))
		          ) ; end N loop
               (setq EY (Lambda.run X))
               (setq err 0.0)
               (setq errScore 0.0)
               (setq avgTopEy 0.0)
               (setq topEyCnt 0)
               (setq avgY (avg Y))
               (setq G (/ N eyGridSize))
               (setq sortedY (|Gv:sort| Y < true)) 
               (loop for n from 0 until N do
                  (setq k sortedY[n])                  
                  (setq ey EY[k])
                  (setq y Y[k])
                  (setq g (divi n G))
                  (setq eyGrid[g] (+ eyGrid[g] ey))
                  (setq eyGridCount[g] (+ eyGridCount[g] 1.0))
                  (setq pct (/ (abs (- ey y)) (+ (abs ey) .000000001)))
                  (+= err pct)
                  (if (= (modi n (divi N 10)) 0) (writeln "[" n "] ey=[" ey "] y=[" y "] err=[" (- ey y) "] err%=[" pct "]"))
                  ) ; end N loop
               (/= err N)
               (loop for g from 0 until eyGridSize do (setq eyGrid[g] (/ eyGrid[g] eyGridCount[g]))) 
               (loop for g from 1 until eyGridSize do (if (<= eyGrid[g] eyGrid[(- g 1)]) (++ errScore)))
               (setq errScore (/ (- eyGridSize errScore 1) (- eyGridSize 1)))
               (writeln "bgmRegress: err=[" err "], avgY=[" avgY "], errScore=[" errScore "], eyGrid=[" (string eyGrid true) "]")
          )) ; end Test Case square
       ;; Test Case tan 
       (if (or (= Test all:) (= Test tan:))
           (begin
		       ;; Create a test polynomial regression where y = C[0]*tan(X[0]) + C[1]*tan(X[1]) + C[2]*tan(X[2]) ...
		       (setq M Ms)
		       (setq N Ns)
		       (setq eyGrid (new Vector: Number: eyGridSize))
		       (setq eyGridCount (new Vector: Number: eyGridSize))
		       (setq X (new Vector: object: N))
		       (setq Y (new Vector: Number: N))
		       (setq C (new Vector: Number: M))
		       (loop for m from 0 until M do
		          (setq C[m] (- (srandom 10.0) 5.0))
		          ) ; end C loop
		       (loop for n from 0 until N do
		          (setq X[n] (new Vector: Number: M))
		          (setq y 0.0)
		          (loop for m from 0 until M do 
		          	 (setq X[n][m] (- (srandom 100.0) 50.0))
                     (setq y (+ y (* (tan X[n][m]) C[m])))
		             ) ; end M loop
		          (setq Y[n] (+ (* y .8) (* y (srandom .4))))
		          ) ; end N loop
		       ;; Train on the training data.
		       (writeln _eol "Starting test case: tan")
		       (setq Lambda (setq Lambda (esm.bgmRegress.train X Y )))
		       (writeln "bgmRegress: N = [" Ns "], M = [" Ms "]")
		       ;; Score on the test case.
		       (loop for n from 0 until N do
		          (setq X[n] (new Vector: Number: M))
		          (setq y 0.0)
                  (setq k (modi n 4)) 
		          (loop for m from 0 until M do 
		          	 (setq X[n][m] (- (srandom 100.0) 50.0))
                     (setq y (+ y (* (tan X[n][m]) C[m])))
		             ) ; end M loop
		          (setq Y[n] (+ (* y .8) (* y (srandom .4))))
		          ) ; end N loop
               (setq EY (Lambda.run X))
               (setq err 0.0)
               (setq errScore 0.0)
               (setq avgTopEy 0.0)
               (setq topEyCnt 0)
               (setq avgY (avg Y))
               (setq G (/ N eyGridSize))
               (setq sortedY (|Gv:sort| Y < true)) 
               (loop for n from 0 until N do
                  (setq k sortedY[n])                  
                  (setq ey EY[k])
                  (setq y Y[k])
                  (setq g (divi n G))
                  (setq eyGrid[g] (+ eyGrid[g] ey))
                  (setq eyGridCount[g] (+ eyGridCount[g] 1.0))
                  (setq pct (/ (abs (- ey y)) (+ (abs ey) .000000001)))
                  (+= err pct)
                  (if (= (modi n (divi N 10)) 0) (writeln "[" n "] ey=[" ey "] y=[" y "] err=[" (- ey y) "] err%=[" pct "]"))
                  ) ; end N loop
               (/= err N)
               (loop for g from 0 until eyGridSize do (setq eyGrid[g] (/ eyGrid[g] eyGridCount[g]))) 
               (loop for g from 1 until eyGridSize do (if (<= eyGrid[g] eyGrid[(- g 1)]) (++ errScore)))
               (setq errScore (/ (- eyGridSize errScore 1) (- eyGridSize 1)))
               (writeln "bgmRegress: err=[" err "], avgY=[" avgY "], errScore=[" errScore "], eyGrid=[" (string eyGrid true) "]")
          )) ; end Test Case tan
       (writeln "bgmRegress.selfTest: completed in [" (/ (setq endTimeT (getTickCount startTimeT)) 60.0) "] minutes.")       
       Lambda) ; end selfTest
    ;; *****************
    ;; Begin main logic.
    ;; ***************** 
    vars:(Lambda bgm)

    ;; Train a new basis grid machine Lambda.
    (setq bgm (new (myself)))
    (setq Lambda (bgm.train X Y))
    Lambda) ; end bgmRegress

















































;;**EXPORTKEY**:esm:ennRegress
(deforphan esm:ennRegress(ObjVector:X NumVector:Y ObjVector:WMM)
;; *******************************************************************
;; name:     ennRegress
;; 
;; summary:  Evolutionary Neural Net
;;
;;           
;;           A Single layer Neural Net is a function, y = fNN{WMM}(x), mapping every x in X to some element y in Y. 
;;           Each single layer neural net is defined by the property WMM, which is an M by M+1 Vector Array of weights.
;; 
;;           Single layer Neural Net regression is equivalent to the problem of finding a Vector of coefficients, C, which 
;;           minimize the least squares error of the linear polynomial y = C[0] + (C[1]*s[1]) + (C[2]*s[2]) + ... + (C[M]*s[M]),
;;           for all y in Y and all s in S. 
;;
;;           The M-Vector s contains the signals generated from the Neural Net's hidden layer. Each M-Vector, s in S, is derrived 
;;           from its corresponding M-Vector, x in X, using the weight array, WMM, as follows.
;;
;;             s[m] = tanh(WMM[m][0] + (WMM[m][1]*x[1]) + (WMM[m][2]*x[2]) + ... + (WMM[m][M]*x[M])) {for all m in [1 ... M]}
;; 
;;           Thus, for every M-Vector, x in X, there is a corresponding M-Vector, s in S, which is derrived as shown above.
;;
;;           Training a single layer neural net involves converting every x in X into its corresponding s in S, using the above formula,
;;           then performing a multivariate regression on the linear polynomial y = C[0] + (C[1]*s[1]) + (C[2]*s[2]) + ... + (C[M]*s[M]),
;;           to find the optimal coefficient vector, C.
;;
;;
;;           A Multi-layer Neural Net supports multiple hidden layers, each providing a signal to the layer above until the final
;;           hidden layer is used for the output linear polynomial y = C[0] + (C[1]*s[H][1]) + (C[2]*s[H][2]) + ... + (C[M]*s{H}[M]), 
;;           where the number of hidden layers varies from 1 thru H. The formulas for each hidden layer is as follows:
;;
;;             s[1][m] = tanh(WMM[1][m][0] + (WMM[1][m][1]*x[1]) + (WMM[1][m][2]*x[2]) + ... + (WMM[1][m][M]*x[M])) {for all m in [1 ... M]}
;;             s[2][m] = tanh(WMM[2][m][0] + (WMM[2][m][1]*s[1][1]) + (WMM[2][m][2]*s[1][2]) + ... + (WMM[2][m][M]*s[1][M])) {for all m in [1 ... M]}
;;                                  ...                         ...                  ...                  ...                  ...
;;             s[H][m] = tanh(WMM[H][m][0] + (WMM[H][m][1]*s[H-1][1]) + (WMM[H][m][2]*s[H-1][2]) + ... + (WMM[H][m][M]*s[H-1][M])) {for all m in [1 ... M]}
;;           
;;           Each Multi-layer neural net, y = fNN{WMM}(x), is defined by the property WMM, which is an H by M by M+1 Vector Array of weights.
;;
;;           Training a multi-layer neural net involves converting every x in X into its corresponding s[1] in S, then
;;           converting every s[1] into its corresponding s[2], until we reach the final hidden layer s[H], at which point we
;;           perform a multivariate regression on the linear polynomial y = C[0] + (C[1]*s[H][1]) + (C[2]*s[H][2]) + ... + (C[M]*s[H][M]),
;;           to find the optimal coefficient vector, C.
;;
;;
;; Parms:    X:         The N by M vector array representing the original observations
;;                      in the form of:    x x ... x
;;                                         x x ... x
;;                                             ... 
;;                                         x x ... x
;;           Y   		The N vector of dependent variables.
;;           WMM        The H by M by M+1 Vector Array of weights for the neural net hidden layers.
;;
;; Return:   Lambda: 	The enhanced neural net Lambda after training is complete.
;;
;;           Note1: See Fausett, "Fundamentals of Neural Networks", ISBN 0133341860.
;;           Note2: See Freeman & Skapura, "Neural Networks: Algorithms, Applications, and Programming techniques", ISBN 0201513765.
;;           Note3: See Eberhart, "Swarm Intelligence", ISBN 1558605959.
;; *******************************************************************
    pvars:(;; Public variables
           NumVector:myC   		      	;; The M+1 vector of output layer regression weights, after training the ENN regression model. 
           Number:Error                 ;; The current absolute error for the SEE regression model (in percent of target). 
           myENNParent                  ;; The parent Lambda of this ENN regression Lambda community. 
           Integer:myH					;; The number of hidden layers in this ENN regression Lambda. 
           Integer:myM					;; The number of elements in each training example (independent variables). 
           Integer:myN					;; The number of training examples (independent variables). 
           myPool                       ;; The pool of fit neural net Lambdas evolved by the trainNeuralNet method. 
           myS                          ;; The N x M matrix of converted training examples, independent variables, for training the ENN regression model. 
           myVerboseSW      			;; True iff we are to display progress on the console. 
           myWMM		      			;; The H x M x M+1 vector array of hidden layer weights, for training the ENN regression model. 
           myX                          ;; The N x M matrix of original training examples, independent variables, for training the ENN regression model. 
           NumVector:myY                ;; The N vector of training examples (dependent variables). 

           ;; Public methods
           clear			       		;; Clear the current enhanced neural net.
           computeError	           		;; Return the average percent error for all of the training examples.
           convertThruLayers		    ;; Convert the specified training example through each hidden layer to produce the final hidden layer signals.
           initWeights 					;; Initialized the evolutionary neural net hidden layer regression weights.
           mergeWeights 				;; Combine two neural net hidden layer regression weight Vectors into a merged hidden layer regression weight Vector.
	       multipleRegression           ;; Performs a Gaussian multiple regression on the myS and myY converted training examples
           newLambda			       		;; Return an Lambda ready to compute the ENN output for specified input vector.
           random                       ;; Pseudo random n umber routine.
           trainNeuralNet		 		;; Train the evolutionary neural net machine on the specified training examples.
           trainRegressionModel 		;; Train the evolutionary neural net machine regression model on the specified training examples.

           ;; Private maintenance child methods
           selfTest                		;; The self test method for this Lambda. 
           ) ;; end of persistent variables
    ;; ***************************
    ;; Define Public Child Lambdas.
    ;; ***************************
    ;; Clear the current enhanced neural net.
    (defun clear()
       (setq myPool #void)
       (setq Error 0.0)
       (setq myH 0) 
       (setq myM 0)
       (setq myN 0)
       (setq myS #void)
       (setq myWMM #void)
       (setq myX #void)
       (setq myY #void)
       true) ; end clear
    ;; Return the average percent error for all of the training examples.
    ;; Note: The regression error is computed as a percent of the target (dependent variable).
    (defun computeError()
       regs:(m M n N)
       regs:(Number:pct Number:s Number:c Number:ey Number:y)
       regs:((Number:err 0.0)(Number:efudge 0.00000000001))
       vars:(NumVector:S)
       ;; Initialize (if necessary)
	   (setq N myN)
	   (setq M myM)
       (setq Error 1.0e+300)
       ;; We recompute the average absolute percent error
       ;; Note: This is a mission critical Lambda and MUST run extra fast.
       (vmregRunInHardware start:)
	   (loop for n from 0 until N do
          (setq S (convertThruLayers myX[n]))
          (setq ey myC[0])
          (loop for m from 0 until M do
            (setq c myC[(+ m 1)])
            (setq s S[m])
            (+ ey (* s c))
            ) ; end M loop
          (setq y myY[n])
          (setq pct (/ (abs (- ey y)) (+ (abs y) efudge))) 
	      (+= err pct)
	      ) ; end error loop
       (vmregRunInHardware stop:)
	   (/= err N)
       (setq Error err)
       err) ; end computeError
    ;; Convert the specified training example through each hidden layer to produce the final hidden layer signals.
    (defun convertThruLayers(NumVector:X)
       regs:(h H ms m M)
       regs:((Number:fudge .00000000000000001))
       regs:(Number:sumA Number:s Number:w Number:x)
       vars:(NumVector:S NumVector:W)
       ;; Initialize registers and local variables.
       (setq H myH)
       (setq M myM)
       (setq S X)
       ;; Promote the input signal through each hidden layer.
       (loop for h from 0 until H do
         (setq X S)
         (setq S (new Vector: Number: M))
         ;; Each hidden layer produces M signals.
         (loop for ms from 0 until M do
           (setq W myWMM[h][ms])
           ;; Sumarize the M+1 arguments for the neuron firing function for each output signal.
           (setq s W[0])
           (setq sumA s)
           (loop for m from 1 to M do
             (setq x X[(- m 1)])
             (setq w W[m])
             (setq s (* x w))
             (+= sumA s)
             ) ; end weight loop
           ;; Save each hidden layer signal.
           (setq s (tanh sumA))          
           (setq S[ms] s)          
           ) ; end outter signal loop
         ) ; end hidden layer loop      
       S) ; end convertThruLayers
   ;; Initialized the evolutionary neural net hidden layer regression weights.
   ;; Args(Optional): H M
   (defun initWeights(...)
      regs:(h H m ms M Number:w)
      vars:(NumVector:W ObjVector:WW)
      ;; Load optional arguments (if present).
      (if (>= (argCount) 1) (setq H (integer (argFetch 0))) (setq H myH))
      (if (>= (argCount) 2) (setq M (integer (argFetch 1))) (setq M myM))
      ;; Initialized the evolutionary neural net hidden layer regression weights.
      (setq WW (new Vector: Object: H))
      (loop for h from 0 until H do
        (setq WW[h] (new Vector: Object: M)) 
        (loop for ms from 0 until M do
          (setq WW[h][ms] (setq W (new Vector: Number: (+ M 1)))) 
          (loop for m from 0 to M do
            (setq w (- (random 1.0) .5))
            (setq W[m] w)
            ) ; end inner M loop
          ) ; end outter M loop
        ) ; end hidden layer loop
      WW) ; end initWeights
   ;; Combine two neural net hidden layer regression weight Vectors into a merged hidden layer regression weight Vector.
   ;; Note: A is always the more fit weight vector, and B always the less fit vector.
   (defun mergeWeights(ObjVector:A ObjVector:B)
      regs:(h H m ms M)
      regs:(Number:wa Number:wb Number:wc)
      regs:(Number:phi Number:delta (Number:range 1.5))
      vars:(ObjVector:C NumVector:WA NumVector:WB NumVector:WC)
      ;; Combine the evolutionary neural net hidden layer regression weight vectors.
      (setq H (length A))
      (setq M (- (length A[0]) 1))
      (setq C (new Vector: Object: H))
      (loop for h from 0 until H do
        (setq C[h] (new Vector: Object: M)) 
        (loop for ms from 0 until M do
          (setq WA A[h][ms])
          (setq WB B[h][ms])
          (setq C[h][ms] (setq WC (new Vector: Number: (+ M 1)))) 
          (loop for m from 0 to M do
            (setq phi (random range))
            (setq wa WA[m])
            (setq wb WB[m])
            (setq delta (- wa wb))
            (setq wc (+ wb (* phi delta)))
            (setq WC[m] wc)
            ) ; end inner M loop
          ) ; end outter M loop
        ) ; end hidden layer loop
      C) ; end mergeWeights
   ;; summary:  Performs a Gaussian multiple regression on the X Y training data
   ;; Return:   myC:     The M+1 weight coefficient vector for the regression model.
   ;; Note1:    Average error statistics are computed as a percent of the target (dependent variable).
   ;; Note2:    See Sedgewick[2] chap 37.
   (defun multipleRegression(ObjVector:X NumVector:Y)
      regs:(m M n N MY NumPointer:pmxy Number:wx Number:y)
      vars:(NumMatrix:Xt NumVector:x NumVector:C NumMatrix:MXY)
      ;; Convert X Y vector input to a Matrix format.
      (setq N (length X))
      (setq M (length X[0]))
      (setq MY (+ M 2))
      (if (<> N (length Y)) (error "ennRegress.multipleRegression: X and Y vectors must be same length"))
      (setq MXY (new Matrix: number: 2 N MY))
      (vmregRunInHardware start:)
      (setq pmxy MXY)
      (loop for n from 0 until N do
        (setq pmxy[0] 1.0)(++ pmxy) ;; Add constant term     
        (setq x X[n])
        (loop for m from 0 until M do
          (setq wx x[m])
          (setq pmxy[0] wx)
          (++ pmxy)
          ) ; end n loop
        (setq y Y[n])
        (setq pmxy[0] y)
        (++ pmxy)
        ) ; end m loop
      (vmregRunInHardware stop:)
      ;; Perform a least squares regression on all the factors.
      (setq Xt (|Gv:makeGaussianMatrix| MXY))
      (setq Xt (|Gv:matrixGaussianEliminate| Xt true))
      (setq myC (|Gv:matrixGaussianSubstitute| Xt))
      ;; Return the validated weight coefficient vector for the regression model.
      myC) ; end multipleRegression
   ;; Return an Lambda ready to compute the ENN output for specified input points.
   (defun newLambda()
       regs:(n m NW)
       vars:(NumVector:hW Vector:hKX ObjVector:hWX)
       vars:(Lambda HRecord)
       (setq Lambda (eval 
        {(lambda(NumVector:xv) 
         pvars:(Strategy NumVector:myC Number:Error Integer:myH Integer:myM myWMM convertThruLayers) 
         (defun convertThruLayers(NumVector:X)
            regs:(h H ms m M)
            regs:((Number:fudge .00000000000000001))
            regs:(Number:sumA Number:s Number:w Number:x)
            vars:(NumVector:S NumVector:W)
            ;; Initialize registers and local variables.
            (setq H myH)
            (setq M myM)
            (setq S X)
            ;; Promote the input signal through each hidden layer.
            (loop for h from 0 until H do
              (setq X S)
              (setq S (new Vector: Number: M))
              ;; Each hidden layer produces M signals.
              (loop for ms from 0 until M do
                (setq W myWMM[h][ms])
                ;; Sumarize the M+1 arguments for the neuron firing function for each output signal.
                (setq s W[0])
                (setq sumA s)
                (loop for m from 1 to M do
                  (setq x X[(- m 1)])
                  (setq w W[m])
                  (setq s (* x w))
                  (+= sumA s)
                  ) ; end weight loop
                ;; Save each hidden layer signal.
                (setq s (tanh sumA))          
                (setq S[ms] s)          
                ) ; end outter signal loop
              ) ; end hidden layer loop      
            S) ; end convertThruLayers
         regs:(m Number:wx Number:sx Number:ey)
         vars:(NumVector:sv) 
         (setq ey myC[0])
         (setq sv (convertThruLayers xv))
         (loop for m from 0 until myM do
            (setq sx sv[m]) 
            (setq wx myC[(+ m 1)]) 
            (+= ey (* wx sx)) 
            )
          ey)
        }))
       ;; Move all important knowledge structures into the trained regression Lambda.
       (setq Lambda.Strategy ENN:)
       (setq Lambda.myC myC)
       (setq Lambda.Error Error)
       (setq Lambda.myH myH)
       (setq Lambda.myM myM)
       (setq Lambda.myWMM myWMM)
       Lambda) ; end newLambda
    ;; Pseudo random n umber routine.
    (defun random(upperLimit)
       regs:(n N)
       vars:((elements #(0.5  0.25  0.125  0.0625  0.03125  0.015625  0.0078125  0.00390625  0.001953125  0.0009765625  0.00048828125  0.000244140625  0.0001220703125  6.103515625E-005  3.0517578125E-005  1.52587890625E-005  7.62939453125E-006  3.814697265625E-006  1.907348632813E-006  9.536743164063E-007  4.768371582031E-007  2.384185791016E-007  1.192092895508E-007  5.960464477539E-008  2.98023223877E-008  1.490116119385E-008  7.450580596924E-009  3.725290298462E-009  1.862645149231E-009  9.313225746155E-010  4.656612873077E-010  2.328306436539E-010  1.164153218269E-010  5.820766091347E-011  2.910383045673E-011  1.455191522837E-011  7.275957614183E-012  3.637978807092E-012  1.818989403546E-012  9.094947017729E-013  4.547473508865E-013  2.273736754432E-013  1.136868377216E-013  5.684341886081E-014  2.84217094304E-014  1.42108547152E-014  7.105427357601E-015)))
       vars:(result)
       ;; Support existing or argument persistent variables structure.
       (setq N (length elements))
       (loop for n from 0 until N do (if (>= (|Gv:srandom| 1.0) .5) (+= result elements[n])))
       (*= result upperLimit)
       result) ; end random
    ;; Train the evolutionary neural net machine on the specified training examples.
    (defun trainNeuralNet(ObjVector:X NumVector:Y Integer:H Integer:maxGenerations Number:stopError)
       regs:(n N nn NN g G)
       regs:(Number:err)
       vars:(Lambda)
       ;; Training parameters
       vars:((Integer:initCount 25) (Integer:poolCount 25))
       ;; Retrieve, save, and validate the arguments.
       (setq myX X) 
       (setq myY Y) 
       (setq myN (length myY))
       (setq myM (length myX[0]))
       (setq myH H)
       (if (<> (type myX[0]) NumVector:) (error "esm.ennRegress.trainNeuralNet: X must be an ObjVector of NumVectors"))
       ;; Initialize a pool of randomly constructed regression Lambdas.
       (if myVerboseSW (writeln "esm.ennRegress.trainNeuralNet: initializing fittness pool"))
       (setq myPool (new Directory:))
       (setq N initCount)
       (loop for n from 0 until N do
         (setq myWMM (initWeights))
         (trainRegressionModel)
         (computeError)
         (setq Lambda (newLambda))
         (setq myPool[Error] Lambda)
         (if (> (length myPool) poolCount) (resize myPool poolCount))
         ) ; end initailize loop
       ;; Evolve the survivor population until we have reached the maximum requested generation count.
       (setq G maxGenerations)
       (loop for g from 0 until G do
         (if myVerboseSW (writeln "esm.ennRegress.trainNeuralNet: starting generation [" (+ g 1) "], Error=[" Error "], BestError=[" myPool[0 1].Error "]"))
         (setq N (length myPool))
         (setq NN (- N 2))
         (loop for nn from 0 until NN do
           (setq myWMM (mergeWeights myPool[(setq n (integer (random (- N 1)))) 1].myWMM myPool[(+ n 1 (integer (random (- N n 1)))) 1].myWMM))
           (trainRegressionModel)
           (computeError)
           (setq Lambda (newLambda))
           (setq myPool[Error] Lambda)
           (if (> (length myPool) poolCount) (resize myPool poolCount))
           ) ; end merge weights loop
         ) ; end evolution loop
       ;; Return the best enhanced neural net regression Lambda.
       (setq Lambda myPool[0 1])
       (setq Error Lambda.Error)
       (setq myWMM Lambda.myWMM)
       (setq myC Lambda.myC)
       Lambda) ; end trainNeuralNet
    ;; Train the evolutionary neural net regression model on the specified training examples.
	;; Return:   myC: 	    The trained ENN output layer regression weights.
    (defun trainRegressionModel()
       regs:(n N)
       vars:(S)
	   ;; Convert the original training examples into their final hidden layer signals.
       (setq N myN)
       (setq myS (new Vector: Object: N))
       (loop for n from 0 until N do
	     (setq myS[n] (convertThruLayers myX[n]))
         ) ; end conversion loop
       (setq myC (multipleRegression myS myY))
       (setq Error 1.0e+300)
       myC) ; end trainRegressionModel
    ;; ****************************************
    ;; Define Private Maintenance Child Lambdas.
    ;; ****************************************
    ;; The self test method for this Lambda.
    (defun selfTest(Test Ms Ns Lc Gs)
       vars:(k m n g G y ey C c X Y Yv avgY avgTopEy topEyCnt
             Lambda err Net pct 
             startTime endTime startTimeT endTimeT
             (checkResults true)
             (tol 0.0) (errStop 0.01) (Cs 1.0)
             ) ; end temporary variables
       (clear)
       (setq startTimeT (getTickCount 0))
       (setq srandom.seed 8192.0)      
       ;; Select the requested test case
       ;; Test Case linear 
       (if (or (= Test all:) (= Test linear:))
           (begin
		       ;; Create a test polynomial regression where y = C[0]*X[0] + C[1]*X[1] - C[2]*X[2] + C[3]*X[3] - C[4]*X[4] ...
		       ;; Note1: We support a bias by having X[0] == 1 for all N.
		       ;; Note2: This algorithm seems to work well when N is at least 25 times M.
		       (setq c Cs)
		       (setq M Ms)
		       (setq N Ns)
		       (setq X (new Vector: object: N))
		       (setq Y (new Vector: Number: N))
		       (setq C (new Vector: Number: M))
		       (setq C[0] c)
		       (loop for m from 1 until M do
		          (setq C[m] (- (srandom 100.0) 50.0))
		          ) ; end C loop
		       (loop for n from 0 until N do
		          (setq X[n] (new Vector: Number: M))
		          (setq X[n][0] 1)
		          (setq y (* C[0] X[n][0]))
		          (loop for m from 1 until M do 
		          	 (setq X[n][m] (- (srandom 100.0) 50.0))
		             (setq y (+ y (* X[n][m] C[m])))
		             ) ; end M loop
		          (setq Y[n] y)
		          ) ; end N loop
		       ;; Train on the test case.
		       (writeln _eol "Starting test case: linear")
		       (setq Lambda (setq Lambda (esm.ennRegress.trainNeuralNet X Y Lc Gs errStop)))
		       (if (= myVerboseSW false) (writeln "ennRegress: N = [" Ns "], M = [" M "], Generations = [" Gs  "], Layers = [" Lc  "], Error=[" Lambda.Error "]")) 
               (if (= checkResults true)
                   (begin
                      (setq err 0.0)
                      (setq avgTopEy 0.0)
                      (setq topEyCnt 0)
                      (setq avgY (avg Y))
		              (loop for n from 0 until N do
		                 (setq y (Lambda X[n]))
                         (setq pct (- Y[n] y))
                         (if (<> Y[n] 0.0) (/= pct Y[n])) 
                         (if (< pct 0.0) (setq pct (- 0.0 pct)))
                         (setq pct (- pct tol))
                         (if (< pct 0.0) (setq pct 0.0))
		                 (+= err pct)
                         (if (= (modi n (divi N 10)) 0) (writeln "[" n "] ey=[" y "] y=[" Y[n] "] err=[" (- Y[n] y) "] err%=[" pct "]"))
		                 ) ; end N loop
                      (/= err N) 
		              (writeln "ennRegress: err=[" err "], avgY=[" avgY "]")
		           )) ; end if
          )) ; end Test Case linear
       ;; Test Case linearSigmoid 
       (if (or (= Test all:) (= Test linearSigmoid:))
           (begin
		       ;; Create a test polynomial regression where y = C[0]*X[0] + C[1]*X[1] - C[2]*X[2] + C[3]*X[3] - C[4]*X[4] ...
		       ;; Note1: The inputs, X, are restricted to the sigmoid domain.
		       ;; Note2: We support a bias by having X[0] == 1 for all N.
		       ;; Note3: This algorithm seems to work well when N is at least 25 times M.
		       (setq c Cs)
		       (setq M Ms)
		       (setq N Ns)
		       (setq X (new Vector: object: N))
		       (setq Y (new Vector: Number: N))
		       (setq C (new Vector: Number: M))
		       (setq C[0] c)
		       (loop for m from 1 until M do
		          (setq C[m] (- (srandom 1) .5))
		          ) ; end C loop
		       (loop for n from 0 until N do
		          (setq X[n] (new Vector: Number: M))
		          (setq X[n][0] 1)
		          (setq y (* C[0] X[n][0]))
		          (loop for m from 1 until M do 
		          	 (setq X[n][m] (- (srandom .999999999) 0))
		             (setq y (+ y (* X[n][m] C[m])))
		             ) ; end M loop
		          (setq Y[n] y)
		          ) ; end N loop
		       ;; Train on the test case.
		       (writeln _eol "Starting test case: linearSigmoid")
		       (setq Lambda (setq Lambda (esm.ennRegress.trainNeuralNet X Y Lc Gs errStop)))
		       (if (= myVerboseSW false) (writeln "ennRegress: N = [" Ns "], M = [" M "], Generations = [" Gs  "], Layers = [" Lc  "], Error=[" Lambda.Error "]")) 
               (if (= checkResults true)
                   (begin
                      (setq err 0.0)
                      (setq avgTopEy 0.0)
                      (setq topEyCnt 0)
                      (setq avgY (avg Y))
		              (loop for n from 0 until N do
		                 (setq y (Lambda X[n]))
                         (if (> y avgY) then (begin (++ topEyCnt) (+= avgTopEy Y[n])))
                         (setq pct (- Y[n] y))
                         (if (<> Y[n] 0.0) (/= pct Y[n])) 
                         (if (< pct 0.0) (setq pct (- 0.0 pct)))
                         (setq pct (- pct tol))
                         (if (< pct 0.0) (setq pct 0.0))
		                 (+= err pct)
                         (if (= (modi n (divi N 10)) 0) (writeln "[" n "] ey=[" y "] y=[" Y[n] "] err=[" (- Y[n] y) "] err%=[" pct "]"))
		                 ) ; end N loop
                      (/= err N) 
		              (writeln "ennRegress: err=[" err "], avgY=[" avgY "]")
		           )) ; end if
          )) ; end Test Case linearSigmoid
       ;; Test Case srandom 
       (if (or (= Test all:) (= Test srandom:))
           (begin
		       ;; Create a test polynomial regression where y = C[0]*X[0] + C[1]*X[1] - C[2]*X[2] + C[3]*X[3] - C[4]*X[4] ...
		       ;; Note1: We support a bias by having X[0] == 1 for all N.
		       ;; Note2: This algorithm seems to work well when N is at least 25 times M.
		       (setq c Cs)
		       (setq M Ms)
		       (setq N Ns)
		       (setq X (new Vector: object: N))
		       (setq Y (new Vector: Number: N))
		       (setq C (new Vector: Number: M))
		       (setq C[0] c)
		       (loop for m from 1 until M do
		          (setq C[m] (- (srandom 100.0) 50.0))
		          ) ; end C loop
		       (loop for n from 0 until N do
		          (setq X[n] (new Vector: Number: M))
		          (setq X[n][0] 1)
		          (setq y (* C[0] X[n][0]))
		          (loop for m from 1 until M do
		          	 (setq X[n][m] (- (srandom 100.0) 50.0))
		             (setq y (+ y (* X[n][m] C[m])))
		             ) ; end M loop
		          (setq Y[n] (+ (* y .8) (* y (srandom .4))))
		          ) ; end N loop
		       ;; Train on the test case.
		       (writeln _eol "Starting test case: srandom")
		       (setq Lambda (setq Lambda (esm.ennRegress.trainNeuralNet X Y Lc Gs errStop)))
		       (if (= myVerboseSW false) (writeln "ennRegress: N = [" Ns "], M = [" M "], Generations = [" Gs  "], Layers = [" Lc  "], Error=[" Lambda.Error "]")) 
               (if (= checkResults true)
                   (begin
                      (setq err 0.0)
                      (setq avgTopEy 0.0)
                      (setq topEyCnt 0)
                      (setq avgY (avg Y))
		              (loop for n from 0 until N do
		                 (setq y (Lambda X[n]))
                         (if (> y avgY) then (begin (++ topEyCnt) (+= avgTopEy Y[n])))
                         (setq pct (- Y[n] y))
                         (if (<> Y[n] 0.0) (/= pct Y[n])) 
                         (if (< pct 0.0) (setq pct (- 0.0 pct)))
                         (setq pct (- pct tol))
                         (if (< pct 0.0) (setq pct 0.0))
		                 (+= err pct)
                         (if (= (modi n (divi N 10)) 0) (writeln "[" n "] ey=[" y "] y=[" Y[n] "] err=[" (- Y[n] y) "] err%=[" pct "]"))
		                 ) ; end N loop
                      (/= err N) 
		              (writeln "ennRegress: err=[" err "], avgY=[" avgY "]")
		           )) ; end if
          )) ; end Test Case srandom
       ;; Test Case mixedRandom 
       (if (or (= Test all:) (= Test mixedRandom:))
           (begin
		       ;; Create a test polynomial linear model where y = C[0]*X[0] + C[1]*X[1] + C[2]*X[2] ...
		       ;; Create a test polynomial square model where y = C[0]*X[0]*X[0] + C[1]*X[1]*X[1] + C[2]*X[2]*X[2] ...
		       ;; Create a test polynomial sin model where y = C[0]*sin(X[0]) + C[1]*sin(X[1]) + C[2]*sin(X[2]) ...
		       ;; Create a test polynomial log model where y = C[0]*log(abs(X[0])+.000001) + C[1]*log(abs(X[1])+.000001) + C[1]*log(abs(X[2])+.000001) ...
               ;; These four models are mixed together and random noise is added.
		       ;; Note1: We support a bias by having X[0] == 1 for all N.
		       ;; Note2: This algorithm seems to work well when N is at least 25 times M.
		       (setq c Cs)
		       (setq M Ms)
		       (setq N Ns)
		       (setq X (new Vector: object: N))
		       (setq Y (new Vector: Number: N))
		       (setq C (new Vector: Number: M))
		       (setq C[0] c)
		       (loop for m from 1 until M do
		          (setq C[m] (- (srandom 100.0) 50.0))
		          ) ; end C loop
		       (loop for n from 0 until N do
		          (setq X[n] (new Vector: Number: M))
                  (setq k (modi n 4)) 
 		          (setq X[n][0] 1.0)
 		          (setq X[n][1] (number k))
		          (setq y (* C[0] X[n][0]))
		          (loop for m from 2 until M do
		          	 (setq X[n][m] (- (srandom 100.0) 50.0))
                     ;; Mix the four models together
                     (cond
                        ;; Linear model
                        ((= k 0) (setq y (+ y (* X[n][m] C[m]))))
                        ;; Square model
                        ((= k 1) (setq y (+ y (* X[n][m] X[n][m] C[m]))))
                        ;; Sine model
                        ((= k 2) (setq y (+ y (* (|Gv:sin| X[n][m]) C[m]))))
                        ;; Log model
                        (else (setq y (+ y (* (|Gv:log| (+ .000001 (|Gv:abs| X[n][m]))) C[m]))))
                        ) ; end cond
		             ) ; end M loop
		          (setq Y[n] (+ (* y .8) (* y (srandom .4))))
		          ) ; end N loop
		       ;; Train on the test case.
		       (writeln _eol "Starting test case: mixedRandom")
		       (setq Lambda (esm.ennRegress.ennTraining X Y kernelID properties))
		       (if (= myVerboseSW false) (writeln "ennRegress: N = [" Ns "], M = [" Lambda.M "], Generations = [" Generations  "], Layers = [" myMaxLayers  "], SVM's = [" Lambda.N "], ETollerance=[" Lambda.ETollerance "], Error=[" Lambda.Error "], ErrorGrid=[" (string Lambda.ErrorGrid true) "]")) 
               (if (= checkResults true)
                   (begin
                      (setq err 0.0)
                      (setq avgTopEy 0.0)
                      (setq topEyCnt 0)
                      (setq avgY (avg Y))
		              (loop for n from 0 until N do
		                 (setq y (Lambda X[n]))
                         (if (> y avgY) then (begin (++ topEyCnt) (+= avgTopEy Y[n])))
                         (setq pct (- Y[n] y))
                         (if (<> Y[n] 0.0) (/= pct Y[n])) 
                         (if (< pct 0.0) (setq pct (- 0.0 pct)))
                         (setq pct (- pct tol))
                         (if (< pct 0.0) (setq pct 0.0))
		                 (+= err pct)
                         (if (= (modi n (divi N 10)) 0) (writeln "[" n "] ey=[" y "] y=[" Y[n] "] err=[" (- Y[n] y) "] err%=[" pct "]"))
		                 ) ; end N loop
                      (/= err N) 
		              (writeln "ennRegress: err=[" err "], avgY=[" avgY "]")
		           )) ; end if
          )) ; end Test Case mixedRandom
       ;; Test Case randomSigmoid 
       (if (or (= Test all:) (= Test randomSigmoid:))
           (begin
		       ;; Create a test polynomial regression where y = C[0]*X[0] + C[1]*X[1] - C[2]*X[2] + C[3]*X[3] - C[4]*X[4] ...
		       ;; Note1: The inputs, X, are restricted to the sigmoid domain.
		       ;; Note2: We support a bias by having X[0] == 1 for all N.
		       ;; Note3: This algorithm seems to work well when N is at least 25 times M.
		       (setq c Cs)
		       (setq M Ms)
		       (setq N Ns)
		       (setq X (new Vector: object: N))
		       (setq Y (new Vector: Number: N))
		       (setq C (new Vector: Number: M))
		       (setq C[0] c)
		       (loop for m from 1 until M do
		          (setq C[m] (- (srandom 1) .5))
		          ) ; end C loop
		       (loop for n from 0 until N do
		          (setq X[n] (new Vector: Number: M))
		          (setq X[n][0] 1)
		          (setq y (* C[0] X[n][0]))
		          (loop for m from 1 until M do 
		          	 (setq X[n][m] (- (srandom .999999999) 0))
		             (setq y (+ y (* X[n][m] C[m])))
		             ) ; end M loop
		          (setq Y[n] (+ (* y .8) (* y (srandom .4))))
		          ) ; end N loop
		       ;; Train on the test case.
		       (writeln _eol "Starting test case: randomSigmoid")
		       (setq Lambda (esm.ennRegress.ennTraining X Y kernelID properties))
		       (if (= myVerboseSW false) (writeln "ennRegress: N = [" Ns "], M = [" Lambda.M "], Generations = [" Generations  "], Layers = [" myMaxLayers  "], SVM's = [" Lambda.N "], ETollerance=[" Lambda.ETollerance "], Error=[" Lambda.Error "], ErrorGrid=[" (string Lambda.ErrorGrid true) "]")) 
               (if (= checkResults true)
                   (begin
                      (setq err 0.0)
                      (setq avgTopEy 0.0)
                      (setq topEyCnt 0)
                      (setq avgY (avg Y))
		              (loop for n from 0 until N do
		                 (setq y (Lambda X[n]))
                         (if (> y avgY) then (begin (++ topEyCnt) (+= avgTopEy Y[n])))
                         (setq pct (- Y[n] y))
                         (if (<> Y[n] 0.0) (/= pct Y[n])) 
                         (if (< pct 0.0) (setq pct (- 0.0 pct)))
                         (setq pct (- pct tol))
                         (if (< pct 0.0) (setq pct 0.0))
		                 (+= err pct)
                         (if (= (modi n (divi N 10)) 0) (writeln "[" n "] ey=[" y "] y=[" Y[n] "] err=[" (- Y[n] y) "] err%=[" pct "]"))
		                 ) ; end N loop
                      (/= err N) 
		              (writeln "ennRegress: err=[" err "], avgY=[" avgY "]")
		           )) ; end if
          )) ; end Test Case randomSigmoid
       ;; Test Case square 
       (if (or (= Test all:) (= Test square:))
           (begin
		       ;; Create a test polynomial regression where y = -11.2 + C[0]*X[0] - C[1]*(X[1]**2) + C[2]*X[2] - C[3]*(X[3]**2) ...
		       ;; Note: We support a bias by having X[0] == 1 for all N.
		       (setq c Cs)
		       (setq M Ms)
		       (setq N Ns)
		       (setq X (new Vector: object: N))
		       (setq Y (new Vector: Number: N))
		       (setq C (new Vector: Number: M))
		       (setq C[0] c)
		       (loop for m from 1 until M do
		          (setq C[m] (- (srandom 100.0) 50.0))
		          ) ; end C loop
		       (loop for n from 0 until N do
		          (setq X[n] (new Vector: Number: M))
		          (setq X[n][0] 1)
		          (setq y (* C[0] X[n][0]))
		          (loop for m from 1 until M do
		          	 (setq X[n][m] (- (srandom 100.0) 50.0))
		             (if (isOdd m)
			             (setq y (+ y (* X[n][m] C[m])))
			             (setq y (+ y (* X[n][m] X[n][m] C[m])))
		                 ) ; end if
		             ) ; end M loop
		          (setq Y[n] y)
		          ) ; end N loop
		       ;; Train on the test case.
		       (writeln _eol "Starting test case: square")
		       (setq Lambda (esm.ennRegress.ennTraining X Y kernelID properties))
		       (if (= myVerboseSW false) (writeln "ennRegress: N = [" Ns "], M = [" Lambda.M "], Generations = [" Generations  "], Layers = [" myMaxLayers  "], SVM's = [" Lambda.N "], ETollerance=[" Lambda.ETollerance "], Error=[" Lambda.Error "], ErrorGrid=[" (string Lambda.ErrorGrid true) "]")) 
               (if (= checkResults true)
                   (begin
                      (setq err 0.0)
                      (setq avgTopEy 0.0)
                      (setq topEyCnt 0)
                      (setq avgY (avg Y))
		              (loop for n from 0 until N do
		                 (setq y (Lambda X[n]))
                         (if (> y avgY) then (begin (++ topEyCnt) (+= avgTopEy Y[n])))
                         (setq pct (- Y[n] y))
                         (if (<> Y[n] 0.0) (/= pct Y[n])) 
                         (if (< pct 0.0) (setq pct (- 0.0 pct)))
                         (setq pct (- pct tol))
                         (if (< pct 0.0) (setq pct 0.0))
		                 (+= err pct)
                         (if (= (modi n (divi N 10)) 0) (writeln "[" n "] ey=[" y "] y=[" Y[n] "] err=[" (- Y[n] y) "] err%=[" pct "]"))
		                 ) ; end N loop
                      (/= err N) 
		              (writeln "ennRegress: err=[" err "], avgY=[" avgY "]")
		           )) ; end if
          )) ; end Test Case square
       ;; Test Case tan 
       (if (or (= Test all:) (= Test tan:))
           (begin
		       ;; Create a test polynomial regression where y = -11.2 + C[0]*X[0] - C[1]*(X[1]**2) + C[2]*X[2] - C[3]*(X[3]**2) ...
		       ;; Note: We support a bias by having X[0] == 1 for all N.
		       (setq c Cs)
		       (setq M Ms)
		       (setq N Ns)
		       (setq X (new Vector: object: N))
		       (setq Y (new Vector: Number: N))
		       (setq C (new Vector: Number: M))
		       (setq C[0] c)
		       (loop for m from 1 until M do
		          (setq C[m] (- (srandom 100.0) 50.0))
		          ) ; end C loop
		       (loop for n from 0 until N do
		          (setq X[n] (new Vector: Number: M))
		          (setq X[n][0] 1)
		          (setq y (* C[0] X[n][0]))
		          (loop for m from 1 until M do
		          	 (setq X[n][m] (- (srandom 100.0) 50.0))
		             (setq y (+ y (* (tan X[n][m]) C[m])))
		             ) ; end M loop
		          (setq Y[n] y)
		          ) ; end N loop
		       ;; Train on the test case.
		       (writeln _eol "Starting test case: tan")
		       (setq Lambda (esm.ennRegress.ennTraining X Y kernelID properties))
		       (if (= myVerboseSW false) (writeln "ennRegress: N = [" Ns "], M = [" Lambda.M "], Generations = [" Generations  "], Layers = [" myMaxLayers  "], SVM's = [" Lambda.N "], ETollerance=[" Lambda.ETollerance "], Error=[" Lambda.Error "], ErrorGrid=[" (string Lambda.ErrorGrid true) "]")) 
               (if (= checkResults true)
                   (begin
                      (setq err 0.0)
                      (setq avgTopEy 0.0)
                      (setq topEyCnt 0)
                      (setq avgY (avg Y))
		              (loop for n from 0 until N do
		                 (setq y (Lambda X[n]))
                         (if (> y avgY) then (begin (++ topEyCnt) (+= avgTopEy Y[n])))
                         (setq pct (- Y[n] y))
                         (if (<> Y[n] 0.0) (/= pct Y[n])) 
                         (if (< pct 0.0) (setq pct (- 0.0 pct)))
                         (setq pct (- pct tol))
                         (if (< pct 0.0) (setq pct 0.0))
		                 (+= err pct)
                         (if (= (modi n (divi N 10)) 0) (writeln "[" n "] ey=[" y "] y=[" Y[n] "] err=[" (- Y[n] y) "] err%=[" pct "]"))
		                 ) ; end N loop
                      (/= err N) 
		              (writeln "ennRegress: err=[" err "], avgY=[" avgY "]")
		           )) ; end if
          )) ; end Test Case tan
       ;; Test Case log 
       (if (or (= Test all:) (= Test log:))
           (begin
		       ;; Create a test polynomial regression where y = -11.2 + C[0]*X[0] - C[1]*(X[1]**2) + C[2]*X[2] - C[3]*(X[3]**2) ...
		       ;; Note: We support a bias by having X[0] == 1 for all N.
		       (setq c Cs)
		       (setq M Ms)
		       (setq N Ns)
		       (setq X (new Vector: object: N))
		       (setq Y (new Vector: Number: N))
		       (setq C (new Vector: Number: M))
		       (setq C[0] c)
		       (loop for m from 1 until M do
		          (setq C[m] (- (srandom 100.0) 50.0))
		          ) ; end C loop
		       (loop for n from 0 until N do
		          (setq X[n] (new Vector: Number: M))
		          (setq X[n][0] 1)
		          (setq y (* C[0] X[n][0]))
		          (loop for m from 1 until M do
		          	 (setq X[n][m] (- (srandom 100.0) 50.0))
		             (setq y (+ y (* (log (+ 1.0 (abs X[n][m]))) C[m])))
		             ) ; end M loop
		          (setq Y[n] y)
		          ) ; end N loop
		       ;; Train on the test case.
		       (writeln _eol "Starting test case: log")
		       (setq Lambda (esm.ennRegress.ennTraining X Y kernelID properties))
		       (if (= myVerboseSW false) (writeln "ennRegress: N = [" Ns "], M = [" Lambda.M "], Generations = [" Generations  "], Layers = [" myMaxLayers  "], SVM's = [" Lambda.N "], ETollerance=[" Lambda.ETollerance "], Error=[" Lambda.Error "], ErrorGrid=[" (string Lambda.ErrorGrid true) "]")) 
               (if (= checkResults true)
                   (begin
                      (setq err 0.0)
                      (setq avgTopEy 0.0)
                      (setq topEyCnt 0)
                      (setq avgY (avg Y))
		              (loop for n from 0 until N do
		                 (setq y (Lambda X[n]))
                         (if (> y avgY) then (begin (++ topEyCnt) (+= avgTopEy Y[n])))
                         (setq pct (- Y[n] y))
                         (if (<> Y[n] 0.0) (/= pct Y[n])) 
                         (if (< pct 0.0) (setq pct (- 0.0 pct)))
                         (setq pct (- pct tol))
                         (if (< pct 0.0) (setq pct 0.0))
		                 (+= err pct)
                         (if (= (modi n (divi N 10)) 0) (writeln "[" n "] ey=[" y "] y=[" Y[n] "] err=[" (- Y[n] y) "] err%=[" pct "]"))
		                 ) ; end N loop
                      (/= err N) 
		              (writeln "ennRegress: err=[" err "], avgY=[" avgY "]")
		           )) ; end if
          )) ; end Test Case log
       (writeln "ennRegress.selfTest: completed in [" (/ (setq endTimeT (getTickCount startTimeT)) 60.0) "] minutes.")       
       Lambda) ; end selfTest
    ;; *****************
    ;; Begin main logic.
    ;; ***************** 
    vars:(Lambda ennLambda)

    ;; Retrieve and save the arguments.
    (clear)
    (setq myX X) 
    (setq myY Y) 
    (setq myWMM WMM) 

    ;; Validate the arguments.
    (setq myN (length myY))
    (setq myM (length myX[0]))
    (setq myH (length myWMM))
    (if (<> (type myX[0]) NumVector:) (error "esm.ennRegress: X must be an ObjVector of NumVectors"))
    (if (or (<> ObjVector: (type myWMM[0])) (<> NumVector: (type myWMM[0][0]))) (error "esm.ennRegress: WMM must be an ObjVector of ObjVectors containing NumVectors"))
    (if (or (<> myM (length myWMM[0])) (<> (+ myM 1) (length myWMM[0][0]))) (error "esm.ennRegress: WMM found to have invalid dimensions"))

    ;; Create and train a new enhanced neural net.
    (setq ennLambda (new (myself)))
    (setq ennLambda.myENNParent ennLambda)
    (ennLambda.trainRegressionModel)
    (setq Lambda (ennLambda.newLambda))
    Lambda) ; end ennRegress

















































;;**EXPORTKEY**:esm:frmRegress
(deforphan esm:frmRegress(X Y)
;; *******************************************************************
;; name:     frmRegress
;; 
;; summary:  Trains a factor model regression Lambda, using a factor model 
;;           algorithm and returns the trained regression Lambda.
;;
;;           Factor model learning machines are regression machines which learn
;;           and make regression estimates on XY vector arrays such as:
;;
;;               XY:  An N by M+1 array representing the original observations
;;                    in the form of:    x x x ...  y
;;                                       x x x ...  y
;;                                           ... 
;;                                       x x x ...  y
;;
;;           Factor Model Machines normalize the X Y training data by converting each
;;           X column and the Y column into unit quantities which reflect the number
;;           units of standard deviation they are away from the mean of the specified
;;           column. For example, if x1 contain a value equal to the mean of x1, then
;;           that value will be converted to zero. Whereas if x1 contains a value which
;;           lies exactly one unit of standard deviation above from the mean of x1, then
;;           such a value would be converted to 1.0.
;;
;;           The FRM regression is essentially a sequencing operation which attempts to
;;           predictively order the X examples in terms of the relative size of their
;;           corresponding Y value. 
;;
;;           The FRM is a normalized learning algorithm, therefore each x variable is
;;           first normalized before learning. Since, relative normalized value, not
;;           absolute value, is important, the FRM should be rescaled before estimates
;;           are made on a new data set. 
;;
;;
;; Parms:    X:         The N by M vector array representing the original observations
;;                      in the form of:    x x ... x
;;                                         x x ... x
;;                                             ... 
;;                                         x x ... x
;;           Y   		The N vector of dependent variables.
;;
;; Return:   Rf:     A new factor model machine Lambda ready for regression.
;; *******************************************************************
    pvars:(;; Public variables
           Number:Error                 ;; The current absolute error for the FRM regression model (in percent of target). 
           myFRMParent                  ;; The parent Lambda of this FRM regression Lambda community. 
           NumVector:myColumnAVGs		;; The vector of average values in each of the training columns (independent variables). 
           NumVector:myColumnSTDs		;; The vector of low values in each of the training columns (independent variables). 
           Number:myYAVG                ;; The average Y value for the FRM regression model.
           Number:myYSTD                ;; The standard deviation of Y value for the FRM regression model.
           Integer:myM					;; The number of elements in each training example (all variables). 
           myVerboseSW      			;; True iff we are to display progress on the console. 
           NumVector:myW                ;; The weight coefficient vector for the FRM regression model.
           ;; Public child methods
           clear			       		;; Clear the current factor model machine.
           frmLambda			       		;; Return an Lambda ready to compute the frm output for specified input vector.
	       multipleRegression           ;; Performs a Gaussian multiple regression on the X Y training data
           train    		       		;; Train the frm machine on the specified training examples.
           rescale   		       		;; Reset the frm machines column highs, lows, and ranges on the specified training examples.
           ;; Private maintenance child methods
           selfTest                		;; The self test method for this Lambda. 
           ) ;; end of persistent variables
    ;; ***************************
    ;; Define Public Child Lambdas.
    ;; ***************************
    ;; Clear the current factor model machine.
    (defun clear()
       (setq Error 0.0)
       (setq myColumnHighs #void) 
       (setq myColumnLows #void) 
       (setq myColumnRanges #void) 
       (setq myGY #void)
       (setq mySelectedColumns #void)
       (setq myK 0)
       (setq myM 0)
       (setq myN 0)
       (setq myW #void)
       (setq myX #void)
       (setq myY #void)
       true) ; end clear
    ; Return an Lambda ready to compute the frm output for specified input points.
    (defun frmLambda()
       regs:(n m NW)
       vars:(NumVector:hW Vector:hKX ObjVector:hWX)
       vars:(Lambda HRecord)
       (setq Lambda (eval 
        {(lambda(NumVector:xv) 
         pvars:(Strategy Number:Error NumVector:myColumnAVGs NumVector:myColumnSTDs Integer:myM NumVector:myW rescale) 
	     (defun rescale(X)
	        regs:(k K m M n N nn)
	        regs:(Number:wx Number:wn Number:wxn Number:ax Number:ey Number:y Number:RN)
	        vars:(result NumVector:x)
		    (setq N (length X))
		    (setq M (length X[0]))
		    (if (<> myM M) (error "frmRegress.rescale: must be at least one column."))
	        (setq myColumnAVGs (new Vector: Number: myM)) 
	        (setq myColumnSTDs (new Vector: Number: myM)) 
			;; Compute the column highs, lows, and ranges.
	        (vmregRunInHardware start:)
	        (setq RN (/ 1.0 N))
	        (loop for m from 0 until M do (setq myColumnAVGs[m] 0.0) (setq myColumnSTDs[m] 0.0))
	        (loop for n from 0 until N do
	           (setq x X[n])
	           (loop for m from 0 until M do 
	              (setq wx x[m]) 
	              (setq wn (* wx RN)) 
	              (setq wxn (* wx wx RN)) 
	              (setq ax myColumnAVGs[m]) 
	              (+= ax wn) 
	              (setq myColumnAVGs[m] ax)
	              (setq ax myColumnSTDs[m]) 
	              (+= ax wxn) 
	              (setq myColumnSTDs[m] ax)
	              ) ; end M loop 
	           ) ; end N loop
	        (loop for m from 0 until M do 
	           (setq wx myColumnSTDs[m]) 
	           (setq ax myColumnAVGs[m]) 
	           (-= wx (* ax ax)) 
	           (setq wx (* (+ (sqrt (abs wx)) .000000000001) 1.000000000001))
	           (setq myColumnSTDs[m] wx)
	           ) ; end M loop 
	        (vmregRunInHardware stop:)
	        true)
         regs:(m Number:wx Number:wn Number:ax Number:sx Number:ey) 
         (setq ey 0.0)
         (loop for m from 0 until myM do
            (setq ax myColumnAVGs[m]) 
            (setq sx myColumnSTDs[m]) 
            (setq wx xv[m]) 
            (-= wx ax)
            (/= wx sx) 
            (setq wn myW[m]) 
            (*= wx wn) 
            (+= ey wx)  
            )
          ey)
        }))
       ;; Move all important knowledge structures into the trained regression Lambda.
       (setq Lambda.Strategy FRM:)
       (setq Lambda.Error 1.0e+300)
       (setq Lambda.myColumnAVGs myColumnAVGs)
       (setq Lambda.myColumnSTDs myColumnSTDs)
       (setq Lambda.myM myM)
       (setq Lambda.myW myW)
       Lambda) ; end frmLambda
    ;; summary:  Performs a Gaussian multiple regression on the X Y training data
    ;; Return:   myW:    The weight coefficient vector for the regression model.
    ;; Note1:    Average error statistics are computed as a percent of the target (dependent variable).
    ;; Note2:    See Sedgewick[2] chap 37.
    (defun multipleRegression(ObjVector:X NumVector:Y)
       regs:(m M n N MY NumPointer:pmxy Number:wx Number:y)
       vars:(NumMatrix:Xt NumVector:x NumVector:C NumMatrix:MXY)
       ;; Convert X Y vector input to a Matrix format.
       (setq N (length X))
       (setq M (length X[0]))
       (setq MY (+ M 1))
       (if (<> N (length Y)) (error "frmRegress.multipleRegression: X and Y vectors must be same length"))
       (setq MXY (new Matrix: number: 2 N MY))
       (vmregRunInHardware start:)
       (setq pmxy MXY)
       (loop for n from 0 until N do
          (setq x X[n])
          (loop for m from 0 until M do
            (setq wx x[m])
            (setq wx (/ (- wx myColumnAVGs[m]) myColumnSTDs[m]))
            (setq pmxy[0] wx)
            (++ pmxy)
            ) ; end n loop
          (setq y Y[n])
          (setq y (/ (- y myYAVG) myYSTD))
          (setq pmxy[0] y)
          (++ pmxy)
          ) ; end m loop
       (vmregRunInHardware stop:)
       ;; Perform a least squares regression on all the factors.
       (setq Xt (|Gv:makeGaussianMatrix| MXY))
       (setq Xt (|Gv:matrixGaussianEliminate| Xt true))
       (setq myW (|Gv:matrixGaussianSubstitute| Xt))
       ;; Return the weight coefficient vector for the regression model.
       myW) ; end multipleRegression
    ;; Train the frm machine on the specified inputs and model.
	;; Parms:    X:         The N by M vector array representing the original observations
	;;                      in the form of:    x x ... x
	;;                                         x x ... x
	;;                                             ... 
	;;                                         x x ... x
	;;           Y   		The N vector of dependent variables.
    ;;
    ;; Return:   Rf:        A new factor model machine Lambda ready for regression.
    (defun train(X NumVector:Y)
        regs:(k K m M n N nn)
        regs:(Number:wx Number:wn Number:wxn Number:ax Number:ey Number:y Number:RN)
        vars:(result NumVector:x)
	    ;; Clear factor model machine for retraining.
	    (clear)
		;; Initialize the persistent variables before proceeding with training.
	    (if (or (<> (isVector X) true) (<> (isVector X[0]) true)) (error "frmRegress.train: X argument must be a Vector Array of rank 2"))
	    (setq N (length X))
	    (if (or (<> (isVector Y) true) (<> (length Y) N)) (error "frmRegress.train: Y argument must be a Vector of length the same as X"))
	    (setq myM (length X[0]))
	    (setq M myM)
	    (if (<= myM 0) (error "frmRegress.train: must be at least one column."))
		(setq Error 1.0e300)
        (setq myColumnAVGs (new Vector: Number: myM)) 
        (setq myColumnSTDs (new Vector: Number: myM)) 
        (setq myYAVG (avg Y)) 
        (setq myYSTD (stdev Y)) 
		;; Compute the column highs, lows, and ranges.
        (vmregRunInHardware start:)
        (setq RN (/ 1.0 N))
        (loop for m from 0 until M do (setq myColumnAVGs[m] 0.0) (setq myColumnSTDs[m] 0.0))
        (loop for n from 0 until N do
           (setq x X[n])
           (loop for m from 0 until M do 
              (setq wx x[m]) 
              (setq wn (* wx RN)) 
              (setq wxn (* wx wx RN)) 
              (setq ax myColumnAVGs[m]) 
              (+= ax wn) 
              (setq myColumnAVGs[m] ax)
              (setq ax myColumnSTDs[m]) 
              (+= ax wxn) 
              (setq myColumnSTDs[m] ax)
              ) ; end M loop 
           ) ; end N loop
        (loop for m from 0 until M do 
           (setq wx myColumnSTDs[m]) 
           (setq ax myColumnAVGs[m]) 
           (-= wx (* ax ax)) 
           (setq wx (* (+ (sqrt (abs wx)) .000000000001) 1.000000000001))
           (setq myColumnSTDs[m] wx)
           ) ; end M loop 
        (vmregRunInHardware stop:)
		;; Perform a multiple linear regression for training.
        (setq myW (multipleRegression X Y))
		;; Return a factor model regression Lambda.
        (setq result (frmLambda))
        result) ; end train
    ;; Reset the frm machines column highs, lows, and ranges on the specified training examples.
    ;; Note: This happens when the FRM has been trained on one data set and is to be tested on another data set.
    ;;       The FRM is a ranked learning machine, so rescaling may be necessary for each test data set.
    (defun rescale(X)
        regs:(k K m M n N)
        regs:(Number:RM)
        regs:(Number:wx Number:ax Number:ey Number:y Number:RN)
        vars:(result NumVector:x NumVector:GYcount)
	    ;; Initialize the untrained FRM model.
	    (if (or (<> (isVector X) true) (<> (isVector X[0]) true)) (error "frmRegress.rescale: X argument must be a Vector Array of rank 2"))
	    (setq M (length X[0]))
	    (setq N (length X))
	    (if (< M myM) (error "frmRegress.rescale: number of columns must match training data set."))
		;; Initialize the persistent variables before proceeding with training.
        (setq myColumnAVGs (new Vector: Number: myM)) 
        (setq myColumnSTDs (new Vector: Number: myM)) 
		;; Compute the column highs, lows, and ranges.
        (vmregRunInHardware start:)
        (setq RN (/ 1.0 N))
        (loop for m from 0 until M do (setq myColumnAVGs[m] 0.0) (setq myColumnSTDs[m] 0.0))
        (loop for n from 0 until N do
           (setq x X[n])
           (loop for m from 0 until M do (setq wx x[m]) (+= myColumnAVGs[m] (* wx RN)))
           ) ; end N loop
        (loop for n from 0 until N do
           (setq x X[n])
           (loop for m from 0 until M do (setq wx x[m]) (setq ax myColumnAVGs[m]) (+= myColumnAVGs[m] (* (- wx ax) (- wx ax) RN)))
           ) ; end N loop
        (loop for m from 0 until M do (setq myColumnSTDs[m] (* (+ (sqrt myColumnSTDs[m]) .000000000001) 1.000000000001)))
        (vmregRunInHardware stop:)
        true) ; end rescale
    ;; ****************************************
    ;; Define Private Maintenance Child Lambdas.
    ;; ****************************************
    ;; The self test method for this Lambda.
    (defun selfTest(Test Ns Ms Es)
       vars:(k m n g G y ey C c X Y Yv avgY avgTopEy topEyCnt
             Lambda err Net pct sortedY 
             eyGrid eyGridCount eyGridSize errScore 
             startTime endTime startTimeT endTimeT
             (checkResults true)
             (tol 0.0) (errStop 0.01) (Cs 1.0)
             ) ; end temporary variables
       (clear)
       (setq startTimeT (getTickCount 0))
       (setq srandom.seed 8192.0)
       (setq eyGridSize Es)
       ;; Select the requested test case
       ;; Test Case srandom 
       (if (or (= Test all:) (= Test linear:))
           (begin
		       ;; Create a test polynomial regression where y = C[0]*X[0] + C[1]*X[1] + C[2]*X[2] + C[3]*X[3] + C[4]*X[4] ...
		       (setq M Ms)
		       (setq N Ns)
		       (setq eyGrid (new Vector: Number: eyGridSize))
		       (setq eyGridCount (new Vector: Number: eyGridSize))
		       (setq X (new Vector: object: N))
		       (setq Y (new Vector: Number: N))
		       (setq C (new Vector: Number: M))
		       (loop for m from 0 until M do
		          (setq C[m] (- (srandom 10.0) 5.0))
		          ) ; end C loop
		       (loop for n from 0 until N do
		          (setq X[n] (new Vector: Number: M))
		          (setq y 0.0)
		          (loop for m from 0 until M do 
		          	 (setq X[n][m] (- (srandom 100.0) 50.0))
		             (setq y (+ y (* X[n][m] C[m])))
		             ) ; end M loop
		          (setq Y[n] (+ (* y .8) (* y (srandom .4))))
		          ) ; end N loop
		       ;; Train on the training data.
		       (writeln _eol "Starting test case: linear")
		       (setq Lambda (setq Lambda (esm.frmRegress.train X Y)))
		       (writeln "frmRegress: N = [" Ns "], M = [" Ms "]")
		       ;; Score on the test case.
		       (loop for n from 0 until N do
		          (setq X[n] (new Vector: Number: M))
		          (setq y 0.0)
		          (loop for m from 0 until M do 
		          	 (setq X[n][m] (- (srandom 100.0) 50.0))
		             (setq y (+ y (* X[n][m] C[m])))
		             ) ; end M loop
		          (setq Y[n] (+ (* y .8) (* y (srandom .4))))
		          ) ; end N loop
               (Lambda.rescale X)
               (setq err 0.0)
               (setq errScore 0.0)
               (setq avgTopEy 0.0)
               (setq topEyCnt 0)
               (setq avgY (avg Y))
               (setq G (/ N eyGridSize))
               (setq sortedY (|Gv:sort| Y < true)) 
               (loop for n from 0 until N do
                  (setq k sortedY[n])                  
                  (setq ey (Lambda X[k]))
                  (setq y Y[k])
                  (setq g (divi n G))
                  (setq eyGrid[g] (+ eyGrid[g] ey))
                  (setq eyGridCount[g] (+ eyGridCount[g] 1.0))
                  (setq pct (/ (abs (- ey y)) (+ (abs ey) .000000001)))
                  (+= err pct)
                  (if (= (modi n (divi N 10)) 0) (writeln "[" n "] ey=[" ey "] y=[" y "] err=[" (- ey y) "] err%=[" pct "]"))
                  ) ; end N loop
               (/= err N)
               (loop for g from 0 until eyGridSize do (setq eyGrid[g] (/ eyGrid[g] eyGridCount[g]))) 
               (loop for g from 1 until eyGridSize do (if (<= eyGrid[g] eyGrid[(- g 1)]) (++ errScore)))
               (setq errScore (/ (- eyGridSize errScore 1) (- eyGridSize 1)))
               (writeln "frmRegress: err=[" err "], avgY=[" avgY "], errScore=[" errScore "], Cols=[" myM "], eyGrid=[" (string eyGrid true) "]")
          )) ; end Test Case linear
       ;; Test Case mixedRandom 
       (if (or (= Test all:) (= Test mixed:))
           (begin
		       ;; Create a test polynomial linear model where y = C[0]*X[0] + C[1]*X[1] + C[2]*X[2] ...
		       ;; Create a test polynomial square model where y = C[0]*X[0]*X[0] + C[1]*X[1]*X[1] + C[2]*X[2]*X[2] ...
		       ;; Create a test polynomial sin model where y = C[0]*sin(X[0]) + C[1]*sin(X[1]) + C[2]*sin(X[2]) ...
		       ;; Create a test polynomial log model where y = C[0]*log(abs(X[0])+.000001) + C[1]*log(abs(X[1])+.000001) + C[1]*log(abs(X[2])+.000001) ...
               ;; These four models are mixed together and random noise is added.
		       (setq M Ms)
		       (setq N Ns)
		       (setq eyGrid (new Vector: Number: eyGridSize))
		       (setq eyGridCount (new Vector: Number: eyGridSize))
		       (setq X (new Vector: object: N))
		       (setq Y (new Vector: Number: N))
		       (setq C (new Vector: Number: M))
		       (loop for m from 0 until M do
		          (setq C[m] (- (srandom 10.0) 5.0))
		          ) ; end C loop
		       (loop for n from 0 until N do
		          (setq X[n] (new Vector: Number: M))
		          (setq y 0.0)
                  (setq k (modi n 4)) 
		          (loop for m from 0 until M do 
		          	 (setq X[n][m] (- (srandom 100.0) 50.0))
		             (setq y (+ y (* X[n][m] C[m])))
                     (cond
                        ;; Linear model
                        ((= k 0) (setq y (+ y (* X[n][m] C[m]))))
                        ;; Square model
                        ((= k 1) (setq y (+ y (* X[n][m] X[n][m] C[m]))))
                        ;; Sine model
                        ((= k 2) (setq y (+ y (* (|Gv:sin| X[n][m]) C[m]))))
                        ;; Log model
                        (else (setq y (+ y (* (|Gv:log| (+ .000001 (|Gv:abs| X[n][m]))) C[m]))))
                        ) ; end cond
		             ) ; end M loop
		          (setq Y[n] (+ (* y .8) (* y (srandom .4))))
		          ) ; end N loop
		       ;; Train on the training data.
		       (writeln _eol "Starting test case: mixed")
		       (setq Lambda (setq Lambda (esm.frmRegress.train X Y)))
		       (writeln "frmRegress: N = [" Ns "], M = [" Ms "]")
		       ;; Score on the test case.
		       (loop for n from 0 until N do
		          (setq X[n] (new Vector: Number: M))
		          (setq y 0.0)
                  (setq k (modi n 4)) 
		          (loop for m from 0 until M do 
		          	 (setq X[n][m] (- (srandom 100.0) 50.0))
                     (cond
                        ;; Linear model
                        ((= k 0) (setq y (+ y (* X[n][m] C[m]))))
                        ;; Square model
                        ((= k 1) (setq y (+ y (* X[n][m] X[n][m] C[m]))))
                        ;; Sine model
                        ((= k 2) (setq y (+ y (* (|Gv:sin| X[n][m]) C[m]))))
                        ;; Log model
                        (else (setq y (+ y (* (|Gv:log| (+ .000001 (|Gv:abs| X[n][m]))) C[m]))))
                        ) ; end cond
		             ) ; end M loop
		          (setq Y[n] (+ (* y .8) (* y (srandom .4))))
		          ) ; end N loop
               (Lambda.rescale X)
               (setq err 0.0)
               (setq errScore 0.0)
               (setq avgTopEy 0.0)
               (setq topEyCnt 0)
               (setq avgY (avg Y))
               (setq G (/ N eyGridSize))
               (setq sortedY (|Gv:sort| Y < true)) 
               (loop for n from 0 until N do
                  (setq k sortedY[n])                  
                  (setq ey (Lambda X[k]))
                  (setq y Y[k])
                  (setq g (divi n G))
                  (setq eyGrid[g] (+ eyGrid[g] ey))
                  (setq eyGridCount[g] (+ eyGridCount[g] 1.0))
                  (setq pct (/ (abs (- ey y)) (+ (abs ey) .000000001)))
                  (+= err pct)
                  (if (= (modi n (divi N 10)) 0) (writeln "[" n "] ey=[" ey "] y=[" y "] err=[" (- ey y) "] err%=[" pct "]"))
                  ) ; end N loop
               (/= err N)
               (loop for g from 0 until eyGridSize do (setq eyGrid[g] (/ eyGrid[g] eyGridCount[g]))) 
               (loop for g from 1 until eyGridSize do (if (<= eyGrid[g] eyGrid[(- g 1)]) (++ errScore)))
               (setq errScore (/ (- eyGridSize errScore 1) (- eyGridSize 1)))
               (writeln "frmRegress: err=[" err "], avgY=[" avgY "], errScore=[" errScore "], Cols=[" myM "], eyGrid=[" (string eyGrid true) "]")
          )) ; end Test Case mixed
       ;; Test Case sigmoid 
       (if (or (= Test all:) (= Test sigmoid:))
           (begin
		       ;; Create a test polynomial regression where y = C[0]*X[0] + C[1]*X[1] + C[2]*X[2] + C[3]*X[3] + C[4]*X[4] ...
		       ;; Note1: The inputs, X, are restricted to the sigmoid domain.
		       (setq M Ms)
		       (setq N Ns)
		       (setq eyGrid (new Vector: Number: eyGridSize))
		       (setq eyGridCount (new Vector: Number: eyGridSize))
		       (setq X (new Vector: object: N))
		       (setq Y (new Vector: Number: N))
		       (setq C (new Vector: Number: M))
		       (loop for m from 0 until M do
		          (setq C[m] (- (srandom 1.0) .50))
		          ) ; end C loop
		       (loop for n from 0 until N do
		          (setq X[n] (new Vector: Number: M))
		          (setq y 0.0)
		          (loop for m from 0 until M do 
		          	 (setq X[n][m] (srandom 1.0))
		             (setq y (+ y (* X[n][m] C[m])))
		             ) ; end M loop
		          (setq Y[n] (+ (* y .8) (* y (srandom .4))))
		          ) ; end N loop
		       ;; Train on the training data.
		       (writeln _eol "Starting test case: sigmoid")
		       (setq Lambda (setq Lambda (esm.frmRegress.train X Y)))
		       (writeln "frmRegress: N = [" Ns "], M = [" Ms "]")
		       ;; Score on the test case.
		       (loop for n from 0 until N do
		          (setq X[n] (new Vector: Number: M))
		          (setq y 0.0)
		          (loop for m from 0 until M do 
		          	 (setq X[n][m] (srandom 1.0))
		             (setq y (+ y (* X[n][m] C[m])))
		             ) ; end M loop
		          (setq Y[n] (+ (* y .8) (* y (srandom .4))))
		          ) ; end N loop
               (Lambda.rescale X)
               (setq err 0.0)
               (setq errScore 0.0)
               (setq avgTopEy 0.0)
               (setq topEyCnt 0)
               (setq avgY (avg Y))
               (setq G (/ N eyGridSize))
               (setq sortedY (|Gv:sort| Y < true)) 
               (loop for n from 0 until N do
                  (setq k sortedY[n])                  
                  (setq ey (Lambda X[k]))
                  (setq y Y[k])
                  (setq g (divi n G))
                  (setq eyGrid[g] (+ eyGrid[g] ey))
                  (setq eyGridCount[g] (+ eyGridCount[g] 1.0))
                  (setq pct (/ (abs (- ey y)) (+ (abs ey) .000000001)))
                  (+= err pct)
                  (if (= (modi n (divi N 10)) 0) (writeln "[" n "] ey=[" ey "] y=[" y "] err=[" (- ey y) "] err%=[" pct "]"))
                  ) ; end N loop
               (/= err N)
               (loop for g from 0 until eyGridSize do (setq eyGrid[g] (/ eyGrid[g] eyGridCount[g]))) 
               (loop for g from 1 until eyGridSize do (if (<= eyGrid[g] eyGrid[(- g 1)]) (++ errScore)))
               (setq errScore (/ (- eyGridSize errScore 1) (- eyGridSize 1)))
               (writeln "frmRegress: err=[" err "], avgY=[" avgY "], errScore=[" errScore "], Cols=[" myM "], eyGrid=[" (string eyGrid true) "]")
          )) ; end Test Case sigmoid
       ;; Test Case square 
       (if (or (= Test all:) (= Test square:))
           (begin
		       ;; Create a test polynomial regression where y = C[0]*X[0] + C[1]*(X[1]**2) + C[2]*X[2] + C[3]*(X[3]**2) ...
		       (setq M Ms)
		       (setq N Ns)
		       (setq eyGrid (new Vector: Number: eyGridSize))
		       (setq eyGridCount (new Vector: Number: eyGridSize))
		       (setq X (new Vector: object: N))
		       (setq Y (new Vector: Number: N))
		       (setq C (new Vector: Number: M))
		       (loop for m from 0 until M do
		          (setq C[m] (- (srandom 10.0) 5.0))
		          ) ; end C loop
		       (loop for n from 0 until N do
		          (setq X[n] (new Vector: Number: M))
		          (setq y 0.0)
		          (loop for m from 0 until M do 
		          	 (setq X[n][m] (- (srandom 100.0) 50.0))
                     (setq y (+ y (* X[n][m] X[n][m] C[m])))
		             ) ; end M loop
		          (setq Y[n] (+ (* y .8) (* y (srandom .4))))
		          ) ; end N loop
		       ;; Train on the training data.
		       (writeln _eol "Starting test case: square")
		       (setq Lambda (setq Lambda (esm.frmRegress.train X Y)))
		       (writeln "frmRegress: N = [" Ns "], M = [" Ms "]")
		       ;; Score on the test case.
		       (loop for n from 0 until N do
		          (setq X[n] (new Vector: Number: M))
		          (setq y 0.0)
                  (setq k (modi n 4)) 
		          (loop for m from 0 until M do 
		          	 (setq X[n][m] (- (srandom 100.0) 50.0))
                     (setq y (+ y (* X[n][m] X[n][m] C[m])))
		             ) ; end M loop
		          (setq Y[n] (+ (* y .8) (* y (srandom .4))))
		          ) ; end N loop
               (Lambda.rescale X)
               (setq err 0.0)
               (setq errScore 0.0)
               (setq avgTopEy 0.0)
               (setq topEyCnt 0)
               (setq avgY (avg Y))
               (setq G (/ N eyGridSize))
               (setq sortedY (|Gv:sort| Y < true)) 
               (loop for n from 0 until N do
                  (setq k sortedY[n])                  
                  (setq ey (Lambda X[k]))
                  (setq y Y[k])
                  (setq g (divi n G))
                  (setq eyGrid[g] (+ eyGrid[g] ey))
                  (setq eyGridCount[g] (+ eyGridCount[g] 1.0))
                  (setq pct (/ (abs (- ey y)) (+ (abs ey) .000000001)))
                  (+= err pct)
                  (if (= (modi n (divi N 10)) 0) (writeln "[" n "] ey=[" ey "] y=[" y "] err=[" (- ey y) "] err%=[" pct "]"))
                  ) ; end N loop
               (/= err N)
               (loop for g from 0 until eyGridSize do (setq eyGrid[g] (/ eyGrid[g] eyGridCount[g]))) 
               (loop for g from 1 until eyGridSize do (if (<= eyGrid[g] eyGrid[(- g 1)]) (++ errScore)))
               (setq errScore (/ (- eyGridSize errScore 1) (- eyGridSize 1)))
               (writeln "frmRegress: err=[" err "], avgY=[" avgY "], errScore=[" errScore "], Cols=[" myM "], eyGrid=[" (string eyGrid true) "]")
          )) ; end Test Case square
       ;; Test Case tan 
       (if (or (= Test all:) (= Test tan:))
           (begin
		       ;; Create a test polynomial regression where y = C[0]*tan(X[0]) + C[1]*tan(X[1]) + C[2]*tan(X[2]) ...
		       (setq M Ms)
		       (setq N Ns)
		       (setq eyGrid (new Vector: Number: eyGridSize))
		       (setq eyGridCount (new Vector: Number: eyGridSize))
		       (setq X (new Vector: object: N))
		       (setq Y (new Vector: Number: N))
		       (setq C (new Vector: Number: M))
		       (loop for m from 0 until M do
		          (setq C[m] (- (srandom 10.0) 5.0))
		          ) ; end C loop
		       (loop for n from 0 until N do
		          (setq X[n] (new Vector: Number: M))
		          (setq y 0.0)
		          (loop for m from 0 until M do 
		          	 (setq X[n][m] (- (srandom 100.0) 50.0))
                     (setq y (+ y (* (tan X[n][m]) C[m])))
		             ) ; end M loop
		          (setq Y[n] (+ (* y .8) (* y (srandom .4))))
		          ) ; end N loop
		       ;; Train on the training data.
		       (writeln _eol "Starting test case: tan")
		       (setq Lambda (setq Lambda (esm.frmRegress.train X Y)))
		       (writeln "frmRegress: N = [" Ns "], M = [" Ms "]")
		       ;; Score on the test case.
		       (loop for n from 0 until N do
		          (setq X[n] (new Vector: Number: M))
		          (setq y 0.0)
		          (loop for m from 0 until M do 
		          	 (setq X[n][m] (- (srandom 100.0) 50.0))
                     (setq y (+ y (* (tan X[n][m]) C[m])))
		             ) ; end M loop
		          (setq Y[n] (+ (* y .8) (* y (srandom .4))))
		          ) ; end N loop
               (Lambda.rescale X)
               (setq err 0.0)
               (setq errScore 0.0)
               (setq avgTopEy 0.0)
               (setq topEyCnt 0)
               (setq avgY (avg Y))
               (setq G (/ N eyGridSize))
               (setq sortedY (|Gv:sort| Y < true)) 
               (loop for n from 0 until N do
                  (setq k sortedY[n])                  
                  (setq ey (Lambda X[k]))
                  (setq y Y[k])
                  (setq g (divi n G))
                  (setq eyGrid[g] (+ eyGrid[g] ey))
                  (setq eyGridCount[g] (+ eyGridCount[g] 1.0))
                  (setq pct (/ (abs (- ey y)) (+ (abs ey) .000000001)))
                  (+= err pct)
                  (if (= (modi n (divi N 10)) 0) (writeln "[" n "] ey=[" ey "] y=[" y "] err=[" (- ey y) "] err%=[" pct "]"))
                  ) ; end N loop
               (/= err N)
               (loop for g from 0 until eyGridSize do (setq eyGrid[g] (/ eyGrid[g] eyGridCount[g]))) 
               (loop for g from 1 until eyGridSize do (if (<= eyGrid[g] eyGrid[(- g 1)]) (++ errScore)))
               (setq errScore (/ (- eyGridSize errScore 1) (- eyGridSize 1)))
               (writeln "frmRegress: err=[" err "], avgY=[" avgY "], errScore=[" errScore "], Cols=[" myM "], eyGrid=[" (string eyGrid true) "]")
          )) ; end Test Case tan
       (writeln "frmRegress.selfTest: completed in [" (/ (setq endTimeT (getTickCount startTimeT)) 60.0) "] minutes.")       
       Lambda) ; end selfTest
    ;; *****************
    ;; Begin main logic.
    ;; ***************** 
    vars:(Lambda frmLambda)

    ;; Train a new factor model machine Lambda.
    (setq frmLambda (new (myself)))
    (setq frmLambda.myFRMParent frmLambda)
    (setq Lambda (frmLambda.train X Y))
    Lambda) ; end frmRegress


















































;;**EXPORTKEY**:esm:linearRegress
(defriend esm:linearRegress(NumVector:X NumVector:Y)
;; *******************************************************************
;; name:     linearRegress
;; 
;; summary:  Returns a vector containing the coefficients resulting
;;           from a linear regression on two variables. If x and y 
;;           are two variables, then (regression w) is: #(a  b  error).
;;           where a + bx = y represents the least squares best fit.
;;           The term, error, is the least squares error = sqr(y - (a + bx)).
;;
;; Parms:    X:       The N Number Vector representing the original observations.
;;           Y        The N Number Vector representing the dependent variables.
;; Return:   C:       The coefficient vector #(num| a b error)
;; *******************************************************************
	regs:(M n N Number:InvN)
	regs:(Number:numerator Number:denominator Number:a Number:b Number:err)
	regs:(Number:xmean Number:xsum Number:x)
	regs:(Number:ymean Number:ysum Number:y)
	regs:(Number:xxdot Number:yydot Number:xydot)
	regs:(NumPointer:pX NumPointer:pY)
	vars:(NumVector:C NumVector:X NumVector:Y)
	(setq C (|Gv:new| Vector: Number: 3))
    (setq N (length X))
    (setq InvN (/ 1.0 (number N)))
    (setq pX X)
    (setq pY Y)
    (loop for n from 0 until N do
       (setq x pX[n])
       (setq y pY[n])
       (+= xsum x)
       (+= ysum y)
       (+= xxdot (* x x))
       (+= xydot (* x y))
       (+= yydot (* y y))
       ) ; end main loop
    (setq xmean (/ xsum N)) 
    (setq ymean (/ ysum N)) 
   	(setq numerator (- xydot (* ysum xmean)))
    (setq denominator (- xxdot (/ (* xsum xsum) N)))
   	(if (= denominator 0.0) (setq b 0.0) (setq b (/ numerator denominator)))
   	(setq a (- ymean (* b xmean)))
    (setq err (+ yydot (* -2.0 b xydot) (* -2.0 a ysum) (* b b xxdot) (* 2.0 a b xsum) (* a a N) ))
   	(setq C[0] a)
   	(setq C[1] b)
    (setq C[2] (* InvN err))
	C) ; linearRegress































;;**EXPORTKEY**:esm:mvlRegress
(deforphan esm:mvlRegress(X Y)
;; *******************************************************************
;; name:     mvlRegress
;; 
;; summary:  Trains a multiple linear regression Lambda, using a the 
;;           gaussian algorithm and returns the trained regression Lambda.
;;
;;           Multivariable regression machines are regression machines which learn
;;           and make regression estimates on XY vector arrays such as:
;;
;;               XY:  An N by M+1 array representing the original observations
;;                    in the form of:    x x x ...  y
;;                                       x x x ...  y
;;                                           ... 
;;                                       x x x ...  y
;;
;; Parms:    X:         The N by M vector array representing the original observations
;;                      in the form of:    x x ... x
;;                                         x x ... x
;;                                             ... 
;;                                         x x ... x
;;           Y   		The N vector of dependent variables.
;;
;; Return:   Rf:     A new learning machine Lambda ready for regression.
;; *******************************************************************
    pvars:(;; Public variables
           Number:Error                 ;; The current absolute error for the MVL regression model (in percent of target). 
           Integer:myM					;; The number of elements in each training example (independent variables). 
           Integer:myN					;; The number of training examples. 
           myMVLParent                  ;; The parent Lambda of this MVL regression Lambda community. 
           myVerboseSW      			;; True iff we are to display progress on the console. 
           NumVector:myW                ;; The weight coefficient vector for the MVL regression model.
           myX                          ;; The vector of training examples (independent variables).
           NumVector:myY                ;; The vector of training example scores (dependent variable).
           ;; Public child methods
           clear			       		;; Clear the current percentile grid machine.
           mvlLambda			       		;; Return an Lambda ready to compute the mvl output for specified input vector.
           mvlTraining		       		;; Train the mvl machine on the specified training examples.
	       multipleRegression           ;; Performs a Gaussian multiple regression on the X Y training data
           ;; Private maintenance child methods
           selfTest                		;; The self test method for this Lambda. 
           ) ;; end of persistent variables
    ;; ***************************
    ;; Define Public Child Lambdas.
    ;; ***************************
    ;; Clear the current percentile grid machine.
    (defun clear()
       (setq Error 0.0)
       (setq myColumnHighs #void) 
       (setq myColumnLows #void) 
       (setq myColumnRanges #void) 
       (setq myGY #void)
       (setq myM 0)
       (setq myN 0)
       (setq myW #void)
       (setq myX #void)
       (setq myY #void)
       true) ; end clear
    ; Return an Lambda ready to compute the mvl output for specified input points.
    (defun mvlLambda()
       regs:(n m NW)
       vars:(NumVector:hW Vector:hKX ObjVector:hWX)
       vars:(Lambda HRecord)
       (setq Lambda (eval 
        {(lambda(NumVector:X) 
         pvars:(Strategy Number:Error Integer:myM NumVector:myW) 
         regs:(k m M Number:x Number:wn Number:dy Number:ey NumPointer:pW NumPointer:pX) 
         (setq M myM) 
         (setq pW myW) 
         (setq pX X) 
         (loop for m from 0 until M do 
            (setq x pX[m]) 
            (setq wn pW[m]) 
            (*= x wn) 
            (+= ey x)  
            )
          ey)
        }))
       ;; Move all important knowledge structures into the trained regression Lambda.
       (setq Lambda.Strategy MVL:)
       (setq Lambda.Error 1.0e+300)
       (setq Lambda.myM myM)
       (setq Lambda.myW myW)
       Lambda) ; end mvlLambda
   ;; summary:  Performs a Gaussian multiple regression on the X Y training data
   ;; Return:   C:     The weight coefficient vector for the regression model.
   ;; Note1:    Average error statistics are computed as a percent of the target (dependent variable).
   ;; Note2:    See Sedgewick[2] chap 37.
   (defun multipleRegression(ObjVector:X NumVector:Y)
       regs:(m M n N MY NumPointer:pmxy Number:wx Number:y)
       vars:(NumMatrix:Xt NumVector:x NumVector:C NumMatrix:MXY)
       ;; Convert X Y vector input to a Matrix format.
       (setq N (length X))
       (setq M (length X[0]))
       (setq MY (+ M 1))
       (if (<> N (length Y)) (error "mvlRegress.multipleRegression: X and Y vectors must be same length"))
       (setq MXY (new Matrix: number: 2 N MY))
       (vmregRunInHardware start:)
       (setq pmxy MXY)
       (loop for n from 0 until N do
          (setq x X[n])
          (loop for m from 0 until M do
            (setq wx x[m])
            (setq pmxy[0] wx)
            (++ pmxy)
            ) ; end n loop
          (setq y Y[n])
          (setq pmxy[0] y)
          (++ pmxy)
          ) ; end m loop
       (vmregRunInHardware stop:)
       ;; Perform a least squares regression on all the factors.
       (setq Xt (|Gv:makeGaussianMatrix| MXY))
       (setq Xt (|Gv:matrixGaussianEliminate| Xt true))
       (setq myW (|Gv:matrixGaussianSubstitute| Xt))
       ;; Return the weight coefficient vector for the regression model.
       myW) ; end multipleRegression
    ;; Train the mvl machine on the specified inputs and model.
	;; Parms:    x:         The N by M vector array representing the original observations
	;;                      in the form of:    x x ... x
	;;                                         x x ... x
	;;                                             ... 
	;;                                         x x ... x
	;;           y   		The N vector of dependent variables.
    ;;
    ;; Return:   Rf:        A new percentile grid machine Lambda ready for regression.
    (defun mvlTraining(X Y)
        regs:(k K m M n N)
        regs:(Number:RGrid Number:RM Number:w Number:tw)
        regs:(Number:wx Number:ey Number:y)
        vars:(result NumVector:GYcount)
	    ;; Clear percentile grid machine for retraining.
	    (clear)
	    ;; Initialize the untrained MVL model.
	    (if (or (<> (isVector X) true) (<> (isVector X[0]) true)) (error "mvlRegress: X argument must be a Vector Array of rank 2"))
	    (setq myM (length X[0]))
	    (setq myN (length X))
	    (if (or (<> (isVector Y) true) (<> (length Y) myN)) (error "mvlRegress: Y argument must be a Vector of length the same as X"))
	    (setq myX X)
	    (setq myY Y)
		;; Initialize the persistent variables before proceeding with training.
		(setq Error 1.0e300)
        (setq myW (new Vector: Number: myM))
		;; Compute the MVL model column weights.
        (setq myW (multipleRegression X Y))
		;; Return a multivariable regression Lambda.
	    (setq myX #void)
	    (setq myY #void)
        (setq result (mvlLambda))
        result) ; end mvlTraining
    ;; ****************************************
    ;; Define Private Maintenance Child Lambdas.
    ;; ****************************************
    ;; The self test method for this Lambda.
    (defun selfTest(Test Ns Ms Gs Es)
       vars:(k m n g G y ey C c X Y Yv avgY avgTopEy topEyCnt
             Lambda err Net pct sortedY 
             eyGrid eyGridCount eyGridSize errScore 
             startTime endTime startTimeT endTimeT
             (checkResults true)
             (tol 0.0) (errStop 0.01) (Cs 1.0)
             ) ; end temporary variables
       (clear)
       (setq startTimeT (getTickCount 0))
       (setq srandom.seed 8192.0)
       (setq eyGridSize Es)
       ;; Select the requested test case
       ;; Test Case srandom 
       (if (or (= Test all:) (= Test linear:))
           (begin
		       ;; Create a test polynomial regression where y = C[0]*X[0] + C[1]*X[1] + C[2]*X[2] + C[3]*X[3] + C[4]*X[4] ...
		       (setq M Ms)
		       (setq N Ns)
		       (setq eyGrid (new Vector: Number: eyGridSize))
		       (setq eyGridCount (new Vector: Number: eyGridSize))
		       (setq X (new Vector: object: N))
		       (setq Y (new Vector: Number: N))
		       (setq C (new Vector: Number: M))
		       (loop for m from 0 until M do
		          (setq C[m] (- (srandom 10.0) 5.0))
		          ) ; end C loop
		       (loop for n from 0 until N do
		          (setq X[n] (new Vector: Number: M))
		          (setq y 0.0)
		          (loop for m from 0 until M do 
		          	 (setq X[n][m] (- (srandom 100.0) 50.0))
		             (setq y (+ y (* X[n][m] C[m])))
		             ) ; end M loop
		          (setq Y[n] (+ (* y .8) (* y (srandom .4))))
		          ) ; end N loop
		       ;; Train on the training data.
		       (writeln _eol "Starting test case: linear")
		       (setq Lambda (setq Lambda (esm.mvlRegress.mvlTraining X Y)))
		       (writeln "mvlRegress: N = [" Ns "], M = [" Ms "]")
		       ;; Score on the test case.
		       (loop for n from 0 until N do
		          (setq X[n] (new Vector: Number: M))
		          (setq y 0.0)
		          (loop for m from 0 until M do 
		          	 (setq X[n][m] (- (srandom 100.0) 50.0))
		             (setq y (+ y (* X[n][m] C[m])))
		             ) ; end M loop
		          (setq Y[n] (+ (* y .8) (* y (srandom .4))))
		          ) ; end N loop
               (setq err 0.0)
               (setq errScore 0.0)
               (setq avgTopEy 0.0)
               (setq topEyCnt 0)
               (setq avgY (avg Y))
               (setq G (/ N eyGridSize))
               (setq sortedY (|Gv:sort| Y < true)) 
               (loop for n from 0 until N do
                  (setq k sortedY[n])                  
                  (setq ey (Lambda X[k]))
                  (setq y Y[k])
                  (setq g (divi n G))
                  (setq eyGrid[g] (+ eyGrid[g] ey))
                  (setq eyGridCount[g] (+ eyGridCount[g] 1.0))
                  (setq pct (/ (abs (- ey y)) (+ (abs ey) .000000001)))
                  (+= err pct)
                  (if (= (modi n (divi N 10)) 0) (writeln "[" n "] ey=[" ey "] y=[" y "] err=[" (- ey y) "] err%=[" pct "]"))
                  ) ; end N loop
               (/= err N)
               (loop for g from 0 until eyGridSize do (setq eyGrid[g] (/ eyGrid[g] eyGridCount[g]))) 
               (loop for g from 1 until eyGridSize do (if (<= eyGrid[g] eyGrid[(- g 1)]) (++ errScore)))
               (setq errScore (/ (- eyGridSize errScore 1) (- eyGridSize 1)))
               (writeln "mvlRegress: err=[" err "], avgY=[" avgY "], errScore=[" errScore "], eyGrid=[" (string eyGrid true) "]")
          )) ; end Test Case linear
       ;; Test Case mixedRandom 
       (if (or (= Test all:) (= Test mixed:))
           (begin
		       ;; Create a test polynomial linear model where y = C[0]*X[0] + C[1]*X[1] + C[2]*X[2] ...
		       ;; Create a test polynomial square model where y = C[0]*X[0]*X[0] + C[1]*X[1]*X[1] + C[2]*X[2]*X[2] ...
		       ;; Create a test polynomial sin model where y = C[0]*sin(X[0]) + C[1]*sin(X[1]) + C[2]*sin(X[2]) ...
		       ;; Create a test polynomial log model where y = C[0]*log(abs(X[0])+.000001) + C[1]*log(abs(X[1])+.000001) + C[1]*log(abs(X[2])+.000001) ...
               ;; These four models are mixed together and random noise is added.
		       (setq M Ms)
		       (setq N Ns)
		       (setq eyGrid (new Vector: Number: eyGridSize))
		       (setq eyGridCount (new Vector: Number: eyGridSize))
		       (setq X (new Vector: object: N))
		       (setq Y (new Vector: Number: N))
		       (setq C (new Vector: Number: M))
		       (loop for m from 0 until M do
		          (setq C[m] (- (srandom 10.0) 5.0))
		          ) ; end C loop
		       (loop for n from 0 until N do
		          (setq X[n] (new Vector: Number: M))
		          (setq y 0.0)
                  (setq k (modi n 4)) 
 		          (setq X[n][1] (number k))
		          (loop for m from 1 until M do 
		          	 (setq X[n][m] (- (srandom 100.0) 50.0))
		             (setq y (+ y (* X[n][m] C[m])))
                     (cond
                        ;; Linear model
                        ((= k 0) (setq y (+ y (* X[n][m] C[m]))))
                        ;; Square model
                        ((= k 1) (setq y (+ y (* X[n][m] X[n][m] C[m]))))
                        ;; Sine model
                        ((= k 2) (setq y (+ y (* (|Gv:sin| X[n][m]) C[m]))))
                        ;; Log model
                        (else (setq y (+ y (* (|Gv:log| (+ .000001 (|Gv:abs| X[n][m]))) C[m]))))
                        ) ; end cond
		             ) ; end M loop
		          (setq Y[n] (+ (* y .8) (* y (srandom .4))))
		          ) ; end N loop
		       ;; Train on the training data.
		       (writeln _eol "Starting test case: mixed")
		       (setq Lambda (setq Lambda (esm.mvlRegress.mvlTraining X Y)))
		       (writeln "mvlRegress: N = [" Ns "], M = [" Ms "]")
		       ;; Score on the test case.
		       (loop for n from 0 until N do
		          (setq X[n] (new Vector: Number: M))
		          (setq y 0.0)
                  (setq k (modi n 4)) 
 		          (setq X[n][1] (number k))
		          (loop for m from 1 until M do 
		          	 (setq X[n][m] (- (srandom 100.0) 50.0))
                     (cond
                        ;; Linear model
                        ((= k 0) (setq y (+ y (* X[n][m] C[m]))))
                        ;; Square model
                        ((= k 1) (setq y (+ y (* X[n][m] X[n][m] C[m]))))
                        ;; Sine model
                        ((= k 2) (setq y (+ y (* (|Gv:sin| X[n][m]) C[m]))))
                        ;; Log model
                        (else (setq y (+ y (* (|Gv:log| (+ .000001 (|Gv:abs| X[n][m]))) C[m]))))
                        ) ; end cond
		             ) ; end M loop
		          (setq Y[n] (+ (* y .8) (* y (srandom .4))))
		          ) ; end N loop
               (setq err 0.0)
               (setq errScore 0.0)
               (setq avgTopEy 0.0)
               (setq topEyCnt 0)
               (setq avgY (avg Y))
               (setq G (/ N eyGridSize))
               (setq sortedY (|Gv:sort| Y < true)) 
               (loop for n from 0 until N do
                  (setq k sortedY[n])                  
                  (setq ey (Lambda X[k]))
                  (setq y Y[k])
                  (setq g (divi n G))
                  (setq eyGrid[g] (+ eyGrid[g] ey))
                  (setq eyGridCount[g] (+ eyGridCount[g] 1.0))
                  (setq pct (/ (abs (- ey y)) (+ (abs ey) .000000001)))
                  (+= err pct)
                  (if (= (modi n (divi N 10)) 0) (writeln "[" n "] ey=[" ey "] y=[" y "] err=[" (- ey y) "] err%=[" pct "]"))
                  ) ; end N loop
               (/= err N)
               (loop for g from 0 until eyGridSize do (setq eyGrid[g] (/ eyGrid[g] eyGridCount[g]))) 
               (loop for g from 1 until eyGridSize do (if (<= eyGrid[g] eyGrid[(- g 1)]) (++ errScore)))
               (setq errScore (/ (- eyGridSize errScore 1) (- eyGridSize 1)))
               (writeln "mvlRegress: err=[" err "], avgY=[" avgY "], errScore=[" errScore "], eyGrid=[" (string eyGrid true) "]")
          )) ; end Test Case mixed
       ;; Test Case sigmoid 
       (if (or (= Test all:) (= Test sigmoid:))
           (begin
		       ;; Create a test polynomial regression where y = C[0]*X[0] + C[1]*X[1] + C[2]*X[2] + C[3]*X[3] + C[4]*X[4] ...
		       ;; Note1: The inputs, X, are restricted to the sigmoid domain.
		       (setq M Ms)
		       (setq N Ns)
		       (setq eyGrid (new Vector: Number: eyGridSize))
		       (setq eyGridCount (new Vector: Number: eyGridSize))
		       (setq X (new Vector: object: N))
		       (setq Y (new Vector: Number: N))
		       (setq C (new Vector: Number: M))
		       (loop for m from 0 until M do
		          (setq C[m] (- (srandom 1.0) .50))
		          ) ; end C loop
		       (loop for n from 0 until N do
		          (setq X[n] (new Vector: Number: M))
		          (setq y 0.0)
		          (loop for m from 0 until M do 
		          	 (setq X[n][m] (srandom 1.0))
		             (setq y (+ y (* X[n][m] C[m])))
		             ) ; end M loop
		          (setq Y[n] (+ (* y .8) (* y (srandom .4))))
		          ) ; end N loop
		       ;; Train on the training data.
		       (writeln _eol "Starting test case: sigmoid")
		       (setq Lambda (setq Lambda (esm.mvlRegress.mvlTraining X Y)))
		       (writeln "mvlRegress: N = [" Ns "], M = [" Ms "]")
		       ;; Score on the test case.
		       (loop for n from 0 until N do
		          (setq X[n] (new Vector: Number: M))
		          (setq y 0.0)
		          (loop for m from 0 until M do 
		          	 (setq X[n][m] (srandom 1.0))
		             (setq y (+ y (* X[n][m] C[m])))
		             ) ; end M loop
		          (setq Y[n] (+ (* y .8) (* y (srandom .4))))
		          ) ; end N loop
               (setq err 0.0)
               (setq errScore 0.0)
               (setq avgTopEy 0.0)
               (setq topEyCnt 0)
               (setq avgY (avg Y))
               (setq G (/ N eyGridSize))
               (setq sortedY (|Gv:sort| Y < true)) 
               (loop for n from 0 until N do
                  (setq k sortedY[n])                  
                  (setq ey (Lambda X[k]))
                  (setq y Y[k])
                  (setq g (divi n G))
                  (setq eyGrid[g] (+ eyGrid[g] ey))
                  (setq eyGridCount[g] (+ eyGridCount[g] 1.0))
                  (setq pct (/ (abs (- ey y)) (+ (abs ey) .000000001)))
                  (+= err pct)
                  (if (= (modi n (divi N 10)) 0) (writeln "[" n "] ey=[" ey "] y=[" y "] err=[" (- ey y) "] err%=[" pct "]"))
                  ) ; end N loop
               (/= err N)
               (loop for g from 0 until eyGridSize do (setq eyGrid[g] (/ eyGrid[g] eyGridCount[g]))) 
               (loop for g from 1 until eyGridSize do (if (<= eyGrid[g] eyGrid[(- g 1)]) (++ errScore)))
               (setq errScore (/ (- eyGridSize errScore 1) (- eyGridSize 1)))
               (writeln "mvlRegress: err=[" err "], avgY=[" avgY "], errScore=[" errScore "], eyGrid=[" (string eyGrid true) "]")
          )) ; end Test Case sigmoid
       ;; Test Case square 
       (if (or (= Test all:) (= Test square:))
           (begin
		       ;; Create a test polynomial regression where y = C[0]*X[0] + C[1]*(X[1]**2) + C[2]*X[2] + C[3]*(X[3]**2) ...
		       (setq M Ms)
		       (setq N Ns)
		       (setq eyGrid (new Vector: Number: eyGridSize))
		       (setq eyGridCount (new Vector: Number: eyGridSize))
		       (setq X (new Vector: object: N))
		       (setq Y (new Vector: Number: N))
		       (setq C (new Vector: Number: M))
		       (loop for m from 0 until M do
		          (setq C[m] (- (srandom 10.0) 5.0))
		          ) ; end C loop
		       (loop for n from 0 until N do
		          (setq X[n] (new Vector: Number: M))
		          (setq y 0.0)
		          (loop for m from 01 until M do 
		          	 (setq X[n][m] (- (srandom 100.0) 50.0))
                     (setq y (+ y (* X[n][m] X[n][m] C[m])))
		             ) ; end M loop
		          (setq Y[n] (+ (* y .8) (* y (srandom .4))))
		          ) ; end N loop
		       ;; Train on the training data.
		       (writeln _eol "Starting test case: square")
		       (setq Lambda (setq Lambda (esm.mvlRegress.mvlTraining X Y)))
		       (writeln "mvlRegress: N = [" Ns "], M = [" Ms "]")
		       ;; Score on the test case.
		       (loop for n from 0 until N do
		          (setq X[n] (new Vector: Number: M))
		          (setq y 0.0)
                  (setq k (modi n 4)) 
 		          (setq X[n][1] (number k))
		          (loop for m from 0 until M do 
		          	 (setq X[n][m] (- (srandom 100.0) 50.0))
                     (setq y (+ y (* X[n][m] X[n][m] C[m])))
		             ) ; end M loop
		          (setq Y[n] (+ (* y .8) (* y (srandom .4))))
		          ) ; end N loop
               (setq err 0.0)
               (setq errScore 0.0)
               (setq avgTopEy 0.0)
               (setq topEyCnt 0)
               (setq avgY (avg Y))
               (setq G (/ N eyGridSize))
               (setq sortedY (|Gv:sort| Y < true)) 
               (loop for n from 0 until N do
                  (setq k sortedY[n])                  
                  (setq ey (Lambda X[k]))
                  (setq y Y[k])
                  (setq g (divi n G))
                  (setq eyGrid[g] (+ eyGrid[g] ey))
                  (setq eyGridCount[g] (+ eyGridCount[g] 1.0))
                  (setq pct (/ (abs (- ey y)) (+ (abs ey) .000000001)))
                  (+= err pct)
                  (if (= (modi n (divi N 10)) 0) (writeln "[" n "] ey=[" ey "] y=[" y "] err=[" (- ey y) "] err%=[" pct "]"))
                  ) ; end N loop
               (/= err N)
               (loop for g from 0 until eyGridSize do (setq eyGrid[g] (/ eyGrid[g] eyGridCount[g]))) 
               (loop for g from 1 until eyGridSize do (if (<= eyGrid[g] eyGrid[(- g 1)]) (++ errScore)))
               (setq errScore (/ (- eyGridSize errScore 1) (- eyGridSize 1)))
               (writeln "mvlRegress: err=[" err "], avgY=[" avgY "], errScore=[" errScore "], eyGrid=[" (string eyGrid true) "]")
          )) ; end Test Case square
       ;; Test Case tan 
       (if (or (= Test all:) (= Test tan:))
           (begin
		       ;; Create a test polynomial regression where y = C[0]*tan(X[0]) + C[1]*tan(X[1]) + C[2]*tan(X[2]) ...
		       (setq M Ms)
		       (setq N Ns)
		       (setq eyGrid (new Vector: Number: eyGridSize))
		       (setq eyGridCount (new Vector: Number: eyGridSize))
		       (setq X (new Vector: object: N))
		       (setq Y (new Vector: Number: N))
		       (setq C (new Vector: Number: M))
		       (loop for m from 0 until M do
		          (setq C[m] (- (srandom 10.0) 5.0))
		          ) ; end C loop
		       (loop for n from 0 until N do
		          (setq X[n] (new Vector: Number: M))
		          (setq y 0.0)
		          (loop for m from 01 until M do 
		          	 (setq X[n][m] (- (srandom 100.0) 50.0))
                     (setq y (+ y (* (tan X[n][m]) C[m])))
		             ) ; end M loop
		          (setq Y[n] (+ (* y .8) (* y (srandom .4))))
		          ) ; end N loop
		       ;; Train on the training data.
		       (writeln _eol "Starting test case: tan")
		       (setq Lambda (setq Lambda (esm.mvlRegress.mvlTraining X Y)))
		       (writeln "mvlRegress: N = [" Ns "], M = [" Ms "]")
		       ;; Score on the test case.
		       (loop for n from 0 until N do
		          (setq X[n] (new Vector: Number: M))
		          (setq y 0.0)
                  (setq k (modi n 4)) 
 		          (setq X[n][1] (number k))
		          (loop for m from 0 until M do 
		          	 (setq X[n][m] (- (srandom 100.0) 50.0))
                     (setq y (+ y (* (tan X[n][m]) C[m])))
		             ) ; end M loop
		          (setq Y[n] (+ (* y .8) (* y (srandom .4))))
		          ) ; end N loop
               (setq err 0.0)
               (setq errScore 0.0)
               (setq avgTopEy 0.0)
               (setq topEyCnt 0)
               (setq avgY (avg Y))
               (setq G (/ N eyGridSize))
               (setq sortedY (|Gv:sort| Y < true)) 
               (loop for n from 0 until N do
                  (setq k sortedY[n])                  
                  (setq ey (Lambda X[k]))
                  (setq y Y[k])
                  (setq g (divi n G))
                  (setq eyGrid[g] (+ eyGrid[g] ey))
                  (setq eyGridCount[g] (+ eyGridCount[g] 1.0))
                  (setq pct (/ (abs (- ey y)) (+ (abs ey) .000000001)))
                  (+= err pct)
                  (if (= (modi n (divi N 10)) 0) (writeln "[" n "] ey=[" ey "] y=[" y "] err=[" (- ey y) "] err%=[" pct "]"))
                  ) ; end N loop
               (/= err N)
               (loop for g from 0 until eyGridSize do (setq eyGrid[g] (/ eyGrid[g] eyGridCount[g]))) 
               (loop for g from 1 until eyGridSize do (if (<= eyGrid[g] eyGrid[(- g 1)]) (++ errScore)))
               (setq errScore (/ (- eyGridSize errScore 1) (- eyGridSize 1)))
               (writeln "mvlRegress: err=[" err "], avgY=[" avgY "], errScore=[" errScore "], eyGrid=[" (string eyGrid true) "]")
          )) ; end Test Case tan
       (writeln "mvlRegress.selfTest: completed in [" (/ (setq endTimeT (getTickCount startTimeT)) 60.0) "] minutes.")       
       Lambda) ; end selfTest
    ;; *****************
    ;; Begin main logic.
    ;; ***************** 
    vars:(Lambda mvlLambda)

    ;; Train a new percentile grid machine Lambda.
    (setq mvlLambda (new (myself)))
    (setq mvlLambda.myMVLParent mvlLambda)
    (setq Lambda (mvlLambda.mvlTraining X Y))
    Lambda) ; end mvlRegress

















































;;**EXPORTKEY**:esm:rowManager
(deforphan esm:rowManager(ObjVector:X ...)
;; *******************************************************************
;; Summary:  Evolutionary Sequencing Machine row mananger which manages 
;;           a set of Number Vectors all having a single time stamp.
;;           This row manager is constructed to work in partnership with
;;           the Selector language to manage a collection of rows 
;;           in memory.
;; Notes:    Each element in the time set is called a record.
;;           The index of a time set record is called its row.
;;           Row numbers begin with index zero.
;; Depends:  esm
;;
;; Args:     X:             - The N by M+2 vector array representing the training data in the form of:
;;                                         x x ... x
;;                                         x x ... x
;;                                             ... 
;;                                         x x ... x
;;           Note1:   	    - The zeroth element (x[0]) of every record may be a unique integer time stamp
;;                              and must be identical for all records in the time set (known as xtime in Selector).
;;           Note3:   	    - The remaining elements (x[1]) thru X[m]) of every record are known as x1 thru xm in Selector.
;;           Note4:   	    - The cdr of every record is the score for the specified record (known as y in Selector).
;;
;; Return:   self           This newly initialized row manager.
;; *******************************************************************
    ;; Persistent Variables
    pvars:(;; Public variables
           allRows           			;; The vector of all rows in this time set (the backup record view vector).
           Integer:colCount    			;; The number of columns in the time set.
           ObjVector:colVector 			;; The time set's vector of column names.
           Integer:myBucketSize        	;; The number of "best" 10% of elements to select from the testing data (as an integer).
           myManagerName        		;; The unique name of this ESM row manager.
           myManagerNotes       		;; The notes Dictionary of this ESM row manager.
           myIndex             			;; The schema repository for ESM time sets.
           myMemoPad           			;; The time set's memo pad (always a Dictionary).
           Number:myAvgY                ;; The average score for all rows in this time set.
           Number:myAvgDevY             ;; The average absolute difference of each y from the average y.
           Number:myMaxY                ;; The maximum y value for this row manager.
           Number:myMinY             	;; The minimum y value for this row manager.
           Integer:myBucketSize05Pct    ;; The number of "best" 5% of elements to select from the testing data (as an integer).
           Number:myScoreBest05Pct      ;; The best score possible (assuming the best 5% selected).
           Number:myScoreRange05Pct   	;; The absolute difference between the best 5% score and worst 5% score possible.
           Number:myScoreWorst05Pct   	;; The worst score possible (assuming the worst 5% selected).
           Integer:myBucketSize10Pct    ;; The number of "best" 10% of elements to select from the testing data (as an integer).
           Number:myScoreBest10Pct      ;; The best score possible (assuming the best 10% selected).
           Number:myScoreRange10Pct   	;; The absolute difference between the best 10% score and worst 10% score possible.
           Number:myScoreWorst10Pct   	;; The worst score possible (assuming the worst 10% selected).
           Integer:myBucketSize15Pct    ;; The number of "best" 15% of elements to select from the testing data (as an integer).
           Number:myScoreBest15Pct      ;; The best score possible (assuming the best 15% selected).
           Number:myScoreRange15Pct   	;; The absolute difference between the best 15% score and worst 15% score possible.
           Number:myScoreWorst15Pct   	;; The worst score possible (assuming the worst 15% selected).
           Integer:myBucketSize20Pct    ;; The number of "best" 20% of elements to select from the testing data (as an integer).
           Number:myScoreBest20Pct      ;; The best score possible (assuming the best 20% selected).
           Number:myScoreRange20Pct   	;; The absolute difference between the best 20% score and worst 20% score possible.
           Number:myScoreWorst20Pct   	;; The worst score possible (assuming the worst 20% selected).
           Integer:myBucketSize25Pct    ;; The number of "best" 25% of elements to select from the testing data (as an integer).
           Number:myScoreBest25Pct      ;; The best score possible (assuming the best 25% selected).
           Number:myScoreRange25Pct   	;; The absolute difference between the best 25% score and worst 25% score possible.
           Number:myScoreWorst25Pct   	;; The worst score possible (assuming the worst 25% selected).
           Integer:myBucketSize30Pct    ;; The number of "best" 30% of elements to select from the testing data (as an integer).
           Number:myScoreBest30Pct      ;; The best score possible (assuming the best 30% selected).
           Number:myScoreRange30Pct   	;; The absolute difference between the best 30% score and worst 30% score possible.
           Number:myScoreWorst30Pct   	;; The worst score possible (assuming the worst 30% selected).
           Integer:myBucketSize35Pct    ;; The number of "best" 35% of elements to select from the testing data (as an integer).
           Number:myScoreBest35Pct      ;; The best score possible (assuming the best 35% selected).
           Number:myScoreRange35Pct   	;; The absolute difference between the best 35% score and worst 35% score possible.
           Number:myScoreWorst35Pct   	;; The worst score possible (assuming the worst 35% selected).
           Integer:myBucketSize40Pct    ;; The number of "best" 40% of elements to select from the testing data (as an integer).
           Number:myScoreBest40Pct      ;; The best score possible (assuming the best 40% selected).
           Number:myScoreRange40Pct   	;; The absolute difference between the best 40% score and worst 40% score possible.
           Number:myScoreWorst40Pct   	;; The worst score possible (assuming the worst 40% selected).
           Integer:myBucketSize45Pct    ;; The number of "best" 45% of elements to select from the testing data (as an integer).
           Number:myScoreBest45Pct      ;; The best score possible (assuming the best 45% selected).
           Number:myScoreRange45Pct   	;; The absolute difference between the best 45% score and worst 45% score possible.
           Number:myScoreWorst45Pct   	;; The worst score possible (assuming the worst 45% selected).
           Integer:myBucketSize50Pct    ;; The number of "best" 50% of elements to select from the testing data (as an integer).
           Number:myScoreBest50Pct      ;; The best score possible (assuming the best 50% selected).
           Number:myScoreRange50Pct   	;; The absolute difference between the best 50% score and worst 50% score possible.
           Number:myScoreWorst50Pct   	;; The worst score possible (assuming the worst 50% selected).
           IntVector:mySortedY          ;; The IntVector of sorted Y indices.
           myTopParent            		;; This parent row manager Lambda.
           ObjVector:myX     		    ;; The ObjVector of X vectors.
           NumVector:myY     		    ;; The NumVector of Y values.
           Number:Penalty           	;; The current penalty for this row manager (0.0 is best 1.0 is worst).
           Integer:recordCount         	;; The time set's current total record count (length of selectedRows).
           Integer:rowCount    			;; The number of rows in the time set.
           selectedRows           		;; The time set's currently selected rows.
           Number:Score             	;; The current score for this row manager (as a percentage bewtween the best and worst possible scores).
           Integer:showLimit           	;; The maximum number of records to display with the show function.
           validFields         			;; The time set's valid fields and types (necessary for query compilation).
           viewDirectory       			;; The the Directory of saved time set record view vectors.
           ;; Public Child Lambdas
           average             			;; Averages a lambda value on each record of a time set.
           averageForAll       			;; Averages a lambda value on ALL records of a time set.
           close               			;; Terminate an update transaction on the time set object.
           deviation           			;; Returns the standard deviation of a lambda value on each record of a time set.
           dropView            			;; Drops the specified record view.
           extractColumns            	;; Extract multiple columns of data from this row manager.
           extractX                     ;; Extract the X training vector array from the specified row manager.
           extractY                     ;; Extract the Y number vector from the specified row manager.
           isView              			;; Returns true iff the specified key is a saved view of this row manager.
           maximum             			;; Returns the max of a lambda value on each record of a time set.
           minimum             			;; Returns the min of a lambda value on each record of a time set.
           newIndex            			;; Creates unique index of each record in a time set.
           newMemoPadIndex     			;; Creates unique memoPad index of each record in a time set.
           read                			;; Read a time set row.
           readByKey           			;; Read a time set row (using the specified index key).
           readLast            			;; Read the last time set row.
           reset               			;; Resets the views and restores the ESM time set.
           restore             			;; Restores the selected rows of the ESM time set to all rows.
           restoreView         			;; Restore the specified record view.
           run                 			;; Run the specified Selector Lambda against this row manager.
           runYEstimator                ;; Run the specified y estimator Lambda on this row manager (selecting the elements with the highest estimated y values).
           saveView            			;; Save the current record view for later retrieval.
           score            			;; Score the current record view (as a percentage of the best worst score range).
           search              			;; Returns the row index for the first record for which a lambda predicate is true.
           sharpe              			;; Returns the sharpe ratio of a lambda value on each record of a time set.
           show                			;; Shows a group of records starting from the specified row.
           sort                			;; Sort the time set rows using a lambda sort function.
           total               			;; Totals a lambda value on each record of a time set.
           totalForAll         			;; Totals a lambda value on ALL records of a time set.
           truncate            			;; Truncates a time set to those records for which a lambda predicate is true.
           ;; Private Child Lambdas
           __clear             			;; Clears the row manager.
           __errorStop         			;; Handles error conditions during sensitive operations.
           __open              			;; Initializes the ESM row manager.
           __statistics        			;; Builds the statistics for this ESM row manager.
           __truncateEnds      			;; Truncate records from both ends of a time set.
           __truncateMid       			;; Truncate records from the center of a time set.
          ) ;; end of persistent variables
    ;; Temporary Variables
    vars:(myCopy)
    ;; Initialize the inline child Lambdas.
    ;; *******************************************************************
    ;; Summary:  Averages a lambda value on each record of a time set.
    ;; *******************************************************************
    (defun average(valueLambda)
       vars:(result) 
       (setq result (total valueLambda))
       (if (isPositive recordCount) 
           (/= result recordCount)
           (setq result #void)
           ) ; end if
       result) ; end average
    ;; *******************************************************************
    ;; Summary:  Averages a lambda value on ALL records of a time set.
    ;; *******************************************************************
    (defun averageForAll(valueLambda)
       vars:(result) 
       (setq result (totalForAll valueLambda))
       (if (isPositive (length allRows)) 
           (/= result (length allRows))
           (setq result #void)
           ) ; end if
       result) ; end averageForAll
    (defun close()
    ;; *******************************************************************
    ;; Summary:  Terminates an update transaction on the row manager.
    ;; *******************************************************************
       (__clear)
       (setq myManagerName #void)
       (setq myIndex #void)
       (setq colVector #void)
       (setq colCount #void)
       (setq validFields #void)
       (setq colVector #void)
       (setq showLimit #void)
       (setq myTopParent #void)
       (setq myManagerNotes #void)
       (setq viewDirectory #void)
       (setq myMemoPad #void)
       (setq colCount #void)
       true) ;; end of close
    (defun deviation(totalLambda)
    ;; *******************************************************************
    ;; Summary:  Returns the standard deviation of a lambda value on each 
    ;;           record of a time set.
    ;; *******************************************************************
       vars:(rowIndex (result 0) score aSum aSsq anAvg)
       (if (<= recordCount 0) (return 0))
       ;; Compute the stadard devisation lambda value for each record in the time set.
       (loop for rowIndex from 0 until recordCount do
           (setq score (totalLambda selectedRows[rowIndex]))
           (setq aSum (+ aSum score))
           (setq aSsq (+ aSsq (* score score)))
           ) ;; end loop
       (setq anAvg (/ aSum recordCount))
       (setq result (sqrt (- (/ aSsq recordCount) (* anAvg anAvg))))
       result) ;; end deviation
    (defun dropView(key)
    ;; *******************************************************************
    ;; Summary: Drops the current record view as specified.
    ;; *******************************************************************
       (setq viewDirectory[key] 1)
       (delete viewDirectory key)
       true) ;; end of dropView
    (defun extractColumns(spineVectorType extractLambda)
    ;; *******************************************************************
    ;; Summary:  Extract multiple columns of data from this row manager.
    ;; *******************************************************************
       regs:(n N)
       vars:(extractedData)
       (if (<= recordCount 0) (return extractedData))
       ;; Select only those rows for which the find Lambda returns true.
       (setq N recordCount)
       (setq extractedData (|Gv:new| Vector: (symbol spineVectorType) N))
       (loop for n from 0 until N do
           (setq extractedData[n] (extractLambda selectedRows[n]))
           ) ;; end loop
       extractedData) ;; end extractColumns
    ;; *******************************************************************
    ;; Summary:  Extract the X training vector array from the specified row manager.
    ;; *******************************************************************
    (defun extractX(expressionVector)
       vars:(m M n N X command XT wff)
       ;; Collect the field expressions arguments.
       (setq XT myTopParent)
       (setq M (length expressionVector))
       (setq N XT.recordCount) 
       (setq command (append "extract new('Vector','Number'," M)) 
       (loop for m from 0 until M do (setq command (append command "," expressionVector[m])))
       (setq command (append command ");"))
       (if (isNumber (find "#void" command)) (return #void))
       ;; Extract raw data from cursor into X vector array.
       ;; Note: also compute column min and max values.
       (setq X (XT.run command))
       X) ; end extractX
    ;; *******************************************************************
    ;; Summary:  Extract the Y number vector array from the specified row manager.
    ;; *******************************************************************
    (defun extractY()
       vars:(n N Y XT)
       ;; Collect the field expressions arguments.
       (setq XT myTopParent)
       (setq N XT.recordCount)
       (setq Y (new Vector: Number: N)) 
       (loop for n from 0 until N do (setq Y[n] (cdr XT.allRows[n])))
       Y) ; end extractY
    ;; Returns true iff the specified key is a saved view of this row manager.
    (defun isView(key) (<> viewDirectory[key] #void)) 
    (defun maximum(totalLambda)
    ;; *******************************************************************
    ;; Summary:  Returns the max of a lambda value on each record of a time set.
    ;; *******************************************************************
       vars:(rowIndex result)
       (if (<= recordCount 0) (return 0))
       (setq result (totalLambda selectedRows[0]))
       ;; Compute the min lambda value for each record in the time set.
       (loop for rowIndex from 1 until recordCount do
           (setq result (max result (totalLambda selectedRows[rowIndex])))
           ) ;; end loop
       result) ;; end maximum
    (defun minimum(totalLambda)
    ;; *******************************************************************
    ;; Summary:  Returns the min of a lambda value on each record of a time set.
    ;; *******************************************************************
       vars:(rowIndex result)
       (if (<= recordCount 0) (return 0))
       (setq result (totalLambda selectedRows[0]))
       ;; Compute the min lambda value for each record in the time set.
       (loop for rowIndex from 1 until recordCount do
           (setq result (min result (totalLambda selectedRows[rowIndex])))
           ) ;; end loop
       result) ;; end minimum
    (defun newIndex(colName)
    ;; *******************************************************************
    ;; Summary:  Creates unique index of each record in a time set.
    ;; *******************************************************************
       vars:(record rowIndex numRecords colIndex)
       ;; Make sure the backup records are used to create the index.
       (setq colIndex (member (symbol colName) colVector))
       (if (= colIndex false) (error (append "rowManager: unknown index field name[" colName "]")))
       (setq numRecords (length allRows))
       (setq myIndex (new Directory:))
       (if (<= numRecords 0) (return numRecords))
       ;; Place all records in the index.
       (loop for rowIndex from 0 until numRecords do
           (setq record allRows[rowIndex])
           (setq myIndex[record[colIndex]] rowIndex)
           ) ;; end loop
       numRecords) ;; end newIndex
    (defun newMemoPadIndex(colName)
    ;; *******************************************************************
    ;; Summary:  Creates unique memoPad index of each record in a time set.
    ;; *******************************************************************
       vars:(record rowIndex numRecords colIndex)
       ;; Make sure the backup records are used to create the index.
       (setq colIndex (member (symbol colName) colVector))
       (if (= colIndex false) (error (append "rowManager: unknown index field name[" colName "]")))
       (setq numRecords (length allRows))
       (setq myMemoPad (new Directory:))
       (if (<= numRecords 0) (return numRecords))
       ;; Place all records in the index.
       (loop for rowIndex from 0 until numRecords do
           (setq record allRows[rowIndex])
           (if (= myMemoPad[record[colIndex]] #void)
               (setq myMemoPad[record[colIndex]] record)
               (error (append "esm.rowManager: duplicate memoPad index key [" colName "," record[colIndex] "," rowIndex "]"))
               ) ; end if
           ) ;; end loop
       numRecords) ;; end newMemoPadIndex
    (defun read(row)
    ;; *******************************************************************
    ;; Summary:  Reads a record from the time set.
    ;; Args:     row      Row number of row to be read.
    ;; Return:   record   The row just read. 
    ;; *******************************************************************
       (if (= row #void) (setq row 0))
       (if (or (< row 0) (> row recordCount))
           (error (append "badRowIndex" 
                          "rowManager:read:" myManagerName " - " row
                          ":An attempt was made to read with a bad row number.")))
       selectedRows[row]) ;; end of read
    (defun readByKey(key)
    ;; *******************************************************************
    ;; Summary:  Reads a record from the time set (using the specified key).
    ;; Args:     key      Key of row to be read (found in index).
    ;; Return:   record   The row just read. 
    ;; *******************************************************************
       vars:(row)
       (setq row myIndex[key])
       (if (= row #void) (return #void))
       (if (or (< row 0) (> row recordCount))
           (error (append "badRowKey" 
                          "rowManager:readByKey:" myManagerName "[" key
                          "] :An attempt was made to read with a bad index key.")))
       selectedRows[row]) ;; end of readByKey
    (defun readLast()
    ;; *******************************************************************
    ;; Summary:  Reads the last record from the time set.
    ;; Args:     none     
    ;; Return:   record   The row just read. 
    ;; *******************************************************************
       (if (<= recordCount 0) (return #void))
       selectedRows[(subi recordCount 1)]) ;; end of readLast
    (defun reset()
    ;; *******************************************************************
    ;; Summary:  Resets the backup copy and views of the ESM time set. 
    ;; *******************************************************************
       ;; Make sure the backup records and views are restored.
       (setq selectedRows allRows)
       (setq recordCount (length selectedRows))
       (setq viewDirectory (new Directory:))
       (setq myIndex (new Directory:))
       true) ;; end of reset
    (defun restore()
    ;; *******************************************************************
    ;; Summary: Restores the backup copy of the ESM time set.
    ;; *******************************************************************
       (setq selectedRows allRows)
       (setq recordCount (length selectedRows))
       true) ;; end of restore
    (defun restoreView(key)
    ;; *******************************************************************
    ;; Summary: Restores the current record view as specified.
    ;; *******************************************************************
       (setq selectedRows viewDirectory[key])
       (if (= selectedRows #void) (setq selectedRows allRows)) 
       (setq selectedRows (copy selectedRows))
       (setq recordCount (length selectedRows))
       true) ;; end of restoreView
    ;; Run the specified Selector Lambda against this row manager.
    (defun run(selectorWff)
       (if (not (isLambda selectorWff))
           (begin
              (setq selectorWff (compile (esm.selector selectorWff)))
              (setq selectorWff (selectorWff))
           )) ; end if
       (selectorWff myTopParent)) ;; end of run
    ;; Run the specified y estimator Lambda on this row manager (selecting the elements with the highest estimated y values).
    ;; Args:    yEst	The y estimator Lambda which, when given a number vector, returns an estimate for y.
    ;; Return:  XT		This row manager with the "best" vector elements having been selected.
    (defun runYEstimator(Lambda:yEst ...)
       regs:(m n N)
       vars:(X NumVector:yest Lambda:XT)
       vars:(IntVector:sortedXT ObjVector:selectedRows ObjVector:allRows)
       ;; Restore the row manager.
       (setq XT myTopParent)
       (XT.restore)
       ;; Compute the estimates for each vector in the testing row manager.
       (setq N XT.recordCount)
       (setq allRows XT.allRows)
       (setq yest (yEst.run allRows))
       ;; Compute the estimates for each vector in the testing row manager.
       (setq N myBucketSize)
       (setq sortedXT (|Gv:sort| yest > true))
       (setq selectedRows (new Vector: Object: N))
       (loop for n from 0 until N do 
          (setq m sortedXT[n])
          (setq selectedRows[n] allRows[m])
          ) ; end select loop
       (setq XT.selectedRows selectedRows)
       (setq XT.recordCount N)
       XT) ; end runYEstimator
    (defun saveView(key)
    ;; *******************************************************************
    ;; Summary: Saves the current record view as specified.
    ;; *******************************************************************
       (setq viewDirectory[key] (copy selectedRows))
       true) ;; end of saveView
    (defun score(wff)
    ;; *******************************************************************
    ;; Summary: Score the current record view (as a percentage of the best 
    ;;          worst score range). The recordCount must match the bucket
    ;;          size specified for this row manager or an error will result.
    ;; *******************************************************************
       regs:(k kk n nn N NN NH)
       regs:(Number:y Number:ey Number:err Number:seqPct Number:avgFactor)
       regs:(Number:yHigh Number:yLow Number:eyHigh Number:eyLow)
       regs:(Integer:buckets Integer:bucketSize Number:bsInv)
       regs:(Number:errHigh Number:errLow Number:errPct)
       regs:(Number:errScore Number:sortScore Number:bucketScore)
       vars:(Number:result NumVector:EY IntVector:sortedEY (Number:bigPositiveNumber 9.0e+300))
       vars:(scoreFocus paretoErrorSW (isEYConstant true))

       ;; Initialize this row manager statistics (if necessary).
       (if (= mySortedY #void) then (__statistics))
       ;; Run the specified command first (if requested).
       (restore)
       (setq scoreFocus esm.myScoreFocus)
       (setq paretoErrorSW esm.myParetoErrorSW)
       (if (= (setq EY (wff.run selectedRows)) false) (return false))
       ;; Score this row manager according to the user requested scoring style
       (cond 
        ;; Sequence the top 5% case
        ((= scoreFocus top05:)
         (begin 
	       ;; Compute the score for this Selector Lambda against the top 5% of this table.
           (setq buckets 20)
	       (setq sortedEY (|Gv:sort| EY > true))
	       (setq NN (length EY))
	       (setq NH (/ NN buckets))
	       (setq bsInv (/ 1.0 (number NH)))
	       (setq k sortedEY[0])
	       (setq y myY[k])
	       (setq ey EY[k])
	       (setq eyLow ey)
           (setq avgFactor (abs (+ myAvgDevY .000000000001))) 
	       (setq err (- ey y))
	       (setq err (abs err))
           (/= err avgFactor) 
           (if (> err 2.0) (setq err (+ 1.0 (expt err .001))))
	       (setq errPct err)
	       (setq sortScore 0.0)
	       (setq bucketScore y)
	       (loop for n from 1 until NH do
	          (setq k sortedEY[n])
	          (setq y myY[k])
	          (setq ey EY[k])
	          (setq err (- ey y))
	          (setq err (abs err))
              (/= err avgFactor) 
              (if (> err 2.0) (setq err (+ 1.0 (expt err .001)))) 
	          (+= errPct err)
	          (+= bucketScore y)
	          (if (<= eyLow ey) then (++ sortScore)) 
	          (setq eyLow ey) 
	          ) ; end scoring loop
	       (/= sortScore (number NH))
	       (/= errPct (number NH))
	       (/= bucketScore (number NH))
	       (setq bucketScore (- 1.0 (/ (- bucketScore myScoreWorst05Pct) myScoreRange05Pct)))
           ;; The final fittness score is the bucket fittness with a small reward for better sequencing.
	       (setq errScore (+ bucketScore (* .001 sortScore)))
	     )) ; end sequence the top 5% case
        ;; Sequence the top 10% case
        ((= scoreFocus top10:)
         (begin 
	       ;; Compute the score for this Selector Lambda against the top 10% of this table.
           (setq buckets 10)
	       (setq sortedEY (|Gv:sort| EY > true))
	       (setq NN (length EY))
	       (setq NH (/ NN buckets))
	       (setq bsInv (/ 1.0 (number NH)))
	       (setq k sortedEY[0])
	       (setq y myY[k])
	       (setq ey EY[k])
	       (setq eyLow ey)
           (setq avgFactor (abs (+ myAvgDevY .000000000001))) 
	       (setq err (- ey y))
	       (setq err (abs err))
           (/= err avgFactor) 
           (if (> err 2.0) (setq err (+ 1.0 (expt err .001)))) 
	       (setq errPct err)
	       (setq sortScore 0.0)
	       (setq bucketScore y)
	       (loop for n from 1 until NH do
	          (setq k sortedEY[n])
	          (setq y myY[k])
	          (setq ey EY[k])
	          (setq err (- ey y))
	          (setq err (abs err))
              (/= err avgFactor) 
              (if (> err 2.0) (setq err (+ 1.0 (expt err .001)))) 
	          (+= errPct err)
	          (+= bucketScore y)
	          (if (<= eyLow ey) then (++ sortScore)) 
	          (setq eyLow ey) 
	          ) ; end scoring loop
	       (/= sortScore (number NH))
	       (/= errPct (number NH))
	       (/= bucketScore (number NH))
	       (setq bucketScore (- 1.0 (/ (- bucketScore myScoreWorst10Pct) myScoreRange10Pct)))
           ;; The final fittness score is the bucket fittness with a small reward for better sequencing.
	       (setq errScore (+ bucketScore (* .001 sortScore)))
	     )) ; end sequence the top 10% case
        ;; Sequence the top 25% case
        ((= scoreFocus top25:)
         (begin 
	       ;; Compute the score for this Selector Lambda against the top 25% of this table.
           (setq buckets 4)
	       (setq sortedEY (|Gv:sort| EY > true))
	       (setq NN (length EY))
	       (setq NH (/ NN buckets))
	       (setq bsInv (/ 1.0 (number NH)))
	       (setq k sortedEY[0])
	       (setq y myY[k])
	       (setq ey EY[k])
	       (setq eyLow ey)
           (setq avgFactor (abs (+ myAvgDevY .000000000001))) 
	       (setq err (- ey y))
	       (setq err (abs err))
           (/= err avgFactor) 
           (if (> err 2.0) (setq err (+ 1.0 (expt err .001)))) 
	       (setq errPct err)
	       (setq sortScore 0.0)
	       (setq bucketScore y)
	       (loop for n from 1 until NH do
	          (setq k sortedEY[n])
	          (setq y myY[k])
	          (setq ey EY[k])
	          (setq err (- ey y))
	          (setq err (abs err))
              (/= err avgFactor) 
              (if (> err 2.0) (setq err (+ 1.0 (expt err .001)))) 
	          (+= errPct err)
	          (+= bucketScore y)
	          (if (<= eyLow ey) then (++ sortScore)) 
	          (setq eyLow ey) 
	          ) ; end scoring loop
	       (/= sortScore (number NH))
	       (/= errPct (number NH))
	       (/= bucketScore (number NH))
	       (setq bucketScore (- 1.0 (/ (- bucketScore myScoreWorst25Pct) myScoreRange25Pct)))
           ;; The final fittness score is the bucket fittness with a small reward for better sequencing.
	       (setq errScore (+ bucketScore (* .001 sortScore)))
	     )) ; end sequence the top 25% case
        ;; Sequence the full 100% case
        ((= scoreFocus full:)
         (begin 
	       ;; Compute the score for this Selector Lambda against this table.
	       (setq NN (length EY))
	       (setq k mySortedY[0])
           (setq avgFactor (abs (+ myAvgDevY .000000000001))) 
	       (setq err (- (setq eyLow EY[k]) myY[k]))
	       (setq err (abs err))
           (/= err avgFactor) 
           (if (> err 2.0) (setq err (+ 1.0 (expt err .001)))) 
	       (setq errPct err)
	       (setq sortScore 0.0)
	       (loop for n from 1 until NN do
	          (setq k mySortedY[n])
	          (setq y myY[k])
	          (setq ey EY[k])
	          (setq err (- ey y))
	          (setq err (abs err))
              (/= err avgFactor) 
              (if (> err 2.0) (setq err (+ 1.0 (expt err .001)))) 
	          (+= errPct err)
	          (if (<= eyLow ey) then (++ sortScore)) 
	          (if (<> eyLow ey) then (setq isEYConstant false)) 
	          (setq eyLow ey) 
	          ) ; end scoring loop
	       (/= sortScore (number NN))
	       (/= errPct (number NN))
           ;; Disallow constant functions where myY is not also a constant. 
           (if (and (<> myAvgDevY 0.0) (= isEYConstant true)) then (setq errPct bigPositiveNumber))
           ;; The final fittness score is the average error with a small reward for better sequencing.
           (if (= paretoErrorSW true)
               (setq errScore (+ errPct (* .001 (length wff.WFF))))
               (setq errScore (+ errPct (* .001 sortScore))) 
               ) ; end if 
	     )) ; end sequence the full 100% case
        (else (error "esm.rowManager.score: invalid myScoreFocus specified"))
        ) ; end scoring style cond

       FinalScore::
       (setq wff.ErrorPct errPct)
       (setq wff.ErrScore errScore)
       (setq wff.Score errScore)
        wff.Score) ;; end of score
    (defun search(findLambda ...)
    ;; *******************************************************************
    ;; Summary:  Returns the row index for the first record for which a lambda predicate is true.
    ;; *******************************************************************
       vars:(startIndex rowIndex n colName colValue (argc 2))
       ;; Define the temporary search lambda.
       (defun __searchLambda(x) vars:(colName colValue) (= x[colName] colValue))
       ;; If the findLambda is a columnName, we must make our own lambda predicate.
       (if (not (isLambda findLambda))
           (begin
              (setq colName findLambda)
              (setq colValue (argFetch 1))
              (++ argc)
              (setq findLambda __searchLambda)
              (setq findLambda.Tv.colName colName)
              (setq findLambda.Tv.colValue colValue)
           )) ; end if
       ;; Make sure we capture the startIndex argument (if any).
       (if (= (argCount) argc)
           (setq startIndex (argFetch (sub1 argc)))
           (setq startIndex 0)
           ) ; end if
       (if (<= recordCount 0) (return false))
       ;; Select the first row for which the find Lambda returns true.
       (setq n (length selectedRows))
       (loop for rowIndex from startIndex until n do
           (if (findLambda selectedRows[rowIndex]) (return rowIndex))
           ) ;; end loop
       false) ;; end search
    (defun sharpe(totalLambda)
    ;; *******************************************************************
    ;; Summary:  Returns the sharpe ratio of a lambda value on each 
    ;;           record of a time set.
    ;; *******************************************************************
       vars:(rowIndex (result 0) score aSum aSsq anAvg aStd)
       (if (<= recordCount 0) (return 0))
       ;; Compute the stadard devisation lambda value for each record in the time set.
       (loop for rowIndex from 0 until recordCount do
           (setq score (totalLambda selectedRows[rowIndex]))
           (setq aSum (+ aSum score))
           (setq aSsq (+ aSsq (* score score)))
           ) ;; end loop
       (setq anAvg (/ aSum recordCount))
       (setq aStd (sqrt (- (/ aSsq recordCount) (* anAvg anAvg))))
       (setq result (/ anAvg aStd))
       result) ;; end sharpe
    (defun show(Integer:startIndex) 
    ;; *******************************************************************
    ;; Summary:  Shows a group of records starting from the specified row.
    ;; *******************************************************************
       regs:(n N) 
       (setq N (integer (min recordCount (addi startIndex showLimit))))
       (loop for n from startIndex until N do
           (writeln "[" n "] " (read n))
           ) ;; end loop
       true) ;; end show
    (defun sort(sortLambda)
    ;; *******************************************************************
    ;; Summary:  Sorts each record in the row manager.
    ;; *******************************************************************
       (if (<= recordCount 0) (return recordCount))       
       (|Gv:sort| selectedRows sortLambda)
       recordCount) ;; end sort
    (defun total(totalLambda)
    ;; *******************************************************************
    ;; Summary:  Totals a lambda value on each record of a time set.
    ;; *******************************************************************
       vars:(rowIndex result)
       (if (<= recordCount 0) (return 0))
       ;; Compute the total lambda value for each record in the time set.
       (loop for rowIndex from 0 until recordCount do
           (+= result (totalLambda selectedRows[rowIndex]))
           ) ;; end loop
       result) ;; end total
    (defun totalForAll(totalLambda)
    ;; *******************************************************************
    ;; Summary:  Totals a lambda value on ALL records of a time set.
    ;; *******************************************************************
       vars:(rowIndex (result 0))
       (if (<= (length allRows) 0) (return 0))
       ;; Compute the total lambda value for each record in the time set.
       (loop for rowIndex from 0 until (length allRows) do
           (+= result (totalLambda allRows[rowIndex]))
           ) ;; end loop
       result) ;; end totalForAll
    (defun truncate(selectLambda)
    ;; *******************************************************************
    ;; Summary:  Truncates a time set to those records for which a lambda predicate is true.
    ;; *******************************************************************
       regs:(m n N rowIndex newIndex maxRecords)
       vars:(vec newRows IntVector:EY)
       (if (<= recordCount 0) (return recordCount))
       ;; If the selectLambda is a number vector, then sort ascending and keep only the first myBucketSize records.
       (if (isVector selectLambda)
           (begin
              (setq EY (|Gv:sort| selectLambda >= true))
              (setq newRows (|Gv:new| Vector: Object: myBucketSize))
              (setq N myBucketSize)
              (loop for n from 0 until N do
                (setq m EY[n])
                (setq newRows[n] selectedRows[m]) 
                ) ; end EY loop
              (setq selectedRows newRows)
              (setq recordCount myBucketSize)
              (return recordCount)
           )) ; end if
       (setq selectedRows (copy selectedRows))
       (setq recordCount (length selectedRows))
       ;; If the selectLambda is a number, then keep only the first N records.
       (if (isNumber selectLambda)
           (begin
              (setq maxRecords (integer (min recordCount selectLambda)))
              (setq selectedRows (resize selectedRows maxRecords))
              (setq recordCount (length selectedRows))
              (return recordCount)
           )) ; end if
       ;; Select only those rows for which the find Lambda returns true.
       (setq maxRecords (length selectedRows))
       (setq vec (|Gv:new| Vector: object: 0))
       (setq newIndex -1)
       (loop for rowIndex from 0 until maxRecords do
           (if (selectLambda selectedRows[rowIndex]) (setq vec[(++ newIndex)] selectedRows[rowIndex]))
           ) ;; end loop
       (setq selectedRows vec)
       (setq recordCount (length selectedRows))
       recordCount) ;; end truncate
    ;; -------------------------------------
    ;; Private methods (not for public use).
    ;; -------------------------------------
    (defun __clear()
       (setq selectedRows (new Vector: object: 0))
       (setq allRows selectedRows)
       (setq viewDirectory (new Directory:))
       (setq myIndex #void)
       (setq recordCount 0)
       true) ;; end of __clear
    (defun __open(ObjVector:X)
    ;; *******************************************************************
    ;; Summary:  Initializes the ESM row manager.
    ;; *******************************************************************
       ;; Search for the specified time set in the reference and blackboard areas.
       regs:(m M n N BR Number:y)
       vars:(NumVector:record Symbol:colName)
       (__clear)
       (setq myManagerName esmRowManager:)
       (setq myX X)
       (setq N (length X))
       (setq recordCount N)
       (setq rowCount N)
       (setq M (length myX[0]))
       (setq colVector (new Vector: Object: M))
       (setq colVector[0] xtime:)
       (setq colVector[1] xid:)
       (loop for m from 2 until M do
          (setq colName (symbol (append "x" (subi m 1))))
          (setq colVector[m] colName)
          ) ; end column names loop
       (setq colCount (length colVector))
       (setq validFields (objectToStructure colVector #(#void)))
       (setq showLimit 10)
       (setq myIndex #void)
       (setq myManagerNotes (new Dictionary:))
       (setq viewDirectory (new Directory:))
       (setq myMemoPad (new Dictionary:))
       (setq selectedRows (copy myX))
       (setq allRows selectedRows)
       (setq myBucketSize (integer (* N .10)))
       ;; Collect the scores for all possible rows in this time set.
       (setq myY #void)
       (setq myAvgY 0.0)
       (setq myAvgDevY 0.0) 
       (setq mySortedY #void)
       true) ;; end of __open
    ;; Builds the statistics for this ESM row manager.
    (defun __statistics()
    ;; *******************************************************************
    ;; Summary:  Initializes the ESM row manager.
    ;; *******************************************************************
       ;; Search for the specified time set in the reference and blackboard areas.
       regs:(m M n N BR Number:y)
       vars:(NumVector:record Symbol:colName)
       ;; Collect the scores for all possible rows in this time set.
       (setq N (length myX))
       (setq myBucketSize (integer (* N .10)))
       (setq myY (new Vector: Number: N))
       (loop for n from 0 until N do (setq myY[n] (number (cdr myX[n]))))
       (setq myAvgY (avg myY))
       (setq myAvgDevY 0.0)
       (setq myMaxY myY[0])
       (setq myMinY myY[0])
       (loop for n from 0 until N do 
         (setq y myY[n])
         (+= myAvgDevY (abs (- y myAvgY)))
         (if (< y myMinY) then (setq myMinY y))
         (if (> y myMaxY) then (setq myMaxY y))
         ) ; end main statistics loop
       (/= myAvgDevY (number N)) 
       (setq mySortedY (|Gv:sort| myY > true))
       ;; Compute the best and worst possible 5% scores.
       (setq myScoreBest05Pct 0.0)
       (setq myScoreWorst05Pct 0.0)
       (setq myBucketSize05Pct (integer (* N .05)))
       (loop for n from 0 until myBucketSize05Pct do
          (setq m mySortedY[n])
          (setq y myY[m])
          (+= myScoreBest05Pct y) 
          (setq m (- N n 1))
          (setq m mySortedY[m])
          (setq y myY[m])
          (+= myScoreWorst05Pct y) 
          ) ; end compute scores
       (/= myScoreBest05Pct myBucketSize05Pct)
       (/= myScoreWorst05Pct myBucketSize05Pct)
       (setq myScoreRange05Pct (- myScoreBest05Pct myScoreWorst05Pct))
       ;; Compute the best and worst possible 10% scores.
       (setq myScoreBest10Pct 0.0)
       (setq myScoreWorst10Pct 0.0)
       (setq myBucketSize10Pct (integer (* N .10)))
       (loop for n from 0 until myBucketSize10Pct do
          (setq m mySortedY[n])
          (setq y myY[m])
          (+= myScoreBest10Pct y) 
          (setq m (- N n 1))
          (setq m mySortedY[m])
          (setq y myY[m])
          (+= myScoreWorst10Pct y) 
          ) ; end compute scores
       (/= myScoreBest10Pct myBucketSize10Pct)
       (/= myScoreWorst10Pct myBucketSize10Pct)
       (setq myScoreRange10Pct (- myScoreBest10Pct myScoreWorst10Pct))
       ;; Compute the best and worst possible 15% scores.
       (setq myScoreBest15Pct 0.0)
       (setq myScoreWorst15Pct 0.0)
       (setq myBucketSize15Pct (integer (* N .15)))
       (loop for n from 0 until myBucketSize15Pct do
          (setq m mySortedY[n])
          (setq y myY[m])
          (+= myScoreBest15Pct y) 
          (setq m (- N n 1))
          (setq m mySortedY[m])
          (setq y myY[m])
          (+= myScoreWorst15Pct y) 
          ) ; end compute scores
       (/= myScoreBest15Pct myBucketSize15Pct)
       (/= myScoreWorst15Pct myBucketSize15Pct)
       (setq myScoreRange15Pct (- myScoreBest15Pct myScoreWorst15Pct))
       ;; Compute the best and worst possible 20% scores.
       (setq myScoreBest20Pct 0.0)
       (setq myScoreWorst20Pct 0.0)
       (setq myBucketSize20Pct (integer (* N .20)))
       (loop for n from 0 until myBucketSize20Pct do
          (setq m mySortedY[n])
          (setq y myY[m])
          (+= myScoreBest20Pct y) 
          (setq m (- N n 1))
          (setq m mySortedY[m])
          (setq y myY[m])
          (+= myScoreWorst20Pct y) 
          ) ; end compute scores
       (/= myScoreBest20Pct myBucketSize20Pct)
       (/= myScoreWorst20Pct myBucketSize20Pct)
       (setq myScoreRange20Pct (- myScoreBest20Pct myScoreWorst20Pct))
       ;; Compute the best and worst possible 25% scores.
       (setq myScoreBest25Pct 0.0)
       (setq myScoreWorst25Pct 0.0)
       (setq myBucketSize25Pct (integer (* N .25)))
       (loop for n from 0 until myBucketSize25Pct do
          (setq m mySortedY[n])
          (setq y myY[m])
          (+= myScoreBest25Pct y) 
          (setq m (- N n 1))
          (setq m mySortedY[m])
          (setq y myY[m])
          (+= myScoreWorst25Pct y) 
          ) ; end compute scores
       (/= myScoreBest25Pct myBucketSize25Pct)
       (/= myScoreWorst25Pct myBucketSize25Pct)
       (setq myScoreRange25Pct (- myScoreBest25Pct myScoreWorst25Pct))
       ;; Compute the best and worst possible 30% scores.
       (setq myScoreBest30Pct 0.0)
       (setq myScoreWorst30Pct 0.0)
       (setq myBucketSize30Pct (integer (* N .30)))
       (loop for n from 0 until myBucketSize30Pct do
          (setq m mySortedY[n])
          (setq y myY[m])
          (+= myScoreBest30Pct y) 
          (setq m (- N n 1))
          (setq m mySortedY[m])
          (setq y myY[m])
          (+= myScoreWorst30Pct y) 
          ) ; end compute scores
       (/= myScoreBest30Pct myBucketSize30Pct)
       (/= myScoreWorst30Pct myBucketSize30Pct)
       (setq myScoreRange30Pct (- myScoreBest30Pct myScoreWorst30Pct))
       ;; Compute the best and worst possible 35% scores.
       (setq myScoreBest35Pct 0.0)
       (setq myScoreWorst35Pct 0.0)
       (setq myBucketSize35Pct (integer (* N .35)))
       (loop for n from 0 until myBucketSize35Pct do
          (setq m mySortedY[n])
          (setq y myY[m])
          (+= myScoreBest35Pct y) 
          (setq m (- N n 1))
          (setq m mySortedY[m])
          (setq y myY[m])
          (+= myScoreWorst35Pct y) 
          ) ; end compute scores
       (/= myScoreBest35Pct myBucketSize35Pct)
       (/= myScoreWorst35Pct myBucketSize35Pct)
       (setq myScoreRange35Pct (- myScoreBest35Pct myScoreWorst35Pct))
       ;; Compute the best and worst possible 40% scores.
       (setq myScoreBest40Pct 0.0)
       (setq myScoreWorst40Pct 0.0)
       (setq myBucketSize40Pct (integer (* N .40)))
       (loop for n from 0 until myBucketSize40Pct do
          (setq m mySortedY[n])
          (setq y myY[m])
          (+= myScoreBest40Pct y) 
          (setq m (- N n 1))
          (setq m mySortedY[m])
          (setq y myY[m])
          (+= myScoreWorst40Pct y) 
          ) ; end compute scores
       (/= myScoreBest40Pct myBucketSize40Pct)
       (/= myScoreWorst40Pct myBucketSize40Pct)
       (setq myScoreRange40Pct (- myScoreBest40Pct myScoreWorst40Pct))
       ;; Compute the best and worst possible 45% scores.
       (setq myScoreBest45Pct 0.0)
       (setq myScoreWorst45Pct 0.0)
       (setq myBucketSize45Pct (integer (* N .45)))
       (loop for n from 0 until myBucketSize45Pct do
          (setq m mySortedY[n])
          (setq y myY[m])
          (+= myScoreBest45Pct y) 
          (setq m (- N n 1))
          (setq m mySortedY[m])
          (setq y myY[m])
          (+= myScoreWorst45Pct y) 
          ) ; end compute scores
       (/= myScoreBest45Pct myBucketSize45Pct)
       (/= myScoreWorst45Pct myBucketSize45Pct)
       (setq myScoreRange45Pct (- myScoreBest45Pct myScoreWorst45Pct))
       ;; Compute the best and worst possible 50% scores.
       (setq myScoreBest50Pct 0.0)
       (setq myScoreWorst50Pct 0.0)
       (setq myBucketSize50Pct (integer (* N .50)))
       (loop for n from 0 until myBucketSize50Pct do
          (setq m mySortedY[n])
          (setq y myY[m])
          (+= myScoreBest50Pct y) 
          (setq m (- N n 1))
          (setq m mySortedY[m])
          (setq y myY[m])
          (+= myScoreWorst50Pct y) 
          ) ; end compute scores
       (/= myScoreBest50Pct myBucketSize50Pct)
       (/= myScoreWorst50Pct myBucketSize50Pct)
       (setq myScoreRange50Pct (- myScoreBest50Pct myScoreWorst50Pct))
       true) ;; end of __statistics
    (defun __errorStop(errMsg)
       (error (mid errMsg 1 (subi (length errMsg) 2)))) ;; end __errorStop
    (defun __truncateEnds(begRowLimit endRowLimit)
    ;; *******************************************************************
    ;; Summary:  Truncate records from both ends of a time set.
    ;; *******************************************************************
       vars:(rowIndex newIndex n vec endRowIndex)
       (if (<= recordCount 0) (return recordCount))
       ;; Select only those rows in between the begin and end row limits.
       (setq n (length selectedRows))
       (setq endRowIndex (subi n endRowLimit))
       (setq vec (^new Vector: object: 0))
       (setq newIndex -1)
       (loop for rowIndex from (addi begRowLimit 1) until endRowIndex do
           (setq vec[(++ newIndex)] selectedRows[rowIndex])
           ) ;; end loop
       (setq selectedRows vec)
       (setq recordCount (length selectedRows))
       recordCount) ;; end truncateEnds
    (defun __truncateMid(begRowLimit endRowLimit)
    ;; *******************************************************************
    ;; Summary:  Truncate records from the center of a time set.
    ;; *******************************************************************
       vars:(rowIndex newIndex n vec endRowIndex)
       (if (<= recordCount 0) (return recordCount))
       ;; Delete those rows in between the begin and end row limits.
       (setq n (length selectedRows))
       (setq endRowIndex (subi n endRowLimit))
       (setq vec (^new Vector: object: 0))
       (setq newIndex -1)
       (loop for rowIndex from 0 until begRowLimit do
           (setq vec[(++ newIndex)] selectedRows[rowIndex])
           ) ;; end loop
       (loop for rowIndex from endRowIndex until n do
           (setq vec[(++ newIndex)] selectedRows[rowIndex])
           ) ;; end loop
       (setq selectedRows vec)
       (setq recordCount (length selectedRows))
       recordCount) ;; end truncateMid
    ;; *******************************************************************
    ;; Begin MAIN logic section.
    ;; Note: Initialize this memory row manager.
    ;; *******************************************************************
    Continue::
    (setq myCopy (copy (myself)))
    (setq myCopy.myTopParent myCopy)
    (myCopy.__open X)
    myCopy) ;; end of rowManager








































;;**EXPORTKEY**:esm:ruleBgm
(defriend esm:ruleBgm(wff)
;; *******************************************************************
;; summary:  The WFF grammar rules for all Selector basis 
;;           grid regression Lambdas. 
;;
;; Main:     Return a Selector basis grid regression statement,
;;           in source format, such as 
;;                    "bgmregress(x2,(x2-x10));" 
;;           as an ASCI source string in grammatically correct selector.
;;
;; args:     wff         The Selector WFF of the basis grid regression statement.
;;           
;; Return:   result      A Selector basis grid regression statement such as "bgmregress(x2,(x2-x10));".
;; *******************************************************************
  	pvars:(;; Public Variables
           (genomeType EXP)         ;; This regression rule uses an expression based genome.
           (lengthExempt false)     ;; This regression rule is NOT exempt from genome length restrictions.
           ;; Public Methods
           chromosomeLength         ;; Return the number of non-empty left-most chromosomes in the genome.
           crossOver                ;; Create two new Selector basis grid regression WFFs by splicing two parent WFFs together using genetic crossover.
           growWFF                  ;; Create a new Selector basis grid regression wff to the current population.
           mutate                   ;; Create a new Selector basis grid regression WFF using genetic mutation.
           wffList                  ;; Convert a Selector basis grid regression wff to a list format.
           wffSource				;; Convert a Selector basis grid regression wff to a source statement such as "bgmregress(x0-x5,abs(x1));". 
           wffString                ;; Convert a Selector basis grid regression wff to a string format such as "(ruleBgm #(obj| (ruleSub x0 x5) (ruleAbs x1)));"
           ) ; end persistant variables
   	;; *******************************************************************************
   	;; Define Public Child Lambdas 
   	;; *******************************************************************************

	(defun chromosomeLength(genome)
	;; *******************************************************************
	;; summary:  Return the number of non-empty left-most chromosomes in the genome. 
	;;
	;; args:     genome     The genome whose chromosome length is to be returned.           
	;;           
	;; Return:   length     The number of non-empty left-most chromosomes in the genome.
	;;
	;; *******************************************************************
	   regs:(m M len)
	
       (setq M (length genome))
	   (loop for m from 0 until M do
          (if (<> genome[m] #void) (++ len))
          ) ; end loop

	   len) ; end chromosomeLength


	(defun crossOver(father mother)
	;; *******************************************************************
	;; summary:  Create two new Selector basis grid regression WFFs, 
    ;;           by splicing two parent WFFs together using genetic crossover.
	;;
	;; args:     father     The father WFF to be mated.           
	;;           mother     The mother WFF to be mated.           
	;;           
	;; Return:   none.
	;;
	;; *******************************************************************
	   regs:(m M n N)
	   vars:(dadGenome momGenome dadRule momRule newGenome)
	   vars:(dadWff momWff newWff (passOne true))
       vars:(dadIndex momIndex dadPair momSlot dadWff chromosomeCrossPct)
	
	   ;; Extract numeric WFFs from the parent Selector Lambdas (where necessary).
	   (if (or (= father #void) (= mother #void)) (return false))
	   (if (isLambda father) (begin (setq dadRule father.Rule) (setq father father.Genome)))
	   (if (isLambda mother) (begin (setq momRule mother.Rule) (setq mother mother.Genome)))
	   (if (isPair father) (begin (setq dadRule father[0]) (setq father father[1])))
	   (if (isPair mother) (begin (setq momRule mother[0]) (setq mother mother[1])))
	   (setq dadGenome father)
	   (setq momGenome mother)

	   ;; Make sure the father and mother are genetically compatible.
	   (if (or (= father #void) (= mother #void)) (return false))
	   (if (<> esm[momRule].genomeType EXP:) (return false))
	
	   Retry::     
	
	   ;; Make copies of the candidate genomes.
	   (setq dadGenome (listWff father))
	   (setq momGenome (listWff mother))

       ;; Do we use a linear genome during cross over operations?
       (if (= myCrossLinearSW true)
           (begin
             ;; Reorder the chromosomes in the candidate genomes occasionally.
             (setq M (chromosomeLength dadGenome))
             (setq N (chromosomeLength momGenome))
             (if (<= (random 1.0) .50)
                 then
                 (begin 
                   (setq dadGenome (copy dadGenome))
                   (setq m (integer (random M)))
                   (setq n (integer (random M)))
                   (setq chromosome dadGenome[m])
                   (setq dadGenome[m] dadGenome[n])
                   (setq dadGenome[n] chromosome)
                 )) ; end if
             (if (<= (random 1.0) .50)
                 then
                 (begin 
                   (setq momGenome (copy momGenome))
                   (setq m (integer (random N)))
                   (setq n (integer (random N)))
                   (setq chromosome momGenome[m])
                   (setq momGenome[m] momGenome[n])
                   (setq momGenome[n] chromosome)
                 )) ; end if
      
             ;; Select chromosomes from each genome.
             (setq genome (copy dadGenome))           
             (setq M (max M N))
             (setq chromosomeCrossPct 1.5)
             (loop for m from 0 until M do
               (if (>= chromosomeCrossPct .50) (-= chromosomeCrossPct .50))
               (cond
                 ((> (random 1.0) chromosomeCrossPct) (setq genome[m] genome[m]))
                 ((= genome[m] #void) (if (<= (random 1.0) .25) then (setq genome[m] momGenome[m])))
                 ((= momGenome[m] #void) (if (<= (random 1.0) .25) then (setq genome[m] (ruleExp.mutateNumericWFF genome[m] (random 1.0)))))
                 ((and (= myCrossMarrySW true) (<= (random 1.0) .20) (<> (setq newWff (ruleExp.marryNumericWFFs genome[m] momGenome[m])) false)) (setq genome[m] newWff))
                 ((= myCrossSpliceSW false) (if (<= (random 1.0) .50) then (setq genome[m] momGenome[m])))
                 (else 
                  (begin
                     (setq dadWff genome[m])                    
                     (setq momSlot (setq momIndex (esm.ruleExp.selectSlotWFF momGenome[m]))[(integer (random (length momIndex)))])
                     (setq dadPair (setq dadIndex (esm.ruleExp.selectPairWFF dadWff))[(integer (random (length dadIndex)))])
                     (cond
                      ((isAtom dadPair) (setq genome[m] momSlot))
                      ((and (isPair momSlot) (= myCrossMarrySW true) (<= (random 1.0) .20) (<> (setq newWff (ruleExp.marryNumericWFFs (listWff dadPair) (listWff momSlot))) false))
                       (begin (setCar dadPair (car newWff))  (setCdr dadPair (cdr newWff))))
                      ((isPair momSlot) (begin (setCar dadPair (car momSlot))  (setCdr dadPair (cdr momSlot))))
                      (else (ruleExp.spliceNumericWFF dadPair momSlot 1.0))
                      ) ; end splice cond
                  )) ; end case
                 ) ; end chromosome cond           
               ) ; end column loop  	
      
             (goto Grow:)
           )) ; end cross over linear genome if

       ;; Do we cross over chromosomes from different columns?
       (if (= myCrossChromosomeSW true)
           (begin
             (setq m (integer (random (chromosomeLength dadGenome))))
             (setq n (integer (random (chromosomeLength momGenome))))
             (setq genome (copy dadGenome))
             
             ;; Select chromosomes for cross over to create child genome.
             (cond 
               ((= dadGenome[m] #void) (setq genome[m] momGenome[n]))
               ((and (= momGenome[n] #void) (= myCrossSpliceSW true)) (setq genome[m] (setq newWff (spliceNumericWFF (listWff dadGenome[m]) (cutoutNumericWFF (listWff dadGenome[m]))))))
               ((and (= momGenome[n] #void) (= myCrossMarrySW true) (<> (setq newWff (ruleExp.marryNumericWFFs dadGenome[m] dadGenome[m])) false)) (setq genome[m] newWff))
               ((= momGenome[n] #void) (setq genome[m] momGenome[n]))
               ((and (= myCrossMarrySW true) (<= (random 1.0) .20) (<> (setq newWff (ruleExp.marryNumericWFFs dadGenome[m] momGenome[n])) false)) (setq genome[m] newWff))
               ((= myCrossSpliceSW false) (setq genome[m] momGenome[n]))
               (else ;; Here myCrossSpliceSW is true and both dadGenome and momGenome are not void. 
                (begin
                   (setq newWff (spliceNumericWFF (listWff dadGenome[m]) (cutoutNumericWFF (listWff momGenome[n]))))
                   (if (> (lengthWFF newWff) myMaxColWFFLen)
                       (setq newWff (spliceNumericWFF (listWff dadGenome[m]) (cutoutNumericWFF (listWff momGenome[n]))))
                       ) ; end if
                   (if (> (lengthWFF newWff) myMaxColWFFLen)
                       (setq newWff (spliceNumericWFF (listWff dadGenome[m]) (cutoutNumericWFF (listWff momGenome[n]))))
                       ) ; end if
                   (if (> (lengthWFF newWff) myMaxColWFFLen)
                       (setq newWff (spliceNumericWFF (listWff dadGenome[m]) (cutoutNumericWFF (listWff momGenome[n]))))
                       ) ; end if
                   (if (> (lengthWFF newWff) myMaxColWFFLen) 
                       (setq newWff momGenome[n])
                       ) ; end if
                   (setq genome[m] newWff)
                )) ; end case
               ) ; end chromosome cond           
             (goto Grow:)
           )) ; end cross over chromosomes if

	   ;; Reorder the chromosomes in the candidate genomes occasionally.
	   (if (<= (random 1.0) .50)
	       then
	       (begin 
	         (setq dadGenome (copy dadGenome))
	         (setq m (integer (random myM)))
	         (setq n (integer (random myM)))
	         (setq chromosome dadGenome[m])
	         (setq dadGenome[m] dadGenome[n])
	         (setq dadGenome[n] chromosome)
	       )) ; end if
	   (if (<= (random 1.0) .50)
	       then
	       (begin 
	         (setq momGenome (copy momGenome))
	         (setq m (integer (random myM)))
	         (setq n (integer (random myM)))
	         (setq chromosome momGenome[m])
	         (setq momGenome[m] momGenome[n])
	         (setq momGenome[n] chromosome)
	       )) ; end if

	   ;; Splice the chromosomes in the candidate genomes to form the progeny.
       (setq genome (new Vector: myM))
       (setq M myM)
       (loop for m from 0 until M do
         ;; Select chromosomes from each genome.
         (cond 
           ((= dadGenome[m] #void) (setq genome[m] momGenome[m]))
           ((= momGenome[m] #void) (setq genome[m] dadGenome[m]))
           ((and (= myCrossMarrySW true) (<= (random 1.0) .20) (<> (setq newWff (ruleExp.marryNumericWFFs dadGenome[m] momGenome[m])) false)) (setq genome[m] newWff))
           ((<= (random 1.0) .10) (setq genome[m] dadGenome[m]))
           ((<= (random 1.0) .10) (setq genome[m] momGenome[m]))
           ((and (= myCrossSpliceSW false) (<= (random 1.0) .50)) (setq genome[m] dadGenome[m]))
           ((= myCrossSpliceSW false) (setq genome[m] momGenome[m]))
           (else 
            (begin
               (setq newWff (spliceNumericWFF (listWff dadGenome[m]) (cutoutNumericWFF (listWff momGenome[m]))))
               (if (> (lengthWFF newWff) myMaxColWFFLen)
                   (setq newWff (spliceNumericWFF (listWff dadGenome[m]) (cutoutNumericWFF (listWff momGenome[m]))))
                   ) ; end if
               (if (> (lengthWFF newWff) myMaxColWFFLen)
                   (setq newWff (spliceNumericWFF (listWff dadGenome[m]) (cutoutNumericWFF (listWff momGenome[m]))))
                   ) ; end if
               (if (> (lengthWFF newWff) myMaxColWFFLen)
                   (setq newWff (spliceNumericWFF (listWff dadGenome[m]) (cutoutNumericWFF (listWff momGenome[m]))))
                   ) ; end if
               (if (> (lengthWFF newWff) myMaxColWFFLen)
                   (if (< (random 1.0) .50) (setq newWff dadGenome[m]) (setq newWff momGenome[m]))
                   ) ; end if
               (setq genome[m] newWff)
            )) ; end case
           ) ; end chromosome cond           
         ) ; end column loop  	

	   ;; Make a Selector basis grid regression wff and add it to the current population.
       Grow::
       (growWFF genome dadGenome)
	
	   ;; Make sure to mate the father with the mother
	   (if (= passOne true) 
	       (begin 
	          (setq passOne false) 
	          (setq dadGenome mother)
	          (setq momGenome father)
	          (goto Retry:)
	       )) ; end if
	
	   true) ; end crossOver
	
	(defun growWFF(command ...)
	;; *******************************************************************
	;; summary:  Grow a new Selector basis grid regression WFF and
	;;           add it to the current population.
	;;
	;; args:     command 	The command to determine the type of WFF to grow.
	;;           prevGenome (Optional)The previous genome (for use by particle swarm operators).
	;;           
	;; Return:   Lambda      A scored Selector basis grid regression Lambda.
	;;
	;; *******************************************************************
	   regs:(k m M mm n N maxChromosomes cntChromosomes)
	   vars:(Lambda genome newGenome prevGenome)
	   vars:(Rf wff)
	   vars:(IntVector:chromosomeIndices IntVector:chromosomeLimits)
	
       ;; Use the command to determine the type of WFF to grow.
       (setq maxChromosomes esm.myBGMMaximum)
       (cond
         ;; Command is a genome vector.
         ((isVector command)
          (begin
            (if (>= (argCount) 2) (setq prevGenome (argFetch 1)))
            (setq genome command) 
          )) ; end case genome vector
         ;; Command requires growth of an mvl using root expressions.
         ((= command root:)
          (begin
		    (setq genome (new Vector: myM))
		    (setq M myM)
		    (loop for m from 0 until M do
		      ;; Select random root expressions for each chromosome in the genome.
              (if (= myGrowWFFStyle full:)
		          (setq genome[m] (ruleExp.growCTermWFF m))
		          (setq genome[m] (ruleExp.growTermWFF -2 m))
                  ) ; end if
		      ) ; end column loop  
          )) ; end case root command
         ;; Command requires growth of an mvl using full expressions.
         ((= command full:)
          (begin
		    (setq genome (new Vector: myM))
		    (setq M myM)
		    (loop for m from 0 until M do
		      ;; Select random full expressions for each chromosome in the genome.
              (if (= myGrowWFFStyle full:)
		          (setq genome[m] (ruleExp.growWFF expression: 0))
		          (setq genome[m] (ruleExp.growRootWFF -2 m))
                  ) ; end if
		      ) ; end column loop  
          )) ; end case full command
         ;; All other commands return an error.
         (else (error (append "esm.ruleBgm.growWFF: invalid command [" command "]")))
         ) ; end command cond
	
	   ;; Create the basis grid regression Selector from the final WFF.
	   Last::
       (if (isLambda genome) (setq genome genome.Genome)) 
       (setq wff (wffList (list ruleBgm: genome)))
       (setq genome wff[1])
       (setq cntChromosomes 0)
       (setq M (length genome))
       (loop for m from 0 until M do (if (<> genome[m] #void) (++ cntChromosomes)))
       (cond
        ;; We have less than the maximum number of chromosomes
	    ((>= maxChromosomes cntChromosomes) (setq Rf (createSelector wff prevGenome)))
        ;; We have more than the maximum number of chromosomes
        ;; Note: We must be an exhaustive search of all possible
        ;;       combinations of the chromosomes in this genome
        ;;       taken 'maxChromosomes' at a time.
	    (else 
         (begin
           (setq chromosomeIndices (new Vector: Integer: maxChromosomes))
           (setq chromosomeLimits (new Vector: Integer: maxChromosomes))
           (loop for mm from 0 until maxChromosomes do (setq chromosomeIndices[mm] mm) (setq chromosomeLimits[mm] (+ (- cntChromosomes maxChromosomes) mm)))
           (while (<= chromosomeIndices[0] chromosomeLimits[0]) do
             (setq newGenome (new Vector: M))
             (loop for mm from 0 until maxChromosomes do (setq k chromosomeIndices[mm]) (setq newGenome[mm] genome[k]))
             (setq Rf (createSelector (list ruleBgm: newGenome) prevGenome))
             ;; Increment the chromosome combinatatorial indices.
             (loop for mm from (- maxChromosomes 1) to 0 by -1 do (++ chromosomeIndices[mm]) (if (<= chromosomeIndices[mm] chromosomeLimits[mm]) (setq mm -1))) 
             (loop for mm from 1 until maxChromosomes do (if (> chromosomeIndices[mm] chromosomeLimits[mm]) (setq chromosomeIndices[mm] (+ chromosomeIndices[(- mm 1)] 1)))) 
             ) ; end while
         )) ; end else
        ) ; end cond
	   Rf) ; end growWFF

	(defun mutate(wff)
	;; *******************************************************************
	;; summary:  Create a new Selector basis grid regression WFF 
    ;;           using genetic mutation.
	;;
	;; args:     wff     The candidate WFF to be mutated.           
	;;           
	;; Return:   none.
	;;
	;; *******************************************************************
	   regs:(m M n N)
	   vars:(rule genome prevGenome newWff chromosome (Number:deathPct .10))
	
	   ;; Extract numeric WFFs from the candidate Selector Lambda (where necessary).
	   (if (= wff #void) (return false))
	   (if (isLambda wff) (setq wff wff.Genome))
	   (if (isPair wff) (setq wff wff[1]))
	   (setq genome wff)
	   (if (= genome #void) (setq genome myColumnGenome))
       (setq prevGenome genome)
	
	   ;; Make a copy of the candidate genome.
	   (setq genome (listWff genome))

       ;; Reorder the chromosomes in the candidate genome occasionally.
       (if (<= (random 1.0) .50)
           then
           (begin 
             (setq m (integer (random myM)))
             (setq n (integer (random myM)))
             (setq chromosome genome[m])
             (setq genome[m] genome[n])
             (setq genome[n] chromosome)
           )) ; end if
       (setq m (integer (random myM))) 
       (if (= myMutateSpliceSW true) (setq newWff (mutateNumericWFF genome[m])) (begin (setq newWff (ruleExp.growRootWFF -2)) (goto Last:)))
       (setq n 0)(while (and (< (++ n) 20) (> (lengthWFF newWff) myMaxColWFFLen)) do (setq newWff (mutateNumericWFF genome[m])))
       (if (> (lengthWFF newWff) myMaxColWFFLen) (return false))
       (if (<= (random 1.0) deathPct) (setq genome[m] #void) (setq genome[m] newWff))

	   ;; Make a Selector multivariate linear regression wff and add it to the current population.
       Last::
       (growWFF genome prevGenome)
	
	   true) ; end mutate
		
    (defun wffList(wff)
	;; *******************************************************************
	;; summary:  Convert a selector basis grid regression wff to a 
    ;;           list format. 
	;;
	;; args:     wff         The basis grid regression wff.
	;;           
	;; Return:   result      A Selector basis grid regression WFF such as '(ruleBgm #(obj| (x0-x5) abs(x1)))
	;;
	;; *******************************************************************
       regs:(m M)
       vars:(wffListFormat rule genome)
       vars:(chromosome0 chromosome1)
       (if (isString wff) (setq wff wff) (setq wff (string wff true)))
       (setq wff (lisp wff)[0])
       (if (or (not (isPair wff)) (<> (length wff) 2) (<> wff[0] ruleBgm:)) (error "esm.ruleBgm.wffList: invalid WFF"))
       (setq rule wff[0])
       (setq genome (sort (copy wff[1]) >))
       ;; Eliminate all collisions before generating bgmregress genome.
       (setq M (length genome))
       (loop for m from 1 until M do
         (setq chromosome0 genome[(- m 1)])
         (setq chromosome1 genome[m])
         (if (and (<> chromosome1 #void) (= chromosome0 chromosome1))
             (setq genome[(- m 1)] #void)
             ) ; end if
         ) ; end loop   
       (setq genome (sort genome >))
       (setq wffListFormat (list rule genome))
       wffListFormat) ; end wffList


	(defun wffSource(genome)
	;; *******************************************************************
	;; summary:  Return a Selector basis grid regression statement 
    ;;           such as 
    ;;                     "bgmregress(x0-x5,abs(x1));"
    ;;  
	;;           as an ASCI string in grammatically correct Selector.
	;;
	;; args:     genome      The sparse vector of chromosome numeric expressions for the bgmregress statement.
	;;           
	;; Return:   result      A Selector basis grid regression statement such as "bgmregress(x0-x5,abs(x1));"
	;;
	;; *******************************************************************
       regs:(m M)
       vars:((wffSourceFormat "") rule genome)
       vars:(chromosome0 chromosome1)
       ;; Eliminate all collisions before generating bgmregress genome.
       (setq genome (sort (copy genome) >))
       (setq M (length genome))
       (loop for m from 1 until M do
         (setq chromosome0 genome[(- m 1)])
         (setq chromosome1 genome[m])
         (if (and (<> chromosome1 #void) (= chromosome0 chromosome1))
             (setq genome[(- m 1)] #void)
             ) ; end if
         ) ; end loop   
       (setq genome (sort genome >))
       ;; Create the final source from the genome.
       (setq M (length genome))
       (loop for m from 0 until M do
         ;; Select chromosomes from each genome.
         (cond 
           ((and (= wffSourceFormat "") (<> genome[m] #void)) (setq wffSourceFormat (ruleExp genome[m])))
           ((and (<> wffSourceFormat "") (<> genome[m] #void)) (setq wffSourceFormat (append wffSourceFormat "," (ruleExp genome[m]))))
           ) ; end chromosome cond
         ) ; end column loop  
	    ;; Generate a gramatically correct Selector basis grid regression statement in source format.
	   (setq wffSourceFormat (append "bgmregress(" wffSourceFormat ");"))
	   wffSourceFormat) ; end wffSource    

   (defun wffString(wff)
	;; *******************************************************************
	;; summary:  Convert a selector basis grid regression wff to a 
    ;;           string format. 
	;;
	;; args:     wff         The basis grid regression wff.
	;;           
	;; Return:   result      A Selector basis grid regression WFF such as "(ruleBgm #(obj| (x0-x5) abs(x1)))"
	;;
	;; *******************************************************************
       regs:(m M)
       vars:(wffStringFormat rule genome)
       vars:(chromosome0 chromosome1)
       (if (isString wff) (setq wff (lisp wff)[0]))
       (if (or (not (isPair wff)) (<> (length wff) 2) (<> wff[0] ruleBgm:)) (error "esm.ruleBgm.wffString: invalid WFF"))
       (setq rule wff[0])
       (setq genome (sort (copy wff[1]) >))
       ;; Eliminate all collisions before generating bgmregress genome.
       (setq M (length genome))
       (loop for m from 1 until M do
         (setq chromosome0 genome[(- m 1)])
         (setq chromosome1 genome[m])
         (if (and (<> chromosome1 #void) (= chromosome0 chromosome1))
             (setq genome[(- m 1)] #void)
             ) ; end if
         ) ; end loop   
       (setq genome (sort genome >))
       (setq wffStringFormat (string (list rule genome) true))
       wffStringFormat) ; end stringWff

   	;; *******************************************************************************
   	;; Begin main logic 
   	;; *******************************************************************************
	vars:(wffSourceFormat)
    
    (setq wffSourceFormat (wffSource wff[1]))
    wffSourceFormat) ; end ruleBgm










































;;**EXPORTKEY**:esm:ruleEnn
(defriend esm:ruleEnn(wff)
;; *******************************************************************
;; summary:  The WFF grammar rules for all Selector evolutionary 
;;           neural net regression Lambdas. 
;;
;; Main:     Return a Selector neural net regression statement,
;;           in source format, such as 
;;                    "ennregress(#(obj| #(obj| #(num| 34.56 -23.4))));" 
;;           as an ASCI source string in grammatically correct selector.
;;
;; args:     wff         The Selector WFF of the n eural net regression statement.
;;           
;; Return:   result      A Selector neural net regression statement such as "ennregress(#(obj| #(obj| #(num| 34.56 -23.4))));".
;; *******************************************************************
  	pvars:(;; Public Variables
           (genomeType ENN)         ;; This regression rule uses an ENN weight genome.
           (lengthExempt true)      ;; This regression rule is exempt from genome length restrictions.
           ;; Public Methods
           crossOver                ;; Create two new Selector neural net regression WFFs by splicing two parent WFFs together using genetic crossover.
           growWFF                  ;; Create a new Selector neural net regression wff to the current population.
           mutate                   ;; Create a new Selector neural net regression WFF using genetic mutation.
           wffList                  ;; Convert a Selector neural net regression wff to a list format.
           wffSource				;; Convert a Selector neural net regression wff to a source statement such as "ennregress(#(obj| #(obj| #(num| 34.56 -23.4))));". 
           wffString                ;; Convert a Selector neural net regression wff to a string format such as "(ruleEnn #(obj| #(obj| #(num| 34.56 -23.4))));"
           ) ; end persistant variables
   	;; *******************************************************************************
   	;; Define Public Child Lambdas 
   	;; *******************************************************************************

	(defun crossOver(father mother)
	;; *******************************************************************
	;; summary:  Create two new Selector neural net regression WFFs, 
    ;;           by splicing two parent WFFs together using genetic crossover.
    ;;
    ;;           For the Evolutionary Neural Net, the crossover operator
    ;;           is a simplified form of particle swarm incremental approach. 
	;;
	;; args:     father     The father WFF to be mated.           
	;;           mother     The mother WFF to be mated.           
	;;           
	;; Return:   none.
	;; 
    ;; Note:     The father is always the more fit of the pair and is the
    ;;           WFF we wish to move incrementally toward.
	;; *******************************************************************
	   vars:(dadGenome momGenome dadRule momRule newGenome)
	
	   ;; Extract numeric WFFs from the parent Selector Lambdas (where necessary).
	   (if (or (= father #void) (= mother #void)) (return false))
	   (if (isLambda father) (begin (setq dadRule father.Rule) (setq father father.Genome)))
	   (if (isLambda mother) (begin (setq momRule mother.Rule) (setq mother mother.Genome)))
	   (if (isPair father) (begin (setq dadRule father[0]) (setq father father[1])))
	   (if (isPair mother) (begin (setq momRule mother[0]) (setq mother mother[1])))
	   (setq dadGenome father)
	   (setq momGenome mother)

	   ;; Make sure the father and mother are genetically compatible.
	   (if (or (= father #void) (= mother #void)) (return false))
	   (if (<> esm[momRule].genomeType ENN:) (return false))
       
	   ;; Reorder the weights in the candidate genomes toward the father.
       (setq newGenome (esm.ennRegress.mergeWeights dadGenome momGenome))

	   ;; Make a Selector neural net regression wff and add it to the current population.
       (growWFF newGenome dadGenome)
		
	   true) ; end crossOver
	
	(defun growWFF(...)
	;; *******************************************************************
	;; summary:  Grow a new Selector neural net regression WFF and
	;;           add it to the current population.
	;;
	;; args:     genome  	(Optional)The command to determine the type of WFF to grow.
	;;           prevGenome (Optional)The previous genome (for use by particle swarm operators).
	;;           
	;; Return:   Lambda      A scored Selector neural net regression Lambda.
	;;
	;; *******************************************************************
	   vars:(genome prevGenome)
	   vars:(Rf wff)

       ;; Grow a new Selector neural net regression WFF and add it to the current population.
       (if (>= (argCount) 1) 
           (setq genome (argFetch 0))          
           (setq genome (esm.ennRegress.initWeights esm.myENNLayers esm.myM))
           ) ; end if
       (if (>= (argCount) 2) (setq prevGenome (argFetch 1)) (setq prevGenome genome))       
       (setq Rf (createSelector (list ruleEnn: genome) prevGenome))
	
	   Rf) ; end growWFF

	(defun mutate(wff)
	;; *******************************************************************
	;; summary:  Create a new Selector neural net regression WFF 
    ;;           using genetic mutation.
	;;
	;; args:     wff     The candidate WFF to be mutated.           
	;;           
	;; Return:   none.
	;;
	;; *******************************************************************
       regs:(h H m ms M Number:w Number:r)
       vars:(NumVector:W ObjVector:WW)
	   vars:(rule genome prevGenome (Number:mutatePct .10))
	
	   ;; Extract numeric WFFs from the candidate Selector Lambda (where necessary).
	   (if (= wff #void) (return false))
	   (if (isLambda wff) (setq wff wff.Genome))
	   (if (isPair wff) (setq wff wff[1]))
	   (setq genome wff)
	   (if (= genome #void) (setq genome (growWFF)))
       (setq prevGenome genome)
	
	   ;; Make a copy of the candidate genome.
	   (setq genome (listWff genome))

 
       ;; Initialized the evolutionary neural net hidden layer regression weights.
       (setq H (length genome))
       (setq M (- (length genome[0]) 1))
       (setq WW genome)
       (loop for h from 0 until H do
         (loop for ms from 0 until M do
           (setq W WW[h][ms]) 
           (loop for m from 0 to M do
             (setq w W[m])
             (if (<= (random 1.0) mutatePct) (*= w (- (random 4.0) 2.0)))
             (setq W[m] w)
             ) ; end inner M loop
           ) ; end outter M loop
         ) ; end hidden layer loop

	   ;; Make a Selector neural net regression wff and add it to the current population.
       (growWFF genome prevGenome)
	
	   true) ; end mutate
		
    (defun wffList(wff)
	;; *******************************************************************
	;; summary:  Convert a selector neural net regression wff to a list format. 
	;;
	;; args:     wff         The neural net regression wff.
	;;           
	;; Return:   result      A Selector neural net regression WFF such as '(ruleEnn #(obj| (x0-x5) abs(x1)))
	;;
	;; *******************************************************************
       regs:(m M)
       vars:(wffListFormat rule genome)
       vars:(chromosome0 chromosome1)
       (if (isString wff) (setq wff wff) (setq wff (string wff true)))
       (setq wff (lisp wff)[0])
       (if (or (not (isPair wff)) (<> (length wff) 2) (<> wff[0] ruleEnn:)) (error "esm.ruleEnn.wffList: invalid WFF"))
       (setq rule wff[0])
       (setq genome wff[1])
       (setq wffListFormat (list rule genome))
       wffListFormat) ; end wffList


	(defun wffSource(genome)
	;; *******************************************************************
	;; summary:  Return a Selector neural net regression statement 
    ;;           such as 
    ;;                     "ennregress(#(obj| #(obj| #(num| 34.56 -23.4))));"
    ;;  
	;;           as an ASCI string in grammatically correct Selector.
	;;
	;; args:     genome      The neural net weight vector for the ennregress statement.
	;;           
	;; Return:   result      A Selector neural net regression statement such as "ennregress(#(obj| #(obj| #(num| 34.56 -23.4))));"
	;;
	;; *******************************************************************
       vars:((wffSourceFormat "") rule genome)
	    ;; Generate a gramatically correct Selector neural net regression statement in source format.
	   (setq wffSourceFormat (append "ennregress(" (string genome true) ");"))
	   wffSourceFormat) ; end wffSource    


   (defun wffString(wff)
	;; *******************************************************************
	;; summary:  Convert a selector neural net regression wff to a 
    ;;           string format. 
	;;
	;; args:     wff         The neural net regression wff.
	;;           
	;; Return:   result      A Selector neural net regression WFF such as "(ruleEnn #(obj| #(obj| #(num| 34.56 -23.4))))"
	;;
	;; *******************************************************************
       regs:(m M)
       vars:(wffStringFormat rule genome)
       vars:(chromosome0 chromosome1)
       (if (isString wff) (setq wff (lisp wff)[0]))
       (if (or (not (isPair wff)) (<> (length wff) 2) (<> wff[0] ruleEnn:)) (error "esm.ruleEnn.wffString: invalid WFF"))
       (setq rule wff[0])
       (setq genome wff[1])
       (setq wffStringFormat (string (list rule genome) true))
       wffStringFormat) ; end stringWff

   	;; *******************************************************************************
   	;; Begin main logic 
   	;; *******************************************************************************
	vars:(wffSourceFormat)
    
    (setq wffSourceFormat (wffSource wff[1]))
    wffSourceFormat) ; end ruleEnn










































;;**EXPORTKEY**:esm:ruleExp
(defriend esm:ruleExp(wff)
;; *******************************************************************
;; summary:  The WFF grammar rules for all Selector numeric expressions. 
;;
;; Main:     Return a Selector numeric expression, in source format, 
;;           such as 
;;                    "(x2*(x2-x10));" 
;;           as an ASCI source string in grammatically correct selector.
;;
;; args:     wff         The Selector WFF of the numeric expression.
;;           
;; Return:   result      A Selector numeric expression such as "(x2*(x2-x10));".
;; *******************************************************************
  pvars:(;; Public Variables
         ;; Expression operators
		 checkNumericWFF   		    ;; Checks a numeric Selector WFF for validity.
		 cutoutNumericWFF   		;; Returns a numeric Selector WFF by randomly cutting out a sub-expression from a candidate Selector WFF {"(x3/sin(x4))" ==> "sin(x4)"}.
         extractNumericWFF		    ;; Return a numeric Selector WFF from any Selector WFF {"regress (x3/x13);" ==> "(x3/x13)"}.
         growWFF                    ;; Returns a randomly grown Selector WFF. 
         growCTermWFF               ;; Returns a sequentially grown regression conditional term Selector WFF based upon a chosen column.
         growRootWFF                ;; Returns a sequentially grown root Selector WFF xtime, thru xm, abs(xtime) thru abs(xm), cos(xtime) thru cos(xm), etc. 
         growTermWFF                ;; Returns a sequentially grown regression basic term Selector WFF based upon a chosen column.
	  	 lengthWFF   		        ;; Returns the left-depth-first length of any Selector WFF.
         marryNumericWFFs		    ;; Return a numeric Selector WFF by marrying two suitor Selector WFFs {"x3" , "x4" ==> "(x3/x4)"}.
		 mutateNumericWFF   	    ;; Returns a numeric Selector WFF by randomly mutating a candidate Selector WFF {"(x3/x4)" ==> "(x3/sin(x4))"}.
		 selectPairWFF    	        ;; Returns the specified WFF as a vector of Pair objects. 
		 selectSlotWFF    	        ;; Returns the specified WFF as a vector of Slot items. 
		 spliceNumericWFF    	    ;; Returns a numeric Selector WFF by randomly splicing one WFF into a candidate Selector WFF {"(x3/sin(x4))" , "log(x10)" ==> "(log(x10)/sin(x4))"}.
         ;; Expression grammar production Rules
         ruleAbs             		;; Produce a Selector numeric absolute function such as "abs(x1)". 
         ruleAdd                   	;; Produce a Selector numeric addition such as x1 + xm. 
         ruleAvg                   	;; Produce a Selector numeric average such as avg(x1,xm). 
         ruleBgm                  	;; Produce a Selector BGM statement such as "bgmregress(sin(x1),x5,(x4/cos(x6)));". 
         ruleCos                   	;; Produce a Selector numeric cosine function such as "cos(x1)". 
         ruleCube                  	;; Produce a Selector numeric cube function such as "(x1*x1*x1)". 
         ruleDiv                   	;; Produce a Selector numeric protected division such as pdiv(x1,xm). (x3 == 0 ? x4 : x4 / x3) 
         ruleEnn                   	;; Produce a Selector ENN statement such as "ennregress(#(obj| #(obj| #(num| 34.56 -23.4))));". 
         ruleExp                   	;; Produce a Selector numeric exp function such as "exp(x1)". 
         ruleExpt                  	;; Produce a Selector exponent function such as "expt(abs(x1),x2)".
         ruleFrm                   	;; Produce a Selector multiple factor regression statement such as "frmregress (.23*x3,34.5*x4);". 
         ruleIf                    	;; Produce a Selector if expression such as "(if (x1 == x2) {5.0} else {1.0})".
         ruleInt                   	;; Produce a Selector numeric integer function such as "number(integer(x1))". 
         ruleInv                   	;; Produce a Selector inversion function such as "(1.0 /x1)". 
         ruleLog                   	;; Produce a Selector numeric log function such as "log(abs(x1))". 
         ruleMax                   	;; Produce a Selector numeric maximum such as max(x1,xm). 
         ruleMin                   	;; Produce a Selector numeric minimum such as min(x1,xm). 
         ruleMod                   	;; Produce a Selector numeric mod such as mod(x1,xm). 
         ruleMul                   	;; Produce a Selector numeric multiplication such as x1 * xm. 
         ruleMvl                   	;; Produce a Selector multiple linear regression statement such as "mvlregress (.23*x3,34.5*x4);". 
         ruleName                  	;; Produce a Selector element name such as xtime, or x1 thru xm.
         ruleNeg                   	;; Produce a Selector numeric negation such as -(x1 * xm). 
         ruleNop                   	;; Produce a Selector no operation.
         ruleNum                   	;; Produce a Selector numeric constant.
         ruleReg                   	;; Produce a Selector regress statement such as "regress x3*x4;". 
         ruleSign                  	;; Produce a Selector numeric sign function such as "sign(x1)". 
         ruleSin                   	;; Produce a Selector numeric sine function such as "sin(x1)". 
         ruleSqrt                  	;; Produce a Selector numeric square root function such as "sqrt(abs(x1))". 
         ruleSquare                	;; Produce a Selector numeric square function such as "(x1*x1)". 
         ruleSub                   	;; Produce a Selector numeric subtraction such as x1 - xm. 
         ruleSvm                   	;; Produce a Selector support vector regression statement such as "svmregress (.23*x3,34.5*x4);". 
         ruleTan                   	;; Produce a Selector numeric tangent function such as "tan(x1)". 
         ruleTanh                  	;; Produce a Selector numeric hyper tangent function such as "tanh(x1)". 
         ) ; end public variables
    ;; *******************************************************************************
    ;; Define Public Child Lambdas 
    ;; *******************************************************************************

    ;; *******************************************************************************
    ;; Define Expression operators 
    ;; *******************************************************************************
   
    (defun checkNumericWFF(wff)
    ;; *******************************************************************
    ;; summary:  Checks a numeric Selector WFF for validity.  
    ;;
    ;; args:     wff       The WFF which is to be verified.
    ;;           
    ;; Return:   wff       The new verified WFF.
    ;;
    ;; *******************************************************************
        vars:(rule)
        ;; Check to make sure the Selector WFF is valid.
        (setq rule wff[0])
        (cond
         ((isNumber wff) true)
         ((isSymbol wff) true)
         ((= rule ruleAbs:)
          (begin
            UnaryCheck::
            (if (<> (length wff) 2) (error (append "esm.ruleExp.checkNumericWFF: invalid length for unary grammar rule [" rule "]")))
          )) ; end ruleAbs case
         ((= rule ruleAdd:)
          (begin
            BinaryCheck::
            (if (<> (length wff) 3) (error (append "esm.ruleExp.checkNumericWFF: invalid length for binary grammar rule [" rule "]")))
          )) ; end ruleAdd case
         ((= rule ruleAvg:) (goto BinaryCheck:))
         ((= rule ruleCos:) (goto UnaryCheck:))
         ((= rule ruleCube:) (goto UnaryCheck:))
         ((= rule ruleDiv:) (goto BinaryCheck:))
         ((= rule ruleName:) (goto UnaryCheck:))
         ((= rule ruleNum:) (goto UnaryCheck:))
         ((= rule ruleExp:) (goto UnaryCheck:))
         ((= rule ruleExpt:) (goto BinaryCheck:))
         ((= rule ruleTanh:) (goto UnaryCheck:))
         ((= rule ruleIf:)
          (begin
            IfCheck::
            (if (<> (length wff) 6) (error (append "esm.ruleExp.checkNumericWFF: invalid length for if grammar rule [" rule "]")))
          )) ; end ruleIf case
         ((= rule ruleInt:) (goto UnaryCheck:))
         ((= rule ruleInv:) (goto UnaryCheck:))
         ((= rule ruleLog:) (goto UnaryCheck:))
         ((= rule ruleMax:) (goto BinaryCheck:))
         ((= rule ruleMin:) (goto BinaryCheck:))
         ((= rule ruleMod:) (goto BinaryCheck:))
         ((= rule ruleMul:) (goto BinaryCheck:))
         ((= rule ruleNeg:) (goto UnaryCheck:))
         ((= rule ruleNop:) (goto UnaryCheck:))
         ((= rule ruleSign:) (goto UnaryCheck:))
         ((= rule ruleSin:) (goto UnaryCheck:))
         ((= rule ruleSqrt:) (goto UnaryCheck:))
         ((= rule ruleSquare:) (goto UnaryCheck:))
         ((= rule ruleSub:) (goto BinaryCheck:))
         ((= rule ruleTan:) (goto UnaryCheck:))
         (else (error (append "esm.ruleExp.checkNumericWFF: unknown WFF grammar rule [" rule "]")))
         ) ; end cond
        wff) ; end checkNumericWFF    
    
	(defun cutoutNumericWFF(wff ...)
	;; *******************************************************************
	;; summary:  Returns a numeric Selector WFF by randomly cutting out a  
	;;           sub-expression from a candidate Selector WFF 
	;;           For example {"(x3/sin(x4))" ==> "sin(x4)"}. 
	;;
	;; args:     wff       The WFF from which the cutout is to be taken.
	;;           point     (Optional)The probability of cutting at this grammar rule.           
	;;           
	;; Return:   wff       The cut out sub-expression from a candidate Selector WFF.
	;;
	;; *******************************************************************
	    regs:(n N Number:point Number:argPoint Number:swPoint)
		vars:(rule stopHereSW)
		vars:((binaryRules #(obj| ruleAdd ruleAvg ruleDiv ruleExpt ruleMax ruleMin ruleMod ruleMul ruleSub)))
		vars:((relationalOperators #(obj| |<| |<=| |>=| |>|)))
		vars:((unaryRules #(obj| ruleAbs ruleCos ruleCube ruleExp ruleInt ruleInv ruleLog ruleNeg ruleSign ruleSin ruleSqrt ruleSquare ruleTan ruleTanh)))
		;; Use the grammar rule to guide the mutation of the candidate Selector WFF.
	    (if (= (argCount) 1) (setq point (random 1.0)) (setq point (argFetch 1)))
	    (if (<= (setq swPoint (random 1.0)) point) (setq stopHereSW true) (setq stopHereSW false))
	    (setq argPoint (random 1.0))
	    (setq rule wff[0])
	    (cond
		 ((isNumber wff) (return wff))
		 ((isSymbol wff) (return wff))
		 ((= rule ruleAbs:)
	      (begin
	        UnaryCutout::
	        (cond
	         ;; Case: Pass the cutout request to the next level.
	         ((and (= stopHereSW false) (isPair wff[1])) (return (cutoutNumericWFF wff[1] point)))
	         ;; Case: Cutout the unary grammar rule of this level.
	         (else (return wff))
	         ) ; end cond
	      )) ; end ruleAbs case
		 ((= rule ruleAdd:)
	      (begin
	        BinaryCutout::
	        (cond
	         ;; Case: Pass the cutout request to the next level through argument one.
	         ((and (= stopHereSW false) (< argPoint .50) (isPair wff[1])) (return (cutoutNumericWFF wff[1] point)))
	         ;; Case: Pass the cutout request to the next level through argument two.
	         ((and (= stopHereSW false) (isPair wff[2])) (return (cutoutNumericWFF wff[2] point)))
	         ;; Case: Cutout the binary grammar rule of this level.
	         (else (return wff))
	         ) ; end cond
	      )) ; end ruleAdd case
		 ((= rule ruleAvg:) (goto BinaryCutout:))
		 ((= rule ruleCos:) (goto UnaryCutout:))
		 ((= rule ruleCube:) (goto UnaryCutout:))
		 ((= rule ruleDiv:) (goto BinaryCutout:))
		 ((= rule ruleExp:) (goto UnaryCutout:))
		 ((= rule ruleExpt:) (goto BinaryCutout:))
		 ((= rule ruleTanh:) (goto UnaryCutout:))
		 ((= rule ruleIf:)
	      (begin
	        IfCutout::
	        (cond
	         ;; Case: Pass the cutout request to the next level through argument one.
	         ((and (= stopHereSW false) (isPair wff[1]) (< argPoint .25)) (return (cutoutNumericWFF wff[1] point)))
	         ;; Case: Pass the cutout request to the next level through argument three.
	         ((and (= stopHereSW false) (isPair wff[3]) (< argPoint .50)) (return (cutoutNumericWFF wff[3] point)))
	         ;; Case: Pass the cutout request to the next level through argument four.
	         ((and (= stopHereSW false) (isPair wff[4]) (< argPoint .75)) (return (cutoutNumericWFF wff[4] point)))
	         ;; Case: Pass the cutout request to the next level through argument five.
	         ((and (= stopHereSW false) (isPair wff[5]) (>= argPoint .75)) (return (cutoutNumericWFF wff[5] point)))
	         ;; Case: Cutout the if expression on this level.
	         (else (return wff))
	         ) ; end cond
	      )) ; end ruleIf case
		 ((= rule ruleInt:) (goto UnaryCutout:))
		 ((= rule ruleInv:) (goto UnaryCutout:))
		 ((= rule ruleLog:) (goto UnaryCutout:))
		 ((= rule ruleMax:) (goto BinaryCutout:))
		 ((= rule ruleMin:) (goto BinaryCutout:))
		 ((= rule ruleMod:) (goto BinaryCutout:))
		 ((= rule ruleMul:) (goto BinaryCutout:))
		 ((= rule ruleName:) (return (esm.ruleExp.ruleName wff[1])))
		 ((= rule ruleNeg:) (goto UnaryCutout:))
		 ((= rule ruleNop:) (goto UnaryCutout:))
		 ((= rule ruleNum:) (goto UnaryCutout:))
		 ((= rule ruleSign:) (goto UnaryCutout:))
		 ((= rule ruleSin:) (goto UnaryCutout:))
		 ((= rule ruleSqrt:) (goto UnaryCutout:))
		 ((= rule ruleSquare:) (goto UnaryCutout:))
		 ((= rule ruleSub:) (goto BinaryCutout:))
		 ((= rule ruleTan:) (goto UnaryCutout:))
	     (else (error (append "esm.ruleExp.cutoutNumericWFF: unknown WFF grammar rule [" rule "]")))
	     ) ; end cond
		wff) ; end cutoutNumericWFF    
	
    (defun extractNumericWFF(inWff ...)
	;; *******************************************************************
	;; summary:  Return a numeric Selector WFF from any Selector WFF.
	;;           For instance: "regress (x3/x13);" ==> "(x3/x13)".
	;;
	;; Note:     If the wff argument is a string, then it is returned "as is".
	;;
	;; args:     inWff       Any Selector input WFF or Selector Lambda.
	;;           sourceSW    (Optional) True iff the result expression is to be returned in source form.
	;;           
	;;           
	;; Return:   expression  The numeric WFF extracted from the specified Selector WFF.
	;;
	;; *******************************************************************
		regs:(m M)
		vars:(sourceSW expression rule wff)
	    ;; If the wff argument is an Lambda, then convert it to a WFF.
	    (if (isLambda inWff) (setq wff inWff.WFF) (setq wff inWff))
	    ;; If the wff argument is a string, then convert it to a list of Pair objects.
	    (if (isString wff) (setq wff (listWff wff)))
	    (setq sourceSW (if (>= (argCount) 2) (argFetch 1) false)) 
		;; Use the grammar rule to guide the extraction 
	    ;; of a numeric expression WFF from the specified 
	    ;; Selector WFF.
	    (setq rule wff[0])
	    (cond
		 ((isNumber wff) (setq expression wff))
		 ((isSymbol wff) (setq expression wff))
		 ((= rule ruleAbs:) (setq expression wff))
		 ((= rule ruleAdd:) (setq expression wff))
		 ((= rule ruleAvg:) (setq expression wff))
		 ((= rule ruleBgm:) (begin (setq M wff[1]) (setq m 0) (while (and (= expression #void) (= wff[1][m] #void)) do (++ m)) (setq expression wff[1][m])))
		 ((= rule ruleCos:) (setq expression wff))
		 ((= rule ruleCube:) (setq expression wff))
		 ((= rule ruleDiv:) (setq expression wff))
		 ((= rule ruleEnn:) (error (append "esm.ruleExp.extractNumericWFF: invalid WFF grammar rule for expression extraction [" rule "]")))
		 ((= rule ruleExp:) (setq expression wff))
		 ((= rule ruleExpt:) (setq expression wff))
		 ((= rule ruleFrm:) (begin (setq M wff[1]) (setq m 0) (while (and (= expression #void) (= wff[1][m] #void)) do (++ m)) (setq expression wff[1][m])))
		 ((= rule ruleIf:) (setq expression wff))
		 ((= rule ruleLog:) (setq expression wff))
		 ((= rule ruleInt:) (setq expression wff))
		 ((= rule ruleInv:) (setq expression wff))
		 ((= rule ruleMax:) (setq expression wff))
		 ((= rule ruleMin:) (setq expression wff))
		 ((= rule ruleMod:) (setq expression wff))
		 ((= rule ruleMul:) (setq expression wff))
		 ((= rule ruleMvl:) (begin (setq M wff[1]) (setq m 0) (while (and (= expression #void) (= wff[1][m] #void)) do (++ m)) (setq expression wff[1][m])))
		 ((= rule ruleName:) (setq expression (esm.ruleExp.ruleName wff[1])))
		 ((= rule ruleNeg:) (setq expression wff))
		 ((= rule ruleNop:) (setq expression wff))
		 ((= rule ruleNum:) (setq expression (esm.ruleExp.ruleNum wff[1])))
		 ((= rule ruleReg:) (begin (setq M wff[1]) (setq m 0) (while (and (= expression #void) (= wff[1][m] #void)) do (++ m)) (setq expression wff[1][m])))
		 ((= rule ruleSign:) (setq expression wff))
		 ((= rule ruleSin:) (setq expression wff))
		 ((= rule ruleSqrt:) (setq expression wff))
		 ((= rule ruleSquare:) (setq expression wff))
		 ((= rule ruleSub:) (setq expression wff))
		 ((= rule ruleSvm:) (begin (setq M wff[1]) (setq m 0) (while (and (= expression #void) (= wff[1][m] #void)) do (++ m)) (setq expression wff[1][m])))
		 ((= rule ruleTan:) (setq expression wff))
		 ((= rule ruleTanh:) (setq expression wff))
	     (else (setq expression x1:))
	     ) ; end cond
	    ;; Return the numeric expression as a Selector WFF or as a Selector source string.
	    (if (= expression #void) (setq expression x1:))
	    (if (= sourceSW true) then (setq expression (evalRule expression))) 
		expression) ; end extractNumericWFF    
	
    (defun growWFF(Symbol:rule Integer:level)
	;; *******************************************************************
	;; summary:  Returns a randomly grown Selector WFF. 
	;;
	;; args:     rule      The grammar rule indicating the type of WFF is to be returned.
	;;                     Note: The values for the rule argument as as follows:
	;;                           binary:		Generates a random binary operator expression.	            
	;;                           condition:     Generates a random "if" expression.	            
	;;                           expression:    Generates a random numeric expression.	            
	;;                           number:		Generates a random number constant.	            
	;;                           unary:		    Generates a random unary operator expression.	            
	;;                           variable:      Generates a random element name (xtime, xid, x1 thru xm).	            
	;;           level     The count of the current level in the expression.
	;;           
	;; Return:   result    The randomly grown Selector WFF.
	;;
	;; *******************************************************************
	    regs:(n N Number:r (maxLevels 4))
		vars:(wff newRule relop)
		vars:(commands expressions percents)
		vars:((binaryRules #(obj| ruleAdd ruleAvg ruleDiv ruleExpt ruleMax ruleMin ruleMod ruleMul ruleSub)))
		vars:((expressionRules #(obj| binary condition unary variable)))
		vars:((relationalOperators #(obj| < <= >= >))) ;; Note: we do not allow == or != as they are too restrictive.
		vars:((unaryRules #(obj| ruleAbs ruleCos ruleCube ruleExp ruleInt ruleInv ruleLog ruleNeg ruleSign ruleSin ruleSqrt ruleSquare ruleTan ruleTanh)))
		;; Use the grammar rule to guide the type of Selector WFF generated.
	    (cond
	     ;; Case binary:		Generates a random binary operator expression.	            
		 ((= rule binary:) 
	      (begin
	        (setq N (length binaryRules))
	        (setq n (integer (random N))) 
	        (setq newRule binaryRules[n])
	        (if (< level maxLevels)
	            (setq wff (list newRule (growWFF expression: (+ level 1)) (growWFF expression: (+ level 1))))
	            (setq wff (list newRule (growWFF variable: (+ level 1)) (growWFF term: (+ level 1))))
	            ) ; end if
	      )) ; end case binary
	
	     ;; Case condition:    Generates a random "if" expression.	            
		 ((= rule condition:) 
	      (begin
	        (setq newRule ruleIf:)
	        (setq N (length relationalOperators))
	        (setq n (integer (random N))) 
	        (setq relop relationalOperators[n])
	        (if (< level maxLevels)
	            (setq wff (list newRule (growWFF variable: (+ level 1)) relop (growWFF expression: (+ level 1)) (growWFF expression: (+ level 1)) (growWFF expression: (+ level 1))))
	            (setq wff (list newRule (growWFF variable: (+ level 1)) relop (growWFF term: (+ level 1)) (growWFF term: (+ level 1)) (growWFF term: (+ level 1))))
	            ) ; end if
	      )) ; end case condition
	
	     ;; Case expression:    Generates a random numeric expression.	            
		 ((= rule expression:) 
	      (begin
	        GrowExpression::
	        (setq N (length expressionRules))
	        (setq n (integer (random N)))
	        (cond 
	         ((< level maxLevels) (setq wff (growWFF (setq newRule expressionRules[n]) (+ level 1))))
	         (else (goto GrowTerm:))
	         ) ; end cond
	      )) ; end case expression
	
	     ;; Case number:    Generates a random number constant.	            
		 ((= rule number:) 
	      (begin
	        GrowNumber::
	        (setq r (- (random 10.0) 5.0))
	        (setq wff r)
	      )) ; end case number
	
	     ;; Case term:    Generates a random number constant or variable name.	            
		 ((= rule term:) 
	      (begin
	        GrowTerm::
	        (if (< (random 1.0) myNumberPct) (goto GrowNumber:) (goto GrowVariable:))
	      )) ; end case number
	
	     ;; Case unary:		Generates a random unary operator expression.	            
		 ((= rule unary:) 
	      (begin
	        (setq N (length unaryRules))
	        (setq n (integer (random N))) 
	        (setq newRule unaryRules[n])
	        (if (< level maxLevels)
	            (setq wff (list newRule (growWFF expression: (+ level 1))))
	            (setq wff (list newRule (growWFF variable: (+ level 1))))
	            ) ; end if
	      )) ; end case unary
	
	     ;; Case variable:		Generates a random element name (xtime, xid, x1 thru xm).	            
		 ((= rule variable:) 
	      (begin
	        GrowVariable::
	        (setq n (integer (random myM))) 
	        (setq wff (esm.ruleExp.ruleName n))
	      )) ; end case variable
	
	     ;; Case else
	     (else (error (append "esm.ruleExp.growWFF: unknown WFF grammar rule [" rule "]")))
	     ) ; end cond
	    ;; Return the randomly grown Selector WFF.
		wff) ; end growWFF    
	
	(defun growCTermWFF(Integer:columnChoice)
	;; *******************************************************************
	;; summary:  Returns a sequentially grown regression conditional term
	;;           Selector WFF based upon a chosen column, such as
	;;             xc 
	;;             xtime, thru xm, 
	;;             abs(ac) thru tan(xc), 
	;;                      ...
	;;             (xc+xtime) thru (xc+xm),
	;;                      ...
	;;             (xtime-xtime) thru (xm-xm),
	;;                      ...
	;;             if (xc+xtime) {xm-xm}, etc.
	;;           
	;;           The term Selector numeric expressions cover the starting
	;;           forms of ALL possible gramatically correct Selector WFFs
	;;           down to the first level of recursion including conditionals.           
	;;
	;; args:     columnChoice   The choice of the regression column for this Selector WFF expression.                        
	;;           
	;; Return:   wff          The sequentially grown conditional term Selector WFF.
	;;
	;; *******************************************************************
	    regs:(n N NN NO)
		vars:(wff)
		vars:((operators #("<" "<=" "==" "<>" ">=" ">")))
	
	    ;; Return the count of basic Selector term WFFs
	    (setq NN (growTermWFF -1 columnChoice))
	    (setq NO (length operators))
	
	    ;; **********************************************************
	    ;; Generate a sequential Selector root WFF without extension.
	    ;; **********************************************************
	
	    (setq choice (random 1.0))
	    (if (>= choice 0.20)
	        ;; Generate a Selector term WFF
	        (setq wff (growTermWFF (integer (random NN)) columnChoice))
	        ;; Generate a Selector conditional term WFF
	        (setq wff (list ruleIf: (growTermWFF (integer (random NN))  columnChoice)
	                                (symbol operators[(integer (random NO))])  
	                                (growTermWFF (integer (random NN))  (integer (random myM)))  
	                                (growTermWFF (integer (random NN))  (integer (random myM)))  
	                                (growTermWFF (integer (random NN))  (integer (random myM)))
	                                )) ; end list
	        ) ; end generate cterm only if
	
		wff) ; end growCTermWFF    
	
	(defun growRootWFF(Integer:rootChoice)
	;; *******************************************************************
	;; summary:  Returns a sequentially grown root Selector WFF such as
	;;             xtime, thru xm, 
	;;             abs(xtime) thru abs(xm), 
	;;             cos(xtime) thru cos(xm),
	;;                      ...
	;;             tan(xtime) thru tan(xm),
	;;             (xtime+xtime) thru (xm+xm),
	;;                      ...
	;;             (xtime-xtime) thru (xm-xm),
	;;           
	;;           The root Selector numeric expressions cover the starting
	;;           forms of ALL possible gramatically correct Selector WFFs
	;;           down to the first level of recursion.           
	;;
	;; args:     rootChoice   The choice of the sequential root WFF expression to return.                        
	;;           Note: if rootChoice <= -2, then a random root WFF is to be generated and extended randomly.
	;;           Note: if rootChoice == -1, then the maximum number of root WFFs is to be returned.
	;;           Note: if rootChoice >=  0, then a random root WFF is to be generated and extended randomly.
	;;                 Otherwise, rootChoice must be the index of the chosen Selector WFF root expression.
	;;           
	;; Return:   wff          The sequentially grown root Selector WFF.
	;;
	;; *******************************************************************
	    regs:(B m M n N NN)
		vars:(wff newRule Integer:choice)
		vars:((binaryRules #(obj| ruleAdd ruleAvg ruleDiv ruleExpt ruleMax ruleMin ruleMod ruleMul ruleSub)))
		vars:((unaryRules #(obj| ruleAbs ruleCos ruleCube ruleExp ruleInt ruleInv ruleLog ruleNeg ruleSign ruleSin ruleSqrt ruleSquare ruleTan ruleTanh)))
	
	    ;; **********************************************************
	    ;; Generate a sequential Selector root WFF without extension.
	    ;; **********************************************************
	    
	    (setq choice rootChoice)
	    (if (>= choice 0)
	        (begin         
	
	          ;; Generate all of the single variable names
	          (if (< choice myM)
	              (begin
	                 (setq wff (esm.ruleExp.ruleName choice))
	                 (return wff)
	              )) ; end variable names
	          (-= choice myM)
	          
	          ;; Generate all of the exponent functions with single variable names
	          (setq N 3)
	          (if (< choice N)
	              (begin
	                 (if (= choice 0) then (begin (setq wff (list ruleExpt: (esm.ruleExp.ruleName m) 0.5)) (return wff)))
	                 (if (= choice 1) then (begin (setq wff (list ruleExpt: (esm.ruleExp.ruleName m) 2.0)) (return wff)))
	                 (if (= choice 2) then (begin (setq wff (list ruleExpt: (esm.ruleExp.ruleName m) 3.0)) (return wff)))
	              )) ; end variable names
	          (-= choice N)
	          
	          ;; Generate all of the unary functions with single variable names
	          (setq N (muli (length unaryRules) myM))
	          (if (< choice N)
	              (begin
	                 (setq n (/ choice myM))
	                 (setq newRule unaryRules[n])
	                 (setq m (modi choice myM))
	                 (setq wff (list newRule (esm.ruleExp.ruleName m)))
	                 (return wff)
	              )) ; end variable names
	          (-= choice N)
	          
	          ;; Generate all of the binary functions with single variable names
	          (setq B (length binaryRules))
	          (setq M (muli myM myM))
	          (setq N (muli B myM myM))
	          (if (< choice N)
	              (begin
	                 (setq n (/ choice M))
	                 (setq newRule binaryRules[n])
	                 (setq n (modi choice M))
	                 (setq m (/ n myM))
	                 (setq n (modi n myM))
	                 (setq wff (list newRule (esm.ruleExp.ruleName m) (esm.ruleExp.ruleName n)))
	                 (return wff)
	              )) ; end variable names
	          (-= choice N)
	          
	          ;; Return void wff showing that there are no more root Selector WFFs available.
	          (return #void)
	          
	        )) ; end generate root only if
	
    ;; **********************************************************
    ;; Return the maximum number of root WFFs.
    ;; **********************************************************
    (setq NN myM)
    (setq N 3)(+= NN N)
    (setq N (muli (length unaryRules) myM))(+= NN N)
    (setq N (muli (length binaryRules) myM myM))(+= NN N)
    (if (= choice -1) (return NN))

    ;; **************************************************************
    ;; Generate a sequential Selector root WFF with random extension.
    ;; **************************************************************
    (setq choice (integer (random NN)))
    (setq wff (growRootWFF choice))
    (setq wff (mutateNumericWFF wff))

	wff) ; end growRootWFF    


	(defun growTermWFF(Integer:rootChoice Integer:columnChoice)
	;; *******************************************************************
	;; summary:  Returns a sequentially grown regression term Selector WFF 
	;;           based upon a chosen column, such as
	;;             xc 
	;;             xtime, thru xm, 
	;;             abs(ac) thru tan(xc), 
	;;                      ...
	;;             (xc+xtime) thru (xc+xm),
	;;                      ...
	;;             (xtime-xtime) thru (xm-xm),
	;;           
	;;           The root Selector numeric expressions cover the starting
	;;           forms of ALL possible gramatically correct Selector WFFs
	;;           down to the first level of recursion.           
	;;
	;; args:     rootChoice     The choice of the sequential root WFF expression to return.                        
	;;                           Note: if rootChoice <  -1, then a random term WFF is to be generated.
	;;                           Note: if rootChoice == -1, then the maximum number of root WFFs is to be returned.
	;;                           Note: if rootChoice >=  0, then a sequential term WFF is to be generated, and the rootChoice must be the index of the chosen Selector WFF.
	;;           columnChoice   The choice of the regression column for this Selector WFF expression.                        
	;;                           Note: if columnChoice <  0, then a random column choice is to be generated.
	;;           
	;; Return:   wff          The sequentially grown root Selector WFF.
	;;
	;; *******************************************************************
	    regs:(B m M mm MM n N nn NN)
		vars:(wff newRule Integer:choice)
		vars:((binaryRules #(obj| ruleAvg ruleAdd ruleDiv ruleExpt ruleMax ruleMod ruleMin ruleMul ruleSub)))
		vars:((unaryRules #(obj| ruleAbs ruleCos ruleCube ruleExp ruleInt ruleInv ruleLog ruleNeg ruleSign ruleSin ruleSqrt ruleSquare ruleTan ruleTanh)))
	
	    ;; Generate a random column choice (if necessary).
	    (if (< columnChoice 0) (setq columnChoice (integer (random myM))))
	    
	    ;; **********************************************************
	    ;; Generate a sequential Selector root WFF without extension.
	    ;; **********************************************************
	    
	    (setq choice rootChoice)
	    (if (>= choice 0)
	        (begin         
	
	          ;; Generate the chosen column variable name
	          (if (= choice 0)
	              (begin
	                 (setq wff (esm.ruleExp.ruleName columnChoice))
	                 (return wff)
	              )) ; end column variable names
	          (-= choice 1)
	
	          ;; Generate all of the root unary functions
	          (setq N (length unaryRules))
	          (if (< choice N)
	              (begin
	                 (setq newRule unaryRules[choice])
	                 (setq wff (list newRule (esm.ruleExp.ruleName columnChoice)))
	                 (return wff)
	              )) ; end root unary functions names
	          (-= choice N)
	          
	          ;; Generate all of the left root binary functions
	          (setq N (muli (length binaryRules) myM))
	          (if (< choice N)
	              (begin
	                 (setq n (/ choice myM))
	                 (setq newRule binaryRules[n])
	                 (setq m (modi choice myM))
	                 (setq wff (list newRule (esm.ruleExp.ruleName columnChoice) (esm.ruleExp.ruleName m)))
	                 (return wff)
	              )) ; end left root binary functions
	          (-= choice N)        
	          
	          ;; Generate all of the right root binary functions
	          (setq N (muli (length binaryRules) myM))
	          (if (< choice N)
	              (begin
	                 (setq n (/ choice myM))
	                 (setq newRule binaryRules[n])
	                 (setq m (modi choice myM))
	                 (setq wff (list newRule (esm.ruleExp.ruleName m) (esm.ruleExp.ruleName columnChoice)))
	                 (return wff)
	              )) ; end right root binary functions
	          (-= choice N) 
	          
	          ;; Generate all of the left binary unary functions
	          (setq N (muli (length binaryRules) (length unaryRules) myM))
	          (if (< choice N)
	              (begin
	                 (setq n (/ choice (muli (length unaryRules) myM)))
	                 (setq m (modi choice (muli (length unaryRules) myM)))
	                 (setq nn (/ m myM))
	                 (setq m (modi m myM))
	                 (setq wff (list binaryRules[n] (list unaryRules[nn] (esm.ruleExp.ruleName columnChoice)) (esm.ruleExp.ruleName m)))
	                 (return wff)
	              )) ; end left binary unary functions
	          (-= choice N)        
	
	          ;; Generate all of the right binary unary functions
	          (setq N (muli (length binaryRules) (length unaryRules) myM))
	          (if (< choice N)
	              (begin
	                 (setq n (/ choice (muli (length unaryRules) myM)))
	                 (setq m (modi choice (muli (length unaryRules) myM)))
	                 (setq nn (/ m myM))
	                 (setq m (modi m myM))
	                 (setq wff (list binaryRules[n] (esm.ruleExp.ruleName columnChoice) (list unaryRules[nn] (esm.ruleExp.ruleName m))))
	                 (return wff)
	              )) ; end right binary unary functions
	          (-= choice N)        
	                            
	          ;; Generate all of the left binary unary functions
	          (setq N (muli (length binaryRules) (length unaryRules) myM))
	          (if (< choice N)
	              (begin
	                 (setq n (/ choice (muli (length unaryRules) myM)))
	                 (setq m (modi choice (muli (length unaryRules) myM)))
	                 (setq nn (/ m myM))
	                 (setq m (modi m myM))
	                 (setq wff (list binaryRules[n] (list unaryRules[nn] (esm.ruleExp.ruleName columnChoice)) (esm.ruleExp.ruleName m)))
	                 (return wff)
	              )) ; end left binary unary functions
	          (-= choice N)        
	
	          ;; Generate all of the left binary unary unary functions
	          (setq N (muli (length binaryRules) (length binaryRules) (length unaryRules) myM))
	          (if (< choice N)
	              (begin
	                 (setq n (/ choice (muli (length binaryRules) (length unaryRules) myM)))
	                 (setq m (modi choice (muli (length binaryRules) (length unaryRules) myM)))
	                 (setq nn (/ m (muli (length unaryRules) myM)))
	                 (setq m (modi m (muli (length unaryRules) myM)))
	                 (setq mm (/ m myM))
	                 (setq m (modi m myM))
	                 (setq wff (list binaryRules[n] (list unaryRules[nn] (esm.ruleExp.ruleName columnChoice)) (list unaryRules[mm] (esm.ruleExp.ruleName m))))
	                 (return wff)
	              )) ; end left binary unary unary functions
	          (-= choice N)        
	
	          ;; Generate all of the right binary unary unary functions
	          (setq N (muli (length binaryRules) (length binaryRules) (length unaryRules) myM))
	          (if (< choice N)
	              (begin
	                 (setq n (/ choice (muli (length binaryRules) (length unaryRules) myM)))
	                 (setq m (modi choice (muli (length binaryRules) (length unaryRules) myM)))
	                 (setq nn (/ m (muli (length unaryRules) myM)))
	                 (setq m (modi m (muli (length unaryRules) myM)))
	                 (setq mm (/ m myM))
	                 (setq m (modi m myM))
	                 (setq wff (list binaryRules[n] (list unaryRules[nn] (esm.ruleExp.ruleName m)) (list unaryRules[mm] (esm.ruleExp.ruleName columnChoice))))
	                 (return wff)
	              )) ; end right binary unary unary functions
	          (-= choice N)        
	
	          ;; Return a random Selector term WFF if there are no more root Selector term WFFs available.
	          (return (growTermWFF -2 columnChoice))
	          
	        )) ; end generate root only if
	
	    ;; **********************************************************
	    ;; Return the maximum number of root WFFs.
	    ;; **********************************************************
	    (setq NN (+ 1 (length unaryRules) (* 2.0 (muli (length binaryRules) myM)) (* 3.0 (muli (length binaryRules) (length unaryRules) myM)) (* 2.0 (muli (length binaryRules) (length binaryRules) (length unaryRules) myM)) ))
	    (if (= choice -1) (return NN))
	
	    ;; **************************************************************
	    ;; Generate a random Selector term WFF.
	    ;; **************************************************************
	    TryAgain::
	    (setq choice (integer (random NN)))
	    (setq wff (growTermWFF choice columnChoice))
	    (if (= wff #void) (begin (writeln "esm.ruleExp.growTermWff: void WFF at [" choice "]") (goto TryAgain:)))
	
		wff) ; end growTermWFF    
	
	(defun lengthWFF(wff)
	;; *******************************************************************
	;; summary:  Returns the left-depth-first length of any Selector WFF. 
	;;
	;; args:     wff       The WFF whose length is to be returned.           
	;;           
	;; Return:   result    The left-depth-first length of the specified WFF.
	;;
	;; *******************************************************************
	    regs:(n N (result 0))
		vars:(rule)
	    ;; If the wff argument is a string, then convert it to a list of Pair objects.
	    (if (isString wff) (setq wff (listWff wff)))
	    ;; The length of a Vector of WFFs is always the
	    ;; sum of the lengths of each WFF in the vector.
	    (if (isVector wff) 
	        then
	        (begin
	          (setq N (length wff))
	          (loop for n from 0 until N do
	            (+= result (lengthWFF wff[n]))
	            ) ; end loop 
	          (return result)
	        )) ; end if
	    ;; The length of an atomic WFF is always 1.
	    (if (isAtom wff) then (return 1))
		;; Use the grammar rule to guide the computation 
	    ;; of the specified Selector WFF length.
	    (setq rule wff[0])
	    (cond
		 ((= rule ruleAbs:) (+= result (lengthWFF wff[1])))
		 ((= rule ruleAdd:) (+= result (+ (lengthWFF wff[1]) (lengthWFF wff[2]))))
		 ((= rule ruleAvg:) (+= result (+ (lengthWFF wff[1]) (lengthWFF wff[2]))))
		 ((= rule ruleBgm:) (+= result (lengthWFF wff[1])))
		 ((= rule ruleCos:) (+= result (lengthWFF wff[1])))
		 ((= rule ruleCube:) (+= result (lengthWFF wff[1])))
		 ((= rule ruleDiv:) (+= result (+ (lengthWFF wff[1]) (lengthWFF wff[2]))))
		 ((= rule ruleEnn:) (+= result 1))
		 ((= rule ruleExp:) (+= result (lengthWFF wff[1])))
		 ((= rule ruleExpt:) (+= result (+ (lengthWFF wff[1]) (lengthWFF wff[2]))))
		 ((= rule ruleFrm:) (+= result (lengthWFF wff[1])))
		 ((= rule ruleIf:) (+= result (+ (lengthWFF wff[1]) (lengthWFF wff[2]) (lengthWFF wff[4]) (lengthWFF wff[5]))))
		 ((= rule ruleInt:) (+= result (lengthWFF wff[1])))
		 ((= rule ruleInv:) (+= result (lengthWFF wff[1])))
		 ((= rule ruleLog:) (+= result (lengthWFF wff[1])))
		 ((= rule ruleMax:) (+= result (+ (lengthWFF wff[1]) (lengthWFF wff[2]))))
		 ((= rule ruleMin:) (+= result (+ (lengthWFF wff[1]) (lengthWFF wff[2]))))
		 ((= rule ruleMod:) (+= result (+ (lengthWFF wff[1]) (lengthWFF wff[2]))))
		 ((= rule ruleMul:) (+= result (+ (lengthWFF wff[1]) (lengthWFF wff[2]))))
		 ((= rule ruleMvl:) (+= result (lengthWFF wff[1])))
		 ((= rule ruleName:) (setq result 1))
		 ((= rule ruleNeg:) (+= result (lengthWFF wff[1])))
		 ((= rule ruleNop:) (+= result (lengthWFF wff[1])))
		 ((= rule ruleNum:) (setq result 1))
		 ((= rule ruleReg:) (+= result (lengthWFF wff[1])))
		 ((= rule ruleSvm:) (+= result (lengthWFF wff[1])))
		 ((= rule ruleSign:) (+= result (lengthWFF wff[1])))
		 ((= rule ruleSin:) (+= result (lengthWFF wff[1])))
		 ((= rule ruleSqrt:) (+= result (lengthWFF wff[1])))
		 ((= rule ruleSquare:) (+= result (lengthWFF wff[1])))
		 ((= rule ruleSub:) (+= result (+ (lengthWFF wff[1]) (lengthWFF wff[2]))))
		 ((= rule ruleSvm:) (+= result (lengthWFF wff[1])))
		 ((= rule ruleTan:) (+= result (lengthWFF wff[1])))
		 ((= rule ruleTanh:) (+= result (lengthWFF wff[1])))
	     (else (error (append "esm.ruleExp.lengthWFF: unknown WFF grammar rule [" rule "]")))
	     ) ; end cond
		result) ; end lengthWFF    
	
	(defun marryNumericWFFs(wff1 wff2 ...)
	;; *******************************************************************
	;; summary:  Return a numeric Selector WFF by marrying two suitor 
	;;           Selector WFFs.
	;;           For instance: "x3" , "x4" ==> "(x3/x4)".
	;;
	;; Note:     If the wff argument is a string, then it is returned "as is".
	;;
	;; args:     wff1      The male WFF suitor for this marriage.           
	;;           wff2      The female WFF suitor for this marriage.           
	;;           operator  (Optional) The binary operator for this marriage.           
	;;           
	;; Return:   expression  The numeric WFF resulting from the marriage.
	;;
	;; *******************************************************************
	    vars:(expression newRule len1 len2)
		vars:((binaryOperators #(obj| ruleAdd ruleAvg ruleDiv ruleExpt ruleMax ruleMin ruleMod ruleMul ruleSub)))
	    ;; If the wff argument is a string, then convert it to a list of Pair objects.
	    (if (= wff1 #void) (return false))
	    (if (= wff2 #void) (return false))
	    (if (isString wff1) (setq wff1 (listWff wff1)))
	    (if (isString wff2) (setq wff2 (listWff wff2)))
	    ;; Make sure we can marry these two WFFs without exceeding the WFF length limit.
	    (setq len1 (lengthWFF wff1))
	    (setq len2 (lengthWFF wff2))
	    (if (>= (+ len1 len2) myMaxColWFFLen) (return false)) 
		;; Use the grammar rule to guide the extraction 
	    ;; of a numeric expression WFF from the specified 
	    ;; Selector WFF.
	    (if (= (argCount) 3) 
	        (setq newRule (argFetch 2))
	        (setq newRule binaryOperators[(integer (random (length binaryOperators)))])
	        ) ; end if
	    (setq expression (list newRule wff1 wff2))
		expression) ; end marryNumericWFFs    
		
	(defun mutateNumericWFF(wff ...)
	;; *******************************************************************
	;; summary:  Returns a numeric Selector WFF by randomly mutating a 
	;;           candidate Selector WFF {"(x3/x4)" ==> "(x3/sin(x4)"}. 
	;;
	;; args:     wff       The WFF which is to be mutated.
	;;           point     (Optional)The probability of mutating this grammar rule.           
	;;           
	;; Return:   wff       The new mutated WFF.
	;;
	;; *******************************************************************
	    regs:(n N Number:point Number:argPoint Number:swPoint)
		vars:(rule stopHereSW)
		vars:((binaryRules #(obj| ruleAdd ruleAvg ruleDiv ruleExpt ruleMax ruleMin ruleMod ruleMul ruleSub)))
		vars:((relationalOperators #(obj| |<| |<=| |>=| |>|)))
		vars:((unaryRules #(obj| ruleAbs ruleCos ruleCube ruleExp ruleInt ruleInv ruleLog ruleNeg ruleSign ruleSin ruleSqrt ruleSquare ruleTan ruleTanh)))
		;; Use the grammar rule to guide the mutation of the candidate Selector WFF.
	    (if (= (argCount) 1) (setq point (random 1.0)) (setq point (argFetch 1)))
	    (if (<= (setq swPoint (random 1.0)) point) (setq stopHereSW true) (setq stopHereSW false))
	    (setq argPoint (random 1.0))
	    (if (isString wff) 
	        (if (isNumber (parse wff)) (setq wff (parse wff)) (setq wff (symbol wff)))
	        ); end if
	    (setq rule wff[0])
	    (cond
		 ((= wff #void) (setq wff (growWFF expression: 0)))
		 ((isNumber wff) (if (< argPoint .80) (setq wff (* wff (- (random 10.0) 5.0))) (setq wff (growWFF expression: 0))))
		 ((isSymbol wff) (if (< argPoint .80) (setq wff (esm.ruleExp.ruleName (integer (random myM)))) (setq wff (growWFF expression: 0))))
		 ((= rule ruleAbs:)
	      (begin
	        UnaryMutate::
	        (cond
	         ;; Case: Pass the mutation request to the next level.
	         ((= stopHereSW false) (setCar (cdr wff) (mutateNumericWFF wff[1] point)))
	         ;; Case: Replace the unary argument of this level.
	         ((>= argPoint .50) (if (isAtom wff[1]) (setCar (cdr wff) (mutateNumericWFF wff[1] point)) (setCar (cdr wff) (growWFF expression: 0))))
	         ;; Case: Replace the unary grammar rule of this level.
	         (else (setCar wff unaryRules[(random (length unaryRules))]))
	         ) ; end cond
	      )) ; end ruleAbs case
		 ((= rule ruleAdd:)
	      (begin
	        BinaryMutate::
	        (cond
	         ;; Case: Pass the mutation request to the next level through argument one.
	         ((and (= stopHereSW false) (< argPoint .50)) (setCar (cdr wff) (mutateNumericWFF wff[1] point)))
	         ;; Case: Pass the mutation request to the next level through argument two.
	         ((= stopHereSW false) (setCar (cddr wff) (mutateNumericWFF wff[2] point)))
	         ;; Case: Replace binary argument two of this level.
	         ((> argPoint .6666) (if (isAtom wff[2]) (setCar (cddr wff) (mutateNumericWFF wff[2] point)) (setCar (cddr wff) (growWFF expression: 0))))
	         ;; Case: Replace binary argument one of this level.
	         ((> argPoint .3333) (if (isAtom wff[1]) (setCar (cdr wff) (mutateNumericWFF wff[1] point)) (setCar (cdr wff) (growWFF expression: 0))))
	         ;; Case: Replace the unary grammar rule of this level.
	         (else (setCar wff binaryRules[(random (length binaryRules))]))
	         ) ; end cond
	      )) ; end ruleAdd case
		 ((= rule ruleAvg:) (goto BinaryMutate:))
		 ((= rule ruleCos:) (goto UnaryMutate:))
		 ((= rule ruleCube:) (goto UnaryMutate:))
		 ((= rule ruleDiv:) (goto BinaryMutate:))
		 ((= rule ruleExp:) (goto UnaryMutate:))
		 ((= rule ruleExpt:) (goto BinaryMutate:))
		 ((= rule ruleIf:)
	      (begin
	        IfMutate::
	        (cond
	         ;; Case: Pass the mutation request to the next level through argument one.
	         ((and (= stopHereSW false) (< argPoint .20)) (setCar (cdr wff) (mutateNumericWFF wff[1] point)))
	         ;; Case: Pass the mutation request to the next level through argument two.
	         ((and (= stopHereSW false) (< argPoint .40)) (setCar (cddr wff) relationalOperators[(random (length relationalOperators))]))
	         ;; Case: Pass the mutation request to the next level through argument three.
	         ((and (= stopHereSW false) (< argPoint .60)) (setCar (cdddr wff) (mutateNumericWFF wff[3] point)))
	         ;; Case: Pass the mutation request to the next level through argument four.
	         ((and (= stopHereSW false) (< argPoint .80)) (setCar (cddddr wff) (mutateNumericWFF wff[4] point)))
	         ;; Case: Pass the mutation request to the next level through argument five.
	         ((and (= stopHereSW false) (>= argPoint .80)) (setCar (cdr (cddddr wff)) (mutateNumericWFF wff[5] point)))
	         ;; Case: Replace argument five of this level.
	         ((>= argPoint .80) (if (isAtom wff[5]) (setCar (cdr (cddddr wff)) (mutateNumericWFF wff[5] point)) (setCar (cdr (cddddr wff)) (growWFF expression: 0))))
	         ;; Case: Replace argument four of this level.
	         ((>= argPoint .60) (if (isAtom wff[4]) (setCar (cddddr wff) (mutateNumericWFF wff[4] point)) (setCar (cddddr wff) (growWFF expression: 0))))
	         ;; Case: Replace argument three of this level.
	         ((>= argPoint .40) (if (isAtom wff[3]) (setCar (cdddr wff) (mutateNumericWFF wff[3] point)) (setCar (cdddr wff) (growWFF expression: 0))))
	         ;; Case: Replace argument two of this level.
	         ((>= argPoint .20) (setCar (cddr wff) relationalOperators[(random (length relationalOperators))]))
	         ;; Case: Replace binary argument one of this level.
	         (else (if (isAtom wff[1]) (setCar (cdr wff) (mutateNumericWFF wff[1] point)) (setCar (cdr wff) (growWFF expression: 0))))
	         ) ; end cond
	      )) ; end ruleIf case
		 ((= rule ruleInt:) (goto UnaryMutate:))
		 ((= rule ruleInv:) (goto UnaryMutate:))
		 ((= rule ruleLog:) (goto UnaryMutate:))
		 ((= rule ruleMax:) (goto BinaryMutate:))
		 ((= rule ruleMin:) (goto BinaryMutate:))
		 ((= rule ruleMod:) (goto BinaryMutate:))
		 ((= rule ruleMul:) (goto BinaryMutate:))
		 ((= rule ruleName:) (setq wff (esm.ruleExp.ruleName (integer (random myM)))))
		 ((= rule ruleNeg:) (goto UnaryMutate:))
		 ((= rule ruleNop:) (goto UnaryMutate:))
		 ((= rule ruleNum:) (if (< argPoint .80) (setq wff[1] (* wff[1] (- (random 10.0) 5.0))) (setq wff[1] (growWFF expression: 0))))
		 ((= rule ruleSign:) (goto UnaryMutate:))
		 ((= rule ruleSin:) (goto UnaryMutate:))
		 ((= rule ruleSqrt:) (goto UnaryMutate:))
		 ((= rule ruleSquare:) (goto UnaryMutate:))
		 ((= rule ruleSub:) (goto BinaryMutate:))
		 ((= rule ruleTan:) (goto UnaryMutate:))
		 ((= rule ruleTanh:) (goto UnaryMutate:))
	     (else (error (append "esm.ruleExp.mutateNumericWFF: unknown WFF grammar rule [" rule "]")))
	     ) ; end cond
		wff) ; end mutateNumericWFF    
	
	(defun selectPairWFF(wff ...)
	;; *******************************************************************
	;; summary:  Returns the specified WFF as a vector of Pair objects. 
	;;
	;; args:     wff       The WFF whose Pair objects are to be returned.           
	;;           index     (Optional)The vector of Pair objects to be returned.           
	;;           
	;; Return:   index     The vector of Pair objects to be returned.
	;;
	;; *******************************************************************
	    regs:(n N)
		vars:(rule index)
	    ;; Retrieve the optional index argument.
        (if (> (argCount) 1) then (setq index (argFetch 1))) 
        (if (not (isVector index)) (setq index (new Vector: Object:)))
	    ;; If the wff argument is a string, then convert it to a list of Pair objects.
	    (if (isString wff) (setq wff (listWff wff)))
	    ;; Return the original argument if it is not a Pair or if the selection index is zero.
	    (if (isAtom wff) then (return index))
		;; Use the grammar rule to guide the computation of the specified Selector WFF length.
	    ;; Note: We are at a Pair object (the head of this list) so we always add it to the index vector.
	    (setq rule wff[0])
        (setq index[(length index)] wff)
	    (cond
		 ((= rule ruleAbs:) (selectPairWFF wff[1] index))
		 ((= rule ruleAdd:) (begin (selectPairWFF wff[1] index) (selectPairWFF wff[2] index)))
		 ((= rule ruleAvg:) (begin (selectPairWFF wff[1] index) (selectPairWFF wff[2] index)))
		 ((= rule ruleCos:) (selectPairWFF wff[1] index))
		 ((= rule ruleCube:) (selectPairWFF wff[1] index))
		 ((= rule ruleDiv:) (begin (selectPairWFF wff[1] index) (selectPairWFF wff[2] index)))
		 ((= rule ruleExp:) (selectPairWFF wff[1] index))
		 ((= rule ruleExpt:) (begin (selectPairWFF wff[1] index) (selectPairWFF wff[2] index)))
		 ((= rule ruleIf:) (begin (selectPairWFF wff[1] index) (selectPairWFF wff[2] index) (selectPairWFF wff[3] index) (selectPairWFF wff[4] index) (selectPairWFF wff[5] index)))
		 ((= rule ruleInt:) (selectPairWFF wff[1] index))
		 ((= rule ruleInv:) (selectPairWFF wff[1] index))
		 ((= rule ruleLog:) (selectPairWFF wff[1] index))
		 ((= rule ruleMax:) (begin (selectPairWFF wff[1] index) (selectPairWFF wff[2] index)))
		 ((= rule ruleMin:) (begin (selectPairWFF wff[1] index) (selectPairWFF wff[2] index)))
		 ((= rule ruleMod:) (begin (selectPairWFF wff[1] index) (selectPairWFF wff[2] index)))
		 ((= rule ruleMul:) (begin (selectPairWFF wff[1] index) (selectPairWFF wff[2] index)))
		 ((= rule ruleName:) (selectPairWFF wff[1] index))
		 ((= rule ruleNeg:) (selectPairWFF wff[1] index))
		 ((= rule ruleNop:) (selectPairWFF wff[1] index))
		 ((= rule ruleNum:) (selectPairWFF wff[1] index))
		 ((= rule ruleSign:) (selectPairWFF wff[1] index))
		 ((= rule ruleSin:) (selectPairWFF wff[1] index))
		 ((= rule ruleSqrt:) (selectPairWFF wff[1] index))
		 ((= rule ruleSquare:) (selectPairWFF wff[1] index))
		 ((= rule ruleSub:) (begin (selectPairWFF wff[1] index) (selectPairWFF wff[2] index)))
		 ((= rule ruleTan:) (selectPairWFF wff[1] index))
		 ((= rule ruleTanh:) (selectPairWFF wff[1] index))
	     (else (error (append "esm.ruleExp.selectPairWFF: unknown WFF grammar rule [" rule "]")))
	     ) ; end cond
		index) ; end selectPairWFF    

	(defun selectSlotWFF(wff ...)
	;; *******************************************************************
	;; summary:  Returns the specified WFF as a vector of Slot items. 
	;;
	;; args:     wff       The WFF whose slot items are to be returned.           
	;;           index     (Optional)The vector of slot items to be returned.           
	;;           
	;; Return:   index     The vector of slot items to be returned.
	;;
	;; *******************************************************************
	    regs:(n N)
		vars:(rule index)
	    ;; Retrieve the optional index argument.
        (if (> (argCount) 1) then (setq index (argFetch 1))) 
        (if (not (isVector index)) (setq index (new Vector:)))
	    ;; If the wff argument is a string, then convert it to a list of Pair objects.
	    (if (isString wff) (setq wff (listWff wff)))
	    ;; Return the original argument if it is not a Pair or if the selection index is zero.
	    (if (isAtom wff) then (return (setq index[(length index)] wff)))
		;; Use the grammar rule to guide the computation of the specified Selector WFF length.
	    ;; Note: We are at a Pair object (the head of this list) so we always add it to the index vector.
	    (setq rule wff[0])
        (setq index[(length index)] wff)
	    (cond
		 ((= rule ruleAbs:) (selectSlotWFF wff[1] index))
		 ((= rule ruleAdd:) (begin (selectSlotWFF wff[1] index) (selectSlotWFF wff[2] index)))
		 ((= rule ruleAvg:) (begin (selectSlotWFF wff[1] index) (selectSlotWFF wff[2] index)))
		 ((= rule ruleCos:) (selectSlotWFF wff[1] index))
		 ((= rule ruleCube:) (selectSlotWFF wff[1] index))
		 ((= rule ruleDiv:) (begin (selectSlotWFF wff[1] index) (selectSlotWFF wff[2] index)))
		 ((= rule ruleExp:) (selectSlotWFF wff[1] index))
		 ((= rule ruleExpt:) (begin (selectSlotWFF wff[1] index) (selectSlotWFF wff[2] index)))
		 ((= rule ruleIf:) (begin (selectSlotWFF wff[1] index) (selectSlotWFF wff[2] index) (selectSlotWFF wff[3] index) (selectSlotWFF wff[4] index) (selectSlotWFF wff[5] index)))
		 ((= rule ruleInt:) (selectSlotWFF wff[1] index))
		 ((= rule ruleInv:) (selectSlotWFF wff[1] index))
		 ((= rule ruleLog:) (selectSlotWFF wff[1] index))
		 ((= rule ruleMax:) (begin (selectSlotWFF wff[1] index) (selectSlotWFF wff[2] index)))
		 ((= rule ruleMin:) (begin (selectSlotWFF wff[1] index) (selectSlotWFF wff[2] index)))
		 ((= rule ruleMod:) (begin (selectSlotWFF wff[1] index) (selectSlotWFF wff[2] index)))
		 ((= rule ruleMul:) (begin (selectSlotWFF wff[1] index) (selectSlotWFF wff[2] index)))
		 ((= rule ruleName:) (selectSlotWFF wff[1] index))
		 ((= rule ruleNeg:) (selectSlotWFF wff[1] index))
		 ((= rule ruleNop:) (selectSlotWFF wff[1] index))
		 ((= rule ruleNum:) (selectSlotWFF wff[1] index))
		 ((= rule ruleSign:) (selectSlotWFF wff[1] index))
		 ((= rule ruleSin:) (selectSlotWFF wff[1] index))
		 ((= rule ruleSqrt:) (selectSlotWFF wff[1] index))
		 ((= rule ruleSquare:) (selectSlotWFF wff[1] index))
		 ((= rule ruleSub:) (begin (selectSlotWFF wff[1] index) (selectSlotWFF wff[2] index)))
		 ((= rule ruleTan:) (selectSlotWFF wff[1] index))
		 ((= rule ruleTanh:) (selectSlotWFF wff[1] index))
	     (else (error (append "esm.ruleExp.selectSlotWFF: unknown WFF grammar rule [" rule "]")))
	     ) ; end cond
		index) ; end selectSlotsWFF    

	(defun spliceNumericWFF(wff swff ...)
	;; *******************************************************************
	;; summary:  Returns a numeric Selector WFF by randomly splicing one WFF 
	;;           into a candidate Selector WFF.  
	;;           For example {"(x3/sin(x4))" , "(log(x10)" ==> "(log(x10)/sin(x4))"}. 
	;;
	;; args:     wff       The WFF which is to be altered.
	;;           swff      The WFF which is to be spliced.
	;;           point     (Optional) The probability of splicing this grammar rule.
    ;;                     Note: If omitted, the probability of splicing this node is 100%.           
	;;           
	;; Return:   wff       The new spliced WFF.
	;;
	;; *******************************************************************
	    regs:(n N Number:point Number:argPoint Number:swPoint)
		vars:(rule stopHereSW)
		vars:((binaryRules #(obj| ruleAdd ruleAvg ruleDiv ruleExpt ruleMax ruleMin ruleMod ruleMul ruleSub)))
		vars:((relationalOperators #(obj| |<| |<=| |>=| |>|)))
		vars:((unaryRules #(obj| ruleAbs ruleCos ruleCube ruleExp ruleInt ruleInv ruleLog ruleNeg ruleSign ruleSin ruleSqrt ruleSquare ruleTan ruleTanh)))
		;; Use the grammar rule to guide the mutation of the candidate Selector WFF.
	    (if (= (argCount) 2) (setq point (random 1.0)) (setq point (argFetch 2)))
	    (if (<= (setq swPoint (random 1.0)) point) (setq stopHereSW true) (setq stopHereSW false))
	    (setq argPoint (random 1.0))
	    (setq rule wff[0])
	    (cond
		 ((isNumber wff) (setq wff swff))
		 ((isSymbol wff) (setq wff swff))
		 ((= rule ruleAbs:)
	      (begin
	        UnarySplice::
	        (cond
	         ;; Case: Pass the splice request to the next level.
	         ((= stopHereSW false) (if (isAtom wff[1]) (setCar (cdr wff) swff) (spliceNumericWFF wff[1] swff point)))
	         ;; Case: Replace unary argument one of this level.
	         (else (setCar (cdr wff) swff))
	         ) ; end cond
	      )) ; end ruleAbs case
		 ((= rule ruleAdd:)
	      (begin
	        BinarySplice::
	        (cond
	         ;; Case: Pass the splice request to the next level through argument one.
	         ((and (= stopHereSW false) (< argPoint .50)) (if (isAtom wff[1]) (setCar (cdr wff) swff) (spliceNumericWFF wff[1] swff point)))
	         ;; Case: Pass the splice request to the next level through argument two.
	         ((= stopHereSW false) (if (isAtom wff[2]) (setCar (cddr wff) swff) (spliceNumericWFF wff[2] swff point)))
	         ;; Case: Replace binary argument two of this level.
	         ((> argPoint .50) (setCar (cddr wff) swff))
	         ;; Case: Replace binary argument one of this level.
	         (else (setCar (cdr wff) swff))
	         ) ; end cond
	      )) ; end ruleAdd case
		 ((= rule ruleAvg:) (goto BinarySplice:))
		 ((= rule ruleCos:) (goto UnarySplice:))
		 ((= rule ruleCube:) (goto UnarySplice:))
		 ((= rule ruleDiv:) (goto BinarySplice:))
		 ((= rule ruleExp:) (goto UnarySplice:))
		 ((= rule ruleExpt:) (goto BinarySplice:))
		 ((= rule ruleIf:)
	      (begin
	        IfSplice::
	        (cond
	         ;; Case: Pass the splice request to the next level through argument one.
	         ((and (= stopHereSW false) (< argPoint .25)) (if (isAtom wff[1]) (setCar (cdr wff) swff) (spliceNumericWFF wff[1] swff point)))
	         ;; Case: Pass the splice request to the next level through argument three.
	         ((and (= stopHereSW false) (< argPoint .50)) (if (isAtom wff[3]) (setCar (cdddr wff) swff) (spliceNumericWFF wff[3] swff point)))
	         ;; Case: Pass the splice request to the next level through argument four.
	         ((and (= stopHereSW false) (< argPoint .75)) (if (isAtom wff[4]) (setCar (cddddr wff) swff) (spliceNumericWFF wff[4] swff point)))
	         ;; Case: Pass the splice request to the next level through argument five.
	         ((and (= stopHereSW false) (>= argPoint .75)) (if (isAtom wff[5]) (setCar (cdr (cddddr wff)) swff) (spliceNumericWFF wff[5] swff point)))
	         ;; Case: Replace argument five of this level.
	         ((>= argPoint .75) (setCar (cdr (cddddr wff)) swff))
	         ;; Case: Replace argument four of this level.
	         ((>= argPoint .50) (setCar (cddddr wff) swff))
	         ;; Case: Replace argument three of this level.
	         ((>= argPoint .25) (setCar (cdddr wff) swff))
	         ;; Case: Replace binary argument one of this level.
	         (else (setCar (cdr wff) swff))
	         ) ; end cond
	      )) ; end ruleIf case
		 ((= rule ruleInt:) (goto UnarySplice:))
		 ((= rule ruleInv:) (goto UnarySplice:))
		 ((= rule ruleLog:) (goto UnarySplice:))
		 ((= rule ruleMax:) (goto BinarySplice:))
		 ((= rule ruleMin:) (goto BinarySplice:))
		 ((= rule ruleMod:) (goto BinarySplice:))
		 ((= rule ruleMul:) (goto BinarySplice:))
		 ((= rule ruleName:) (setq wff  swff))
		 ((= rule ruleNeg:) (goto UnarySplice:))
		 ((= rule ruleNop:) (goto UnarySplice:))
		 ((= rule ruleNum:) (setq wff  swff))
		 ((= rule ruleSign:) (goto UnarySplice:))
		 ((= rule ruleSin:) (goto UnarySplice:))
		 ((= rule ruleSqrt:) (goto UnarySplice:))
		 ((= rule ruleSquare:) (goto UnarySplice:))
		 ((= rule ruleSub:) (goto BinarySplice:))
		 ((= rule ruleTan:) (goto UnarySplice:))
		 ((= rule ruleTanh:) (goto UnarySplice:))
	     (else (error (append "esm.ruleExp.spliceNumericWFF: unknown WFF grammar rule [" rule "]")))
	     ) ; end cond
		wff) ; end spliceNumericWFF    
	
    ;; *******************************************************************************
    ;; Define Expression grammar production Rules 
    ;; *******************************************************************************

	(defun ruleAbs(wff1)
	;; *******************************************************************
	;; summary:  Return a Selector numeric absolute function such as "abs(x1)", 
	;;           as an ASCI string in grammatically correct selector.
	;;
	;; args:     wff1     The first expression.
	;;           
	;; Return:   source   The resulting numeric Selector WFF expression
	;;
	;; *******************************************************************
	    vars:(source)
	    (setq source (append "abs(" (evalRule wff1) ")"))
	    source) ; end ruleAbs    
	
	(defun ruleAdd(wff1 wff2)
	;; *******************************************************************
	;; summary:  Return a Selector numeric addition such as "x1 + xm", 
	;;           as an ASCI string in grammatically correct selector.
	;;
	;; args:     wff1     The first expression.
	;;           wff2     The second expression.
	;;           
	;; Return:   source   The resulting numeric Selector WFF expression
	;;
	;; *******************************************************************
	    vars:(source)
	    (setq source (append "(" (evalRule wff1) " + " (evalRule wff2) ")"))
	    source) ; end ruleAdd    
	    
	(defun ruleAvg(wff1 wff2 ...)
	;; *******************************************************************
	;; summary:  Return a Selector numeric average such as "avg(x1,xm)", 
	;;           as an ASCI string in grammatically correct selector.
	;;
	;; args:     wff1     The first expression.
	;;           wff2     The second expression.
	;;           
	;; Return:   source   The resulting numeric Selector WFF expression
	;;
	;; *******************************************************************
        regs:(n N)
	    vars:(source wff)
        (setq N (argCount))
	    (setq source (append "avg(" (evalRule wff1) " , " (evalRule wff2)))
        (loop for n from 2 until N do (setq wff (argFetch n)) (setq source (append source " , " (evalRule wff))))
	    (setq source (append source ")"))
	    source) ; end ruleAvg    
	    
	(defun ruleBgm(genome)
	;; *******************************************************************
	;; summary:  Return a Selector BGM statement such as "bgmregress(x1,x2,x3);". 
	;;           as an ASCI string in grammatically correct selector.
	;;
	;; args:     genome      The sparse vector of numeric expressions for the bgmregress statement.
	;;           
	;; Return:   result      A Selector regress statement such as "bgmregress(x3,x5);"
	;;
	;; *******************************************************************
	    vars:(source)
        (setq source (esm.ruleBgm.wffSource genome))
	    source) ; end ruleBgm    
	    
	(defun ruleCos(wff1)
	;; *******************************************************************
	;; summary:  Return a Selector numeric cosine function such as "cos(x1)", 
	;;           as an ASCI string in grammatically correct selector.
	;;
	;; args:     wff1     The first expression.
	;;           
	;; Return:   source   The resulting numeric Selector WFF expression
	;;
	;; *******************************************************************
	    vars:(source)
	    (setq source (append "cos(" (evalRule wff1) ")"))
	    source) ; end ruleCos    
	    
	(defun ruleCube(wff1)
	;; *******************************************************************
	;; summary:  Return a Selector numeric cube function such as "(x1*x1*x1)", 
	;;           as an ASCI string in grammatically correct selector.
	;;
	;; args:     wff1     The first expression.
	;;           
	;; Return:   source   The resulting numeric Selector WFF expression
	;;
	;; *******************************************************************
	    vars:(source)
	    (setq source (append "(" (evalRule wff1) "*" (evalRule wff1) "*" (evalRule wff1) ")"))
	    source) ; end ruleCube    
	    
	(defun ruleDiv(wff1 wff2)
	;; *******************************************************************
	;; summary:  Return a Selector numeric protected division such as "(x1/xm)", 
	;;           as an ASCI string in grammatically correct selector.
	;;
	;; args:     wff1     The first expression.
	;;           wff2     The second expression.
	;;           
	;; Return:   source   The resulting numeric Selector WFF expression
	;;
	;; *******************************************************************
	    vars:(source)
	    (setq source (append "(" (evalRule wff1) " / " (evalRule wff2) ")"))
	    source) ; end ruleDiv    
	    
	(defun ruleEnn(genome)
	;; *******************************************************************
	;; summary:  Return a Selector ENN statement such as "ennregress(#(obj| #(obj| #(num| 34.56 -23.4))));". 
	;;           as an ASCI string in grammatically correct selector.
	;;
	;; args:     genome      The vector of hidden layer weights for the ennregress statement.
	;;           
	;; Return:   result      A Selector regress statement such as "ennregress(#(obj| #(obj| #(num| 34.56 -23.4))));".
	;;
	;; *******************************************************************
	    vars:(source)
        (setq source (esm.ruleEnn.wffSource genome))
	    source) ; end ruleEnn    
	    
	(defun ruleExp(wff1)
	;; *******************************************************************
	;; summary:  Return a Selector numeric exp function such as "exp(x1)", 
	;;           as an ASCI string in grammatically correct selector.
	;;
	;; args:     wff1     The first expression.
	;;           
	;; Return:   source   The resulting numeric Selector WFF expression
	;;
	;; *******************************************************************
	    vars:(source)
	    (setq source (append "exp(" (evalRule wff1) ")"))
	    source) ; end ruleExp    
	    
	(defun ruleExpt(wff1 wff2)
	;; *******************************************************************
	;; summary:  Return a Selector exponent function such as "expt(abs(x1),x2)", 
	;;           as an ASCI string in grammatically correct selector.
	;;
	;; args:     wff1     The first expression.
	;;           wff2     The second expression.
	;;           
	;; Return:   source   The resulting numeric Selector WFF expression
	;;
	;; *******************************************************************
	    vars:(source)
	    (setq source (append "expt(abs(" (evalRule wff1) ") ," (evalRule wff2) ")"))
	    source) ; end ruleExpt    
	    
	(defun ruleFrm(genome)
	;; *******************************************************************
	;; summary:  Return a Selector multiple factor regression statement 
    ;;           such as 
    ;;                     "frmregress (23.4*x3,2.1*x10);"
    ;;  
	;;           as an ASCI string in grammatically correct selector.
	;;
	;; args:     genome      The sparse vector of numeric expressions for the regress statement.
	;;           
	;; Return:   result      A Selector multiple factor regression statement such as "frmregress (23.4*x3,2.1*x10);"
	;;
	;; *******************************************************************
	    vars:(source)
        (setq source (esm.ruleFrm.wffSource genome))
	    source) ; end ruleFrm    
	    
	(defun ruleIf(wff1 relop wff2 wff3 wff4)
	;; *******************************************************************
	;; summary:  Return a Selector if expression such as "(if (x1 == x2) {5.0} else {1.0})", 
	;;           as an ASCI string in grammatically correct selector.
	;;
	;; args:     wff1     The first expression.
	;;           relop    The relational operator (< <= == != >= >).
	;;           wff2     The second expression.
	;;           wff3     The then expression.
	;;           wff4     The else expression.
	;;           
	;; Return:   source   The resulting numeric Selector WFF expression
	;;
	;; *******************************************************************
	    vars:(source)
	    (setq source (append "(if (" (evalRule wff1) " " (string relop true) " " (evalRule wff2) ") {" (evalRule wff3)  "} else {" (evalRule wff4) "})"))
	    source) ; end ruleIf    
	    
	
	(defun ruleInt(wff1)
	;; *******************************************************************
	;; summary:  Return a Selector numeric integer function such as "number(integer(x1))", 
	;;           as an ASCI string in grammatically correct selector.
	;;
	;; args:     wff1     The first expression.
	;;           
	;; Return:   source   The resulting numeric Selector WFF expression
	;;
	;; *******************************************************************
	    vars:(source)
	    (setq source (append "ninteger(" (evalRule wff1) ")"))
	    source) ; end ruleInt    
	    
	(defun ruleInv(wff)
	;; *******************************************************************
	;; summary:  Return a Selector inverted numeric WFF, such as (1.0 / x4), 
	;;           as an ASCI string in grammatically correct selector.
	;;
	;; args:     wff         A Selector numeric WFF.
	;;           
	;; Return:   expression  The inverion of the specified wff.
	;;
	;; *******************************************************************
	    vars:(expression rule)
	    ;; Return the negative of the specified WFF.
	    (setq rule wff[0])
	    (cond
	     ((isNumber wff) (return (string (/ wff))))
	     ((= rule ruleBgm:) (error (append "esm.ruleExp.ruleInv: invalid WFF grammar rule for inversion [" rule "]")))
	     ((= rule ruleEnn:) (error (append "esm.ruleExp.ruleInv: invalid WFF grammar rule for inversion [" rule "]")))
	     ((= rule ruleFrm:) (error (append "esm.ruleExp.ruleInv: invalid WFF grammar rule for inversion [" rule "]")))
	     ((= rule ruleMvl:) (error (append "esm.ruleExp.ruleInv: invalid WFF grammar rule for inversion [" rule "]")))
	     ((= rule ruleReg:) (error (append "esm.ruleExp.ruleInv: invalid WFF grammar rule for inversion [" rule "]")))
	     ((= rule ruleSvm:) (error (append "esm.ruleExp.ruleInv: invalid WFF grammar rule for inversion [" rule "]")))
	     (else (setq expression (append "(1.0 / " (evalRule wff) ")")))
	     ) ; end cond
	    expression) ; end ruleInv    
	
	(defun ruleLog(wff1)
	;; *******************************************************************
	;; summary:  Return a Selector numeric log function such as "log(abs(x1))", 
	;;           as an ASCI string in grammatically correct selector.
	;;
	;; args:     wff1     The first expression.
	;;           
	;; Return:   source   The resulting numeric Selector WFF expression
	;;
	;; *******************************************************************
	    vars:(source)
	    (setq source (append "log(abs(" (evalRule wff1) "))"))
	    source) ; end ruleLog    
	    
	(defun ruleMax(wff1 wff2 ...)
	;; *******************************************************************
	;; summary:  Return a Selector numeric maximum such as "max(x1,xm)", 
	;;           as an ASCI string in grammatically correct selector.
	;;
	;; args:     wff1     The first expression.
	;;           wff2     The second expression.
	;;           
	;; Return:   source   The resulting numeric Selector WFF expression
	;;
	;; *******************************************************************
        regs:(n N)
	    vars:(source wff)
        (setq N (argCount))
	    (setq source (append "max(" (evalRule wff1) " , " (evalRule wff2)))
        (loop for n from 2 until N do (setq wff (argFetch n)) (setq source (append source " , " (evalRule wff))))
	    (setq source (append source ")"))
	    source) ; end ruleMax    
	
	(defun ruleMin(wff1 wff2 ...)
	;; *******************************************************************
	;; summary:  Return a Selector numeric minimum such as "min(x1,xm)", 
	;;           as an ASCI string in grammatically correct selector.
	;;
	;; args:     wff1     The first expression.
	;;           wff2     The second expression.
	;;           
	;; Return:   source   The resulting numeric Selector WFF expression
	;;
	;; *******************************************************************
        regs:(n N)
	    vars:(source wff)
        (setq N (argCount))
	    (setq source (append "min(" (evalRule wff1) " , " (evalRule wff2)))
        (loop for n from 2 until N do (setq wff (argFetch n)) (setq source (append source " , " (evalRule wff))))
	    (setq source (append source ")"))
	    source) ; end ruleMin    
	    
	(defun ruleMod(wff1 wff2)
	;; *******************************************************************
	;; summary:  Return a Selector numeric modulus such as "(x1%xm)", 
	;;           as an ASCI string in grammatically correct selector.
	;;
	;; args:     wff1     The first expression.
	;;           wff2     The second expression.
	;;           
	;; Return:   source   The resulting numeric Selector WFF expression
	;;
	;; *******************************************************************
	    vars:(source)
	    ;; Return the modulus of the specified arguments.
	    (setq source (append "(" (evalRule wff1) " % " (evalRule wff2) ")"))
	    source) ; end ruleMod    
	    
	(defun ruleMul(wff1 wff2)
	;; *******************************************************************
	;; summary:  Return a Selector numeric multiplication such as "x1 * xm", 
	;;           as an ASCI string in grammatically correct selector.
	;;
	;; args:     wff1     The first expression.
	;;           wff2     The second expression.
	;;           
	;; Return:   source   The resulting numeric Selector WFF expression
	;;
	;; *******************************************************************
	    vars:(source)
	    (setq source (append "(" (evalRule wff1) " * " (evalRule wff2) ")"))
	    source) ; end ruleMul    
	    
	(defun ruleMvl(genome)
	;; *******************************************************************
	;; summary:  Return a Selector multiple linear regression statement 
    ;;           such as 
    ;;                     "mvlregress (23.4*x3,2.1*x10);"
    ;;  
	;;           as an ASCI string in grammatically correct selector.
	;;
	;; args:     genome      The sparse vector of numeric expressions for the regress statement.
	;;           
	;; Return:   result      A Selector multiple linear regression statement such as "mvlregress (23.4*x3,2.1*x10);"
	;;
	;; *******************************************************************
	    vars:(source)
        (setq source (esm.ruleMvl.wffSource genome))
        source) ; end ruleMvl    
	    
	(defun ruleName(elementID)
	;; *******************************************************************
	;; summary:  Return an element name xtime, xid, x1 thru xm only as 
	;;           an ASCI string in grammatically correct selector.
	;;
	;; args:     elementID   An Integer in the range 0 thru M.
	;;           
	;; Return:   name        An element name xtime, xid, or x1 thru xm.
	;;
	;; *******************************************************************
	    vars:(name)
	    ;; Return an element name WGE.
	    (cond
	      ((and (= elementID 0) (= esm.myTimeON true)) (setq name "xtime"))
	      (else (setq name (append "x" elementID)))
	      ) ; end cond
	    (symbol name)) ; end ruleName    
	    
	(defun ruleNeg(wff)
	;; *******************************************************************
	;; summary:  Return a Selector negative numeric WFF, such as -x1 or -(x2 / 4), 
	;;           as an ASCI string in grammatically correct selector.
	;;
	;; args:     wff         A Selector numeric WFF.
	;;           
	;; Return:   expression  The negation of the specified wff.
	;;
	;; *******************************************************************
	    vars:(expression rule)
	    ;; Return the negative of the specified WFF.
	    (setq rule wff[0])
	    (cond
	     ((isNumber wff) (return (string (- wff))))
	     ((= rule ruleAdd:) (setq expression (evalRule (list rule (list ruleNeg: wff[1])  (list ruleNeg: wff[2])))))
	     ((= rule ruleAvg:) (setq expression (evalRule (list rule (list ruleNeg: wff[1])  (list ruleNeg: wff[2])))))
	     ((= rule ruleBgm:) (error (append "esm.ruleExp.ruleNeg: invalid WFF grammar rule for negation [" rule "]")))
	     ((= rule ruleDiv:) (setq expression (if (isNumber wff[2]) (evalRule (list rule wff[1] (list ruleNeg: wff[2]))) (evalRule (list rule (list ruleNeg: wff[1]) wff[2])))))
	     ((= rule ruleEnn:) (error (append "esm.ruleExp.ruleNeg: invalid WFF grammar rule for negation [" rule "]")))
	     ((= rule ruleFrm:) (error (append "esm.ruleExp.ruleNeg: invalid WFF grammar rule for negation [" rule "]")))
	     ((= rule ruleIf:) (setq expression (evalRule (list rule wff[1] wff[2] wff[3] (list ruleNeg: wff[4])  (list ruleNeg: wff[5])))))
	     ((= rule ruleMul:) (setq expression (if (isNumber wff[2]) (evalRule (list rule wff[1] (list ruleNeg: wff[2]))) (evalRule (list rule (list ruleNeg: wff[1]) wff[2])))))
	     ((= rule ruleMvl:) (error (append "esm.ruleExp.ruleNeg: invalid WFF grammar rule for negation [" rule "]")))
	     ((= rule ruleNeg:) (setq expression (evalRule wff[1])))
	     ((= rule ruleSign:) (setq expression (evalRule (list rule (list ruleNeg: wff[1])))))
	     ((= rule ruleSub:) (setq expression (evalRule (list rule (list ruleNeg: wff[1])  (list ruleNeg: wff[2])))))
	     ((= rule ruleReg:) (error (append "esm.ruleExp.ruleNeg: invalid WFF grammar rule for negation [" rule "]")))
	     ((= rule ruleSvm:) (error (append "esm.ruleExp.ruleNeg: invalid WFF grammar rule for negation [" rule "]")))
	     (else (setq expression (append "(-" (evalRule wff) ")")))
	     ) ; end cond
	    expression) ; end ruleNeg    
	
	(defun ruleNop(wff)
	;; *******************************************************************
	;; summary:  Return a Selector no operation numeric WFF, such as x1 or (x2 / 4), 
	;;           as an ASCI string in grammatically correct selector.
	;;
	;; args:     wff         A Selector numeric WFF.
	;;           
	;; Return:   expression  The specified wff.
	;;
	;; *******************************************************************
	    ;; Return the specified WFF.
	    (evalRule wff)) ; end ruleNop    
	
	(defun ruleNum(Number:constant)
	;; *******************************************************************
	;; summary:  Return a numeric constant as an ASCI string in 
	;;           grammatically correct selector.
	;;
	;; args:     constant    A numeric constant.
	;;           
	;; Return:   number      A numeric constant.
	;;
	;; *******************************************************************
	    ;; Return a numeric constant.
	    constant) ; end ruleNum    
	    
	(defun ruleReg(genome)
	;; *******************************************************************
	;; summary:  Return a Selector regress statement such as "regress x3;" 
	;;           as an ASCI string in grammatically correct selector.
	;;
	;; args:     genome      The sparse vector of numeric expressions for the regress statement.
	;;           
	;; Return:   result      A Selector regress statement such as "regress x3*x5;"
	;;
	;; *******************************************************************
	    vars:(source)
        (setq source (esm.ruleReg.wffSource genome))
	    source) ; end ruleReg    
	    
	(defun ruleSign(wff1)
	;; *******************************************************************
	;; summary:  Return a Selector numeric sign function such as "sign(x1)", 
	;;           as an ASCI string in grammatically correct selector.
	;;
	;; args:     wff1     The first expression.
	;;           
	;; Return:   source   The resulting numeric Selector WFF expression
	;;
	;; *******************************************************************
	    vars:(source)
	    (setq source (append "sign(" (evalRule wff1) ")"))
	    source) ; end ruleSign    
	
	
	(defun ruleSin(wff1)
	;; *******************************************************************
	;; summary:  Return a Selector numeric sine function such as "sin(x1)", 
	;;           as an ASCI string in grammatically correct selector.
	;;
	;; args:     wff1     The first expression.
	;;           
	;; Return:   source   The resulting numeric Selector WFF expression
	;;
	;; *******************************************************************
	    vars:(source)
	    (setq source (append "sin(" (evalRule wff1) ")"))
	    source) ; end ruleSin    
	
	(defun ruleSqrt(wff1)
	;; *******************************************************************
	;; summary:  Return a Selector numeric sqrt function such as "sqrt(abs(x1))", 
	;;           as an ASCI string in grammatically correct selector.
	;;
	;; args:     wff1     The first expression.
	;;           
	;; Return:   source   The resulting numeric Selector WFF expression
	;;
	;; *******************************************************************
	    vars:(source)
	    (setq source (append "sqrt(abs(" (evalRule wff1) "))"))
	    source) ; end ruleSqrt    
	    
	(defun ruleSquare(wff1)
	;; *******************************************************************
	;; summary:  Return a Selector numeric square function such as "(x1*x1)", 
	;;           as an ASCI string in grammatically correct selector.
	;;
	;; args:     wff1     The first expression.
	;;           
	;; Return:   source   The resulting numeric Selector WFF expression
	;;
	;; *******************************************************************
	    vars:(source)
	    (setq source (append "(" (evalRule wff1) "*" (evalRule wff1) ")"))
	    source) ; end ruleSquare    
	    
	(defun ruleSub(wff1 wff2)
	;; *******************************************************************
	;; summary:  Return a Selector numeric subtraction such as "x1 - xm", 
	;;           as an ASCI string in grammatically correct selector.
	;;
	;; args:     wff1     The first expression.
	;;           wff2     The second expression.
	;;           
	;; Return:   source   The resulting numeric Selector WFF expression
	;;
	;; *******************************************************************
	    vars:(source)
	    (setq source (append "(" (evalRule wff1) " - " (evalRule wff2) ")"))
	    source) ; end ruleSub    
	    
	(defun ruleSvm(genome)
	;; *******************************************************************
	;; summary:  Return a Selector SVM statement such as "svmregress(x1,x2,x3);". 
	;;           as an ASCI string in grammatically correct selector.
	;;
	;; args:     genome      The sparse vector of numeric expressions for the svmregress statement.
	;;           
	;; Return:   result      A Selector regress statement such as "svmregress(x3,x5);"
	;;
	;; *******************************************************************
	    vars:(source)
        (setq source (esm.ruleSvm.wffSource genome))
	    source) ; end ruleSvm    

	    
	(defun ruleTan(wff1)
	;; *******************************************************************
	;; summary:  Return a Selector numeric tangent function such as "tan(x1)", 
	;;           as an ASCI string in grammatically correct selector.
	;;
	;; args:     wff1     The first expression.
	;;           
	;; Return:   source   The resulting numeric Selector WFF expression
	;;
	;; *******************************************************************
	    vars:(source)
	    (setq source (append "tan(" (evalRule wff1) ")"))
	    source) ; end ruleTan    
	    
	(defun ruleTanh(wff1)
	;; *******************************************************************
	;; summary:  Return a Selector numeric hyper tangent function such as "tanh(x1)", 
	;;           as an ASCI string in grammatically correct selector.
	;;
	;; args:     wff1     The first expression.
	;;           
	;; Return:   source   The resulting numeric Selector WFF expression
	;;
	;; *******************************************************************
	    vars:(source)
	    (setq source (append "tanh(" (evalRule wff1) ")"))
	    source) ; end ruleTanh    
	    
	;; *******************************************************************
    ;; Begin main logic
	;; *******************************************************************
    vars:(wffHdr sourceWff)
    (if (isString wff) then (setq wff (listWff wff)))
    (if (isPair wff) 
        (setq sourceWff (apply (setq wffHdr esm.ruleExp[(car wff)]) (cdr wff))) 
        (setq sourceWff (string wff true))
        ) ; end if
    sourceWff) ; end ruleExp










































;;**EXPORTKEY**:esm:ruleFrm
(defriend esm:ruleFrm(wff)
;; *******************************************************************
;; summary:  The WFF grammar rules for all Selector multivariate 
;;           factor regression Lambdas. 
;;
;; Main:     Return a Selector multivariate factor regression statement,
;;           in source format, such as 
;;                    "frmregress(x2,(x2-x10));" 
;;           as an ASCI source string in grammatically correct selector.
;;
;; args:     wff         The Selector WFF of the multivariate factor regression statement.
;;           
;; Return:   result      A Selector multivariate factor regression statement such as "frmregress(x2,(x2-x10));".
;; *******************************************************************
  	pvars:(;; Public Variables
           (genomeType EXP)         ;; This regression rule uses an expression based genome.
           (lengthExempt false)     ;; This regression rule is NOT exempt from genome length restrictions.
           ;; Public Methods
           chromosomeLength         ;; Return the number of non-empty left-most chromosomes in the genome.
           crossOver                ;; Create two new Selector multivariate factor regression WFFs by splicing two parent WFFs together using genetic crossover.
           growWFF                  ;; Create a new Selector multivariate factor regression wff to the current population.
           mutate                   ;; Create a new Selector multivariate factor regression WFF using genetic mutation.
           wffList                  ;; Convert a Selector multivariate factor regression wff to a list format.
           wffSource				;; Convert a Selector multivariate factor regression wff to a source statement such as "frmregress(x0-x5,abs(x1));". 
           wffString                ;; Convert a Selector multivariate factor regression wff to a string format such as "(ruleFrm #(obj| (ruleSub x0 x5) (ruleAbs x1)));"
           ) ; end persistant variables
   	;; *******************************************************************************
   	;; Define Public Child Lambdas 
   	;; *******************************************************************************

	(defun chromosomeLength(genome)
	;; *******************************************************************
	;; summary:  Return the number of non-empty left-most chromosomes in the genome. 
	;;
	;; args:     genome     The genome whose chromosome length is to be returned.           
	;;           
	;; Return:   length     The number of non-empty left-most chromosomes in the genome.
	;;
	;; *******************************************************************
	   regs:(m M len)
	
       (setq M (length genome))
	   (loop for m from 0 until M do
          (if (<> genome[m] #void) (++ len))
          ) ; end loop

	   len) ; end chromosomeLength


	(defun crossOver(father mother)
	;; *******************************************************************
	;; summary:  Create two new Selector multivariate factor regression WFFs, 
    ;;           by splicing two parent WFFs together using genetic crossover.
	;;
	;; args:     father     The father WFF to be mated.           
	;;           mother     The mother WFF to be mated.           
	;;           
	;; Return:   none.
	;;
	;; *******************************************************************
	   regs:(m M n N)
	   vars:(dadGenome momGenome dadRule momRule newGenome)
	   vars:(dadWff momWff newWff (passOne true))
       vars:(dadIndex momIndex dadPair momSlot dadWff chromosomeCrossPct)
	
	   ;; Extract numeric WFFs from the parent Selector Lambdas (where necessary).
	   (if (or (= father #void) (= mother #void)) (return false))
	   (if (isLambda father) (begin (setq dadRule father.Rule) (setq father father.Genome)))
	   (if (isLambda mother) (begin (setq momRule mother.Rule) (setq mother mother.Genome)))
	   (if (isPair father) (begin (setq dadRule father[0]) (setq father father[1])))
	   (if (isPair mother) (begin (setq momRule mother[0]) (setq mother mother[1])))
	   (setq dadGenome father)
	   (setq momGenome mother)

	   ;; Make sure the father and mother are genetically compatible.
	   (if (or (= father #void) (= mother #void)) (return false))
	   (if (<> esm[momRule].genomeType EXP:) (return false))
	
	   Retry::     
	
	   ;; Make copies of the candidate genomes.
	   (setq dadGenome (listWff father))
	   (setq momGenome (listWff mother))

       ;; Do we use a linear genome during cross over operations?
       (if (= myCrossLinearSW true)
           (begin
             ;; Reorder the chromosomes in the candidate genomes occasionally.
             (setq M (chromosomeLength dadGenome))
             (setq N (chromosomeLength momGenome))
             (if (<= (random 1.0) .50)
                 then
                 (begin 
                   (setq dadGenome (copy dadGenome))
                   (setq m (integer (random M)))
                   (setq n (integer (random M)))
                   (setq chromosome dadGenome[m])
                   (setq dadGenome[m] dadGenome[n])
                   (setq dadGenome[n] chromosome)
                 )) ; end if
             (if (<= (random 1.0) .50)
                 then
                 (begin 
                   (setq momGenome (copy momGenome))
                   (setq m (integer (random N)))
                   (setq n (integer (random N)))
                   (setq chromosome momGenome[m])
                   (setq momGenome[m] momGenome[n])
                   (setq momGenome[n] chromosome)
                 )) ; end if
      
             ;; Select chromosomes from each genome.
             (setq genome (copy dadGenome))           
             (setq M (max M N))
             (setq chromosomeCrossPct 1.5)
             (loop for m from 0 until M do
               (if (>= chromosomeCrossPct .50) (-= chromosomeCrossPct .50))
               (cond
                 ((> (random 1.0) chromosomeCrossPct) (setq genome[m] genome[m]))
                 ((= genome[m] #void) (if (<= (random 1.0) .25) then (setq genome[m] momGenome[m])))
                 ((= momGenome[m] #void) (if (<= (random 1.0) .25) then (setq genome[m] (ruleExp.mutateNumericWFF genome[m] (random 1.0)))))
                 ((and (= myCrossMarrySW true) (<= (random 1.0) .20) (<> (setq newWff (ruleExp.marryNumericWFFs genome[m] momGenome[m])) false)) (setq genome[m] newWff))
                 ((= myCrossSpliceSW false) (if (<= (random 1.0) .50) then (setq genome[m] momGenome[m])))
                 (else 
                  (begin
                     (setq dadWff genome[m])                    
                     (setq momSlot (setq momIndex (esm.ruleExp.selectSlotWFF momGenome[m]))[(integer (random (length momIndex)))])
                     (setq dadPair (setq dadIndex (esm.ruleExp.selectPairWFF dadWff))[(integer (random (length dadIndex)))])
                     (cond
                      ((isAtom dadPair) (setq genome[m] momSlot))
                      ((and (isPair momSlot) (= myCrossMarrySW true) (<= (random 1.0) .20) (<> (setq newWff (ruleExp.marryNumericWFFs (listWff dadPair) (listWff momSlot))) false))
                       (begin (setCar dadPair (car newWff))  (setCdr dadPair (cdr newWff))))
                      ((isPair momSlot) (begin (setCar dadPair (car momSlot))  (setCdr dadPair (cdr momSlot))))
                      (else (ruleExp.spliceNumericWFF dadPair momSlot 1.0))
                      ) ; end splice cond
                  )) ; end case
                 ) ; end chromosome cond           
               ) ; end column loop  	
      
             (goto Grow:)
           )) ; end cross over linear genome if

       ;; Do we cross over chromosomes from different columns?
       (if (= myCrossChromosomeSW true)
           (begin
             (setq m (integer (random (chromosomeLength dadGenome))))
             (setq n (integer (random (chromosomeLength momGenome))))
             (setq genome (copy dadGenome))
             
             ;; Select chromosomes for cross over to create child genome.
             (cond 
               ((= dadGenome[m] #void) (setq genome[m] momGenome[n]))
               ((and (= momGenome[n] #void) (= myCrossSpliceSW true)) (setq genome[m] (setq newWff (spliceNumericWFF (listWff dadGenome[m]) (cutoutNumericWFF (listWff dadGenome[m]))))))
               ((and (= momGenome[n] #void) (= myCrossMarrySW true) (<> (setq newWff (ruleExp.marryNumericWFFs dadGenome[m] dadGenome[m])) false)) (setq genome[m] newWff))
               ((= momGenome[n] #void) (setq genome[m] momGenome[n]))
               ((and (= myCrossMarrySW true) (<= (random 1.0) .20) (<> (setq newWff (ruleExp.marryNumericWFFs dadGenome[m] momGenome[n])) false)) (setq genome[m] newWff))
               ((= myCrossSpliceSW false) (setq genome[m] momGenome[n]))
               (else ;; Here myCrossSpliceSW is true and both dadGenome and momGenome are not void. 
                (begin
                   (setq newWff (spliceNumericWFF (listWff dadGenome[m]) (cutoutNumericWFF (listWff momGenome[n]))))
                   (if (> (lengthWFF newWff) myMaxColWFFLen)
                       (setq newWff (spliceNumericWFF (listWff dadGenome[m]) (cutoutNumericWFF (listWff momGenome[n]))))
                       ) ; end if
                   (if (> (lengthWFF newWff) myMaxColWFFLen)
                       (setq newWff (spliceNumericWFF (listWff dadGenome[m]) (cutoutNumericWFF (listWff momGenome[n]))))
                       ) ; end if
                   (if (> (lengthWFF newWff) myMaxColWFFLen)
                       (setq newWff (spliceNumericWFF (listWff dadGenome[m]) (cutoutNumericWFF (listWff momGenome[n]))))
                       ) ; end if
                   (if (> (lengthWFF newWff) myMaxColWFFLen) 
                       (setq newWff momGenome[n])
                       ) ; end if
                   (setq genome[m] newWff)
                )) ; end case
               ) ; end chromosome cond           
             (goto Grow:)
           )) ; end cross over chromosomes if

	   ;; Reorder the chromosomes in the candidate genomes occasionally.
	   (if (<= (random 1.0) .50)
	       then
	       (begin 
	         (setq dadGenome (copy dadGenome))
	         (setq m (integer (random myM)))
	         (setq n (integer (random myM)))
	         (setq chromosome dadGenome[m])
	         (setq dadGenome[m] dadGenome[n])
	         (setq dadGenome[n] chromosome)
	       )) ; end if
	   (if (<= (random 1.0) .50)
	       then
	       (begin 
	         (setq momGenome (copy momGenome))
	         (setq m (integer (random myM)))
	         (setq n (integer (random myM)))
	         (setq chromosome momGenome[m])
	         (setq momGenome[m] momGenome[n])
	         (setq momGenome[n] chromosome)
	       )) ; end if

	   ;; Splice the chromosomes in the candidate genomes to form the progeny.
       (setq genome (new Vector: myM))
       (setq M myM)
       (loop for m from 0 until M do
         ;; Select chromosomes from each genome.
         (cond 
           ((= dadGenome[m] #void) (setq genome[m] momGenome[m]))
           ((= momGenome[m] #void) (setq genome[m] dadGenome[m]))
           ((and (= myCrossMarrySW true) (<= (random 1.0) .20) (<> (setq newWff (ruleExp.marryNumericWFFs dadGenome[m] momGenome[m])) false)) (setq genome[m] newWff))
           ((<= (random 1.0) .10) (setq genome[m] dadGenome[m]))
           ((<= (random 1.0) .10) (setq genome[m] momGenome[m]))
           ((and (= myCrossSpliceSW false) (<= (random 1.0) .50)) (setq genome[m] dadGenome[m]))
           ((= myCrossSpliceSW false) (setq genome[m] momGenome[m]))
           (else 
            (begin
               (setq newWff (spliceNumericWFF (listWff dadGenome[m]) (cutoutNumericWFF (listWff momGenome[m]))))
               (if (> (lengthWFF newWff) myMaxColWFFLen)
                   (setq newWff (spliceNumericWFF (listWff dadGenome[m]) (cutoutNumericWFF (listWff momGenome[m]))))
                   ) ; end if
               (if (> (lengthWFF newWff) myMaxColWFFLen)
                   (setq newWff (spliceNumericWFF (listWff dadGenome[m]) (cutoutNumericWFF (listWff momGenome[m]))))
                   ) ; end if
               (if (> (lengthWFF newWff) myMaxColWFFLen)
                   (setq newWff (spliceNumericWFF (listWff dadGenome[m]) (cutoutNumericWFF (listWff momGenome[m]))))
                   ) ; end if
               (if (> (lengthWFF newWff) myMaxColWFFLen)
                   (if (< (random 1.0) .50) (setq newWff dadGenome[m]) (setq newWff momGenome[m]))
                   ) ; end if
               (setq genome[m] newWff)
            )) ; end case
           ) ; end chromosome cond           
         ) ; end column loop  	

	   ;; Make a Selector multivariate factor regression wff and add it to the current population.
       Grow::
       (growWFF genome dadGenome)
	
	   ;; Make sure to mate the father with the mother
	   (if (= passOne true) 
	       (begin 
	          (setq passOne false) 
	          (setq dadGenome mother)
	          (setq momGenome father)
	          (goto Retry:)
	       )) ; end if
	
	   true) ; end crossOver
	
	(defun growWFF(command ...)
	;; *******************************************************************
	;; summary:  Grow a new Selector multivariate factor regression WFF and
	;;           add it to the current population.
	;;
	;; args:     command 	The command to determine the type of WFF to grow.
	;;           prevGenome (Optional)The previous genome (for use by particle swarm operators).
	;;           
	;; Return:   Lambda      A scored Selector multivariate factor regression Lambda.
	;;
	;; *******************************************************************
	   regs:(k m M mm n N maxChromosomes cntChromosomes)
	   vars:(Lambda genome newGenome prevGenome)
	   vars:(Rf wff)
	   vars:(IntVector:chromosomeIndices IntVector:chromosomeLimits)
	
       ;; Use the command to determine the type of WFF to grow.
       (setq maxChromosomes esm.myFRMMaximum)
       (cond
         ;; Command is a genome vector.
         ((isVector command)
          (begin
            (if (>= (argCount) 2) (setq prevGenome (argFetch 1)))
            (setq genome command) 
          )) ; end case genome vector
         ;; Command requires growth of an mvl using root expressions.
         ((= command root:)
          (begin
		    (setq genome (new Vector: myM))
		    (setq M myM)
		    (loop for m from 0 until M do
		      ;; Select random root expressions for each chromosome in the genome.
              (if (= myGrowWFFStyle full:)
		          (setq genome[m] (ruleExp.growCTermWFF m))
		          (setq genome[m] (ruleExp.growTermWFF -2 m))
                  ) ; end if
		      ) ; end column loop  
          )) ; end case root command
         ;; Command requires growth of an mvl using full expressions.
         ((= command full:)
          (begin
		    (setq genome (new Vector: myM))
		    (setq M myM)
		    (loop for m from 0 until M do
		      ;; Select random full expressions for each chromosome in the genome.
              (if (= myGrowWFFStyle full:)
		          (setq genome[m] (ruleExp.growWFF expression: 0))
		          (setq genome[m] (ruleExp.growRootWFF -2 m))
                  ) ; end if
		      ) ; end column loop  
          )) ; end case full command
         ;; All other commands return an error.
         (else (error (append "esm.ruleFrm.growWFF: invalid command [" command "]")))
         ) ; end command cond
	
	   ;; Create the multivariate factor regression Selector from the final WFF.
	   Last::
       (if (isLambda genome) (setq genome genome.Genome)) 
       (setq wff (wffList (list ruleFrm: genome)))
       (setq genome wff[1])
       (setq cntChromosomes 0)
       (setq M (length genome))
       (loop for m from 0 until M do (if (<> genome[m] #void) (++ cntChromosomes)))
       (cond
        ;; We have less than the maximum number of chromosomes
	    ((>= maxChromosomes cntChromosomes) (setq Rf (createSelector wff prevGenome)))
        ;; We have more than the maximum number of chromosomes
        ;; Note: We must be an exhaustive search of all possible
        ;;       combinations of the chromosomes in this genome
        ;;       taken 'maxChromosomes' at a time.
	    (else 
         (begin
           (setq chromosomeIndices (new Vector: Integer: maxChromosomes))
           (setq chromosomeLimits (new Vector: Integer: maxChromosomes))
           (loop for mm from 0 until maxChromosomes do (setq chromosomeIndices[mm] mm) (setq chromosomeLimits[mm] (+ (- cntChromosomes maxChromosomes) mm)))
           (while (<= chromosomeIndices[0] chromosomeLimits[0]) do
             (setq newGenome (new Vector: M))
             (loop for mm from 0 until maxChromosomes do (setq k chromosomeIndices[mm]) (setq newGenome[mm] genome[k]))
             (setq Rf (createSelector (list ruleFrm: newGenome) prevGenome))
             ;; Increment the chromosome combinatatorial indices.
             (loop for mm from (- maxChromosomes 1) to 0 by -1 do (++ chromosomeIndices[mm]) (if (<= chromosomeIndices[mm] chromosomeLimits[mm]) (setq mm -1))) 
             (loop for mm from 1 until maxChromosomes do (if (> chromosomeIndices[mm] chromosomeLimits[mm]) (setq chromosomeIndices[mm] (+ chromosomeIndices[(- mm 1)] 1)))) 
             ) ; end while
         )) ; end else
        ) ; end cond
	   Rf) ; end growWFF

	(defun mutate(wff)
	;; *******************************************************************
	;; summary:  Create a new Selector multivariate factor regression WFF 
    ;;           using genetic mutation.
	;;
	;; args:     wff     The candidate WFF to be mutated.           
	;;           
	;; Return:   none.
	;;
	;; *******************************************************************
	   regs:(m M n N)
	   vars:(rule genome prevGenome newWff chromosome (Number:deathPct .10))
	
	   ;; Extract numeric WFFs from the candidate Selector Lambda (where necessary).
	   (if (= wff #void) (return false))
	   (if (isLambda wff) (setq wff wff.Genome))
	   (if (isPair wff) (setq wff wff[1]))
	   (setq genome wff)
	   (if (= genome #void) (setq genome myColumnGenome))
       (setq prevGenome genome)
	
	   ;; Make a copy of the candidate genome.
	   (setq genome (listWff genome))

       ;; Reorder the chromosomes in the candidate genome ocasionally.
       (if (<= (random 1.0) .50)
           then
           (begin 
             (setq m (integer (random myM)))
             (setq n (integer (random myM)))
             (setq chromosome genome[m])
             (setq genome[m] genome[n])
             (setq genome[n] chromosome)
           )) ; end if
       (setq m (integer (random myM))) 
       (if (= myMutateSpliceSW true) (setq newWff (mutateNumericWFF genome[m])) (begin (setq newWff (ruleExp.growRootWFF -2)) (goto Last:)))
       (setq n 0)(while (and (< (++ n) 20) (> (lengthWFF newWff) myMaxColWFFLen)) do (setq newWff (mutateNumericWFF genome[m])))
       (if (> (lengthWFF newWff) myMaxColWFFLen) (return false))
       (if (<= (random 1.0) deathPct) (setq genome[m] #void) (setq genome[m] newWff))

	   ;; Make a Selector multivariate linear regression wff and add it to the current population.
       Last::
       (growWFF genome prevGenome)
	
	   true) ; end mutate
		
    (defun wffList(wff)
	;; *******************************************************************
	;; summary:  Convert a selector multivariate factor regression wff to a 
    ;;           list format. 
	;;
	;; args:     wff         The multivariate factor regression wff.
	;;           
	;; Return:   result      A Selector multivariate factor regression WFF such as '(ruleFrm #(obj| (x0-x5) abs(x1)))
	;;
	;; *******************************************************************
       regs:(m M)
       vars:(wffListFormat rule genome)
       vars:(chromosome0 chromosome1)
       (if (isString wff) (setq wff wff) (setq wff (string wff true)))
       (setq wff (lisp wff)[0])
       (if (or (not (isPair wff)) (<> (length wff) 2) (<> wff[0] ruleFrm:)) (error "esm.ruleFrm.wffList: invalid WFF"))
       (setq rule wff[0])
       (setq genome (sort (copy wff[1]) >))
       ;; Eliminate all collisions before generating frmregress genome.
       (setq M (length genome))
       (loop for m from 1 until M do
         (setq chromosome0 genome[(- m 1)])
         (setq chromosome1 genome[m])
         (if (and (<> chromosome1 #void) (= chromosome0 chromosome1))
             (setq genome[(- m 1)] #void)
             ) ; end if
         ) ; end loop   
       (setq genome (sort genome >))
       (setq wffListFormat (list rule genome))
       wffListFormat) ; end wffList


	(defun wffSource(genome)
	;; *******************************************************************
	;; summary:  Return a Selector multivariate factor regression statement 
    ;;           such as 
    ;;                     "frmregress(x0-x5,abs(x1));"
    ;;  
	;;           as an ASCI string in grammatically correct Selector.
	;;
	;; args:     genome      The sparse vector of chromosome numeric expressions for the frmregress statement.
	;;           
	;; Return:   result      A Selector multivariate factor regression statement such as "frmregress(x0-x5,abs(x1));"
	;;
	;; *******************************************************************
       regs:(m M)
       vars:((wffSourceFormat "") rule genome)
       vars:(chromosome0 chromosome1)
       ;; Eliminate all collisions before generating bgmregress genome.
       (setq genome (sort (copy genome) >))
       (setq M (length genome))
       (loop for m from 1 until M do
         (setq chromosome0 genome[(- m 1)])
         (setq chromosome1 genome[m])
         (if (and (<> chromosome1 #void) (= chromosome0 chromosome1))
             (setq genome[(- m 1)] #void)
             ) ; end if
         ) ; end loop   
       (setq genome (sort genome >))
       ;; Create the final source from the genome.
       (setq M (length genome))
       (loop for m from 0 until M do
         ;; Select chromosomes from each genome.
         (cond 
           ((and (= wffSourceFormat "") (<> genome[m] #void)) (setq wffSourceFormat (ruleExp genome[m])))
           ((and (<> wffSourceFormat "") (<> genome[m] #void)) (setq wffSourceFormat (append wffSourceFormat "," (ruleExp genome[m]))))
           ) ; end chromosome cond
         ) ; end column loop  
	    ;; Generate a gramatically correct Selector multivariate factor regression statement in source format.
	   (setq wffSourceFormat (append "frmregress(" wffSourceFormat ");"))
	   wffSourceFormat) ; end wffSource    

   (defun wffString(wff)
	;; *******************************************************************
	;; summary:  Convert a selector multivariate factor regression wff to a 
    ;;           string format. 
	;;
	;; args:     wff         The multivariate factor regression wff.
	;;           
	;; Return:   result      A Selector multivariate factor regression WFF such as "(ruleFrm #(obj| (x0-x5) abs(x1)))"
	;;
	;; *******************************************************************
       regs:(m M)
       vars:(wffStringFormat rule genome)
       vars:(chromosome0 chromosome1)
       (if (isString wff) (setq wff (lisp wff)[0]))
       (if (or (not (isPair wff)) (<> (length wff) 2) (<> wff[0] ruleFrm:)) (error "esm.ruleFrm.wffString: invalid WFF"))
       (setq rule wff[0])
       (setq genome (sort (copy wff[1]) >))
       ;; Eliminate all collisions before generating frmregress genome.
       (setq M (length genome))
       (loop for m from 1 until M do
         (setq chromosome0 genome[(- m 1)])
         (setq chromosome1 genome[m])
         (if (and (<> chromosome1 #void) (= chromosome0 chromosome1))
             (setq genome[(- m 1)] #void)
             ) ; end if
         ) ; end loop   
       (setq genome (sort genome >))
       (setq wffStringFormat (string (list rule genome) true))
       wffStringFormat) ; end stringWff

   	;; *******************************************************************************
   	;; Begin main logic 
   	;; *******************************************************************************
	vars:(wffSourceFormat)
    
    (setq wffSourceFormat (wffSource wff[1]))
    wffSourceFormat) ; end ruleFrm










































;;**EXPORTKEY**:esm:ruleMvl
(defriend esm:ruleMvl(wff)
;; *******************************************************************
;; summary:  The WFF grammar rules for all Selector multivariate 
;;           linear regression Lambdas. 
;;
;; Main:     Return a Selector multivariate linear regression statement,
;;           in source format, such as 
;;                    "mvlregress(x2,(x2-x10));" 
;;           as an ASCI source string in grammatically correct selector.
;;
;; args:     wff         The Selector WFF of the multivariate linear regression statement.
;;           
;; Return:   result      A Selector multivariate linear regression statement such as "mvlregress(x2,(x2-x10));".
;; *******************************************************************
  	pvars:(;; Public Variables
           (genomeType EXP)         ;; This regression rule uses an expression based genome.
           (lengthExempt false)     ;; This regression rule is NOT exempt from genome length restrictions.
           ;; Public Methods
           chromosomeLength         ;; Return the number of non-empty left-most chromosomes in the genome.
           crossOver                ;; Create two new Selector multivariate linear regression WFFs by splicing two parent WFFs together using genetic crossover.
           growWFF                  ;; Create a new Selector multivariate linear regression wff to the current population.
           mutate                   ;; Create a new Selector multivariate linear regression WFF using genetic mutation.
           wffList                  ;; Convert a Selector multivariate linear regression wff to a list format.
           wffSource				;; Convert a Selector multivariate linear regression wff to a source statement such as "mvlregress(x0-x5,abs(x1));". 
           wffString                ;; Convert a Selector multivariate linear regression wff to a string format such as "(ruleMvl #(obj| (ruleSub x0 x5) (ruleAbs x1)));"
           ) ; end persistant variables
   	;; *******************************************************************************
   	;; Define Public Child Lambdas 
   	;; *******************************************************************************

	(defun chromosomeLength(genome)
	;; *******************************************************************
	;; summary:  Return the number of non-empty left-most chromosomes in the genome. 
	;;
	;; args:     genome     The genome whose chromosome length is to be returned.           
	;;           
	;; Return:   length     The number of non-empty left-most chromosomes in the genome.
	;;
	;; *******************************************************************
	   regs:(m M len)
	
       (setq M (length genome))
	   (loop for m from 0 until M do
          (if (<> genome[m] #void) (++ len))
          ) ; end loop

	   len) ; end chromosomeLength


	(defun crossOver(father mother)
	;; *******************************************************************
	;; summary:  Create two new Selector multivariate linear regression WFFs, 
    ;;           by splicing two parent WFFs together using genetic crossover.
	;;
	;; args:     father     The father WFF to be mated.           
	;;           mother     The mother WFF to be mated.           
	;;           
	;; Return:   none.
	;;
	;; *******************************************************************
	   regs:(m M n N)
	   vars:(dadGenome momGenome dadRule momRule newGenome)
	   vars:(dadWff momWff newWff (passOne true))
       vars:(dadIndex momIndex dadPair momSlot dadWff chromosomeCrossPct)
	
	   ;; Extract numeric WFFs from the parent Selector Lambdas (where necessary).
	   (if (or (= father #void) (= mother #void)) (return false))
	   (if (isLambda father) (begin (setq dadRule father.Rule) (setq father father.Genome)))
	   (if (isLambda mother) (begin (setq momRule mother.Rule) (setq mother mother.Genome)))
	   (if (isPair father) (begin (setq dadRule father[0]) (setq father father[1])))
	   (if (isPair mother) (begin (setq momRule mother[0]) (setq mother mother[1])))
	   (setq dadGenome father)
	   (setq momGenome mother)

	   ;; Make sure the father and mother are genetically compatible.
	   (if (or (= father #void) (= mother #void)) (return false))
	   (if (<> esm[momRule].genomeType EXP:) (return false))
	
	   Retry::     
	
	   ;; Make copies of the candidate genomes.
	   (setq dadGenome (listWff father))
	   (setq momGenome (listWff mother))

       ;; Do we use a linear genome during cross over operations?
       (if (= myCrossLinearSW true)
           (begin
             ;; Reorder the chromosomes in the candidate genomes occasionally.
             (setq M (chromosomeLength dadGenome))
             (setq N (chromosomeLength momGenome))
             (if (<= (random 1.0) .50)
                 then
                 (begin 
                   (setq dadGenome (copy dadGenome))
                   (setq m (integer (random M)))
                   (setq n (integer (random M)))
                   (setq chromosome dadGenome[m])
                   (setq dadGenome[m] dadGenome[n])
                   (setq dadGenome[n] chromosome)
                 )) ; end if
             (if (<= (random 1.0) .50)
                 then
                 (begin 
                   (setq momGenome (copy momGenome))
                   (setq m (integer (random N)))
                   (setq n (integer (random N)))
                   (setq chromosome momGenome[m])
                   (setq momGenome[m] momGenome[n])
                   (setq momGenome[n] chromosome)
                 )) ; end if
      
             ;; Select chromosomes from each genome.
             (setq genome (copy dadGenome))           
             (setq M (max M N))
             (setq chromosomeCrossPct 1.5)
             (loop for m from 0 until M do
               (if (>= chromosomeCrossPct .50) (-= chromosomeCrossPct .50))
               (cond
                 ((> (random 1.0) chromosomeCrossPct) (setq genome[m] genome[m]))
                 ((= genome[m] #void) (if (<= (random 1.0) .25) then (setq genome[m] momGenome[m])))
                 ((= momGenome[m] #void) (if (<= (random 1.0) .25) then (setq genome[m] (ruleExp.mutateNumericWFF genome[m] (random 1.0)))))
                 ((and (= myCrossMarrySW true) (<= (random 1.0) .20) (<> (setq newWff (ruleExp.marryNumericWFFs genome[m] momGenome[m])) false)) (setq genome[m] newWff))
                 ((= myCrossSpliceSW false) (if (<= (random 1.0) .50) then (setq genome[m] momGenome[m])))
                 (else 
                  (begin
                     (setq dadWff genome[m])                    
                     (setq momSlot (setq momIndex (esm.ruleExp.selectSlotWFF momGenome[m]))[(integer (random (length momIndex)))])
                     (setq dadPair (setq dadIndex (esm.ruleExp.selectPairWFF dadWff))[(integer (random (length dadIndex)))])
                     (cond
                      ((isAtom dadPair) (setq genome[m] momSlot))
                      ((and (isPair momSlot) (= myCrossMarrySW true) (<= (random 1.0) .20) (<> (setq newWff (ruleExp.marryNumericWFFs (listWff dadPair) (listWff momSlot))) false))
                       (begin (setCar dadPair (car newWff))  (setCdr dadPair (cdr newWff))))
                      ((isPair momSlot) (begin (setCar dadPair (car momSlot))  (setCdr dadPair (cdr momSlot))))
                      (else (ruleExp.spliceNumericWFF dadPair momSlot 1.0))
                      ) ; end splice cond
                  )) ; end case
                 ) ; end chromosome cond           
               ) ; end column loop  	
      
             (goto Grow:)
           )) ; end cross over linear genome if

       ;; Do we cross over chromosomes from different columns?
       (if (= myCrossChromosomeSW true)
           (begin
             (setq m (integer (random (chromosomeLength dadGenome))))
             (setq n (integer (random (chromosomeLength momGenome))))
             (setq genome (copy dadGenome))
             
             ;; Select chromosomes for cross over to create child genome.
             (cond 
               ((= dadGenome[m] #void) (setq genome[m] momGenome[n]))
               ((and (= momGenome[n] #void) (= myCrossSpliceSW true)) (setq genome[m] (setq newWff (spliceNumericWFF (listWff dadGenome[m]) (cutoutNumericWFF (listWff dadGenome[m]))))))
               ((and (= momGenome[n] #void) (= myCrossMarrySW true) (<> (setq newWff (ruleExp.marryNumericWFFs dadGenome[m] dadGenome[m])) false)) (setq genome[m] newWff))
               ((= momGenome[n] #void) (setq genome[m] momGenome[n]))
               ((and (= myCrossMarrySW true) (<= (random 1.0) .20) (<> (setq newWff (ruleExp.marryNumericWFFs dadGenome[m] momGenome[n])) false)) (setq genome[m] newWff))
               ((= myCrossSpliceSW false) (setq genome[m] momGenome[n]))
               (else ;; Here myCrossSpliceSW is true and both dadGenome and momGenome are not void. 
                (begin
                   (setq newWff (spliceNumericWFF (listWff dadGenome[m]) (cutoutNumericWFF (listWff momGenome[n]))))
                   (if (> (lengthWFF newWff) myMaxColWFFLen)
                       (setq newWff (spliceNumericWFF (listWff dadGenome[m]) (cutoutNumericWFF (listWff momGenome[n]))))
                       ) ; end if
                   (if (> (lengthWFF newWff) myMaxColWFFLen)
                       (setq newWff (spliceNumericWFF (listWff dadGenome[m]) (cutoutNumericWFF (listWff momGenome[n]))))
                       ) ; end if
                   (if (> (lengthWFF newWff) myMaxColWFFLen)
                       (setq newWff (spliceNumericWFF (listWff dadGenome[m]) (cutoutNumericWFF (listWff momGenome[n]))))
                       ) ; end if
                   (if (> (lengthWFF newWff) myMaxColWFFLen) 
                       (setq newWff momGenome[n])
                       ) ; end if
                   (setq genome[m] newWff)
                )) ; end case
               ) ; end chromosome cond           
             (goto Grow:)
           )) ; end cross over chromosomes if

	   ;; Reorder the chromosomes in the candidate genomes occasionally.
	   (if (<= (random 1.0) .50)
	       then
	       (begin 
	         (setq dadGenome (copy dadGenome))
	         (setq m (integer (random myM)))
	         (setq n (integer (random myM)))
	         (setq chromosome dadGenome[m])
	         (setq dadGenome[m] dadGenome[n])
	         (setq dadGenome[n] chromosome)
	       )) ; end if
	   (if (<= (random 1.0) .50)
	       then
	       (begin 
	         (setq momGenome (copy momGenome))
	         (setq m (integer (random myM)))
	         (setq n (integer (random myM)))
	         (setq chromosome momGenome[m])
	         (setq momGenome[m] momGenome[n])
	         (setq momGenome[n] chromosome)
	       )) ; end if

	   ;; Splice the chromosomes in the candidate genomes to form the progeny.
       (setq genome (new Vector: myM))
       (setq M myM)
       (loop for m from 0 until M do
         ;; Select chromosomes from each genome.
         (cond 
           ((= dadGenome[m] #void) (setq genome[m] momGenome[m]))
           ((= momGenome[m] #void) (setq genome[m] dadGenome[m]))
           ((and (= myCrossMarrySW true) (<= (random 1.0) .20) (<> (setq newWff (ruleExp.marryNumericWFFs dadGenome[m] momGenome[m])) false)) (setq genome[m] newWff))
           ((<= (random 1.0) .10) (setq genome[m] dadGenome[m]))
           ((<= (random 1.0) .10) (setq genome[m] momGenome[m]))
           ((and (= myCrossSpliceSW false) (<= (random 1.0) .50)) (setq genome[m] dadGenome[m]))
           ((= myCrossSpliceSW false) (setq genome[m] momGenome[m]))
           (else 
            (begin
               (setq newWff (spliceNumericWFF (listWff dadGenome[m]) (cutoutNumericWFF (listWff momGenome[m]))))
               (if (> (lengthWFF newWff) myMaxColWFFLen)
                   (setq newWff (spliceNumericWFF (listWff dadGenome[m]) (cutoutNumericWFF (listWff momGenome[m]))))
                   ) ; end if
               (if (> (lengthWFF newWff) myMaxColWFFLen)
                   (setq newWff (spliceNumericWFF (listWff dadGenome[m]) (cutoutNumericWFF (listWff momGenome[m]))))
                   ) ; end if
               (if (> (lengthWFF newWff) myMaxColWFFLen)
                   (setq newWff (spliceNumericWFF (listWff dadGenome[m]) (cutoutNumericWFF (listWff momGenome[m]))))
                   ) ; end if
               (if (> (lengthWFF newWff) myMaxColWFFLen)
                   (if (< (random 1.0) .50) (setq newWff dadGenome[m]) (setq newWff momGenome[m]))
                   ) ; end if
               (setq genome[m] newWff)
            )) ; end case
           ) ; end chromosome cond           
         ) ; end column loop  	

	   ;; Make a Selector multivariate linear regression wff and add it to the current population.
       Grow::
       (growWFF genome dadGenome)
	
	   ;; Make sure to mate the father with the mother
	   (if (= passOne true) 
	       (begin 
	          (setq passOne false) 
	          (setq dadGenome mother)
	          (setq momGenome father)
	          (goto Retry:)
	       )) ; end if
	
	   true) ; end crossOver
	
	(defun growWFF(command ...)
	;; *******************************************************************
	;; summary:  Grow a new Selector multivariate linear regression WFF and
	;;           add it to the current population.
	;;
	;; args:     command 	The command to determine the type of WFF to grow..
	;;           prevGenome (Optional)The previous genome (for use by particle swarm operators).
	;;           
	;; Return:   Lambda      A scored Selector multivariate linear regression Lambda.
	;;
	;; *******************************************************************
	   regs:(k m M mm n N maxChromosomes cntChromosomes)
	   vars:(Lambda genome newGenome prevGenome)
	   vars:(Rf wff)
	   vars:(IntVector:chromosomeIndices IntVector:chromosomeLimits)
	
       ;; Use the command to determine the type of WFF to grow.
       (setq maxChromosomes esm.myMVLMaximum)
       (cond
         ;; Command is a genome vector.
         ((isVector command)
          (begin
            (if (>= (argCount) 2) (setq prevGenome (argFetch 1)))
            (setq genome command) 
          )) ; end case genome vector
         ;; Command requires growth of an mvl using root expressions.
         ((= command root:)
          (begin
		    (setq genome (new Vector: myM))
		    (setq M myM)
		    (loop for m from 0 until M do
		      ;; Select random root expressions for each chromosome in the genome.
              (if (= myGrowWFFStyle full:)
		          (setq genome[m] (ruleExp.growCTermWFF m))
		          (setq genome[m] (ruleExp.growTermWFF -2 m))
                  ) ; end if
		      ) ; end column loop  
          )) ; end case root command
         ;; Command requires growth of an mvl using full expressions.
         ((= command full:)
          (begin
		    (setq genome (new Vector: myM))
		    (setq M myM)
		    (loop for m from 0 until M do
		      ;; Select random full expressions for each chromosome in the genome.
              (if (= myGrowWFFStyle full:)
		          (setq genome[m] (ruleExp.growWFF expression: 0))
		          (setq genome[m] (ruleExp.growRootWFF -2 m))
                  ) ; end if
		      ) ; end column loop  
          )) ; end case full command
         ;; All other commands return an error.
         (else (error (append "esm.ruleMvl.growWFF: invalid command [" command "]")))
         ) ; end command cond
	
	   ;; Create the multivariate linear regression Selector from the final WFF.
	   Last::
       (if (isLambda genome) (setq genome genome.Genome)) 
       (setq wff (wffList (list ruleMvl: genome)))
       (setq genome wff[1])
       (setq cntChromosomes 0)
       (setq M (length genome))
       (loop for m from 0 until M do (if (<> genome[m] #void) (++ cntChromosomes)))
       (cond
        ;; We have less than the maximum number of chromosomes
	    ((>= maxChromosomes cntChromosomes) (setq Rf (createSelector wff prevGenome)))
        ;; We have more than the maximum number of chromosomes
        ;; Note: We must be an exhaustive search of all possible
        ;;       combinations of the chromosomes in this genome
        ;;       taken 'maxChromosomes' at a time.
	    (else 
         (begin
           (setq chromosomeIndices (new Vector: Integer: maxChromosomes))
           (setq chromosomeLimits (new Vector: Integer: maxChromosomes))
           (loop for mm from 0 until maxChromosomes do (setq chromosomeIndices[mm] mm) (setq chromosomeLimits[mm] (+ (- cntChromosomes maxChromosomes) mm)))
           (while (<= chromosomeIndices[0] chromosomeLimits[0]) do
             (setq newGenome (new Vector: M))
             (loop for mm from 0 until maxChromosomes do (setq k chromosomeIndices[mm]) (setq newGenome[mm] genome[k]))
             (setq Rf (createSelector (list ruleMvl: newGenome) prevGenome))
             ;; Increment the chromosome combinatatorial indices.
             (loop for mm from (- maxChromosomes 1) to 0 by -1 do (++ chromosomeIndices[mm]) (if (<= chromosomeIndices[mm] chromosomeLimits[mm]) (setq mm -1))) 
             (loop for mm from 1 until maxChromosomes do (if (> chromosomeIndices[mm] chromosomeLimits[mm]) (setq chromosomeIndices[mm] (+ chromosomeIndices[(- mm 1)] 1)))) 
             ) ; end while
         )) ; end else
        ) ; end cond
	   Rf) ; end growWFF

	(defun mutate(wff)
	;; *******************************************************************
	;; summary:  Create a new Selector multivariate linear regression WFF 
    ;;           using genetic mutation.
	;;
	;; args:     wff     The candidate WFF to be mutated.           
	;;           
	;; Return:   none.
	;;
	;; *******************************************************************
	   regs:(m M n N)
	   vars:(rule genome prevGenome newWff chromosome (Number:deathPct .10))
	
	   ;; Extract numeric WFFs from the candidate Selector Lambda (where necessary).
	   (if (= wff #void) (return false))
	   (if (isLambda wff) (setq wff wff.Genome))
	   (if (isPair wff) (setq wff wff[1]))
	   (setq genome wff)
	   (if (= genome #void) (setq genome myColumnGenome))
       (setq prevGenome genome)
	
	   ;; Make a copy of the candidate genome.
	   (setq genome (listWff genome))

       ;; Reorder the chromosomes in the candidate genome ocasionally.
       (if (<= (random 1.0) .50)
           then
           (begin 
             (setq m (integer (random myM)))
             (setq n (integer (random myM)))
             (setq chromosome genome[m])
             (setq genome[m] genome[n])
             (setq genome[n] chromosome)
           )) ; end if
       (setq m (integer (random myM))) 
       (if (= myMutateSpliceSW true) (setq newWff (mutateNumericWFF genome[m])) (begin (setq newWff (ruleExp.growRootWFF -2)) (goto Last:)))
       (setq n 0)(while (and (< (++ n) 20) (> (lengthWFF newWff) myMaxColWFFLen)) do (setq newWff (mutateNumericWFF genome[m])))
       (if (> (lengthWFF newWff) myMaxColWFFLen) (return false))
       (if (<= (random 1.0) deathPct) (setq genome[m] #void) (setq genome[m] newWff))

	   ;; Make a Selector multivariate linear regression wff and add it to the current population.
       Last::
       (growWFF genome prevGenome)
	
	   true) ; end mutate
		
    (defun wffList(wff)
	;; *******************************************************************
	;; summary:  Convert a selector multivariate linear regression wff to a 
    ;;           list format. 
	;;
	;; args:     wff         The multivariate linear regression wff.
	;;           
	;; Return:   result      A Selector multivariate linear regression WFF such as '(ruleMvl #(obj| (x0-x5) abs(x1)))
	;;
	;; *******************************************************************
       regs:(m M)
       vars:(wffListFormat rule genome)
       vars:(chromosome0 chromosome1)
       (if (isString wff) (setq wff wff) (setq wff (string wff true)))
       (setq wff (lisp wff)[0])
       (if (or (not (isPair wff)) (<> (length wff) 2) (<> wff[0] ruleMvl:)) (error "esm.ruleMvl.wffList: invalid WFF"))
       (setq rule wff[0])
       (setq genome (sort (copy wff[1]) >))
       ;; Eliminate all collisions before generating mvlregress genome.
       (setq M (length genome))
       (loop for m from 1 until M do
         (setq chromosome0 genome[(- m 1)])
         (setq chromosome1 genome[m])
         (if (and (<> chromosome1 #void) (= chromosome0 chromosome1))
             (setq genome[(- m 1)] #void)
             ) ; end if
         ) ; end loop   
       (setq genome (sort genome >))
       (setq wffListFormat (list rule genome))
       wffListFormat) ; end wffList


	(defun wffSource(genome)
	;; *******************************************************************
	;; summary:  Return a Selector multivariate linear regression statement 
    ;;           such as 
    ;;                     "mvlregress(x0-x5,abs(x1));"
    ;;  
	;;           as an ASCI string in grammatically correct Selector.
	;;
	;; args:     genome      The sparse vector of chromosome numeric expressions for the mvlregress statement.
	;;           
	;; Return:   result      A Selector multivariate linear regression statement such as "mvlregress(x0-x5,abs(x1));"
	;;
	;; *******************************************************************
       regs:(m M)
       vars:((wffSourceFormat "") rule genome)
       vars:(chromosome0 chromosome1)
       ;; Eliminate all collisions before generating bgmregress genome.
       (setq genome (sort (copy genome) >))
       (setq M (length genome))
       (loop for m from 1 until M do
         (setq chromosome0 genome[(- m 1)])
         (setq chromosome1 genome[m])
         (if (and (<> chromosome1 #void) (= chromosome0 chromosome1))
             (setq genome[(- m 1)] #void)
             ) ; end if
         ) ; end loop   
       (setq genome (sort genome >))
       ;; Create the final source from the genome.
       (setq M (length genome))
       (loop for m from 0 until M do
         ;; Select chromosomes from each genome.
         (cond 
           ((and (= wffSourceFormat "") (<> genome[m] #void)) (setq wffSourceFormat (ruleExp genome[m])))
           ((and (<> wffSourceFormat "") (<> genome[m] #void)) (setq wffSourceFormat (append wffSourceFormat "," (ruleExp genome[m]))))
           ) ; end chromosome cond
         ) ; end column loop  
	    ;; Generate a gramatically correct Selector multivariate linear regression statement in source format.
	   (setq wffSourceFormat (append "mvlregress(" wffSourceFormat ");"))
	   wffSourceFormat) ; end wffSource    

   (defun wffString(wff)
	;; *******************************************************************
	;; summary:  Convert a selector multivariate linear regression wff to a 
    ;;           string format. 
	;;
	;; args:     wff         The multivariate linear regression wff.
	;;           
	;; Return:   result      A Selector multivariate linear regression WFF such as "(ruleMvl #(obj| (x0-x5) abs(x1)))"
	;;
	;; *******************************************************************
       regs:(m M)
       vars:(wffStringFormat rule genome)
       vars:(chromosome0 chromosome1)
       (if (isString wff) (setq wff (lisp wff)[0]))
       (if (or (not (isPair wff)) (<> (length wff) 2) (<> wff[0] ruleMvl:)) (error "esm.ruleMvl.wffString: invalid WFF"))
       (setq rule wff[0])
       (setq genome (sort (copy wff[1]) >))
       ;; Eliminate all collisions before generating mvlregress genome.
       (setq M (length genome))
       (loop for m from 1 until M do
         (setq chromosome0 genome[(- m 1)])
         (setq chromosome1 genome[m])
         (if (and (<> chromosome1 #void) (= chromosome0 chromosome1))
             (setq genome[(- m 1)] #void)
             ) ; end if
         ) ; end loop   
       (setq genome (sort genome >))
       (setq wffStringFormat (string (list rule genome) true))
       wffStringFormat) ; end stringWff

   	;; *******************************************************************************
   	;; Begin main logic 
   	;; *******************************************************************************
	vars:(wffSourceFormat)
    
    (setq wffSourceFormat (wffSource wff[1]))
    wffSourceFormat) ; end ruleMvl











































;;**EXPORTKEY**:esm:ruleReg
(defriend esm:ruleReg(wff)
;; *******************************************************************
;; summary:  The WFF grammar rules for all Selector linear regression Lambdas. 
;;
;; Main:     Return a Selector linear regression statement,
;;           in source format, such as 
;;                    "regress(x2*(x2-x10));" 
;;           as an ASCI source string in grammatically correct selector.
;;
;; args:     wff         The Selector WFF of the linear regression statement.
;;           
;; Return:   result      A Selector linear regression statement such as "regress(x2*(x2-x10));".
;; *******************************************************************
  	pvars:(;; Public Variables
           (genomeType EXP)         ;; This regression rule uses an expression based genome.
           (lengthExempt false)     ;; This regression rule is NOT exempt from genome length restrictions.
           ;; Public Methods
           chromosomeLength         ;; Return the number of non-empty left-most chromosomes in the genome.
           crossOver                ;; Create two new Selector linear regression WFFs by splicing two parent WFFs together using genetic crossover.
           growWFF                  ;; Create a new Selector linear regression wff to the current population.
           mutate                   ;; Create a new Selector linear regression WFF using genetic mutation.
           wffList                  ;; Convert a Selector linear regression wff to a list format.
           wffSource				;; Convert a Selector linear regression wff to a source statement such as "regress(x0-x5*abs(x1));". 
           wffString                ;; Convert a Selector linear regression wff to a string format such as "(ruleReg #(obj| (ruleSub x0 x5) (ruleAbs x1)));"
           ) ; end persistant variables
   	;; *******************************************************************************
   	;; Define Public Child Lambdas 
   	;; *******************************************************************************

	(defun chromosomeLength(genome)
	;; *******************************************************************
	;; summary:  Return the number of non-empty left-most chromosomes in the genome. 
	;;
	;; args:     genome     The genome whose chromosome length is to be returned.           
	;;           
	;; Return:   length     The number of non-empty left-most chromosomes in the genome.
	;;
	;; *******************************************************************
	   regs:(m M len)
	
       (setq M (length genome))
	   (loop for m from 0 until M do
          (if (<> genome[m] #void) (++ len))
          ) ; end loop

	   len) ; end chromosomeLength


	(defun crossOver(father mother)
	;; *******************************************************************
	;; summary:  Create two new Selector linear regression WFFs, 
    ;;           by splicing two parent WFFs together using genetic crossover.
	;;
	;; args:     father     The father WFF to be mated.           
	;;           mother     The mother WFF to be mated.           
	;;           
	;; Return:   none.
	;;
	;; *******************************************************************
	   regs:(m M n N)
	   vars:(dadGenome momGenome dadRule momRule newGenome)
	   vars:(dadWff momWff newWff (passOne true))
       vars:(dadIndex momIndex dadPair momSlot dadWff chromosomeCrossPct)
	
	   ;; Extract numeric WFFs from the parent Selector Lambdas (where necessary).
	   (if (or (= father #void) (= mother #void)) (return false))
	   (if (isLambda father) (begin (setq dadRule father.Rule) (setq father father.Genome)))
	   (if (isLambda mother) (begin (setq momRule mother.Rule) (setq mother mother.Genome)))
	   (if (isPair father) (begin (setq dadRule father[0]) (setq father father[1])))
	   (if (isPair mother) (begin (setq momRule mother[0]) (setq mother mother[1])))
	   (setq dadGenome father)
	   (setq momGenome mother)

	   ;; Make sure the father and mother are genetically compatible.
	   (if (or (= father #void) (= mother #void)) (return false))
	   (if (<> esm[momRule].genomeType EXP:) (return false))
	
	   Retry::     
	
	   ;; Make copies of the candidate genomes.
	   (setq dadGenome (listWff father))
	   (setq momGenome (listWff mother))

       ;; Do we use a linear genome during cross over operations?
       (if (= myCrossLinearSW true)
           (begin
             ;; Reorder the chromosomes in the candidate genomes occasionally.
             (setq M (chromosomeLength dadGenome))
             (setq N (chromosomeLength momGenome))
             (if (<= (random 1.0) .50)
                 then
                 (begin 
                   (setq dadGenome (copy dadGenome))
                   (setq m (integer (random M)))
                   (setq n (integer (random M)))
                   (setq chromosome dadGenome[m])
                   (setq dadGenome[m] dadGenome[n])
                   (setq dadGenome[n] chromosome)
                 )) ; end if
             (if (<= (random 1.0) .50)
                 then
                 (begin 
                   (setq momGenome (copy momGenome))
                   (setq m (integer (random N)))
                   (setq n (integer (random N)))
                   (setq chromosome momGenome[m])
                   (setq momGenome[m] momGenome[n])
                   (setq momGenome[n] chromosome)
                 )) ; end if
      
             ;; Select chromosomes from each genome.
             (setq genome (copy dadGenome))           
             (setq M (max M N))
             (setq chromosomeCrossPct 1.5)
             (loop for m from 0 until M do
               (if (>= chromosomeCrossPct .50) (-= chromosomeCrossPct .50))
               (cond
                 ((> (random 1.0) chromosomeCrossPct) (setq genome[m] genome[m]))
                 ((= genome[m] #void) (if (<= (random 1.0) .25) then (setq genome[m] momGenome[m])))
                 ((= momGenome[m] #void) (if (<= (random 1.0) .25) then (setq genome[m] (ruleExp.mutateNumericWFF genome[m] (random 1.0)))))
                 ((and (= myCrossMarrySW true) (<= (random 1.0) .20) (<> (setq newWff (ruleExp.marryNumericWFFs genome[m] momGenome[m])) false)) (setq genome[m] newWff))
                 ((= myCrossSpliceSW false) (if (<= (random 1.0) .50) then (setq genome[m] momGenome[m])))
                 (else 
                  (begin
                     (setq dadWff genome[m])                    
                     (setq momSlot (setq momIndex (esm.ruleExp.selectSlotWFF momGenome[m]))[(integer (random (length momIndex)))])
                     (setq dadPair (setq dadIndex (esm.ruleExp.selectPairWFF dadWff))[(integer (random (length dadIndex)))])
                     (cond
                      ((isAtom dadPair) (setq genome[m] momSlot))
                      ((and (isPair momSlot) (= myCrossMarrySW true) (<= (random 1.0) .20) (<> (setq newWff (ruleExp.marryNumericWFFs (listWff dadPair) (listWff momSlot))) false))
                       (begin (setCar dadPair (car newWff))  (setCdr dadPair (cdr newWff))))
                      ((isPair momSlot) (begin (setCar dadPair (car momSlot))  (setCdr dadPair (cdr momSlot))))
                      (else (ruleExp.spliceNumericWFF dadPair momSlot 1.0))
                      ) ; end splice cond
                  )) ; end case
                 ) ; end chromosome cond           
               ) ; end column loop  	
      
             (goto Grow:)
           )) ; end cross over linear genome if

       ;; Do we cross over chromosomes from different columns?
       (if (= myCrossChromosomeSW true)
           (begin
             (setq m (integer (random (chromosomeLength dadGenome))))
             (setq n (integer (random (chromosomeLength momGenome))))
             (setq genome (copy dadGenome))
             
             ;; Select chromosomes for cross over to create child genome.
             (cond 
               ((= dadGenome[m] #void) (setq genome[m] momGenome[n]))
               ((and (= momGenome[n] #void) (= myCrossSpliceSW true)) (setq genome[m] (setq newWff (spliceNumericWFF (listWff dadGenome[m]) (cutoutNumericWFF (listWff dadGenome[m]))))))
               ((and (= momGenome[n] #void) (= myCrossMarrySW true) (<> (setq newWff (ruleExp.marryNumericWFFs dadGenome[m] dadGenome[m])) false)) (setq genome[m] newWff))
               ((= momGenome[n] #void) (setq genome[m] momGenome[n]))
               ((and (= myCrossMarrySW true) (<= (random 1.0) .20) (<> (setq newWff (ruleExp.marryNumericWFFs dadGenome[m] momGenome[n])) false)) (setq genome[m] newWff))
               ((= myCrossSpliceSW false) (setq genome[m] momGenome[n]))
               (else ;; Here myCrossSpliceSW is true and both dadGenome and momGenome are not void. 
                (begin
                   (setq newWff (spliceNumericWFF (listWff dadGenome[m]) (cutoutNumericWFF (listWff momGenome[n]))))
                   (if (> (lengthWFF newWff) myMaxColWFFLen)
                       (setq newWff (spliceNumericWFF (listWff dadGenome[m]) (cutoutNumericWFF (listWff momGenome[n]))))
                       ) ; end if
                   (if (> (lengthWFF newWff) myMaxColWFFLen)
                       (setq newWff (spliceNumericWFF (listWff dadGenome[m]) (cutoutNumericWFF (listWff momGenome[n]))))
                       ) ; end if
                   (if (> (lengthWFF newWff) myMaxColWFFLen)
                       (setq newWff (spliceNumericWFF (listWff dadGenome[m]) (cutoutNumericWFF (listWff momGenome[n]))))
                       ) ; end if
                   (if (> (lengthWFF newWff) myMaxColWFFLen) 
                       (setq newWff momGenome[n])
                       ) ; end if
                   (setq genome[m] newWff)
                )) ; end case
               ) ; end chromosome cond           
             (goto Grow:)
           )) ; end cross over chromosomes if

	   ;; Reorder the chromosomes in the candidate genomes occasionally.
	   (if (<= (random 1.0) .50)
	       then
	       (begin 
	         (setq dadGenome (copy dadGenome))
	         (setq m (integer (random myM)))
	         (setq n (integer (random myM)))
	         (setq chromosome dadGenome[m])
	         (setq dadGenome[m] dadGenome[n])
	         (setq dadGenome[n] chromosome)
	       )) ; end if
	   (if (<= (random 1.0) .50)
	       then
	       (begin 
	         (setq momGenome (copy momGenome))
	         (setq m (integer (random myM)))
	         (setq n (integer (random myM)))
	         (setq chromosome momGenome[m])
	         (setq momGenome[m] momGenome[n])
	         (setq momGenome[n] chromosome)
	       )) ; end if

	   ;; Splice the chromosomes in the candidate genomes to form the progeny.
       (setq genome (new Vector: myM))
       (setq M myM)
       (loop for m from 0 until M do
         ;; Select chromosomes from each genome.
         (cond 
           ((= dadGenome[m] #void) (setq genome[m] momGenome[m]))
           ((= momGenome[m] #void) (setq genome[m] dadGenome[m]))
           ((and (= myCrossMarrySW true) (<= (random 1.0) .20) (<> (setq newWff (ruleExp.marryNumericWFFs dadGenome[m] momGenome[m])) false)) (setq genome[m] newWff))
           ((<= (random 1.0) .10) (setq genome[m] dadGenome[m]))
           ((<= (random 1.0) .10) (setq genome[m] momGenome[m]))
           ((and (= myCrossSpliceSW false) (<= (random 1.0) .50)) (setq genome[m] dadGenome[m]))
           ((= myCrossSpliceSW false) (setq genome[m] momGenome[m]))
           (else 
            (begin
               (setq newWff (spliceNumericWFF (listWff dadGenome[m]) (cutoutNumericWFF (listWff momGenome[m]))))
               (if (> (lengthWFF newWff) myMaxColWFFLen)
                   (setq newWff (spliceNumericWFF (listWff dadGenome[m]) (cutoutNumericWFF (listWff momGenome[m]))))
                   ) ; end if
               (if (> (lengthWFF newWff) myMaxColWFFLen)
                   (setq newWff (spliceNumericWFF (listWff dadGenome[m]) (cutoutNumericWFF (listWff momGenome[m]))))
                   ) ; end if
               (if (> (lengthWFF newWff) myMaxColWFFLen)
                   (setq newWff (spliceNumericWFF (listWff dadGenome[m]) (cutoutNumericWFF (listWff momGenome[m]))))
                   ) ; end if
               (if (> (lengthWFF newWff) myMaxColWFFLen)
                   (if (< (random 1.0) .50) (setq newWff dadGenome[m]) (setq newWff momGenome[m]))
                   ) ; end if
               (setq genome[m] newWff)
            )) ; end case
           ) ; end chromosome cond           
         ) ; end column loop  	

	   ;; Make a Selector linear regression wff and add it to the current population.
       Grow::
       (growWFF genome)
	
	   ;; Make sure to mate the father with the mother
	   (if (= passOne true) 
	       (begin 
	          (setq passOne false) 
	          (setq dadGenome mother)
	          (setq momGenome father)
	          (goto Retry:)
	       )) ; end if
	
	   true) ; end crossOver
	
	(defun growWFF(command ...)
	;; *******************************************************************
	;; summary:  Grow a new Selector linear regression WFF and
	;;           add it to the current population.
	;;
	;; args:     command 	The command to determine the type of WFF to grow..
	;;           colChoice  (Optional)The choice of root column.
	;;           
	;; Return:   Lambda      A scored Selector linear regression Lambda.
	;;
	;; *******************************************************************
	   regs:(k m M mm n N maxChromosomes cntChromosomes)
	   vars:(Lambda genome newGenome colChoice)
	   vars:(Rf wff)
	   vars:(IntVector:chromosomeIndices IntVector:chromosomeLimits)
	
       ;; Use the command to determine the type of WFF to grow.
       (setq maxChromosomes esm.myREGMaximum)
       (if (>= (argCount) 2) (setq colChoice (argFetch 1)))
       (if (isNumber colChoice) (setq colChoice (integer colChoice)) (setq colChoice (integer (random myM))))
       (cond
         ;; Command is a genome vector.
         ((isVector command)
          (begin 
            (setq genome command) 
          )) ; end case genome vector
         ;; Command requires growth of an mvl using root expressions.
         ((= command root:)
          (begin
            (if (= myREGMultiple true)
                (begin
		          ;; Select random root expressions for each chromosome in the genome.
		          (setq genome (new Vector: myM))
		          (setq M myM)
		          (loop for m from 0 until M do
		            ;; Select random root expressions for each chromosome in the genome.
                    (if (= myGrowWFFStyle full:)
		                (setq genome[m] (ruleExp.growCTermWFF m))
		                (setq genome[m] (ruleExp.growTermWFF -2 m))
                        ) ; end if
		            ) ; end column loop  
                ) else
                (begin
		          ;; Select random root expressions for the only one random chromosome in the genome.
		          (setq genome (new Vector: myM))
                  (if (= myGrowWFFStyle full:)
		              (setq genome[0] (ruleExp.growCTermWFF colChoice))
		              (setq genome[0] (ruleExp.growTermWFF -2 colChoice))
                      ) ; end if
                )) ; end multiple columns if
          )) ; end case root command
         ;; Command requires growth of an mvl using full expressions.
         ((= command full:)
          (begin
            (if (= myREGMultiple true)
                (begin
		          ;; Select random full expressions for each chromosome in the genome.
		          (setq genome (new Vector: myM))
		          (setq M myM)
		          (loop for m from 0 until M do
                    (if (= myGrowWFFStyle full:)
		                (setq genome[m] (ruleExp.growWFF expression: 0))
		                (setq genome[m] (ruleExp.growRootWFF -2 m))
                        ) ; end if
		            ) ; end column loop  
                ) else
                (begin
		          ;; Select random full expressions for the only one random chromosome in the genome.
		          (setq genome (new Vector: myM))
		          ;; Select random full expressions for each chromosome in the genome.
                  (if (= myGrowWFFStyle full:)
		              (setq genome[0] (ruleExp.growWFF expression: 0))
		              (setq genome[0] (ruleExp.growRootWFF -2 colChoice))
                      ) ; end if
                )) ; end multiple columns if
          )) ; end case full command
         ;; All other commands return an error.
         (else (error (append "esm.ruleReg.growWFF: invalid command [" command "]")))
         ) ; end command cond
	
	   ;; Create the linear regression Selector from the final WFF.
	   Last::
       (if (isLambda genome) (setq genome genome.Genome)) 
       (setq wff (wffList (list ruleReg: genome)))
       (setq genome wff[1])
       (setq cntChromosomes 0)
       (setq M (length genome))
       (loop for m from 0 until M do (if (<> genome[m] #void) (++ cntChromosomes)))
       (cond
        ;; We have less than the maximum number of chromosomes
	    ((>= maxChromosomes cntChromosomes) (setq Rf (createSelector wff)))
        ;; We have more than the maximum number of chromosomes
        ;; Note: We must be an exhaustive search of all possible
        ;;       combinations of the chromosomes in this genome
        ;;       taken 'maxChromosomes' at a time.
	    (else 
         (begin
           (setq chromosomeIndices (new Vector: Integer: maxChromosomes))
           (setq chromosomeLimits (new Vector: Integer: maxChromosomes))
           (loop for mm from 0 until maxChromosomes do (setq chromosomeIndices[mm] mm) (setq chromosomeLimits[mm] (+ (- cntChromosomes maxChromosomes) mm)))
           (while (<= chromosomeIndices[0] chromosomeLimits[0]) do
             (setq newGenome (new Vector: M))
             (loop for mm from 0 until maxChromosomes do (setq k chromosomeIndices[mm]) (setq newGenome[mm] genome[k]))
             (setq Rf (createSelector (list ruleReg: newGenome)))
             ;; Increment the chromosome combinatatorial indices.
             (loop for mm from (- maxChromosomes 1) to 0 by -1 do (++ chromosomeIndices[mm]) (if (<= chromosomeIndices[mm] chromosomeLimits[mm]) (setq mm -1))) 
             (loop for mm from 1 until maxChromosomes do (if (> chromosomeIndices[mm] chromosomeLimits[mm]) (setq chromosomeIndices[mm] (+ chromosomeIndices[(- mm 1)] 1)))) 
             ) ; end while
         )) ; end else
        ) ; end cond
	   Rf) ; end growWFF

	(defun mutate(wff)
	;; *******************************************************************
	;; summary:  Create a new Selector linear regression WFF 
    ;;           using genetic mutation.
	;;
	;; args:     wff     The candidate WFF to be mutated.           
	;;           
	;; Return:   none.
	;;
	;; *******************************************************************
	   regs:(m M n N)
	   vars:(rule genome prevGenome newWff chromosome (Number:deathPct .10))
	
	   ;; Extract numeric WFFs from the candidate Selector Lambda (where necessary).
	   (if (= wff #void) (return false))
	   (if (isLambda wff) (setq wff wff.Genome))
	   (if (isPair wff) (setq wff wff[1]))
	   (setq genome wff)
	   (if (= genome #void) (setq genome myColumnGenome))
       (setq prevGenome genome)
	
	   ;; Make a copy of the candidate genome.
	   (setq genome (listWff genome))

       ;; Reorder the chromosomes in the candidate genome ocasionally.
       (if (<= (random 1.0) .50)
           then
           (begin 
             (setq m (integer (random myM)))
             (setq n (integer (random myM)))
             (setq chromosome genome[m])
             (setq genome[m] genome[n])
             (setq genome[n] chromosome)
           )) ; end if
       (setq m (integer (random myM))) 
       (if (= myMutateSpliceSW true) (setq newWff (mutateNumericWFF genome[m])) (begin (setq newWff (ruleExp.growRootWFF -2)) (goto Last:)))
       (setq n 0)(while (and (< (++ n) 20) (> (lengthWFF newWff) myMaxColWFFLen)) do (setq newWff (mutateNumericWFF genome[m])))
       (if (> (lengthWFF newWff) myMaxColWFFLen) (return false))
       (if (<= (random 1.0) deathPct) (setq genome[m] #void) (setq genome[m] newWff))

	   ;; Make a Selector multivariate linear regression wff and add it to the current population.
       Last::
       (growWFF genome)
	
	   true) ; end mutate
		
    (defun wffList(wff)
	;; *******************************************************************
	;; summary:  Convert a selector linear regression wff to a 
    ;;           list format. 
	;;
	;; args:     wff         The linear regression wff.
	;;           
	;; Return:   result      A Selector linear regression WFF such as '(ruleReg #(obj| (x0-x5) abs(x1)))
	;;
	;; *******************************************************************
       regs:(m M)
       vars:(wffListFormat rule genome)
       vars:(chromosome0 chromosome1)
       (if (isString wff) (setq wff wff) (setq wff (string wff true)))
       (setq wff (lisp wff)[0])
       (if (or (not (isPair wff)) (<> (length wff) 2) (<> wff[0] ruleReg:)) (error "esm.ruleReg.wffList: invalid WFF"))
       (setq rule wff[0])
       (setq genome (sort (copy wff[1]) >))
       (setq wffListFormat (list rule genome))
       wffListFormat) ; end wffList


	(defun wffSource(genome)
	;; *******************************************************************
	;; summary:  Return a Selector linear regression statement 
    ;;           such as 
    ;;                     "regress(x0-x5*abs(x1));"
    ;;  
	;;           as an ASCI string in grammatically correct Selector.
	;;
	;; args:     genome      The sparse vector of chromosome numeric expressions for the regress statement.
	;;           
	;; Return:   result      A Selector linear regression statement such as "regress(x0-x5*abs(x1));"
	;;
	;; *******************************************************************
       regs:(m M)
       vars:((wffSourceFormat "") rule genome)
       vars:(chromosome0 chromosome1)
       ;; Create the final source from the genome.
       (setq genome (sort (copy genome) >))
       (setq M (length genome))
       (loop for m from 0 until M do
         ;; Select chromosomes from each genome.
         (cond 
           ((and (= wffSourceFormat "") (<> genome[m] #void)) (setq wffSourceFormat (ruleExp genome[m])))
           ((and (<> wffSourceFormat "") (<> genome[m] #void)) (setq wffSourceFormat (append wffSourceFormat myREGOperatorJoin (ruleExp genome[m]))))
           ) ; end chromosome cond
         ) ; end column loop  
	    ;; Generate a gramatically correct Selector linear regression statement in source format.
	   (setq wffSourceFormat (append "regress(" wffSourceFormat ");"))
	   wffSourceFormat) ; end wffSource    

   (defun wffString(wff)
	;; *******************************************************************
	;; summary:  Convert a selector linear regression wff to a 
    ;;           string format. 
	;;
	;; args:     wff         The linear regression wff.
	;;           
	;; Return:   result      A Selector linear regression WFF such as "(ruleReg #(obj| (x0-x5) abs(x1)))"
	;;
	;; *******************************************************************
       regs:(m M)
       vars:(wffStringFormat rule genome)
       vars:(chromosome0 chromosome1)
       (if (isString wff) (setq wff (lisp wff)[0]))
       (if (or (not (isPair wff)) (<> (length wff) 2) (<> wff[0] ruleReg:)) (error "esm.ruleReg.wffString: invalid WFF"))
       (setq rule wff[0])
       (setq genome (sort (copy wff[1]) >))
       (setq wffStringFormat (string (list rule genome) true))
       wffStringFormat) ; end stringWff

   	;; *******************************************************************************
   	;; Begin main logic 
   	;; *******************************************************************************
	vars:(wffSourceFormat)
    
    (setq wffSourceFormat (wffSource wff[1]))
    wffSourceFormat) ; end ruleReg










































;;**EXPORTKEY**:esm:ruleSvm
(defriend esm:ruleSvm(wff)
;; *******************************************************************
;; summary:  The WFF grammar rules for all Selector support 
;;           vector regression Lambdas. 
;;
;; Main:     Return a Selector support vector regression statement,
;;           in source format, such as 
;;                    "svmregress(x2,(x2-x10));" 
;;           as an ASCI source string in grammatically correct selector.
;;
;; args:     wff         The Selector WFF of the support vector regression statement.
;;           
;; Return:   result      A Selector support vector regression statement such as "svmregress(x2,(x2-x10));".
;; *******************************************************************
  	pvars:(;; Public Variables
           (genomeType EXP)         ;; This regression rule uses an expression based genome.
           (lengthExempt false)     ;; This regression rule is NOT exempt from genome length restrictions.
           ;; Public Methods
           chromosomeLength         ;; Return the number of non-empty left-most chromosomes in the genome.
           crossOver                ;; Create two new Selector support vector regression WFFs by splicing two parent WFFs together using genetic crossover.
           growWFF                  ;; Create a new Selector support vector regression wff to the current population.
           mutate                   ;; Create a new Selector support vector regression WFF using genetic mutation.
           wffList                  ;; Convert a Selector support vector regression wff to a list format.
           wffSource				;; Convert a Selector support vector regression wff to a source statement such as "svmregress(x0-x5,abs(x1));". 
           wffString                ;; Convert a Selector support vector regression wff to a string format such as "(ruleSvm #(obj| (ruleSub x0 x5) (ruleAbs x1)));"
           ) ; end persistant variables
   	;; *******************************************************************************
   	;; Define Public Child Lambdas 
   	;; *******************************************************************************

	(defun chromosomeLength(genome)
	;; *******************************************************************
	;; summary:  Return the number of non-empty left-most chromosomes in the genome. 
	;;
	;; args:     genome     The genome whose chromosome length is to be returned.           
	;;           
	;; Return:   length     The number of non-empty left-most chromosomes in the genome.
	;;
	;; *******************************************************************
	   regs:(m M len)
	
       (setq M (length genome))
	   (loop for m from 0 until M do
          (if (<> genome[m] #void) (++ len))
          ) ; end loop

	   len) ; end chromosomeLength


	(defun crossOver(father mother)
	;; *******************************************************************
	;; summary:  Create two new Selector support vector regression WFFs, 
    ;;           by splicing two parent WFFs together using genetic crossover.
	;;
	;; args:     father     The father WFF to be mated.           
	;;           mother     The mother WFF to be mated.           
	;;           
	;; Return:   none.
	;;
	;; *******************************************************************
	   regs:(m M n N)
	   vars:(dadGenome momGenome dadRule momRule newGenome)
	   vars:(dadWff momWff newWff (passOne true))
       vars:(dadIndex momIndex dadPair momSlot dadWff chromosomeCrossPct)
	
	   ;; Extract numeric WFFs from the parent Selector Lambdas (where necessary).
	   (if (or (= father #void) (= mother #void)) (return false))
	   (if (isLambda father) (begin (setq dadRule father.Rule) (setq father father.Genome)))
	   (if (isLambda mother) (begin (setq momRule mother.Rule) (setq mother mother.Genome)))
	   (if (isPair father) (begin (setq dadRule father[0]) (setq father father[1])))
	   (if (isPair mother) (begin (setq momRule mother[0]) (setq mother mother[1])))
	   (setq dadGenome father)
	   (setq momGenome mother)

	   ;; Make sure the father and mother are genetically compatible.
	   (if (or (= father #void) (= mother #void)) (return false))
	   (if (<> esm[momRule].genomeType EXP:) (return false))
	
	   Retry::     
	
	   ;; Make copies of the candidate genomes.
	   (setq dadGenome (listWff father))
	   (setq momGenome (listWff mother))

       ;; Do we use a linear genome during cross over operations?
       (if (= myCrossLinearSW true)
           (begin
             ;; Reorder the chromosomes in the candidate genomes occasionally.
             (setq M (chromosomeLength dadGenome))
             (setq N (chromosomeLength momGenome))
             (if (<= (random 1.0) .50)
                 then
                 (begin 
                   (setq dadGenome (copy dadGenome))
                   (setq m (integer (random M)))
                   (setq n (integer (random M)))
                   (setq chromosome dadGenome[m])
                   (setq dadGenome[m] dadGenome[n])
                   (setq dadGenome[n] chromosome)
                 )) ; end if
             (if (<= (random 1.0) .50)
                 then
                 (begin 
                   (setq momGenome (copy momGenome))
                   (setq m (integer (random N)))
                   (setq n (integer (random N)))
                   (setq chromosome momGenome[m])
                   (setq momGenome[m] momGenome[n])
                   (setq momGenome[n] chromosome)
                 )) ; end if
      
             ;; Select chromosomes from each genome.
             (setq genome (copy dadGenome))           
             (setq M (max M N))
             (setq chromosomeCrossPct 1.5)
             (loop for m from 0 until M do
               (if (>= chromosomeCrossPct .50) (-= chromosomeCrossPct .50))
               (cond
                 ((> (random 1.0) chromosomeCrossPct) (setq genome[m] genome[m]))
                 ((= genome[m] #void) (if (<= (random 1.0) .25) then (setq genome[m] momGenome[m])))
                 ((= momGenome[m] #void) (if (<= (random 1.0) .25) then (setq genome[m] (ruleExp.mutateNumericWFF genome[m] (random 1.0)))))
                 ((and (= myCrossMarrySW true) (<= (random 1.0) .20) (<> (setq newWff (ruleExp.marryNumericWFFs genome[m] momGenome[m])) false)) (setq genome[m] newWff))
                 ((= myCrossSpliceSW false) (if (<= (random 1.0) .50) then (setq genome[m] momGenome[m])))
                 (else 
                  (begin
                     (setq dadWff genome[m])                    
                     (setq momSlot (setq momIndex (esm.ruleExp.selectSlotWFF momGenome[m]))[(integer (random (length momIndex)))])
                     (setq dadPair (setq dadIndex (esm.ruleExp.selectPairWFF dadWff))[(integer (random (length dadIndex)))])
                     (cond
                      ((isAtom dadPair) (setq genome[m] momSlot))
                      ((and (isPair momSlot) (= myCrossMarrySW true) (<= (random 1.0) .20) (<> (setq newWff (ruleExp.marryNumericWFFs (listWff dadPair) (listWff momSlot))) false))
                       (begin (setCar dadPair (car newWff))  (setCdr dadPair (cdr newWff))))
                      ((isPair momSlot) (begin (setCar dadPair (car momSlot))  (setCdr dadPair (cdr momSlot))))
                      (else (ruleExp.spliceNumericWFF dadPair momSlot 1.0))
                      ) ; end splice cond
                  )) ; end case
                 ) ; end chromosome cond           
               ) ; end column loop  	
      
             (goto Grow:)
           )) ; end cross over linear genome if

       ;; Do we cross over chromosomes from different columns?
       (if (= myCrossChromosomeSW true)
           (begin
             (setq m (integer (random (chromosomeLength dadGenome))))
             (setq n (integer (random (chromosomeLength momGenome))))
             (setq genome (copy dadGenome))
             
             ;; Select chromosomes for cross over to create child genome.
             (cond 
               ((= dadGenome[m] #void) (setq genome[m] momGenome[n]))
               ((and (= momGenome[n] #void) (= myCrossSpliceSW true)) (setq genome[m] (setq newWff (spliceNumericWFF (listWff dadGenome[m]) (cutoutNumericWFF (listWff dadGenome[m]))))))
               ((and (= momGenome[n] #void) (= myCrossMarrySW true) (<> (setq newWff (ruleExp.marryNumericWFFs dadGenome[m] dadGenome[m])) false)) (setq genome[m] newWff))
               ((= momGenome[n] #void) (setq genome[m] momGenome[n]))
               ((and (= myCrossMarrySW true) (<= (random 1.0) .20) (<> (setq newWff (ruleExp.marryNumericWFFs dadGenome[m] momGenome[n])) false)) (setq genome[m] newWff))
               ((= myCrossSpliceSW false) (setq genome[m] momGenome[n]))
               (else ;; Here myCrossSpliceSW is true and both dadGenome and momGenome are not void. 
                (begin
                   (setq newWff (spliceNumericWFF (listWff dadGenome[m]) (cutoutNumericWFF (listWff momGenome[n]))))
                   (if (> (lengthWFF newWff) myMaxColWFFLen)
                       (setq newWff (spliceNumericWFF (listWff dadGenome[m]) (cutoutNumericWFF (listWff momGenome[n]))))
                       ) ; end if
                   (if (> (lengthWFF newWff) myMaxColWFFLen)
                       (setq newWff (spliceNumericWFF (listWff dadGenome[m]) (cutoutNumericWFF (listWff momGenome[n]))))
                       ) ; end if
                   (if (> (lengthWFF newWff) myMaxColWFFLen)
                       (setq newWff (spliceNumericWFF (listWff dadGenome[m]) (cutoutNumericWFF (listWff momGenome[n]))))
                       ) ; end if
                   (if (> (lengthWFF newWff) myMaxColWFFLen) 
                       (setq newWff momGenome[n])
                       ) ; end if
                   (setq genome[m] newWff)
                )) ; end case
               ) ; end chromosome cond           
             (goto Grow:)
           )) ; end cross over chromosomes if

	   ;; Reorder the chromosomes in the candidate genomes occasionally.
	   (if (<= (random 1.0) .50)
	       then
	       (begin 
	         (setq dadGenome (copy dadGenome))
	         (setq m (integer (random myM)))
	         (setq n (integer (random myM)))
	         (setq chromosome dadGenome[m])
	         (setq dadGenome[m] dadGenome[n])
	         (setq dadGenome[n] chromosome)
	       )) ; end if
	   (if (<= (random 1.0) .50)
	       then
	       (begin 
	         (setq momGenome (copy momGenome))
	         (setq m (integer (random myM)))
	         (setq n (integer (random myM)))
	         (setq chromosome momGenome[m])
	         (setq momGenome[m] momGenome[n])
	         (setq momGenome[n] chromosome)
	       )) ; end if

	   ;; Splice the chromosomes in the candidate genomes to form the progeny.
       (setq genome (new Vector: myM))
       (setq M myM)
       (loop for m from 0 until M do
         ;; Select chromosomes from each genome.
         (cond 
           ((= dadGenome[m] #void) (setq genome[m] momGenome[m]))
           ((= momGenome[m] #void) (setq genome[m] dadGenome[m]))
           ((and (= myCrossMarrySW true) (<= (random 1.0) .20) (<> (setq newWff (ruleExp.marryNumericWFFs dadGenome[m] momGenome[m])) false)) (setq genome[m] newWff))
           ((<= (random 1.0) .10) (setq genome[m] dadGenome[m]))
           ((<= (random 1.0) .10) (setq genome[m] momGenome[m]))
           ((and (= myCrossSpliceSW false) (<= (random 1.0) .50)) (setq genome[m] dadGenome[m]))
           ((= myCrossSpliceSW false) (setq genome[m] momGenome[m]))
           (else 
            (begin
               (setq newWff (spliceNumericWFF (listWff dadGenome[m]) (cutoutNumericWFF (listWff momGenome[m]))))
               (if (> (lengthWFF newWff) myMaxColWFFLen)
                   (setq newWff (spliceNumericWFF (listWff dadGenome[m]) (cutoutNumericWFF (listWff momGenome[m]))))
                   ) ; end if
               (if (> (lengthWFF newWff) myMaxColWFFLen)
                   (setq newWff (spliceNumericWFF (listWff dadGenome[m]) (cutoutNumericWFF (listWff momGenome[m]))))
                   ) ; end if
               (if (> (lengthWFF newWff) myMaxColWFFLen)
                   (setq newWff (spliceNumericWFF (listWff dadGenome[m]) (cutoutNumericWFF (listWff momGenome[m]))))
                   ) ; end if
               (if (> (lengthWFF newWff) myMaxColWFFLen)
                   (if (< (random 1.0) .50) (setq newWff dadGenome[m]) (setq newWff momGenome[m]))
                   ) ; end if
               (setq genome[m] newWff)
            )) ; end case
           ) ; end chromosome cond           
         ) ; end column loop  	

	   ;; Make a Selector support vector regression wff and add it to the current population.
       Grow::
       (growWFF genome dadGenome)
	
	   ;; Make sure to mate the father with the mother
	   (if (= passOne true) 
	       (begin 
	          (setq passOne false) 
	          (setq dadGenome mother)
	          (setq momGenome father)
	          (goto Retry:)
	       )) ; end if
	
	   true) ; end crossOver
	
	(defun growWFF(command ...)
	;; *******************************************************************
	;; summary:  Grow a new Selector support vector regression WFF and
	;;           add it to the current population.
	;;
	;; args:     command 	The command to determine the type of WFF to grow..
	;;           prevGenome (Optional)The previous genome (for use by particle swarm operators).
	;;           
	;; Return:   Lambda      A scored Selector support vector regression Lambda.
	;;
	;; *******************************************************************
	   regs:(k m M mm n N maxChromosomes cntChromosomes)
	   vars:(Lambda genome newGenome prevGenome)
	   vars:(Rf wff)
	   vars:(IntVector:chromosomeIndices IntVector:chromosomeLimits)
	
       ;; Use the command to determine the type of WFF to grow.
       (setq maxChromosomes esm.mySVMMaximum)
       (cond
         ;; Command is a genome vector.
         ((isVector command)
          (begin
            (if (>= (argCount) 2) (setq prevGenome (argFetch 1)))
            (setq genome command) 
          )) ; end case genome vector
         ;; Command requires growth of an mvl using root expressions.
         ((= command root:)
          (begin
		    (setq genome (new Vector: myM))
		    (setq M myM)
		    (loop for m from 0 until M do
		      ;; Select random root expressions for each chromosome in the genome.
              (if (= myGrowWFFStyle full:)
		          (setq genome[m] (ruleExp.growCTermWFF m))
		          (setq genome[m] (ruleExp.growTermWFF -2 m))
                  ) ; end if
		      ) ; end column loop  
          )) ; end case root command
         ;; Command requires growth of an mvl using full expressions.
         ((= command full:)
          (begin
		    (setq genome (new Vector: myM))
		    (setq M myM)
		    (loop for m from 0 until M do
		      ;; Select random full expressions for each chromosome in the genome.
              (if (= myGrowWFFStyle full:)
		          (setq genome[m] (ruleExp.growWFF expression: 0))
		          (setq genome[m] (ruleExp.growRootWFF -2 m))
                  ) ; end if
		      ) ; end column loop  
          )) ; end case full command
         ;; All other commands return an error.
         (else (error (append "esm.ruleSvm.growWFF: invalid command [" command "]")))
         ) ; end command cond
	
	   ;; Create the support vector regression Selector from the final WFF.
	   Last::
       (if (isLambda genome) (setq genome genome.Genome)) 
       (setq wff (wffList (list ruleSvm: genome)))
       (setq genome wff[1])
       (setq cntChromosomes 0)
       (setq M (length genome))
       (loop for m from 0 until M do (if (<> genome[m] #void) (++ cntChromosomes)))
       (cond
        ;; We have less than the maximum number of chromosomes
	    ((>= maxChromosomes cntChromosomes) (setq Rf (createSelector wff prevGenome)))
        ;; We have more than the maximum number of chromosomes
        ;; Note: We must be an exhaustive search of all possible
        ;;       combinations of the chromosomes in this genome
        ;;       taken 'maxChromosomes' at a time.
	    (else 
         (begin
           (setq chromosomeIndices (new Vector: Integer: maxChromosomes))
           (setq chromosomeLimits (new Vector: Integer: maxChromosomes))
           (loop for mm from 0 until maxChromosomes do (setq chromosomeIndices[mm] mm) (setq chromosomeLimits[mm] (+ (- cntChromosomes maxChromosomes) mm)))
           (while (<= chromosomeIndices[0] chromosomeLimits[0]) do
             (setq newGenome (new Vector: M))
             (loop for mm from 0 until maxChromosomes do (setq k chromosomeIndices[mm]) (setq newGenome[mm] genome[k]))
             (setq Rf (createSelector (list ruleSvm: newGenome) prevGenome))
             ;; Increment the chromosome combinatatorial indices.
             (loop for mm from (- maxChromosomes 1) to 0 by -1 do (++ chromosomeIndices[mm]) (if (<= chromosomeIndices[mm] chromosomeLimits[mm]) (setq mm -1))) 
             (loop for mm from 1 until maxChromosomes do (if (> chromosomeIndices[mm] chromosomeLimits[mm]) (setq chromosomeIndices[mm] (+ chromosomeIndices[(- mm 1)] 1)))) 
             ) ; end while
         )) ; end else
        ) ; end cond
	   Rf) ; end growWFF

	(defun mutate(wff)
	;; *******************************************************************
	;; summary:  Create a new Selector support vector regression WFF 
    ;;           using genetic mutation.
	;;
	;; args:     wff     The candidate WFF to be mutated.           
	;;           
	;; Return:   none.
	;;
	;; *******************************************************************
	   regs:(m M n N)
	   vars:(rule genome prevGenome newWff chromosome (Number:deathPct .10))
	
	   ;; Extract numeric WFFs from the candidate Selector Lambda (where necessary).
	   (if (= wff #void) (return false))
	   (if (isLambda wff) (setq wff wff.Genome))
	   (if (isPair wff) (setq wff wff[1]))
	   (setq genome wff)
	   (if (= genome #void) (setq genome myColumnGenome))
       (setq prevGenome genome)
	
	   ;; Make a copy of the candidate genome.
	   (setq genome (listWff genome))

       ;; Reorder the chromosomes in the candidate genome ocasionally.
       (if (<= (random 1.0) .50)
           then
           (begin 
             (setq m (integer (random myM)))
             (setq n (integer (random myM)))
             (setq chromosome genome[m])
             (setq genome[m] genome[n])
             (setq genome[n] chromosome)
           )) ; end if
       (setq m (integer (random myM))) 
       (if (= myMutateSpliceSW true) (setq newWff (mutateNumericWFF genome[m])) (begin (setq newWff (ruleExp.growRootWFF -2)) (goto Last:)))
       (setq n 0)(while (and (< (++ n) 20) (> (lengthWFF newWff) myMaxColWFFLen)) do (setq newWff (mutateNumericWFF genome[m])))
       (if (> (lengthWFF newWff) myMaxColWFFLen) (return false))
       (if (<= (random 1.0) deathPct) (setq genome[m] #void) (setq genome[m] newWff))

	   ;; Make a Selector multivariate linear regression wff and add it to the current population.
       Last::
       (growWFF genome prevGenome)
	
	   true) ; end mutate
		
    (defun wffList(wff)
	;; *******************************************************************
	;; summary:  Convert a selector support vector regression wff to a 
    ;;           list format. 
	;;
	;; args:     wff         The support vector regression wff.
	;;           
	;; Return:   result      A Selector support vector regression WFF such as '(ruleSvm #(obj| (x0-x5) abs(x1)))
	;;
	;; *******************************************************************
       regs:(m M)
       vars:(wffListFormat rule genome)
       vars:(chromosome0 chromosome1)
       (if (isString wff) (setq wff wff) (setq wff (string wff true)))
       (setq wff (lisp wff)[0])
       (if (or (not (isPair wff)) (<> (length wff) 2) (<> wff[0] ruleSvm:)) (error "esm.ruleSvm.wffList: invalid WFF"))
       (setq rule wff[0])
       (setq genome (sort (copy wff[1]) >))
       ;; Eliminate all collisions before generating svmregress genome.
       (setq M (length genome))
       (loop for m from 1 until M do
         (setq chromosome0 genome[(- m 1)])
         (setq chromosome1 genome[m])
         (if (and (<> chromosome1 #void) (= chromosome0 chromosome1))
             (setq genome[(- m 1)] #void)
             ) ; end if
         ) ; end loop   
       (setq genome (sort genome >))
       (setq wffListFormat (list rule genome))
       wffListFormat) ; end wffList


	(defun wffSource(genome)
	;; *******************************************************************
	;; summary:  Return a Selector support vector regression statement 
    ;;           such as 
    ;;                     "svmregress(x0-x5,abs(x1));"
    ;;  
	;;           as an ASCI string in grammatically correct Selector.
	;;
	;; args:     genome      The sparse vector of chromosome numeric expressions for the svmregress statement.
	;;           
	;; Return:   result      A Selector support vector regression statement such as "svmregress(x0-x5,abs(x1));"
	;;
	;; *******************************************************************
       regs:(m M)
       vars:((wffSourceFormat "") rule genome)
       vars:(chromosome0 chromosome1)
       ;; Eliminate all collisions before generating bgmregress genome.
       (setq genome (sort (copy genome) >))
       (setq M (length genome))
       (loop for m from 1 until M do
         (setq chromosome0 genome[(- m 1)])
         (setq chromosome1 genome[m])
         (if (and (<> chromosome1 #void) (= chromosome0 chromosome1))
             (setq genome[(- m 1)] #void)
             ) ; end if
         ) ; end loop   
       (setq genome (sort genome >))
       ;; Create the final source from the genome.
       (setq M (length genome))
       (loop for m from 0 until M do
         ;; Select chromosomes from each genome.
         (cond 
           ((and (= wffSourceFormat "") (<> genome[m] #void)) (setq wffSourceFormat (ruleExp genome[m])))
           ((and (<> wffSourceFormat "") (<> genome[m] #void)) (setq wffSourceFormat (append wffSourceFormat "," (ruleExp genome[m]))))
           ) ; end chromosome cond
         ) ; end column loop  
	    ;; Generate a gramatically correct Selector support vector regression statement in source format.
	   (setq wffSourceFormat (append "svmregress(" wffSourceFormat ");"))
	   wffSourceFormat) ; end wffSource    

   (defun wffString(wff)
	;; *******************************************************************
	;; summary:  Convert a selector support vector regression wff to a 
    ;;           string format. 
	;;
	;; args:     wff         The support vector regression wff.
	;;           
	;; Return:   result      A Selector support vector regression WFF such as "(ruleSvm #(obj| (x0-x5) abs(x1)))"
	;;
	;; *******************************************************************
       regs:(m M)
       vars:(wffStringFormat rule genome)
       vars:(chromosome0 chromosome1)
       (if (isString wff) (setq wff (lisp wff)[0]))
       (if (or (not (isPair wff)) (<> (length wff) 2) (<> wff[0] ruleSvm:)) (error "esm.ruleSvm.wffString: invalid WFF"))
       (setq rule wff[0])
       (setq genome (sort (copy wff[1]) >))
       ;; Eliminate all collisions before generating svmregress genome.
       (setq M (length genome))
       (loop for m from 1 until M do
         (setq chromosome0 genome[(- m 1)])
         (setq chromosome1 genome[m])
         (if (and (<> chromosome1 #void) (= chromosome0 chromosome1))
             (setq genome[(- m 1)] #void)
             ) ; end if
         ) ; end loop   
       (setq genome (sort genome >))
       (setq wffStringFormat (string (list rule genome) true))
       wffStringFormat) ; end stringWff

   	;; *******************************************************************************
   	;; Begin main logic 
   	;; *******************************************************************************
	vars:(wffSourceFormat)
    
    (setq wffSourceFormat (wffSource wff[1]))
    wffSourceFormat) ; end ruleSvm










































;;**EXPORTKEY**:esm:selector
(defriend esm:selector(_input)
;; ********************************************************************
;; summary:  The esm.selector compiler generated from esm:selector:DEFINITION.
;; Summary:  This Lambda implements the esm.selector compiler as defined
;;           in the esm:selector:DEFINITION compiler definition file.
;;           Much code has been marked with a boxed comment lines for
;;           ease of human understanding.
;; Note:     This code was machine generated by ParseLib.
;; Parms:    _input   The esm.selector language source string
;; return:   _result  The Lambda resulting from compiling the _input source.
;; Modification history:
;; TM Jan 15 99 Added Console Error suppression (see _consoleError, _makeError and _lastError)
;; TM Jan 20 99 Added _verboseLexIn - a directory of routines to be _verbose in
;; TM Jan 20 99 Added _verboseSynIn - a directory of routines to be _verbose in
;; TM Jan 20 99 Added _verboseSemIn - a directory of routines to be _verbose in
;; TM Jan 20 99 Changes _verbosexxxIn so that you supply a stop count
;;              example: (setq esm:selector._verboseSynIn.MYRULE 2})  ; error after 2nd pass
;;                       (setq esm:selector._verboseSynIn.MYRULE 0})  ; verbose on every pass
;;                       (setq esm:selector._verboseSynIn.MYRULE: -1}) ; not verbose
;; TM Nov 13 01 Added _verboseLex   - flag forcing _verbose only in Lex
;;                    _verboseSyn   - flag forcing _verbose only in Syntax
;;                    _verboseSem   - flag forcing _verbose only in Semantic
;; TM Nov 19 10 Added new error handling that automatically turns verbose on near the 
;;              parse error. This makes most of the other error handling in the tool 
;;              obsolete except when you want to see a full trace of all rule attempts from
;;              the begining of one of the passes.
;; ********************************************************************
   pvars:(_changeCount             ;; Number of rule based substitutions
          _explainOnOff            ;; Switch for saving semantic explanation steps in the _explanation variable 
          _explanation             ;; Variable for saving semantic explanation steps (see _explainOnOff) 
          _indent                  ;; Indent for displaying each explanation step on the console
          _io                      ;; Current parse tree index object
          _ip                      ;; Current parse tree index pointer
          (_maxPasses 200)         ;; Maximum number of passes before issuing error (singlePass = false)
          (morphFail |*failure*|:) ;; Morph rule failure RHS value
          _name                    ;; Name of the current compiler definition
          _parseLen                ;; Current parse tree length
          _parseTree               ;; Current head of the parse tree during recognition
          _passCount               ;; Current number of passes already executed by apply
          _result                  ;; Final result of parsing the submitted compiler definition rules 
          _semanticRule            ;; Current semantic rule to be applied by _applyRule 
          _semanticStack           ;; Current semantic stack for use by _applyRule 
          _semanticVerbose         ;; Switch for displaying each semantic explanation step on the console 
          _showTokens              ;; Show only the token list resulting from the lexical analyzer
          _syntaxFeatures          ;; Syntax features supplied in the compiler definition
          _tkIN                    ;; Place holder for the input source string (see $IN) 
          _tkLIST                  ;; The output token list from the lexer rules.
          _tkOUT                   ;; Output a feature based token to the token list.
          tokenDirectory           ;; Lexicon of token and their attributes
          _userFunctions           ;; User functions source code supplied in the compiler definition
          _verbose                 ;; Switch for displaying each explanation step on the console
          _verboseHold 
           ;; Switch for displaying each explanation step on the console
          _verboseLex			   ;; Switch for displaying only lexical steps
          _verboseSyn              ;; Switch for displaying only syntax steps
          _verboseSem              ;; Switch for displaying only sematic steps
          _verboseLexIn            ;; Structure of lex parse routines in which we should force _verbose true
          _verboseSynIn            ;; Structure of syntax parse routines in which we should force _verbose true
          _verboseSemIn            ;; Structure of semantic parse routines in which we should force _verbose true
          _verboseLexCount         ;; Structure of Counts used by _verboseLexCount (not set by user!)
          _verboseSynCount         ;; Structure of Counts used by _verboseSynCount (not set by user!)
          _verboseSemCount         ;; Structure of Counts used by _verboseSemCount (not set by user!)
          _lastError               ;; Error encountered during parse 
          
          _ruleCount               ;; Number of rules tried 
          _ruleCountLex
          _ruleCountSyn
          _ruleCountSem
          _failurePass
          _failureIn
          _verboseState
          (_verboseTrigger -1)     ;; If more than zero this indicates at what _ruleCount _verbose should be turned on
     
          _incCount				   ;; increment number of rules tried and test for error condition
          _startLog
           
          (_consoleErrors true)    ;; Error messaging flag is false no messages will be printed to console
          ;; Methods list 
          _findLineNum             ;; find the line the error ocurred in
          _makeError               ;; _error function wrapper that creates pretty errors and sets _lastError values
          _error                   ;; error function wrapper allowing silent or console messaging
          _apply                   ;; Apply the specified semantic rule to the result
          _applyRule               ;; Apply the current semantic rule to a sub list
          appendList               ;; Append multiple arguments into a list
          defaultLexer             ;; Default lexical analyzer for recognizing input symbols
          defaultTokenRule         ;; Modified default rule for adding attributes to a parsed token
          _eofToken                ;; Return true if we are at the end of the parse tree
          _errorHandler            ;; Handle any errors which may occur during compilation
          _getToken                ;; Get the next attributed token in the parse tree
          _Initialize              ;; Initialization routine for setting token dicrectory, etc.
          _initializeSW            ;; Initialization switch (set true after first initialization).
          initRule                 ;; User defined initialization routine for setting token dicrectory, etc.
          _lastIp                  ;; Move the current parse tree index to the previous position
          _lenIp                   ;; Return the length of the current parse tree
          _nextIp                  ;; Move the current parse tree index to the next position
          outputRule               ;; Default rule for returning the final output from the compiler
          _popIp                   ;; Pop the current parse tree index up one level to the next position
          preLexRule               ;; Default rule for any pre-lexical compiler operations.
          _pushIp                  ;; Push the current parse tree index down one level to the next position
          _setLexicalFeature       ;; Assign a set of letters to the specified lexical feature
          _setSyntaxFeature        ;; Set a whole class of syntax tokens with the specified attribute
          _setWordFeatures         ;; Set or enhance a whole word with the specified features and feature values
          _showInput               ;; Show a fragment of the input source string in lexical verbose mode
          _showSource              ;; Show a fragment of the input source string in syntax verbose mode
          startRule                ;; Default rule for starting the compiler
          _writeRule               ;; Display the results of a rule firing in verbose mode
          $IN                      ;; Place holder for the input source string (see _tkIN)
          $LIST                    ;; The output token list from the lexer rules.
          $OUT                     ;; Output a feature based token to the token list.
          $ASIS                    ;; Output a feature based token to the token list (as is).









          ;; Variables to hold lexical feature bit maps
          _LF_Digit
          _LF_Alpha
          _LF_AlphaNum
          _LF_NameChar
          _LF_Letter
          _LF_NameStart
          _LF_Special
          _LF_DQuote
          _LF_NotDQuote
          _LF_Quote
          _LF_NotQuote
          _LF_Whitespace
          _LF_Eol
          _LF_NotEol
          _LF_Period
          _LF_Exponent
          _LF_Sign
          ;; Functions to implement Lexical Rules
          _LEXRULE_MAIN
          ;; Functions to implement Syntax Rules
          _SYNRULE_ARGLIST
          _SYNRULE_CFCALL
          _SYNRULE_CHILD
          _SYNRULE_CLASS
          _SYNRULE_CUT
          _SYNRULE_EXPRESSION
          _SYNRULE_FIELDLIST
          _SYNRULE_FILTER
          _SYNRULE_FRIEND
          _SYNRULE_FSTATEMENT
          _SYNRULE_FSTMTLIST
          _SYNRULE_FUNCTION
          _SYNRULE_MAIN
          _SYNRULE_METHOD
          _SYNRULE_NAME
          _SYNRULE_NUMLIST
          _SYNRULE_ORPHAN
          _SYNRULE_PARMLIST
          _SYNRULE_PHRASE
          _SYNRULE_QUALIFY
          _SYNRULE_REFLIST
          _SYNRULE_SCORE
          _SYNRULE_SEXPRESSION
          _SYNRULE_SSTATEMENT
          _SYNRULE_STATEMENT
          _SYNRULE_STMTLIST
          _SYNRULE_TERM
          _SYNRULE_VAR
          _SYNRULE_WEIGHTLIST
          ;; Functions to implement Semantic Rules
          _SEMRULE_LAMBDA
          _SEMRULE_LET
          _SEMRULE_LETOMIT
          _SEMRULE_LETTRUNCATE
          _SEMRULE_MAIN
          _SEMRULE_OMIT
          _SEMRULE_REFOMIT
          _SEMRULE_REFTRUNCATE
          _SEMRULE_TRUNCATE
         ) ;; end of persistent variables
   vars:(i verboseHold outString outExplain)
   ;; ***************************************************
   ;; Define the child Lambdas which belong to this parent
   ;; ***************************************************

   ;;*********************************************************************
   ;; 
   ;;*********************************************************************
   (defun _incCount()
   	(setq _ruleCount (+ _ruleCount 1)) 
   	(if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog))
   	
   	true)

   ;;*********************************************************************
   ;; 
   ;;*********************************************************************
   (defun _startLog()
   		vars:(i len)
		(setq _verbose true)
		(writeln "*********Lexed Tokens with Features Near Failure*********")
		(setq len (min (+ _ip 50) (length _parseTree)))
		(loop for i from _ip until len do
			(writeln "[" i "] " _parseTree[i])
		) ; end loop
		(writeln "*********Source near Failure***********")
		(writeln "Charpos=" _parseTree[_ip].Charpos)
		(writeln (substring $IN _parseTree[_ip].Charpos (+ _parseTree[_ip].Charpos 1000)))
		(writeln "*********Rules starting near Failure*****")
   			
   true)


   ;;*********************************************************************
   ;; Centralized print routine for logging errors to console. 
   ;;*********************************************************************
   (defun _logLine(lineArg show)
   		vars:(lineText)
   		(setq lineText (new Vector: byte: 30000))
   		(cond
   			((= show source:) (setq lineText (appendWriteln lineText (rept " " _indent) lineArg (_showSource 20))))
   			((= show input:) (setq lineText (appendWriteln lineText (rept " " _indent) lineArg (_showInput 20))))
   			((= show none:) (setq lineText (appendWriteln lineText (rept " " _indent) lineArg)))
		);cond   			
 		(writeln lineText)  
   true)

   ;;*********************************************************************
   ;; Finds the line number given a character position
   ;;*********************************************************************
   (defun _findLineNum(pos)
      vars: (i j l)
      (setq j 0)
      (setq l (length $IN))
      (if (< pos l) (setq l pos))
      (loop for i from 0 until l do
         (if (= $IN[i] 10) (setq j (iadd j 1)))
         ); end loop
      j; return number of linefeeds found
      ); end findLineNum

   ;;*********************************************************************
   ;; Construct Error
   ;;*********************************************************************
   (defun _makeError(errorKey pos desc)
      vars: (i j line1 line2 line3 line4 eof nontabs tabs temp result)

      (setq eof (length $IN))
      (setq nontabs 0)
      (setq tabs 0)


      ;find start of error line - line2
      (setq i pos)
      (while (and (>= i 0) (<> $IN[i] 10) (<> $IN[i] 13)) (setq i (isub i 1)))
      (if (> i 0) (setq line2 (iadd i 1)) (setq line2 0))

      ;Count number of tabs and non-tabs up to error in error line
      (setq j line2)
      (while (< j pos) (if (= $IN[j] 9) (setq tabs (iadd tabs 1)) (setq nontabs (iadd nontabs 1))) (setq j (iadd j 1)))

      ;find start of line1
      (while (and (>= i 0) (or (= $IN[i] 10) (= $IN[i] 13))) (setq i (isub i 1))) 
      (while (and (>= i 0) (<> $IN[i] 10) (<> $IN[i] 13)) (setq i (isub i 1)))
      (if (> i 0) (setq line1 (iadd i 1)) (setq line1 0))

      ;find start of line 3
      (setq i pos) ; reset to error position
      (while (and (< i eof) (<> $IN[i] 10) (<> $IN[i] 13)) (setq i (iadd i 1)))
      (while (and (< i eof) (or (= $IN[i] 10) (= $IN[i] 13))) (setq i (iadd i 1)))
       (setq line3 i)

      ;find start of line 4
      (while (and (< i eof) (<> $IN[i] 10) (<> $IN[i] 13)) (setq i (iadd i 1)))
      (while (and (< i eof) (or (= $IN[i] 10) (= $IN[i] 13))) (setq i (iadd i 1)))
      (setq line4 i)

      ;Create error window
      (setq temp (append 
           (if (> (- line2 line1) 0) (substring $IN line1 (isub line2 1)) "") ;line 1
           (if (> (- line3 line2) 0) (substring $IN line2 (isub line3 1)) "") ;line 2 
           (if (= line3 eof) _eol ""); add an _eol if line2 is the only line!
           (rept " " nontabs) (rept (string (char 9)) tabs) "^error" _eol 
           (if (> line4 line3) (substring $IN line3 (isub line4 1)) "")
           ))

      ; Normalize tabs to 4 chars each
      (setq j (length temp))
      (setq result "")
      (loop for i from 0 until j do
         (setq result (append result (if (= temp[i] 9) "    " (string temp[i]))))
         ); end loop

      ;insert values into the _lastError structure
      (setq _lastError.errorKey errorKey)
      (setq _lastError.line (_findLineNum pos))
      (setq _lastError.desc desc)
      (setq _lastError.charpos pos)
      (setq _lastError.message result)

      ;(_error errorKey (append " " desc " Line:" _lastError.line " Charpos:" pos _eol _lastError.message))
      (_error (append " " desc " Line:" _lastError.line " Char:" pos _eol _lastError.message))

      ); end _makeError

   ;; _error wraps the builtin error function so that it 
   ;; is possible to disable console errors for silent
   ;; operation. The _consoleErrors variable determines
   ;; the function of _error.
   (defun _error ( ... )
      vars:(argc i e)

      (setq argc (argCount))
      (setq e (new Vector: argc))
      (loop for i from 0 until argc do
         (setq e[i] (argFetch i))
         ); end loop

       (if _consoleErrors (apply error e) (error e[0]))

      false ; we will never actually get here but put in a return anyway
      ); of defun _error

   ;; Append multiple arguments into a list.
   ;; Note: This Lambda is here as a builtin function for
   ;;       use in the output section of any rule definition. 
   (defun appendList(one two ...)
       vars:(result argc i)
       (cond 
           ((and (isPair one) (isPair two)) (setq result (append one two)))
           ((= one #void) (setq result two))
           ((and (isPair one) (= two #void)) (setq result one))
           ((isPair one) (setq result (append one (list two))))
           ((= two #void) (setq result (list one)))
           (else (setq result (list one two)))
           ) ; end cond
       (setq argc (argCount))
       (loop for i from 2 until argc do
           (setq result (appendList result (argFetch i)))
           ) ;; end loop
       result) ;; end appendList
   ;; Apply the specified semantic rule to the result.
   ;; Note: This Lambda is here as a builtin function for
   ;;       use in the output section of any rule definition. 
   (defun _apply(theRule multiplePass)
       vars:(outList outString outExplain)
       ;(if _semanticVerbose (setq outString (append "Replacing: " (string _result true) " ==> ")))
       ;(if _explainOnOff (setq outExplain (setq outExplain (append "Replacing: " (string _result true) " ==> "))))
       (setq outList _result)
       (setq _passCount 0)
       (setq _semanticRule theRule)
       Retry::
       (if (> _passCount _maxPasses) (_error "ParseLib_Pass" "Exceeded maximum number of apply rules."))
       (setq _changeCount 0)
       (setq outList (morph (list outList) _applyRule morphFail))
       (if (isPair outList) (setq outList (car outList)))
       (if (and (> _changeCount 0) (= multiplePass true) (isPair outList)) (goto Retry:))
       ;(if (= _semanticVerbose true) (writeln outString  (string outList true)))
       ;(if _explainOnOff (setq _explanation (append _explanation outExplain (string _result true) _eol)))
       (setq _result outList)
       _result) ;; end of _apply
   ;; Apply the current semantic rule to a sub list
   ;; Note: This Lambda is called by morph for every sub list
   ;;       in the larger result list. 
   (defun _applyRule(sexp)
      vars:(ret outString outExplain)
      (setq _ip -1)
      (setq _io sexp)
      (if _semanticVerbose (setq outString (append "Replacing: " (string sexp true) " ==> ")))
      (if _explainOnOff (setq outExplain (setq outExplain (append "Replacing: " (string sexp true) " ==> "))))
      (setq _semanticStack (new Structure: _io _ip))
      (if (<> (setq ret (_semanticRule)) morphFail)
          (begin
             (++ _changeCount)
             (if (= _semanticVerbose true) (writeln outString  (string ret true)))
             (if _explainOnOff (setq _explanation (append _explanation outExplain (string ret true) _eol)))
             (return ret)
          )) ;; end if
      morphFail) ;; end of _applyRule
   ;; Modified default rule for adding attributes to a parsed token.
   ;; Note: This Lambda is here in case the user does not 
   ;;       define one of his/her own.
   (defun defaultTokenRule(token)
       vars:(result tokenLen tokenEnd)
       ;; Is this token a delimited constant?
       (if (isVector token) 
           (begin
              (setq result (new Structure: Value: token[1] token[0] true  Constant: true))
              (return result)
              )) ;; end if delimited constant
       ;; Is this token an integer constant?
       (if (isInteger token) 
           (begin
              (setq result (new Structure: Value: token  Integer: true  Number: true  Constant: true))
              (return result)
              )) ;; end if integer constant
       ;; Is this token an numeric constant?
       (if (isNumber token) 
           (begin
              (setq result (new Structure: Value: token  Number: true  Constant: true))
              (return result)
              )) ;; end if numeric constant
       ;; Is this token a name token?
       (if (isCharName token) 
           (begin
              (setq result (new Structure: Value: token  Name: true  Default: true))
              ;; Add the token to the directory so we don't have to do this again.
              (setq tokenDirectory[token] result) 
              (return result)
              )) ;; end if numeric constant
       ;; Create a default attributed structure for this token
       (setq result (new Structure: Value: token Default: true))
       ;; Add the token to the directory so we don't have to do this again.
       (setq tokenDirectory[token] result) 
       result) ;; end defaultTokenRule
   ;; Return true if the we are at the end of the parse tree.
   (defun _eofToken()
       (if (>= (_nextIp) (_lenIp)) true (_lastIp))) ;; end _eofToken
   ;; Manages any errors which may occur during compilation.
   (defun _errorHandler(errMsg)
      vars:(stemp n)
      (setq stemp (string errMsg true))
      (setq n (length stemp))
      (setq stemp (right (left stemp (- n 2)) (- n 4)))
      (error stemp)) ;; end of _errorHandler
   ;; Get the next token in the current parse tree.
   (defun _getToken()
       vars:(result i io n)
       ;; Load the next token in the parse tree.
       ;(if (>= (_nextIp) (_lenIp)) (begin (setq result morphFail) (_lastIp) (return result)))
       (if (>= (_nextIp) (_lenIp)) (return morphFail))
       (if (isNumber _ip) (return _io[_ip]))
       (setq n (subi (length _ip) 1))
       (setq io _io)
       (loop for i from 0 until n do
           (setq io io[_ip[i]])          
           ) ; end loop
       (setq result io[_ip[n]]) 
       result) ;; end of _getToken
   ;; Initialization routine for setting token dicrectory, etc.
   ;; Note: This Lambda is run once during the lifetime of the
   ;;       parent Lambda.
   (defun _Initialize()
       (setq _initializeSW true)
       ;; Reset the verbose mode indent.
       (setq _indent 0)
       ;; Create the token directory for the compiler definition language.
       (setq tokenDirectory (new Directory:))
       ;; Adjust the lexical analyzer for the compiler definition language.
       (defaultLexer._Initialize)









       ;; Initialization of Delimited Strings
       (defaultLexer.addStringDelimiters   String: {"} {"})
       (defaultLexer.addStringDelimiters   Symbol: {'} {'})
       (defaultLexer.addStringDelimiters   Whitespace: {/*} {*/})
       (defaultLexer.addStringDelimiters   Whitespace2: {//} _eol
)
         ;; Initialization of Lexical Features
       (setq _LF_Digit (_setLexicalFeature _LF_Digit #( 1 48 57)))
       (setq _LF_Alpha (_setLexicalFeature _LF_Alpha #( 1 97 122 1 65 90)))
       (setq _LF_AlphaNum (_setLexicalFeature _LF_AlphaNum #( 1 97 122 1 65 90 1 48 57)))
       (setq _LF_NameChar (_setLexicalFeature _LF_NameChar #( 1 97 122 1 65 90 1 48 57 1 95 95)))
       (setq _LF_Letter (_setLexicalFeature _LF_Letter #( 1 97 122 1 65 90)))
       (setq _LF_NameStart (_setLexicalFeature _LF_NameStart #( 1 97 122 1 65 90 1 95 95)))
       (setq _LF_Special (_setLexicalFeature _LF_Special #( 1 60 60 1 62 62 1 61 61 1 38 38 1 37 37 1 33 33 1 94 94 1 126 126 1 43 43 1 47 47 1 42 42 1 45 45 1 124 124 1 35 35)))
       (setq _LF_DQuote (_setLexicalFeature _LF_DQuote #( 1 34 34)))
       (setq _LF_NotDQuote (_setLexicalFeature _LF_NotDQuote #( 1 0 255 0 34 34)))
       (setq _LF_Quote (_setLexicalFeature _LF_Quote #( 1 39 39)))
       (setq _LF_NotQuote (_setLexicalFeature _LF_NotQuote #( 1 0 255 0 39 39)))
       (setq _LF_Whitespace (_setLexicalFeature _LF_Whitespace #( 1 0 32)))
       (setq _LF_Eol (_setLexicalFeature _LF_Eol #( 1 10 10 1 13 13)))
       (setq _LF_NotEol (_setLexicalFeature _LF_NotEol #( 1 0 255 0 10 10 0 13 13)))
       (setq _LF_Period (_setLexicalFeature _LF_Period #( 1 46 46)))
       (setq _LF_Exponent (_setLexicalFeature _LF_Exponent #( 1 101 101 1 69 69)))
       (setq _LF_Sign (_setLexicalFeature _LF_Sign #( 1 43 43 1 45 45)))
         ;; Initialization of Syntax Features
       (_setSyntaxFeature Operator: #( |+| |-| |*| |/| |%| "&&" "||" "#") #void)
       (_setSyntaxFeature Lisp: #( |+| |-| |*| |/| |%| "&&" "||" "#") #(+ - * pdiv pmod and or pdiv ))
       (_setSyntaxFeature Charop: #( |+| |-| |*| |/| |%| "&&" "||") #(cadd csub cmul pdiv pmod and or ))
       (_setSyntaxFeature Chartyp: #( |+| |-| |*| |/| |%| "&&" "||") #(char char char char char bool bool ))
       (_setSyntaxFeature Boolop: #( |+| |-| |*| |/| |%| "&&" "||") #(badd bsub bmul pdiv pmod and or ))
       (_setSyntaxFeature Booltyp: #( |+| |-| |*| |/| |%| "&&" "||") #(bool bool bool bool bool bool bool ))
       (_setSyntaxFeature Intop: #( |+| |-| |*| |/| |%| "&&" "||") #(iadd isub imul pdiv pmod and or ))
       (_setSyntaxFeature Inttyp: #( |+| |-| |*| |/| |%| "&&" "||") #(int int int int int bool bool ))
       (_setSyntaxFeature Floatop: #( |+| |-| |*| |/| |%| "&&" "||") #(nadd nsub nmul pdiv pmod and or ))
       (_setSyntaxFeature Floattyp: #( |+| |-| |*| |/| |%| "&&" "||") #(float float float float float bool bool ))
       (_setSyntaxFeature Operator: #( |==| |<| |<=| |>| |>=| |!=|) #void)
       (_setSyntaxFeature Lisp: #( |==| |<| |<=| |>| |>=| |!=|) #(= < <= > >= <> ))
       (_setSyntaxFeature Boolop: #( |==| |<| |<=| |>| |>=| |!=|) #(bcompareEQ bcompareLT bcompareLE bcompareGT bcompareGE bcompareNE ))
       (_setSyntaxFeature Booltyp: #( |==| |<| |<=| |>| |>=| |!=|) #(bool bool bool bool bool bool bool ))
       (_setSyntaxFeature Charop: #( |==| |<| |<=| |>| |>=| |!=|) #(ccompareEQ ccompareLT ccompareLE ccompareGT ccompareGE ccompareNE ))
       (_setSyntaxFeature Chartyp: #( |==| |<| |<=| |>| |>=| |!=|) #(bool bool bool bool bool bool bool ))
       (_setSyntaxFeature Intop: #( |==| |<| |<=| |>| |>=| |!=|) #(icompareEQ icompareLT icompareLE icompareGT icompareGE icompareNE ))
       (_setSyntaxFeature Inttyp: #( |==| |<| |<=| |>| |>=| |!=|) #(bool bool bool bool bool bool bool ))
       (_setSyntaxFeature Floatop: #( |==| |<| |<=| |>| |>=| |!=|) #(ncompareEQ ncompareLT ncompareLE ncompareGT ncompareGE ncompareNE ))
       (_setSyntaxFeature Floattyp: #( |==| |<| |<=| |>| |>=| |!=|) #(bool bool bool bool bool bool bool ))
       (_setSyntaxFeature MathAssignmentOperator: #( |+=| |-=| |/=| |*=| "%=") #void)
       (_setSyntaxFeature Lisp: #( |+=| |-=| |/=| |*=| |%| |=|) #(+ - pdiv * pmod ))
       (_setSyntaxFeature Charop: #( |+=| |-=| |/=| |*=| |%| |=|) #(cadd csub cmul pdiv pmod ))
       (_setSyntaxFeature Chartyp: #( |+=| |-=| |/=| |*=| |%| |=|) #(char char char char char ))
       (_setSyntaxFeature Boolop: #( |+=| |-=| |/=| |*=| |%| |=|) #(badd bsub bmul pdiv pmod ))
       (_setSyntaxFeature Booltyp: #( |+=| |-=| |/=| |*=| |%| |=|) #(bool bool bool bool bool ))
       (_setSyntaxFeature Intop: #( |+=| |-=| |/=| |*=| |%| |=|) #(iadd isub imul pdiv pmod ))
       (_setSyntaxFeature Inttyp: #( |+=| |-=| |/=| |*=| |%| |=|) #(int int int int int int ))
       (_setSyntaxFeature Floatop: #( |+=| |-=| |/=| |*=| |%| |=|) #(nadd nsub nmul pdiv pmod ))
       (_setSyntaxFeature Floattyp: #( |+=| |-=| |/=| |*=| |%| |=|) #(float float float float float ))
       (_setSyntaxFeature AssignmentOperator: #( |=|) #void)
       (_setSyntaxFeature Lisp: #( |=|) #(setq ))
       (_setSyntaxFeature InitializeOperator: #( |=|) #void)
       (_setSyntaxFeature RelationOperator: #( |==| |<| |<=| |>| |>=| |!=|) #void)
       (_setSyntaxFeature Boolean: #( |true| |false|) #(true false ))
       (_setSyntaxFeature Term: #( |true| |false|) #void)
       (_setSyntaxFeature Increment: #( |++| |--|) #void)
       (_setSyntaxFeature Lisp: #( |++| |--|) #(+ - ))
       (_setSyntaxFeature Boolop: #( |++| |--|) #(badd bsub ))
       (_setSyntaxFeature Booltyp: #( |++| |--|) #(bool bool ))
       (_setSyntaxFeature Charop: #( |++| |--|) #(cadd csub ))
       (_setSyntaxFeature Chartyp: #( |++| |--|) #(char char ))
       (_setSyntaxFeature Intop: #( |++| |--|) #(iadd isub ))
       (_setSyntaxFeature Inttyp: #( |++| |--|) #(int int ))
       (_setSyntaxFeature Floatop: #( |++| |--|) #(nadd nsub ))
       (_setSyntaxFeature Floattyp: #( |++| |--|) #(float float ))
       (_setSyntaxFeature Logical: #( |!|) #void)
       (_setSyntaxFeature Lisp: #( |!|) #(not ))
       (_setSyntaxFeature For: #( |for|) #void)
       (_setSyntaxFeature Function: #( |function|) #void)
       (_setSyntaxFeature Friend: #( |friend|) #void)
       (_setSyntaxFeature Class: #( |class|) #void)
       (_setSyntaxFeature Extends: #( |extends|) #void)
       (_setSyntaxFeature Child: #( |child|) #void)
       (_setSyntaxFeature Orphan: #( |orphan|) #void)
       (_setSyntaxFeature Method: #( |method|) #void)
       (_setSyntaxFeature If: #( |if|) #void)
       (_setSyntaxFeature Name: #( |int| |float| |char| |bool| |obj| |text| |symbol| |string| |stc| |dir| |dic| |vec| |bitvec| |bytvec| |numvec| |intvec| |fltvec| |objvec| |pcdvec| |matrix| |nummat|) #void)
       (_setSyntaxFeature Of: #( |of|) #void)
       (_setSyntaxFeature Type: #( |int| |float| |char| |bool| |obj| |text| |symbol| |string| |stc| |dir| |dic| |vec| |bitvec| |bytvec| |numvec| |intvec| |fltvec| |objvec| |pcdvec| |matrix| |nummat|) #void)
       (_setSyntaxFeature Reftyp: #( |int| |float| |char| |bool| |obj| |text| |symbol| |string| |stc| |dir| |dic| |vec| |bitvec| |bytvec| |numvec| |intvec| |fltvec| |objvec| |pcdvec| |matrix| |nummat|) #(int float char bool obj char char char obj obj obj obj int char float int float obj int obj float ))
       (_setSyntaxFeature Refop: #( |int| |float| |char| |bool| |obj| |text| |symbol| |string| |stc| |dir| |dic| |vec| |bitvec| |bytvec| |numvec| |intvec| |fltvec| |objvec| |pcdvec| |matrix| |nummat|) #(ref ref ref ref ref reftext refSymbol refString ref ref ref refVector refBitVector refBytVector refNumVector refIntVector refFltVector refObjVector refPcdVector ref ref ))
       (_setSyntaxFeature Setop: #( |int| |float| |char| |bool| |obj| |text| |symbol| |string| |stc| |dir| |dic| |vec| |bitvec| |bytvec| |numvec| |intvec| |fltvec| |objvec| |pcdvec| |matrix| |nummat|) #(setq setq setq setq setq setq setq setString setq setq setq setVector setBitVector setBytVector setNumVector setIntVector setFltVector setObjVector setPcdVector setq setq ))
       (_setSyntaxFeature While: #( |while|) #void)
       (_setSyntaxFeature Else: #( |else|) #void)
       (_setSyntaxFeature Reg: #( |reg|) #void)
       (_setSyntaxFeature Var: #( |var|) #void)
       (_setSyntaxFeature Pvar: #( |pvar|) #void)
       (_setSyntaxFeature Cvar: #( |cvar|) #void)
       (_setSyntaxFeature Semicolon: #( ";") #void)
       (_setSyntaxFeature Colon: #( ":") #void)
       (_setSyntaxFeature Question: #( "?") #void)
       (_setSyntaxFeature LeftParen: #( "(") #void)
       (_setSyntaxFeature RightParen: #( ")") #void)
       (_setSyntaxFeature LeftBrace: #( "{") #void)
       (_setSyntaxFeature RightBrace: #( "}") #void)
       (_setSyntaxFeature LeftBracket: #( "[") #void)
       (_setSyntaxFeature RightBracket: #( "]") #void)
       (_setSyntaxFeature Comma: #( ",") #void)
       (_setSyntaxFeature DotOperator: #( ".") #void)
       (_setSyntaxFeature Pound: #( |#|) #void)
       (_setSyntaxFeature Bar: #( "|") #void)
       (_setSyntaxFeature Reserved: #( |if| |then| |else| |while| |do| |reg| |var| |pvar| |cvar| |for| |function| |orphan| |friend| |child| |class| |method| |extends|) #void)
       (_setSyntaxFeature Select: #( |select|) #void)
       (_setSyntaxFeature Percent: #( |%|) #void)
       (_setSyntaxFeature Sort: #( |sort|) #void)
       (_setSyntaxFeature Backup: #( |backup|) #void)
       (_setSyntaxFeature Direction: #( |up| |down|) #void)
       (_setSyntaxFeature All: #( |all|) #void)
       (_setSyntaxFeature Scale: #( |scale|) #void)
       (_setSyntaxFeature Regress: #( |regress|) #void)
       (_setSyntaxFeature Bgmregress: #( |bgmregress|) #void)
       (_setSyntaxFeature Frmregress: #( |frmregress|) #void)
       (_setSyntaxFeature Mvlregress: #( |mvlregress|) #void)
       (_setSyntaxFeature Ennregress: #( |ennregress|) #void)
       (_setSyntaxFeature Svmregress: #( |svmregress|) #void)
       (_setSyntaxFeature Highest: #( |highest|) #void)
       (_setSyntaxFeature Extract: #( |extract|) #void)
       (_setSyntaxFeature Omit: #( |omit|) #void)
       (_setSyntaxFeature Check: #( |check|) #void)
       (_setSyntaxFeature Checkoff: #( |checkoff| |nocheck|) #void)
       (_setSyntaxFeature Checkon: #( |checkon|) #void)
       (_setSyntaxFeature Cut: #( |bottom| |top|) #void)
       (_setSyntaxFeature Slice: #( |slice|) #void)
       (_setSyntaxFeature Lisp: #( |slice| |bottom| |up| |top| |down| |highest| |all|) #(<= <= <= >= >= >= true ))
       (_setSyntaxFeature Score: #( |score|) #void)
       (_setSyntaxFeature ScoreCommand: #( |average| |averageForAll| |total| |totalForAll| |maximum| |minimum| |deviation| |sharpe|) #void)
       (_setSyntaxFeature Set: #( |Set| |set|) #void)
       (_setSyntaxFeature Setnr: #( |Setnr| |setnr|) #void)
       (_setSyntaxFeature Run: #( |run|) #void)
       (_setSyntaxFeature Restore: #( |restore|) #void)
       (_setSyntaxFeature Reserved: #( |all| |average| |averageForAll| |backup| |bottom| |check| |checkoff| |checkon| |down| |filter|) #void)
       (_setSyntaxFeature Reserved: #( |nocheck| |omit| |restore| |run| |score| |set| |sort| |up| |top| |total| |totalForAll|) #void)
       ;; Call the user defined initialization routine
       (initRule)
       true) ;; end _Initialize
   ;; Default rule for user defined compiler initialization tasks.
   ;; Note: This Lambda is here in case the user does not 
   ;;       define one of his/her own.
   (defun initRule()
       true) ;; end initRule
   ;; Move the current parse tree index to the previous position.
   (defun _lastIp()
       (if (isNumber _ip) (return (setq _ip (subi _ip 1))))
       (setq _ip[(subi (length _ip) 1)] (subi _ip[(sub1 (length _ip))] 1))
       _ip[(subi (length _ip) 1)]) ;; end _lastIp
   ;; Return the length of the current parse.
   (defun _lenIp()
       vars:(i io n)
       (if (isNumber _ip) (return (length _io)))
       (setq n (sub1 (length _ip)))
       (setq io _io)
       (loop for i from 0 until n do
           (setq io io[_ip[i]])          
           ) ; end loop
       (length io)) ;; end _lenIp
   ;; Default main Lexical Rule for starting the compiler.
   ;; Note: This Lambda is here in case the user does not 
   ;;       define one of his/her own.
   (defun _LEXRULE_MAIN()
       (defaultLexer $IN)) ;; end _LEXRULE_MAIN
   ;; Move the current parse tree index to the next position.
   (defun _nextIp()
       (if (isNumber _ip) (return (setq _ip (addi _ip 1))))
       (setq _ip[(subi (length _ip) 1)] (addi _ip[(sub1 (length _ip))] 1))
       _ip[(subi (length _ip) 1)]) ;; end _nextIp
   ;; Default rule for returning the final output from the compiler.
   ;; Note: This Lambda is here in case the user does not 
   ;;       define one of his/her own.
   (defun outputRule(result)
       result) ;; end outputRule
   ;; Pop the current parse tree index up one level to the next position.
   (defun _popIp()
       (if (isNumber _ip) (return true))
       (if (<= (length _ip) 1) (begin (setq _ip _ip[0]) (return true)))
       (resize _ip (subi (length _ip) 1))
       true) ;; end _popIp
   ;; Default rule for any pre-lexical compiler operations.
   (defun preLexRule(input) input)
   ;; Push the current parse tree index down one level to the next position.
   (defun _pushIp()
       (if (isNumber _ip) (setq _ip (new Vector: 1 _ip)))
       (setq _ip[(length _ip)] -1)
       true) ;; end _pushIp
   ;; Assign a set of letters to the specified lexical feature
   (defun _setLexicalFeature(letterBitMap values)
       vars:(i j valueLen start end bit)
       (setq valueLen (length values))
       (if (= letterBitMap #void) (setq letterBitMap (new Vector: bit: 255)))
       (loop for i from 0 until valueLen by 3 do
          (setq bit values[i])
          (setq start (integer (min values[(+ i 1)] values[(+ i 2)])))
          (setq end (integer (max values[(+ i 1)] values[(+ i 2)])))
          (loop for j from start to end do
              (setq letterBitMap[j] bit)
              ) ; end j loop
           ) ; end i loop
       letterBitMap) ; end _setLexicalFeature
   ;; Set a whole class of syntax tokens with the specified attribute and values
   (defun _setSyntaxFeature(name words values)
       vars:(i wordLen token)
       (setq wordLen (length words))
       (loop for i from 0 until wordLen do
           (setq token tokenDirectory[words[i]])
           (if (= token #void) (setq token (new Structure:)))
           (if (= values[i] #void)
               (setq token[name] true)
               (setq token[name] values[i])
               ) ; end if
           (setq token.Value words[i])
           (setq tokenDirectory[words[i]] token)
           ) ; end loop
       true) ; end _setSyntaxFeature 
   ;; Set or enhance a whole word with the specified features and feature values
   ;; Note1: The word may be a single word or a vector starting with the word
   ;;        and proceeding with all of its gramatical synonyms. 
   ;;        For instance:   #(give gives gave given giving) 
   ;;        Each synonym may be a single word or a vector starting with the word
   ;;        and proceeding with all of its special features. 
   ;;        For instance:   #(give gives gave given #(giving Noun))
   ;; Note2: Each feature may be a single word or a vector starting with the feature
   ;;        and proceeding with the value of the feature. A singleton feature is
   ;;        assumed to have a value of true. 
   ;;        For instance:   Noun: Name: #(Color blue) 
   (defun _setWordFeatures(words commonFeatures)
       vars:(k K m M n N token feature featureName word extraFeatures)
       ;; If there are more than one grammatical synonym, then define each one.
       (if (not (isVector words)) (setq words (new Vector: 1 words)))
       (setq M (length words)) 
	   (setq N (length commonFeatures))
       ;; Extract the common feature name from the first word.
       ;; Note: Words with extra features are entered as Vectors
       (if (isVector words[0])
           then
           ;; The common word has extra features
           (setq featureName (downcase (makeString words[0][0])))
           else
           ;; The common word has no extra features
           (setq featureName (downcase (makeString words[0])))
           ) ; end if
       (setq featureName[0] (upcase featureName[0]))
       (setq featureName (symbol featureName))
       ;; Set all words to upper case.
       (loop for m from 0 until M do
           ;; Each word is stored upper case.
           ;; Note: Words with extra features are entered as Vectors
           (if (isVector words[m])
               then
               ;; This word has extra features
               (begin
                  (setq extraFeatures words[m])
                  (setq K (length extraFeatures))
                  (setq word (makeString extraFeatures[0]))
               ) ; end then
               else
               ;; This word has no extra features
               (begin
                  (setq extraFeatures #void)
                  (setq K 0)
                  (setq word (makeString words[m]))
               )) ; end if
	       (setq token tokenDirectory[word])
	       (if (= token #void) (setq token (new Structure:)))
	       (setq token.Value word)
           ;; Each word has its main word as a feature.
	       (setq token[featureName] true)
           ;; Each word has itself as a feature.
	       (setq feature (downcase (makeString word)))
	       (setq feature[0] (upcase word[0]))
	       (setq feature (symbol feature))
	       (setq token[feature] true)
	       ;; Add common features to this word.
	       (loop for n from 0 until N do
	           (setq feature commonFeatures[n])
	           (if (isVector feature) 
	               (setq token[feature[0]] feature[1])
	               (setq token[feature] true)
	               ) ; end if
	           ) ; end feature loop
	       ;; Add extra features to this word.
	       (loop for k from 1 until K do
	           (setq feature extraFeatures[k])
	           (if (isVector feature) 
	               (setq token[feature[0]] feature[1])
	               (setq token[feature] true)
	               ) ; end if
	           ) ; end feature loop
           (setq tokenDirectory[word] token)
	       ) ; end word loop
       true) ; end _setWordFeatures
   ;; Show a fragment of the input source string in lexical verbose mode
   (defun _showInput(size)
       vars:(sourceIndex sourceLen result)
       (setq sourceLen (length $IN))
       (setq sourceIndex (addi _ip 1))
       (if (< sourceIndex sourceLen)
           (setq result (mid $IN sourceIndex size))
           (setq result "...End Of File...")
           ) ; end if
       result) ; end _showInput
   ;; Show a fragment of the input source string in syntax verbose mode
   (defun _showSource(size)
       vars:(treeIndex treeLen result)
       (setq treeLen (length _parseTree))
       (setq treeIndex (addi _ip 1))
       (if (< treeIndex treeLen)
           (setq result (mid $IN _parseTree[treeIndex].Charpos size))
           (setq result "...End Of File...")
           ) ; end if
       result) ; end _showSource
   ;; Default rule for starting the compiler.
   ;; Note: This Lambda is here in case the user does not 
   ;;       define one of his/her own.
   (defun startRule()
       true) ;; end startRule
   ;; Default main Syntax Rule for starting the compiler.
   ;; Note: This Lambda is here in case the user does not 
   ;;       define one of his/her own.
   (defun _SYNRULE_MAIN()
       _parseTree) ;; end _SYNRULE_MAIN
   ;; Display the results of a rule firing in verbose mode.
   (defun writeRule(ruleID _ret t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10)
   	   vars:(ruleText)
   	   (setq ruleText (new Vector: byte: 5000))
   	   (appendWriteln ruleText 
   	   		"Firing " 
   	   		ruleID
	       	(if (<> t1 #void)  (append "  " (rept " " _indent) "t1 = " (string t1 true) _eol) "")
	       	(if (<> t2 #void)  (append "  " (rept " " _indent) "t2 = " (string t2 true) _eol) "")
	       	(if (<> t3 #void)  (append "  " (rept " " _indent) "t3 = " (string t3 true) _eol) "")
	       	(if (<> t4 #void)  (append "  " (rept " " _indent) "t4 = " (string t4 true) _eol) "")
	       	(if (<> t5 #void)  (append "  " (rept " " _indent) "t5 = " (string t5 true) _eol) "")
	       	(if (<> t6 #void)  (append "  " (rept " " _indent) "t6 = " (string t6 true) _eol) "")
	       	(if (<> t7 #void)  (append "  " (rept " " _indent) "t7 = " (string t7 true) _eol) "")
	       	(if (<> t8 #void)  (append "  " (rept " " _indent) "t8 = " (string t8 true) _eol) "")
	       	(if (<> t9 #void)  (append "  " (rept " " _indent) "t9 = " (string t9 true) _eol) "")
	       	(if (<> t10 #void)  (append "  " (rept " " _indent) "t10 = "(string t10 true) _eol) "")
       		(append "  " (rept " " _indent) " ==> " (string _ret true)))
       (_logLine ruleText none:)
       true) ;; end writeRule
    ;; Output a feature based token to the token list (output as is).
    (defun _tkASIS(charpos value ...)
       vars:(parseTree treeIndex treeLen tokenAttr argc featureName featureValue argIndex)
       ;; Check the number of arguments for validity.
       (if (< (argCount) 2) (error "$ASIS must have at least two arguments"))
       (if (isOdd (argCount)) (error "$ASIS must have an even number of arguments"))
       ;; Make sure the token list is a vector.
       (if (= _tkLIST #void) (setq _tkLIST (new Vector: 0)))
       (setq $LIST _tkLIST)
       ;; Use these value as is. Do not use the syntax feature directory.
       ;; Note: Create an attributed token using the user supplied features.
       (setq tokenAttr (new Structure:))
       (setq tokenAttr.Value value)
       (setq tokenAttr.Charpos charpos)
       (loop for argIndex from 2 until (argCount) by 2 do
          (setq featureName (argFetch argIndex))
          (setq featureValue (argFetch (iadd argIndex 1)))
          (setq tokenAttr[(symbol featureName)] featureValue)
          ) ; end feature loop
       ;; Set the displacement of the token in the source string
       (setq _tkLIST[(length _tkLIST)] tokenAttr)
       tokenAttr) ;; end of _tkASIS
    ;; Output a feature based token to the token list.
    (defun _tkOUT(charpos value ...)
       vars:(parseTree treeIndex treeLen tokenAttr argc featureName featureValue argIndex)
       ;; Check the number of arguments for validity.
       (if (< (argCount) 2) (_error "$OUT must have at least two arguments"))
       (if (isOdd (argCount)) (_error "$OUT must have an even number of arguments"))
       ;; Make sure the token list is a vector.
       (if (= _tkLIST #void) (setq _tkLIST (new Vector: 0)))
       (setq $LIST _tkLIST)
       ;; Load any syntax features for this value.
       (setq tokenAttr tokenDirectory[value])
       (if (= tokenAttr #void)
           then
           ;; This value is not found in the syntax feature directory.
           ;; Note: Create an attributed token using the user supplied features.
           (begin
              (setq tokenAttr (new Structure:))
              (setq tokenAttr.Value value)
              (setq tokenAttr.Charpos charpos)
              (loop for argIndex from 2 until (argCount) by 2 do
                 (setq featureName (argFetch argIndex))
                 (setq featureValue (argFetch (iadd argIndex 1)))
                 (setq tokenAttr[(symbol featureName)] featureValue)
                 ) ; end feature loop
              ) ; end then
           else
           ;; This value is found in the syntax feature directory.
           ;; Note: Copy the features from the dictionary and set the value.
           (begin
              (setq tokenAttr (copy tokenAttr))
              (setq tokenAttr.Value value)
              (setq tokenAttr.Charpos charpos)
              ) ; end else
           ) ; end if
       ;; Set the displacement of the token in the source string
       (setq _tkLIST[(length _tkLIST)] tokenAttr)
       tokenAttr) ;; end of _tkOUT

	(defun _substitute(begIp endIp newItem)
		vars:(n N)
		(setq _ip (- begIp 1))
		(setq N (+ (- endIp begIp) 1))
		(setq _parseTree[begIp] newItem)
		(++ begIp)
		(loop for n from 1 until N do
			(delete _parseTree begIp)
		);n
		true)

   ;; ************************************************
   ;; Define the main entry code for this parent Lambda
   ;; ************************************************
   ;; ************************************************
   ;; Define the main entry code for this parent Lambda
   ;; ************************************************
   ;; Perform any pre-lexical work required before compilation
   (setq _verboseState _verbose)
   (setq _verboseTrigger -1)
   (setq _ruleCount 0)
   (setq _failurePass false)
   RESTART::	; come here if we have an error so we can turn _verbose on close to the failure
   (if (> _ruleCount 0) ; this means we are restarting after a failure
    	(begin
    	(setq _failurePass true)
   	    (cond
   	    ((= _failureIn lex:) ; error found in Lexical Pass
	   		(setq _verboseTrigger (max (- _ruleCount 15) 15))) ; show at least 15 rules before failure
	   	((= _failureIn syn:) ; error found in syntax pass
	   		(setq _verboseTrigger (max (- _ruleCount 15) _ruleCountSyn))) ; make sure we don't start _verbose in lexical analysis
	   	((= _failureIn sem:) ; error found in semantic pass
	   		(setq _verboseTrigger (max (- _ruleCount 15) _ruleCountSem))) ; make sure we dont start _verbose in syntax analysis
   		)));if

   (setq _ruleCount 0)
   (setq _verbose _verboseState)
   (setq _lastError (new Structure:)) ;; Clear _lastError 
   (onError _errorHandler)
   (setq _verboseLexCount (new Structure:)) ;Clear routine pass counts
   (setq _verboseSynCount (new Structure:))
   (setq _verboseSemCount (new Structure:))

   (setq _input (preLexRule _input))
   ;; Initialize the parent Lambda once and only once.
   (setq _indent 0)
   (setq _tkIN _input)
   (setq $IN _input)
   (setq $OUT _tkOUT)
   (setq $ASIS _tkASIS)
   (if (= _initializeSW #void) (_Initialize))
   (setq defaultLexer.defaultTokenRule defaultTokenRule)
   (setq defaultLexer.tokenDirectory tokenDirectory)
   
   ;; In verbose mode, display the source string contents
   (if _verbose (writeln _eol "************Input Source************" _eol $IN))
   ;; Run the user defined start rule.
   (startRule)


   ;; Run the Lexical Rules
   ;; Note: Create the attributed token stream parse tree.
   (setq _verboseHold _verbose)
   (setq _verbose (or _verboseHold _verboseLex))
   (if _verbose (writeln "*********Lexical Rule Firings*********"))
   (setq _tkLIST #void)
   (setq $LIST _tkLIST)
   (setq _ip -1)
   (setq _indent 0)
   (setq _parseTree (_LEXRULE_MAIN)) 
   (if (and (not _failurePass) (not _verbose)(= _parseTree |*failure*|:)) (begin (setq _failureIn lex:) (goto RESTART:)))
   ;; In verbose mode, display the parse tree contents
   (if _showTokens
      (begin
         (writeln "*********Lexed Tokens with Features*********")
         (loop for i from 0 until (length _parseTree) do
             (writeln "[" i "] " _parseTree[i])
             ) ; end loop
         )) ; end if
   (if (= _parseTree |*failure*|:) (goto END:))
   
    ;; Run the Syntax Rules
   (setq _verbose (or _verboseHold _verboseSyn))
   (if _verbose (writeln "*********Syntax Rule Firings*********"))
   (setq _ruleCountSyn _ruleCount)
   (setq _ip -1)
   (setq _io _parseTree)
   (setq _parseLen (length _parseTree))
   (setq _indent 0)
   (setq _result (_SYNRULE_MAIN))
   (if (and (not _failurePass) (not _verbose) (= _result |*failure*|:)) (begin (setq _failureIn syn:) (goto RESTART:)))
   (if (= _result |*failure*|:) (goto END:))
   
   ;; Run the Semantic Rules
   (if _verbose (writeln "*********Semantic Rule Firings*********"))
   (setq _ruleCountSem _ruleCount)
   (setq _verbose (or _verboseHold _verboseSem))
   (setq _explanation #void)
   
   (if _verbose (setq outString (append "Reducing:= " (string _result true) " ==> " )))
   (if _explainOnOff (setq outExplain (append "Reducing:= " (string _result true) " ==> " )))
   (setq _indent 0)

   ;; Perform semantic passes (if any)
   (_apply _SEMRULE_MAIN true)
   ;; Return the final output by invoking the user defined output rule.
   (if _semanticVerbose (writeln outString (string _result true)))
   (if _explainOnOff (setq _explanation (append _explanation outExplain (string _result true) _eol)))
   (setq _verbose verboseHold)
   
   (setq _result (outputRule _result))
   (setq _verbose _verboseState)   
   (if (and (not _failurePass) (not _verbose) (= _result |*failure*|:)) (begin (setq _failureIn sem:) (goto RESTART:)))
   (if _verbose (writeln "*********Final Output*********" _eol _result)) 
   
   END::
   
   _result) ;; end of compiler















;;**EXPORTKEY**:esm:selector:%COMPILER_USERFUNCTIONS
;; ********************************************************************
;; summary:  selector User defined functions
;;
;; 			 Compile time child Lambdas for the selector compiler.
;; 			 These child Lambdas support the selector compilation process.
;;
;; Notes:    Requires the browseLib, the ParseLib, and the compiler
;;           definition source must be checked into the file cabinet 
;;           under the key: |esm.selector:%DEFINITION|.
;;
;; ********************************************************************

;;*********************************************************************
;; Function start pushes a new variable dictionary on the stack
;; Note: These persistant variables will extend the java environment
;;*********************************************************************
(defchild esm.selector startRule()
    pvars:(pvarVector           ;; Vector of declared persistant variables.
           cvarVector           ;; Vector of declared class variables. 
           gvarVector           ;; Vector of declared global variables.
           varVector            ;; Vector of declared temporary variables.
           regVector            ;; Vector of declared register variables.
           avarVector           ;; Vector of declared argument variables.
           startRuleFilterStmt  ;; Filter statement extended start rule.
           ) ; end of persistant variables
    ;; Initialize the variable vectors.
    (setq gvarVector (new Vector: 1 (new Dictionary:)))
    (setq pvarVector (new Vector: 0))
    (setq cvarVector (new Vector: 0))
    (setq regVector (new Vector: 0))
    (setq varVector (new Vector: 0))
    (setq avarVector (new Vector: 0))
    ;; Extend this start rule for the Filter statement.
    (startRuleFilterStmt)
    true) ; end startRule

;;*********************************************************************
;; Convert a record field name into an integer index. 
;;*********************************************************************
(defchild esm.selector cnvFieldName(name)
    vars:(xRef num)
    ;; Convert an element name of a record in the row manager.
    ;; Selector makes the following translations:
    ;;
    ;;    X       ==>  XT.selectedRows
    ;;    xv      ==>  ...the current row being studied...
    ;;    yv      ==>  ...the current second row being studied in a sort...
    ;;    xtime   ==>  xv[0] ...the time stamp element of the vector
    ;;    xn      ==>  xv[n] ...the nth element of the vector
    ;;    y       ==>  (cdr xv) ...the score element of the vector
    ;;
    (cond
     ((= name "xtime") (setq xRef (list ref: xv: 0)))
     ((= name "y") (setq xRef (list cdr: xv:)))
     ((and (= name[0] #\x) (isCharNumeric (setq num (mid name 1 10000)))) (setq xRef (list ref: xv: (setq num (integer num))))) 
     (else (error (append "esm.selector.cnvFieldname: invalid field name [" name "]")))  
     ) ; end cond
    xRef) ; end cnvFieldName

;;*********************************************************************
;; Function start pushes a new variable dictionary on the stack
;; Note: We use the persistant variables from startRule
;;*********************************************************************
(defchild esm.selector pushVars(struct)
    ;; Create a new variable dictionary for this function.
    (setq pvarVector[(length pvarVector)] (new Dictionary:))
    (setq cvarVector[(length cvarVector)] (new Dictionary:))
    (setq varVector[(length varVector)] (new Dictionary:))
    (setq avarVector[(length avarVector)] (new Dictionary:))
    struct) ; end pushVars

;;*********************************************************************
;; Function end pops the variable dictionary from the stack
;; Note: We use the persistant variables from startRule
;;*********************************************************************
(defchild esm.selector popVars()
    vars:(result pvarDic cvarDic varDic vecLast varLen i varList)
    ;; Pop the variable dictionaries from the stack.
    (setq vecLast (subi (length pvarVector) 1))
    (setq pvarDic pvarVector[vecLast])
    (setq cvarDic cvarVector[vecLast])
    (setq varDic varVector[vecLast])
    (resize pvarVector vecLast)
    (resize cvarVector vecLast)
    (resize varVector vecLast)
    (resize avarVector vecLast)
    ;; If there are any pvars, append them to the result.
    (if (> (length pvarDic) 0)
        (begin
           (setq result (list (makeQuotedSymbol "pvars")))
           (setq varLen (length pvarDic))
           (setq varList #void)
           (loop for i from 0 until varLen do
               (setq varList (appendList varList pvarDic[i 0]))
               ) ; end loop
           (if (not (isPair varList)) (setq varList (list varList)))
           (setq result (appendList result (list varList)))
           )) ; end if pvars
    ;; If there are any cvars, append them to the result.
    (if (> (length cvarDic) 0)
        (begin
           (setq result (list (makeQuotedSymbol "cvars")))
           (setq varLen (length cvarDic))
           (setq varList #void)
           (loop for i from 0 until varLen do
               (setq varList (appendList varList cvarDic[i 0]))
               ) ; end loop
           (if (not (isPair varList)) (setq varList (list varList)))
           (setq result (appendList result (list varList)))
           )) ; end if cvars
    ;; If there are any vars, append them to the result.
    (if (> (length varDic) 0)
        (begin
           (setq result (appendList result (list (makeQuotedSymbol "vars"))))
           (setq varLen (length varDic))
           (setq varList #void)
           (loop for i from 0 until varLen do
               (setq varList (appendList varList varDic[i 0]))
               ) ; end loop
           (if (not (isPair varList)) (setq varList (list varList)))
           (setq result (appendList result (list varList)))
           )) ; end if vars
    result) ; end popVars

;;*********************************************************************
;; Add a new variable to the pvars or the vars variable list
;; Note: We use the persistant variables from startRule
;;*********************************************************************
(defchild esm.selector addVar(var vtype name struct struct2)
    pvars:(addToBeginList)
    vars:(vecLast)
    ;; Add a new variable to the variable dictionary for this function.
    (setq vecLast (subi (length var) 1))
    ;; Manage variable declarations at the global level.
    (if (< vecLast 0)
        (begin
           ;; At the global level, we always substitute the global 
           ;; variable vector (gvarVector) as the variable vector.
           (setq var gvarVector)
           (setq vecLast (subi (length var) 1))
        )) ; end if
    (setq var[vecLast][(symbol name)] vtype)
    (addToBeginList struct struct2.Value)
    struct) ; end addVar

;;*********************************************************************
;; The the variable type of the Name token just recognized.
;; Note: We use the persistant variables from startRule
;;*********************************************************************
(defchild esm.selector setVType(token)
    vars:(vecLast)
    ;; Locate a new variable in the variable dictionary for this level.
    (setq vecLast (subi (length varVector) 1))
    ;; If this is a constant, then set its type accordingly.
    (cond
       ;; If the token is an Integer constant, set the type to int.
       ((= token.Integer true)
        (setq token.VType int:))
       ;; If the token is a Number constant, set the type to float.
       ((= token.Number true)
        (setq token.VType float:))
       ;; If the token is a Boolean constant, set the type to bool.
       ((= token.Boolean true)
        (setq token.VType bool:))
       ;; If the token is a Character constant, set the type to char.
       ((= token.Character true)
        (setq token.VType char:))
       ;; If the token is a Name, set the type to char.
       ((= token.Name true)
        (begin
           ;; Set all Name tokens to type obj as a default.
           (setq token.VType obj:)
           ;; If the Name is a registered global, set its type as registered.
           (if (<> gvarVector[0][token.Value] #void) 
               (setq token.VType gvarVector[0][token.Value]))
           ;; If the Name is a registered cvar, set its type as registered.
           (if (<> cvarVector[vecLast][token.Value] #void) 
               (setq token.VType cvarVector[vecLast][token.Value]))
           ;; If the Name is a registered pvar, set its type as registered.
           (if (<> pvarVector[vecLast][token.Value] #void) 
               (setq token.VType pvarVector[vecLast][token.Value]))
           ;; If the Name is a registered var, set its type as registered.
           (if (<> varVector[vecLast][token.Value] #void) 
               (setq token.VType varVector[vecLast][token.Value]))
           ;; If the Name is a registered arg, set its type as registered.
           (if (<> avarVector[vecLast][token.Value] #void) 
               (setq token.VType avarVector[vecLast][token.Value]))
           )) ; end begin
       ;; Otherwise, set the type to obj.
       (else 
        (setq token.VType obj:))
       ) ; end cond
    token) ; end setVType

;;*********************************************************************
;; The the variable type of the binary expression just recognized.
;; Note: We use the persistant variables from startRule
;;*********************************************************************
(defchild esm.selector setExpVType(struct token1 optok token2)
    vars:(op resultTyp typ1 typ2)
    ;; Determine the type of both token and the operator.
    (setq typ1 token1.VType)
    (setq typ2 token2.VType)
    (cond
       ;; Is this an Boolean expression?
       ((and (= typ1 bool:) (or (= typ2 int:) (= typ2 char:) (= typ2 bool:)))
        (begin
           (setq op optok.Boolop)
           (setq resultTyp optok.Booltyp)
        )) ; end Boolean expression
       ;; Is this an Character expression?
       ((and (= typ1 char:) (or (= typ2 int:) (= typ2 char:) (= typ2 bool:)))
        (begin
           (setq op optok.Charop)
           (setq resultTyp optok.Chartyp)
        )) ; end Character expression
       ;; Is this an Integer expression?
       ((and (= typ1 int:) (or (= typ2 int:) (= typ2 char:) (= typ2 bool:)))
        (begin
           (setq op optok.Intop)
           (setq resultTyp optok.Inttyp)
        )) ; end Integer expression
       ;; Is this an Number expression?
       ((and (= typ1 float:) (= typ2 float:))
        (begin
           (setq op optok.Floatop)
           (setq resultTyp optok.Floattyp)
        )) ; end Number expression
       ;; Is this any other expression?
       (else
        (begin
           (setq op optok.Lisp)
           (setq resultTyp obj:)
        )) ; end any other expression
       ) ; end cond
    (setq struct.Value (list op token1.Value token2.Value))
    (setq struct.VType resultTyp)
    struct) ; end setExpVType

;;*********************************************************************
;; Appends iterative results into a single structure 
;;*********************************************************************
(defchild esm.selector addToList(struct x) 
    (setq struct.Value (appendList struct.Value x))) ; end addToList

;;*********************************************************************
;; Constructs a function call argument list from an argument vector.
;;*********************************************************************
(defchild esm.selector argList(struct name args)
    ;; Create a function call with or without arguments.
    (if (= args #void) 
        (setq struct.Value (list name))
        (setq struct.Value (append (list name) (objectToList args)))
        ) ; end if
    (setq struct.Term true)
    struct) ; end argList

;;*********************************************************************
;; Constructs a defstruct statement from an argument vector.
;;*********************************************************************
(defchild esm.selector fieldList(struct newType parent args)
    ;; Create a defstruct with or without fields.
    (cond
       ;; Build a defstruct with no parent and no field list.
       ((and (= args #void) (= parent #void)) 
        (setq struct.Value (list defstruct: (makeQuotedSymbol newType))))
       ;; Build a defstruct with no parent and a field list.
       ((and (<> args #void) (= parent #void)) 
        (setq struct.Value (append (list defstruct: (makeQuotedSymbol newType)) (objectToList args))))
       ;; Build a defstruct with a parent and no field list.
       ((and (= args #void) (<> parent #void)) 
        (setq struct.Value (list defstruct: (makeQuotedSymbol newType) ''include parent)))
       ;; Build a defstruct with a parent and a field list.
       ((and (<> args #void) (<> parent #void)) 
        (setq struct.Value (append (list defstruct: (makeQuotedSymbol newType) ''include parent) (objectToList args))))
        ) ; end cond
    struct) ; end fieldList

;;*********************************************************************
;; Constructs a message send argument list from an argument vector.
;;*********************************************************************
(defchild esm.selector sendList(struct method receiver args)
    ;; Return if there are no arguments.
    (if (= args #void) 
        (setq struct.Value (list send: (makeQuotedSymbol method) receiver))
        (setq struct.Value (append (list send: (makeQuotedSymbol method) receiver) (objectToList args)))
        ) ; end if
    (setq struct.Term true)
    struct) ; end sendList

;;*********************************************************************
;; AddToBeginList
;; Note: Avoid using append for repeatedly linking long lists together.
;;       Use the setCdr function, it is much faster. Append was never
;;       meant to be used for long lists repeatedly.
;;*********************************************************************
(defchild esm.selector addToBeginList(struct x)
    ;; Return if there is nothing to append.
    (if (= x #void) (return struct))
    (cond
        ;; Set up begin list header the first time.
        ((= struct.Value #void)
         (begin 
            (setq struct.Value x)
            (setq struct._noBeginList true)
            ))
        ;; Set up begin list header the second time.
        ((= struct._noBeginList true)
         (begin
            ;; Singletons can be ignored because they have no side effects.
            (if (= (isPair struct.Value) false)
                then
                (setq struct.Value x)
                else
                (begin 
                   (setq struct._noBeginList false)
                   (setq struct.Value (list |begin|: struct.Value))
                   (setCdr struct (last struct.Value))
                   (setCdr (cdr struct) (list x))
                   (setCdr struct (last (cdr struct)))
                   ) ; end else
                ) ; end if
            ))
        ;; Append to the begin list the remaining times.
        (else 
         (begin
            (setCdr (cdr struct) (list x))
            (setCdr struct (last (cdr struct)))
            ))
        ) ; end conf
    struct) ; end addToBeginList

;;*********************************************************************
;; Constructs a java child definition from an argument list.
;;*********************************************************************
(defchild esm.selector childList(defname struct parent name args body)
    vars:(varList)
    ;; Retrieve all the declare variables
    (setq varList (popVars))
    ;; Set up |()| if there are no arguments.
    (if (= args.Value #void)
        (setq args (symbol "()"))
        (setq args (objectToList args.Value))
        ) ; end if
    ;; Create the function declaration up to the variables
    (setq struct.Value (list defname (makeQuotedSymbol parent.Value) name.Value args))
    ;; Append the variable declarations (if any)
    (if (<> varList #void)
        (setq struct.Value (appendList struct.Value varList (list body.Value)))
        (setq struct.Value (appendList struct.Value (list body.Value)))
        ) ; end if
    (setq struct.Func true)
    struct) ; end childList

;;*********************************************************************
;; Modified default rule for adding Term attributes to all name tokens.
;;*********************************************************************
(defchild esm.selector defaultTokenRule(token)
    vars:(result tokenLen tokenEnd)
    ;; Is this token a delimited constant?
    (if (isVector token) 
        (begin
           (setq result (new Structure: Value: token[1] token[0] true  Constant: true))
           ;; If this is a String constant, make it a Term also and remove the enclosing quotes
           (if (= token[0] String:) 
               (begin
                  (setq result.Value (mid result.Value 1 (subi (length result.Value) 2)))
                  (setq result.Term true)
                  )) ; end if
           ;; If this is a Symbol constant, make it a Term also and remove the enclosing quotes
           (if (= token[0] Symbol:) 
               (begin
                  (setq result.Value (makeQuotedSymbol (mid result.Value 1 (subi (length result.Value) 2))))
                  (setq result.Term true)
                  )) ; end if
           (return result)
           )) ;; end if delimited constant
    ;; Is this token an integer constant?
    (if (isInteger token) 
        (begin
           (setq result (new Structure: Value: token  Integer: true  Number: true  Term: true))
           (return result)
           )) ;; end if integer constant
    ;; Is this token a numeric constant?
    (if (isNumber token) 
        (begin
           (setq result (new Structure: Value: token  Number: true  Term: true))
           (return result)
           )) ;; end if numeric constant
    ;; Is this token a name token?
    (if (isCharName token) 
        (begin
           (setq result (new Structure: Value: token  Name: true  Term: true))
           (return result)
           )) ;; end if numeric constant
    ;; Create a default attributed structure for this token
    (setq result (new Structure: Value: token Default: true))
    result) ;; end defaultTokenRule

;;*********************************************************************
;; Constant Folding Function 
;;*********************************************************************
(defchild esm.selector foldConstants(struct op x y) 
    vars:(f n)
    (setq f (getGlobalValue (symbol op)))
    (cond 
      ((= op |+|:) (setq n (+ x y)))
      ((= op |-|:) (setq n (- x y)))
      ((= op |*|:) (setq n (* x y)))
      ((= op |/|:) (setq n (pdiv x y)))
      ((= op |pdiv|:) (setq n (pdiv x y)))
      ((= op |%|:) (setq n (pmod x y)))
      ((= op |#|:) (setq n (pmod x y)))
      ((= op |pmod|:) (setq n (pmod x y)))
      ((= op |and|:) (setq n (and x y)))
      ((= op |or|:) (setq n (or x y)))
      ((= op |<|:) (setq n (< x y)))
      ((= op |<=|:) (setq n (<= x y)))
      ((= op |=|:) (setq n (= x y)))
      ((= op |!=|:) (setq n (<> x y)))
      ((= op |>=|:) (setq n (>= x y)))
      ((= op |>|:) (setq n (> x y)))
      (else (error (append "esm.selector.foldConstants: unknown operator=[" op "]")))
      ) ; end cond
    (if (<> struct #void)
        (begin
           (cond
              ((isInteger n) (setq struct.Integer true))
              ((isBoolean n) (setq struct.Boolean true))
              ((isChar n) (setq struct.Character true))
              ((isNumber n) (setq struct.Number true))
              ) ; end cond
           (setq struct.Value (appendList struct.Value n))
           (setVType struct))
        else
        n)) ; end foldConstants

;;*********************************************************************
;; Constructs a java function definition from an argument list.
;;*********************************************************************
(defchild esm.selector funList(struct name args body)
    vars:(varList)
    ;; Retrieve all the declare variables
    (setq varList (popVars))
    ;; Set up |()| if there are no arguments.
    (if (= args.Value #void)
        (setq args (symbol "()"))
        (setq args (objectToList args.Value))
        ) ; end if
    ;; Create the function declaration up to the variables
    ;; Note: A function name of #void indicates an unnamed lambda
    (if (= name #void)
        (setq struct.Value (list |lambda|: args))
        (setq struct.Value (list |defun|: name.Value args))
        ) ; end if
    ;; Append the variable declarations (if any)
    (if (<> varList #void)
        (setq struct.Value (appendList struct.Value varList (list body.Value)))
        (setq struct.Value (appendList struct.Value (list body.Value)))
        ) ; end if
    (setq struct.Func true)
    struct) ; end funList

;;*********************************************************************
;; Appends iterative results into a single structure 
;;*********************************************************************
(defchild esm.selector postfix(struct token optok style)
    vars:(op resultTyp inc)
    ;; Determine the type of this operation.
    (cond
       ;; Manage a Boolean operation.
       ((= token.VType bool:)
        (begin
           (setq op optok.Boolop)
           (setq inc 1)
           (setq resultTyp optok.Booltyp)
        )) ; end Boolean operation
       ;; Manage a Character operation.
       ((= token.VType char:)
        (begin
           (setq op optok.Charop)
           (setq inc 1)
           (setq resultTyp optok.Chartyp)
        )) ; end Character operation
       ;; Manage an Integer operation.
       ((= token.VType int:)
        (begin
           (setq op optok.Intop)
           (setq inc 1)
           (setq resultTyp optok.Inttyp)
        )) ; end Integer operation
       ;; Manage a Number operation.
       ((= token.VType float:)
        (begin
           (setq op optok.Floatop)
           (setq inc 1.0)
           (setq resultTyp optok.Floattyp)
        )) ; end Number operation
       ;; Manage all other types.
       (else
        (begin
           (setq op optok.Lisp)
           (setq inc 1)
           (setq resultTyp obj:)
        )) ; end all other types
       ) ; end cond
    (setq struct.VType resultTyp)
    (if (= style false)
        (setq struct.Value (list |setq|: token.Value (list op token.Value inc)))
        else
        (setq struct.Value 
               (list |begin|: 
                   (list |setq|: |_currentResult|: token.Value) 
                   (list |setq|: token.Value (list op token.Value inc))
               _currentResult:))
        ) ; end if
    struct) ; end postfix

;;*********************************************************************
;; Appends iterative results into a single structure 
;;*********************************************************************
(defchild esm.selector qualifyName(struct target index indexExp)
    vars:(refOp setOp)
    (if (= target.VType #void) (setVType target))
    (setq struct.VType tokenDirectory[target.VType].Reftyp)
    ;; Determine the reference operator for this name qualifier.
    (cond
        ;; Manage a single strongly typped index expression.
        ((and (or (= index.VType int:) (= index.VType char:) (= index.VType bool:)) (<> target.VType #void) (<= (length indexExp) 1))
         (begin
	         (setq refOp tokenDirectory[target.VType].Refop)
		     (if (isVector indexExp) 
			     (setq struct.Value (append (list refOp target.Value) (objectToList indexExp)))
			     (setq struct.Value (list refOp target.Value indexExp))
		         ) ; end if
	          (setq struct.Setop tokenDirectory[target.VType].Setop)
         )) ; end strong typped case 
        ;; Any other type of reference uses a generic ref qualfier.
        (else
         (begin
		     (if (isVector indexExp) 
	             (setq struct.Value (append (list |ref|: target.Value) (objectToList indexExp)))
	             (setq struct.Value (list |ref|: target.Value indexExp))
		         ) ; end if
	         (setq struct.Setop |setq|:)
		 )) ; end else case
        ) ; end cond
    (setq struct.Name true)
    (setq struct.Term true)
    (setq struct.NAME true)) ; end qualifyName 
    
;;*********************************************************************
;; Sets up an assignment into a singleton or collection target 
;;*********************************************************************
(defchild esm.selector assignMe(struct op target newValue)
    ;; Compute assignment operator.
    (cond
        ((and (<> target.Setop #void) (<> target.Setop |setq|:) (isPair target.Value)) 
         (begin
            (setq struct.Value (appendList target.Value (list newValue.Value)))
            (setCar struct.Value target.Setop) 
         )) ; end strong typped case
        (else  
         (setq struct.Value (appendList |setq|: target.Value (list newValue.Value)))
         ) ; end else case
       ) ; end cond
    struct) ; end assignMe 
    
;;*********************************************************************
;; Sets up an math assignment into a singleton or collection target 
;;*********************************************************************
(defchild esm.selector assignMath(struct optok target newValue)
    vars:(temp)
    (setq temp (copy struct))
    (setq temp (setExpVType temp target optok newValue))
    (setq struct (assignMe struct optok target temp))
    struct) ; end assignMath 
    
;;*********************************************************************
;; Finds the line number given a character position
;;*********************************************************************
(defchild esm.selector findLineNum(pos)
   vars: (i j l)
   (setq l (length $IN))
   (if (< pos l) (setq l pos))
   (loop for i from 0 until l do
      (if (= $IN[i] 10) (setq j (iadd j 1)))
      ); end loop
   j; return number of linefeeds found
   ); end findLineNum

;; --------------------------------------------------------------------
;; User functions for Filter extensions
;; --------------------------------------------------------------------

;;*********************************************************************
;; Extend the normal selector startRule with these initialization
;; functions required for the Filter extensions.
;; Note: This function will run right after the normal startRule.
;;*********************************************************************
(defchild esm.selector startRuleFilterStmt()
    pvars:(autoCheck           ;; The function for generating auto check truncation.
           autoCheckSW         ;; The switch for auto checking on/off.
           autoXReferentSW     ;; The switch for automatic field names to x referents on/off.
           haveSeenDataCmdsSW  ;; The we have seen dataMineLib filter/score commands switch.
           fieldRefs           ;; The dictionary of field name references seen.
           KB                  ;; The rule base for x to y referent conversion.
           OKB                 ;; The rule base for optimizing the final filter Lambda.
          ) ; end of persistent variables
    ;; Initialize the automatic field names to x referents switch.
    (setq autoXReferentSW false)
    (setq haveSeenDataCmdsSW false)
    ;; Initialize the variable vectors.
    (setq pvarVector (new Vector: 0))
    (setq varVector (new Vector: 0))
    (setq avarVector (new Vector: 0))
    ;; Initialize the field references Dictionary.
    (setq autoCheckSW true)
    (setq fieldRefs (new Dictionary:))
    ;; Initialize the rulesLib for swapping x and y referents.
    (setq KB (new rulesLib))
    (KB.assert '(ref xv $X) '(ref yv $X))
    (setq KB.singlePass false)
    ;; Initialize the rulesLib for optimizing the final filter Lambda.
    (setq OKB (new rulesLib))
    (OKB.assert '(($X*)) '$X)
    (OKB.assert '(begin $X) '$X)
    ;; ---------------------------------------------------------------
    ;; merge filter statement All commands to reduce table passes.
    ;; ---------------------------------------------------------------
    (OKB.assert '($A* ((ref XT truncate:) (lambda (xv) $E $P1)) 
                      ((ref XT truncate:) (lambda (xv) $E $P2)) $B*)                  
                  '(<$> $A (((ref XT truncate:) (lambda (xv) $E (and $P1 $P2)))) $B))

    (OKB.assert '($A* ((ref XT truncate:) (let $V (lambda (xv) $E $P1))) 
                      ((ref XT truncate:) (let $V (lambda (xv) $E $P2))) $B*)                  
                  '(<$> $A (((ref XT truncate:) (let $V (lambda (xv) $E (and $P1 $P2))))) $B))
    ;; ---------------------------------------------------------------
    ;; merge filter statement All commands with field check at beginning
    ;;                        of sort command to reduce table passes.
    ;; ---------------------------------------------------------------
    (OKB.assert '($A* ((ref XT truncate:) (lambda (xv) $E1 $P1)) 
                      (begin ((ref XT truncate:) (lambda (xv) $E $P2)) $B*) $C*)                  
                  '(<$> $A (((ref XT truncate:) (lambda (xv) $E (and $P1 $P2)))) $B $C))

    (OKB.assert '($A* ((ref XT truncate:) (let $V (lambda (xv) $E1 $P1))) 
                      (begin ((ref XT truncate:) (let $V (lambda (xv) $E $P2))) $B*) $C*)                  
                  '(<$> $A (((ref XT truncate:) (let $V (lambda (xv) $E (and $P1 $P2))))) $B $C))
    ;; ---------------------------------------------------------------
    ;; merge filter statement Omit commands to reduce table passes.
    ;; ---------------------------------------------------------------
    (OKB.assert '($A* ((ref XT omit:) (lambda (xv) $E $P1)) 
                      ((ref XT omit:) (lambda (xv) $E $P2)) $B*)                  
                  '(<$> $A (((ref XT omit:) (lambda (xv) $E (or $P1 $P2)))) $B))

    (OKB.assert '($A* ((ref XT omit:) (let $V (lambda (xv) $E $P1))) 
                      ((ref XT omit:) (let $V (lambda (xv) $E $P2))) $B*)                  
                  '(<$> $A (((ref XT omit:) (let $V (lambda (xv) $E (or $P1 $P2))))) $B))

    ;; ---------------------------------------------------------------
    ;; Turn off all rulesLib assertions so we can use the new compiled semantic rules.
    ;; ---------------------------------------------------------------
    (setq OKB (new rulesLib))

    ;; Initialize the rulesLib for single pass.
    (setq OKB.singlePass false)
    true) ; end startRule

;;*********************************************************************
;; Create a lambda list enclosed in a let expression, which contains
;; definitions of all the locally defined variables.
;; Note: We use the persistant variables from startRule
;;*********************************************************************
(defchild esm.selector letLambda(...)
    vars:(letList lambdaList result i argc varCount vecLast)
    ;; Set up the letList expression with the locally defined variables.
    (setq letList #void)
    (setq vecLast (subi (length varVector) 1))
    ;; If the Name is a registered cvar, then add it to the let lambda list.
    (setq varCount (length cvarVector[vecLast]))
    (loop for i from 0 until varCount do
       (if (= letList #void) (setq letList (list (list _hdr: 1))))
       (setq letList (append letList (list (list cvarVector[vecLast][i 0] cvarVector[vecLast][i 0]))))
       ) ; end cvar loop
    ;; If the Name is a registered pvar, then add it to the let lambda list.
    (setq varCount (length pvarVector[vecLast]))
    (loop for i from 0 until varCount do
       (if (= letList #void) (setq letList (list (list _hdr: 1))))
       (setq letList (append letList (list (list pvarVector[vecLast][i 0] pvarVector[vecLast][i 0]))))
       ) ; end pvar loop
    ;; If the Name is a registered var, then add it to the let lambda list.
    (setq varCount (length varVector[vecLast]))
    (loop for i from 0 until varCount do
       (if (= letList #void) (setq letList (list (list _hdr: 1))))
       (setq letList (append letList (list (list varVector[vecLast][i 0] varVector[vecLast][i 0]))))
       ) ; end var loop
    ;; If the Name is a registered arg, then add it to the let lambda list.
    (setq varCount (length avarVector[vecLast]))
    (loop for i from 0 until varCount do
       (if (= letList #void) (setq letList (list (list _hdr: 1))))
       (setq letList (append letList (list (list avarVector[vecLast][i 0] avarVector[vecLast][i 0]))))
       ) ; end avar loop
    ;; Add all the arguments to the let lambda list.
    (setq argc (argCount))
    (setq lambdaList (list lambda:))
    (loop for i from 0 until argc do
       (setq lambdaList (append lambdaList (list (argFetch i)))) 
       ) ; end arg loop
    ;; Add the let list and the let lambda list together.
    (if (= letList #void)
        (setq result lambdaList)
        (setq result (list let: letList lambdaList))    
        ) ; end if
    result) ; end letLambda

;;*********************************************************************
;; Run the named current project filter on the current selected row set.
;; Note: We use the persistant variables from startRule
;;*********************************************************************
(defchild esm.selector runFilter(struct filterName)
    vars:(cutLambda errLambda)
    ;; If filter name is void, then return true.
    (if (= filterName #void) (return (setq struct.Value true)))
    ;; Create a call to the specified function.
    (if (isPair filterName) (return (setq struct.Value filterName)))
    ;; Create a call to the runProjectFilter function from the arguments.
    (setq struct.Value (list (list |ref|: (list |ref|: dataMineLib: ''miner) ''runProjectFilter) filterName XT:))
    (setq autoXReferentSW false)
    struct) ; end runFilter

;;*********************************************************************
;; Create an updateView lambda of the current selected row set.
;; Note: We use the persistant variables from startRule
;;*********************************************************************
(defchild esm.selector filterUpdate(struct name setExp resetSW)
    vars:(setLambda errLambda)
    ;; Create an on error lambda from the arguments.
    (setq errLambda (list onError: (list lambda: (list s:) false)))
    ;; Create a set lambda from the arguments.
    (setq setLambda (letLambda (list  xv:) errLambda (list setq: name setExp)))
    ;; Use the command to create the proper truncate call for the cut lambda.
    (if (= resetSW true)
        then
        (setq struct.Value (list (list |ref|: XT: ''updateView) setLambda))
        else
        (setq struct.Value (list (list |ref|: XT: ''updateView) setLambda ''noreset))
        ) ; end if
    (setq autoXReferentSW false)
    (setq fieldRefs (new Dictionary:))
    struct) ; end filterUpdate

;;*********************************************************************
;; Create a find cut of the current selected row set.
;; Note: We use the persistant variables from startRule
;;*********************************************************************
(defchild esm.selector fieldCut(struct cutExp command)
    vars:(cutLambda errLambda)
    ;; If cut expression is void, then return true.
    (if (= cutExp #void) (return (setq struct.Value true)))
    ;; If the command is to omit, then turn auto checking off.
    (if (= command "omit") (setq autoCheckSW false))
    ;; Create an on error lambda from the arguments.
    (setq errLambda (list onError: (list lambda: (list s:) false)))
    ;; Create a cut lambda from the arguments.
    (setq cutLambda (letLambda (list xv:) errLambda cutExp))
    ;; Use the command to create the proper truncate call for the cut lambda.
    (if (= command "omit")
        (setq struct.Value (list (list |ref|: XT: ''omit) cutLambda))
        (setq struct.Value (list (list |ref|: XT: ''truncate) cutLambda))
        ) ; end if
    (setq autoXReferentSW false)
    struct) ; end fieldCut

;;*********************************************************************
;; Extract columns from the the current selected row set.
;; Note: We use the persistant variables from startRule
;;*********************************************************************
(defchild esm.selector extractColumns(struct spineVectorType extractExp)
    vars:(extractLambda errLambda)
    ;; If cut expression is void, then return true.
    (if (= extractExp #void) (return (setq struct.Value true)))
    ;; Create an on error lambda from the arguments.
    (setq errLambda (list onError: (list lambda: (list s:) #void)))
    ;; Create an extraction lambda from the arguments.
    (setq extractLambda (letLambda (list xv:) errLambda extractExp))
    ;; Create a truncate lambda to eliminate any void fields.
    (addToBeginList struct (autoCheck))
    ;; Create an extract lambda to eliminate any void fields.
    (addToBeginList struct (list (list |ref|: XT: ''extractColumns) (makeQuotedSymbol spineVectorType) extractLambda))
    (setq autoXReferentSW false)
    struct) ; end extractColumns

;;*********************************************************************
;; Create a find cut of the current selected row set.
;; Note: We use the persistant variables from startRule
;;*********************************************************************
(defchild esm.selector filterCut(struct cutExp command)
    vars:(cutLambda errLambda)
    ;; If cut expression is void, then return true.
    (if (= cutExp #void) (return (setq struct.Value true)))
    ;; If the command is to omit, then turn auto checking off.
    (if (= command "omit") (setq autoCheckSW false))
    ;; Create an on error lambda from the arguments.
    (setq errLambda (list onError: (list lambda: (list s:) false)))
    ;; Create a cut lambda from the arguments.
    (setq cutLambda (letLambda (list xv:) errLambda cutExp))
    ;; Create a truncate lambda to eliminate any void fields.
    (addToBeginList struct (autoCheck))
    ;; Use the command to create the proper truncate call for the cut lambda.
    (if (= command "omit")
        (addToBeginList struct (list (list |ref|: XT: ''omit) cutLambda))
        (addToBeginList struct (list (list |ref|: XT: ''truncate) cutLambda))
        ) ; end if
    (setq autoXReferentSW false)
    struct) ; end filterCut

;;*********************************************************************
;; Create an inline sort followed by a cut of the all row set.
;; Note: We use the persistant variables from startRule
;;*********************************************************************
(defchild esm.selector highestCut(struct eyExp cutExp)
    vars:(selectLambda eyLambda)
    ;; If regress expression is void, then return true.
    (if (= eyExp #void) (return (setq struct.Value true)))
    ;; Create a select lambda from the arguments.
    (setq eyLambda (string eyExp true))
    ;; Create a validate lambda to eliminate any void fields.
    (addToBeginList struct (autoCheck))
    ;; Create the proper expression for the select loop.
    (setq selectLambda  "(lambda (XT) pvars:(Number:B Number:Score Number:Penalty History PHistory Notes WFF Rule Genome pGenome) vars:(n N Number:ey Number:ty) vars:(NumVector:xv NumVector:EY rows) (onError (lambda(s) false)) (setq rows XT.selectedRows) (setq N (length rows)) (setq EY (|Gv:new| Vector: Number: N)) (loop for n from 0 until N do (setq xv rows[n]) (setq ey {***replace me***}) (setq EY[n] ey)) (XT.truncate EY) XT)" )
    (setq selectLambda (substitute selectLambda "{***replace me***}" eyLambda))
    (addToBeginList struct (lisp selectLambda)[0])
    (setq autoXReferentSW false)
    (setq haveSeenDataCmdsSW false)
    struct) ; end highestCut

;;*********************************************************************
;; Create a sort followed by a tile of the current selected row set.
;; Note: We use the persistant variables from startRule
;;*********************************************************************
(defchild esm.selector sliceCut(struct relOp fieldExp tileIndexExp tileCountExp)
    vars:(yReferent sortLambda errLambda)
    ;; Create a y referent version of the field expression.
    (setq yReferent (KB.apply (copy fieldExp)))
    ;; Create an on error lambda from the arguments.
    (setq errLambda (list onError: (list lambda: (list s:) false)))
    ;; Create a sort lambda from the arguments.
    (setq sortLambda (letLambda (list  xv:  yv:) errLambda (list relOp fieldExp yReferent)))
    ;; Create a truncate lambda to eliminate any void fields.
    (addToBeginList struct (autoCheck))
    ;; Create sort and tile commands from the various lambdas.
    (addToBeginList struct (list (list |ref|: XT: ''sort) sortLambda))
    (addToBeginList struct (list (list |ref|: XT: ''tile) tileCountExp tileIndexExp))
    (setq autoXReferentSW false)
    struct) ; end sliceCut

;;*********************************************************************
;; Create a sort followed by a cut of the current selected row set.
;; Note: We use the persistant variables from startRule
;;*********************************************************************
(defchild esm.selector sortCut(struct relOp fieldExp cutExp)
    vars:(yReferent sortLambda errLambda)
    ;; Create a y referent version of the field expression.
    (setq yReferent (KB.apply (copy fieldExp)))
    ;; Create an on error lambda from the arguments.
    (setq errLambda (list onError: (list lambda: (list s:) false)))
    ;; Create a sort lambda from the arguments.
    (setq sortLambda (letLambda (list xv: yv:) errLambda (list relOp fieldExp yReferent)))
    ;; Create a truncate lambda to eliminate any void fields.
    ;(addToBeginList struct (autoCheck))
    ;; Create sort and cut commands from the various lambdas.
    (if (< cutExp -1)
        (addToBeginList struct (list (list |ref|: XT: ''sort) sortLambda ''backup))
        (addToBeginList struct (list (list |ref|: XT: ''sort) sortLambda))
        ) ; end if
    (cond
        ;; Create cut command for integer cuts
        ((>= cutExp 1) (addToBeginList struct (list (list |ref|: XT: ''truncate) cutExp)))
        ;; Create cut command for percent cuts
        ((>= cutExp 0) (addToBeginList struct 
                           (list (list |ref|: XT: ''truncate) 
                                 (list integer: 
                                       (list *: (list |ref|: XT: ''recordCount) cutExp)))))
        ;; Do not create a cut command for negative cuts
        ) ; end cond
    (setq autoXReferentSW false)
    struct) ; end sortCut

;;*********************************************************************
;; Test to see if the expression is a record field name. 
;;*********************************************************************
(defchild esm.selector isFieldName(nameStruct)
    vars:(result)
    ;; Check to see if the name is an element name of a record in the selected row set.
    ;; Selector makes the following translations:
    ;;
    ;;    X       ==>  XT.selectedRows
    ;;    xv      ==>  ...the current row being studied...
    ;;    yv      ==>  ...the current second row being studied in a sort...
    ;;    xtime   ==>  xv[0] ...the time stamp element of the vector
    ;;    xn      ==>  xv[n] ...the nth element of the vector
    ;;    y       ==>  (cdr xv) ...the score element of the vector
    ;;
    (cond
     ;; Recognize xm
     ((and (= nameStruct.Value[0] #\x) (isCharNumeric (mid nameStruct.Value 1 10000))) (setq result true))
     ;; Recognize xtime
     ((= nameStruct.Value "xtime") (setq result true))
     ;; Recognize y
     ((= nameStruct.Value "y") (setq result true))
     ;; else
     (else (setq result false))
     ) ; end cond
    ;; Return true so the recognition step will continue
    result) ; end isFieldName

;;*********************************************************************
;; Set automatic field name to x referent conversion on. 
;;*********************************************************************
(defchild esm.selector xReferentOn() (setq haveSeenDataCmdsSW (setq autoXReferentSW true)))

;;*********************************************************************
;; Set automatic field name to x referent conversion off. 
;;*********************************************************************
(defchild esm.selector xReferentOff() (setq autoXReferentSW false) (setq haveSeenDataCmdsSW true))

;;*********************************************************************
;; Create an x referent expression from a record field name. 
;;*********************************************************************
(defchild esm.selector xReferent(struct nameStruct)
    vars:(vname fieldNameSW vecLast)
    ;; Determine whether or not the variable is 
    ;; a field name, or a registered variable name.
    (setq vname nameStruct.Value) 
    (setq fieldNameSW true)
    (setq vecLast (subi (length varVector) 1))
    (if (<> gvarVector[0][vname] #void) (setq fieldNameSW false))
    ;; If the Name is a registered cvar, then this is not a field name.
    (if (<> cvarVector[vecLast][vname] #void) (setq fieldNameSW false)) 
    ;; If the Name is a registered pvar, then this is not a field name.
    (if (<> pvarVector[vecLast][vname] #void) (setq fieldNameSW false)) 
    ;; If the Name is a registered var, then this is not a field name.
    (if (<> varVector[vecLast][vname] #void) (setq fieldNameSW false)) 
    ;; If the Name is a registered arg, then this is not a field name.
    (if (<> avarVector[vecLast][vname] #void) (setq fieldNameSW false))
    ;; Check to see if the name is a valid column name.
    (if (and (and autoXReferentSW fieldNameSW (<> vname void:) (<> vname nil:)) (isFieldName nameStruct))
        then
        (begin
           ;; If a column name, then record the field in the field references dictionary.
           ;(setq fieldRefs[vname] true)
           (setq fieldRefs[vname] #void)
           ;; If a column name, then return an x reference to the record.
           (setq struct.Value (cnvFieldName vname))
           ) ; end then
        else
        (begin
           ;; If not column name, then delete the name from the field references dictionary.
           (setq fieldRefs[vname] #void)
           ;; If not a column name, then return the name as entered.
           (setq struct.Value vname)
        )) ; end if
    (setq struct[Charpos:] nameStruct.Charpos)) ; end xReferent

;;*********************************************************************
;; Add an automatic Check command for all record field names seen. 
;;*********************************************************************
(defchild esm.selector autoCheck()
    vars:(i checkExp xRef result)
    ;; Set automatic field name to x referent conversion off.
    (setq autoXReferentSW false)
    ;; If auto checking is off, then just return true.
    (if (= autoCheckSW false) (return true))
    ;; Build the auto check command for all referenced fields.
    (loop for i from 0 until (length fieldRefs) do
        ;; Return an x reference to the record field name.
        (setq xRef (list |<>|: (list ref: xv: (makeQuotedSymbol fieldRefs[i 0])) #void))
        ;; Are there previous field references recorded?
        (if (= checkExp #void)
            then
            (setq checkExp xRef)
            else
            (setq checkExp (list and: checkExp xRef))
            ) ; end if
        ) ; end loop
    ;; Create a truncate expression.
    (setq result (fieldCut (new Structure:) checkExp "all"))
    (setq fieldRefs (new Dictionary:))
    result.Value) ; end autoCheck

;;*********************************************************************
;; Turn off automatic Checking for all record field names seen. 
;;*********************************************************************
(defchild esm.selector autoCheckOff(struct)
    ;; Turn off auto checking.
    (setq autoCheckSW false)
    (setq fieldRefs (new Dictionary:))
    (setq struct.Value true)) ; end autoCheckOff

;;*********************************************************************
;; Turn on automatic Checking for all record field names seen. 
;;*********************************************************************
(defchild esm.selector autoCheckOn(struct)
    ;; Turn on auto checking.
    (setq autoCheckSW true)
    (setq fieldRefs (new Dictionary:))
    (setq struct.Value true)) ; end autoCheckOn

;;*********************************************************************
;; Create a final filter expression on the current selected row set.
;; Note: We use the persistant variables from startRule
;;*********************************************************************
(defchild esm.selector filterFinal(filterExp)
    vars:(result)
    ;; Do nothing, if we've never seen a dataMineLib filter command.
    (if (<> haveSeenDataCmdsSW true) (return filterExp))
    ;; We must add a lambda wrapper if we are stand alone or global.
    (if (<= (length pvarVector) 0)
        then
        ;; If we are stand alone or global, add a lambda wrapper.
        (setq result (list lambda: (list XT:) ''|pvars| (list ''Number B: ''Number Score: ''Number Penalty: History: WFF: Rule: Genome: pGenome:) filterExp))
        else
        ;; If we are inside an existing function, add a begin wrapper.
        (setq result (list (symbol "begin") filterExp))
        ) ;; end if
    (setq fieldRefs (new Dictionary:))
    (setq result (OKB.apply (copy result)))
    (setq autoXReferentSW false)
    (if (= result '(begin true #void)) (setq result (list )))
    (if (= result '(lambda (XT) true #void XT)) (setq result (list )))
    result) ; end filterFinal

;;*********************************************************************
;; Create a score cut of the current selected row set.
;; Note: We use the persistant variables from startRule
;;*********************************************************************
(defchild esm.selector scoreCut(struct scoreExp command)
    vars:(scoreLambda errLambda)
    ;; If score expression is void, then return zero.
    (if (= scoreExp #void) (return (setq struct.Value 0)))
    ;; Create an on error lambda from the arguments.
    (setq errLambda (list onError: (list lambda: (list s:) false)))
    ;; Create a score lambda from the arguments.
    (setq scoreLambda (letLambda (list xv:) errLambda scoreExp))
    ;; Create a score command from the various lambda.
    (setq struct.Value (list (list |ref|: XT: (makeQuotedSymbol command)) scoreLambda))
    (setq autoXReferentSW false)
    struct) ; end scoreCut

;;*********************************************************************
;; Create a final score expression on the current selected row set.
;; Note: We use the persistant variables from startRule
;;*********************************************************************
(defchild esm.selector scoreFinal(scoreExp)
    vars:(result)
    ;; Do nothing, if we've never seen a dataMineLib score command.
    (if (<> haveSeenDataCmdsSW true) (return scoreExp))
    ;; We must add a lambda wrapper if we are stand alone or global.
    (if (<= (length pvarVector) 0)
        then
        ;; If we are stand alone or global, add a lambda wrapper.
        (setq result (list lambda: (list XT:) scoreExp))
        else
        ;; If we are inside an existing function, add a begin wrapper.
        (setq result (list (symbol "begin") scoreExp))
        ) ;; end if
    (setq fieldRefs (new Dictionary:))
    (setq result (OKB.apply (copy result)))
    result) ; end scoreFinal

;;*********************************************************************
;; Create a regression Lambda for the current selected row set.
;; Note: We use the persistant variables from startRule
;;*********************************************************************
(defchild esm.selector filterScale(struct regressExp)
    vars:(regressLambda eyLambda)
    ;; If regress expression is void, then return true.
    (if (= regressExp #void) (return (setq struct.Value true)))
    ;; Create a regress lambda from the arguments.
    (setq eyLambda (string regressExp true))
    ;; Create a regress lambda to eliminate any void fields.
    (addToBeginList struct (autoCheck))
    ;; Create the proper expression for the regression loop.
    (setq regressLambda "(lambda (NumVector:xv) regs:(Number:ey) (onError (lambda(s) false)) (setq ey {***replace me***}) (VALIDATE ey))" )
    (setq regressLambda (substitute regressLambda "{***replace me***}" eyLambda))
    (addToBeginList struct (lisp regressLambda)[0])
    (setq autoXReferentSW false)
    (setq haveSeenDataCmdsSW false)
    struct) ; end filterScale

;;*********************************************************************
;; Create a regression Lambda for the current selected row set.
;; Note: We use the persistant variables from startRule
;;*********************************************************************
(defchild esm.selector filterRegress(struct regressExp)
    vars:(regressLambda eyLambda)
    ;; If regress expression is void, then return true.
    (if (= regressExp #void) (return (setq struct.Value true)))
    ;; Create a regress lambda from the arguments.
    (setq eyLambda (string regressExp true))
    ;; Create a regress lambda to eliminate any void fields.
    (addToBeginList struct (autoCheck))
    ;; Create the proper expression for the regression loop.
    (setq regressLambda (append {(lambda (NumVector:xv) 
                                    pvars:(Integer:ID (Train full:) Number:A Number:B Number:AvgYSq Number:Score Number:BandScore Number:ErrScore Number:Error (Number:ErrorPct 999999.0) NumVector:EY Svm History WFF Rule Genome pGenome run train) 
                                    (defun train(ObjVector:X NumVector:Y ...) 
                                       regs:(n N) 
                                       regs:(Number:InvN Number:numerator Number:denominator Number:a Number:b Number:err) 
                                       regs:(Number:xmean Number:xsum Number:x) 
                                       regs:(Number:ymean Number:ysum Number:y) 
                                       regs:(Number:xxdot Number:yydot Number:xydot) 
                                       vars:(NumVector:xv) 
                                       (onError (lambda(s) false)) 
                                       (setq N (length X)) 
                                       (setq EY (new Vector: Number: N)) 
                                       (setq InvN (/ 1.0 (number N))) 
                                       (loop for n from 0 until N do 
                                          (setq y Y[n]) 
                                          (setq xv X[n]) 
                                          (setq x {***replace me***}) 
                                          (setq x (VALIDATE x)) 
                                          (setq EY[n] x) 
                                          (+= xsum x) 
                                          (+= ysum y) 
                                          (+= xxdot (* x x)) 
                                          (+= xydot (* x y)) 
                                          (+= yydot (* y y)))                                           
                                       (setq xmean (/ xsum N)) 
                                       (setq ymean (/ ysum N)) 
                                       (setq AvgY ymean) 
                                       (setq numerator (- xydot (* ysum xmean))) 
                                       (setq denominator (- xxdot (/ (* xsum xsum) N))) 
                                       (if (= denominator 0.0) (setq b 0.0) (setq b (/ numerator denominator))) 
                                       (setq a (- ymean (* b xmean))) 
                                       (setq err (+ yydot (* -2.0 b xydot) (* -2.0 a ysum) (* b b xxdot) (* 2.0 a b xsum) (* a a N) )) 
                                       (setq A a) 
                                       (setq B b) 
                                       (setq Error (abs (* InvN err))) 
                                       (setq AvgYSq (abs (* InvN yydot))) 
                                       (setq Score (sqrt Error)) 
                                       (if (<> yydot 0.0) (/= Score (sqrt yydot))) 
                                       true)
                                    (defun run(ObjVector:X) 
                                       regs:(n N) 
                                       regs:(Number:y Number:fx) 
                                       vars:(NumVector:xv NumVector:ey) 
                                       (onError (lambda(s) false)) 
                                       (setq N (length X)) 
                                       (setq ey (new Vector: Number: N)) 
                                       (loop for n from 0 until N do 
                                          (setq xv X[n]) 
                                          (setq fx {***replace me***}) 
                                          (setq y B) 
                                          (*= y fx) 
                                          (+= y A) 
                                          (setq y (VALIDATE y)) 
                                          (setq ey[n] y)) 
                                        ey)
                                    regs:(Number:y Number:fx) 
                                    (setq fx {***replace me***}) 
                                    (setq y B) 
                                    (*= y fx) 
                                    (+= y A) 
                                    (setq y (VALIDATE y)) 
                                    y)
                                }))
    (setq regressLambda (substitute regressLambda "{***replace me***}" eyLambda))
    (addToBeginList struct (lisp regressLambda)[0])
    (setq autoXReferentSW false)
    (setq haveSeenDataCmdsSW false)
    struct) ; end filterRegress

;;*********************************************************************
;; Create a regression Lambda for the current selected row set.
;; Note: We use the persistant variables from startRule
;;*********************************************************************
(defchild esm.selector filterRegressSvm(struct ...)
    regs:(n N)
    vars:(regressLambda eyLambda cvLambda expressionVector)
    ;; Load the expression vector (if one was provided).
    (if (>= (argCount) 2) (setq expressionVector (argFetch 1)) (setq expressionVector #void))
    ;; Create the proper Selector SVM Lambda depending upon the contents of the expression vector.
    (cond
     ;; Data input expressions have been provided.
     ((isVector expressionVector)
      (begin
         ;; Create a regress lambda from the arguments.
         (setq eyLambda "(Svm xv)")
         ;; Create a conversion lambda from the expression vector.
         (setq N (length expressionVector))
         (setq cvLambda (append "(setq xxv (new Vector: Number: " N "))")) 
         (loop for n from 0 until N do
            (setq cvLambda (append cvLambda " (setq fx " (string expressionVector[n] true) ") (setq xxv[" n "] fx)"))
            ) ; end expression loop
         ;; Create a regress lambda to eliminate any void fields.
         (addToBeginList struct (autoCheck))
         ;; Create the proper expression for the regression loop.
         (setq regressLambda (append {(lambda (NumVector:xv) 
                                         pvars:(Integer:ID (Train full:) Number:A Number:B Number:AvgYSq Number:Score Number:BandScore Number:ErrScore Number:Error (Number:ErrorPct 999999.0) NumVector:EY Svm History WFF Rule Genome pGenome convert run train)  
                                         (defun convert(NumVector:xv) 
                                            regs:(n N) 
                                            regs:(Number:fx) 
                                            vars:(NumVector:xxv) 
                                            {***convert me***} 
                                            xxv) 
                                         (defun run(ObjVector:X) 
                                            regs:(n N) 
                                            regs:(Number:y Number:fx) 
                                            vars:(NumVector:xv NumVector:ey) 
                                            (onError (lambda(s) false)) 
                                            (setq N (length X)) 
                                            (setq ey (new Vector: Number: N)) 
                                            (loop for n from 0 until N do 
                                               (setq xv X[n]) 
                                               (setq xv (convert xv)) 
                                               (setq fx {***replace me***}) 
                                               (setq y B) 
                                               (*= y fx) 
                                               (+= y A) 
                                               (setq y (VALIDATE y)) 
                                               (setq ey[n] y)) 
                                            ey) 
                                         (defun train(ObjVector:X NumVector:Y ...) 
                                            regs:(n N) 
                                            regs:(Number:InvN Number:numerator Number:denominator Number:a Number:b Number:err) 
                                            regs:(Number:xmean Number:xsum Number:x) 
                                            regs:(Number:ymean Number:ysum Number:y) 
                                            regs:(Number:xxdot Number:yydot Number:xydot) 
                                            vars:(ObjVector:XX NumVector:xv)  
                                            (onError (lambda(s) false))  
                                            (setq XX (copy X))  
                                            (setq N (length X))  
                                            (loop for n from 0 until N do (setq XX[n] (convert X[n])))  
                                            (setq X XX)  
                                            (setq Svm (esm.svmRegress X Y))  
                                            (setq EY #void)  
                                            (setq A 0.0)   
                                            (setq B 1.0)   
                                            (setq Error 0.0)   
                                            (setq AvgYSq 0.0)   
                                            (setq Score 0.0)   
                                            true) 
                                         regs:(Number:y Number:fx) 
                                         (setq xv (convert xv))  
                                         (setq fx {***replace me***})  
                                         (setq y B)  
                                         (*= y fx)  
                                         (+= y A)  
                                         (setq y (VALIDATE y))  
                                         y)
                                     }))
         (setq regressLambda (substitute regressLambda "{***replace me***}" eyLambda))
         (setq regressLambda (substitute regressLambda "{***convert me***}" cvLambda))
      )) ; end case Data input expressions
     ;; No data input expressions have been provided.
     (else
      (begin
         ;; Create a regress lambda from the arguments.
         (setq eyLambda "(Svm xv)")
         ;; Create a regress lambda to eliminate any void fields.
         (addToBeginList struct (autoCheck))
         ;; Create the proper expression for the regression loop.
         (setq regressLambda (append {(lambda (NumVector:xv)  
                                         pvars:(Integer:ID (Train full:) Number:A Number:B Number:AvgYSq Number:Score Number:BandScore Number:ErrScore Number:Error (Number:ErrorPct 999999.0) NumVector:EY Svm History WFF Rule Genome pGenome run train)  
                                         (defun run(ObjVector:X)  
                                            regs:(n N)   
                                            regs:(Number:y Number:fx)   
                                            vars:(NumVector:xv NumVector:ey)   
                                            (onError (lambda(s) false))   
                                            (setq N (length X))   
                                            (setq ey (new Vector: Number: N))   
                                            (loop for n from 0 until N do   
                                               (setq xv X[n])    
                                               (setq fx {***replace me***})    
                                               (setq y B)    
                                               (*= y fx)    
                                               (+= y A)    
                                               (setq y (VALIDATE y))    
                                               (setq ey[n] y))    
                                            ey) 
                                         (defun train(ObjVector:X NumVector:Y ...)    
                                            regs:(n N)     
                                            regs:(Number:InvN Number:numerator Number:denominator Number:a Number:b Number:err)     
                                            regs:(Number:xmean Number:xsum Number:x)     
                                            regs:(Number:ymean Number:ysum Number:y)     
                                            regs:(Number:xxdot Number:yydot Number:xydot)     
                                            vars:(NumVector:xv)     
                                            (onError (lambda(s) false))     
                                            (setq Svm (esm.svmRegress X Y))     
                                            (setq N (length X))     
                                            (setq EY #void)  
                                            (setq A 0.0)   
                                            (setq B 1.0)   
                                            (setq Error 0.0)   
                                            (setq AvgYSq 0.0)   
                                            (setq Score 0.0)   
                                            true) 
                                         regs:(Number:y Number:fx)       
                                         (setq fx {***replace me***})        
                                         (setq y B)        
                                         (*= y fx)        
                                         (+= y A)        
                                         (setq y (VALIDATE y)) 
                                         y)
                                     }))
         (setq regressLambda (substitute regressLambda "{***replace me***}" eyLambda))
      )) ; end case Data input expressions
     ) ; end cond 
    ;; Complete the final Selector SVM Lambda as a ParseLib attributed grammar token.
    (addToBeginList struct (lisp regressLambda)[0])
    (setq autoXReferentSW false)
    (setq haveSeenDataCmdsSW false)
    struct) ; end filterRegressSvm

;;*********************************************************************
;; Create a regression Lambda for the current selected row set.
;; Note: We use the persistant variables from startRule
;;*********************************************************************
(defchild esm.selector filterRegressMvl(struct expressionVector)
    regs:(n N)
    vars:(regressLambda eyLambda cvLambda )
    ;; Create the proper Selector MVL Lambda based upon the contents of the expression vector.
    ;; Create a regress lambda from the arguments.
    (setq eyLambda "(Mvl xv)")
    ;; Create a conversion lambda from the expression vector.
    (setq N (length expressionVector))
    (setq cvLambda (append "(setq xxv (new Vector: Number: " N "))")) 
    (loop for n from 0 until N do
       (setq cvLambda (append cvLambda " (setq fx " (string expressionVector[n] true) ") (setq xxv[" n "] fx)"))
       ) ; end expression loop
    ;; Create a regress lambda to eliminate any void fields.
    (addToBeginList struct (autoCheck))
    ;; Create the proper expression for the regression loop.
    (setq regressLambda (append {(lambda (NumVector:xv) 
                                    pvars:(Integer:ID (Train full:) Number:A Number:B Number:AvgYSq Number:Score Number:BandScore Number:ErrScore Number:Error (Number:ErrorPct 999999.0) NumVector:EY Mvl History WFF Rule Genome pGenome convert run train) 
                                    (defun convert(NumVector:xv) 
                                       regs:(n N) 
                                       regs:(Number:fx) 
                                       vars:(NumVector:xxv) 
                                       {***convert me***} 
                                       xxv)
                                    (defun run(ObjVector:X) 
                                       regs:(n N) 
                                       regs:(Number:y Number:fx) 
                                       vars:(NumVector:xv NumVector:ey) 
                                       (onError (lambda(s) false)) 
                                       (setq N (length X)) 
                                       (setq ey (new Vector: Number: N)) 
                                       (loop for n from 0 until N do 
                                          (setq xv X[n]) 
                                          (setq xv (convert xv)) 
                                          (setq fx {***replace me***}) 
                                          (setq y B) 
                                          (*= y fx) 
                                          (+= y A) 
                                          (setq y (VALIDATE y)) 
                                          (setq ey[n] y)) 
                                       ey)
                                    (defun train(ObjVector:X NumVector:Y ...) 
                                       regs:(n N) 
                                       regs:(Number:InvN Number:numerator Number:denominator Number:a Number:b Number:err) 
                                       regs:(Number:xmean Number:xsum Number:x) 
                                       regs:(Number:ymean Number:ysum Number:y) 
                                       regs:(Number:xxdot Number:yydot Number:xydot) 
                                       vars:(ObjVector:XX NumVector:xv) 
                                       (onError (lambda(s) false)) 
                                       (setq XX (copy X)) 
                                       (setq N (length X)) 
                                       (loop for n from 0 until N do (setq XX[n] (convert X[n]))) 
                                       (setq X XX) 
                                       (setq Mvl (esm.mvlRegress X Y)) 
                                       (setq EY #void)  
                                       (setq A 0.0)   
                                       (setq B 1.0)   
                                       (setq Error 0.0)   
                                       (setq AvgYSq 0.0)   
                                       (setq Score 0.0)   
                                       true)
                                    regs:(Number:y Number:fx) 
                                    (setq xv (convert xv)) 
                                    (setq fx {***replace me***}) 
                                    (setq y B) 
                                    (*= y fx) 
                                    (+= y A) 
                                    (setq y (VALIDATE y)) 
                                    y)
                                }))
    (setq regressLambda (substitute regressLambda "{***replace me***}" eyLambda))
    (setq regressLambda (substitute regressLambda "{***convert me***}" cvLambda))
    ;; Complete the final Selector SVM Lambda as a ParseLib attributed grammar token.
    (addToBeginList struct (lisp regressLambda)[0])
    (setq autoXReferentSW false)
    (setq haveSeenDataCmdsSW false)
    struct) ; end filterRegressMvl

;;*********************************************************************
;; Create a regression Lambda for the current selected row set.
;; Note: We use the persistant variables from startRule
;;*********************************************************************
(defchild esm.selector filterRegressBgm(struct expressionVector)
    regs:(n N)
    vars:(regressLambda replaceMeLambda convertMeLambda )
    ;; Create the proper Selector BGM Lambda based upon the contents of the expression vector.
    ;; Create a regress lambda from the arguments, and
    ;; Create a conversion lambda from the expression vector.
    (setq N (length expressionVector))
    (setq convertMeLambda (append "(setq xxv (new Vector: Number: " N "))")) 
    (setq replaceMeLambda "(Bgm xv)")
    (loop for n from 0 until N do
       (setq convertMeLambda (append convertMeLambda " (setq fx " (string expressionVector[n] true) ") (setq xxv[" n "] fx)"))
       ) ; end expression loop
    ;; Create a regress lambda to eliminate any void fields.
    (addToBeginList struct (autoCheck))
    ;; Create the proper expression for the regression loop.
    (setq regressLambda (append {
                                (lambda (NumVector:xv) 
                                   pvars:(Integer:ID (Train full:) Number:A Number:B Number:AvgYSq Number:Score Number:BandScore Number:ErrScore Number:Error (Number:ErrorPct 999999.0) NumVector:EY Bgm History WFF Rule Genome pGenome convert run train) 
                                   (defun convert(NumVector:xv) 
                                      regs:(n N) 
                                      regs:(Number:fx) 
                                      vars:(NumVector:xxv) 
                                      {***convert me***} 
                                      xxv)
                                   (defun run(ObjVector:X) 
                                      regs:(n N) 
                                      regs:(Number:y Number:fx) 
                                      vars:(NumVector:xv NumVector:ey) 
                                      vars:(ObjVector:XX) 
                                      (onError (lambda(s) false)) 
                                      (setq N (length X)) 
                                      (setq XX (copy X)) 
                                      (loop for n from 0 until N do (setq XX[n] (convert X[n]))) 
                                      (setq X XX) 
                                      (if (= (setq ey (Bgm.run X)) false) (return false))
                                      (loop for n from 0 until N do 
                                         (setq fx ey[n]) 
                                         (setq y B) 
                                         (*= y fx) 
                                         (+= y A) 
                                         (setq y (VALIDATE y)) 
                                         (setq ey[n] y)
                                         ) 
                                      ey)
                                   (defun train(ObjVector:X NumVector:Y ...) 
                                      regs:(n N) 
                                      regs:(Number:InvN Number:numerator Number:denominator Number:a Number:b Number:err) 
                                      regs:(Number:xmean Number:xsum Number:x) 
                                      regs:(Number:ymean Number:ysum Number:y) 
                                      regs:(Number:xxdot Number:yydot Number:xydot) 
                                      vars:(ObjVector:XX NumVector:xv) 
                                      (onError (lambda(s) false)) 
                                      (setq XX (copy X)) 
                                      (setq N (length X)) 
                                      (loop for n from 0 until N do (setq XX[n] (convert X[n]))) 
                                      (setq X XX) 
                                      (setq Bgm (esm.bgmRegress X Y)) 
                                      (setq EY #void)  
                                      (setq A 0.0)   
                                      (setq B 1.0)   
                                      (setq Error 0.0)   
                                      (setq AvgYSq 0.0)   
                                      (setq Score 0.0)   
                                      true)
                                   regs:(Number:y Number:fx) 
                                   (setq xv (convert xv)) 
                                   (setq fx {***replace me***}) 
                                   (setq y B) 
                                   (*= y fx) 
                                   (+= y A) 
                                   (setq y (VALIDATE y)) 
                                   y)
                                }))
    (setq regressLambda (substitute regressLambda "{***replace me***}" replaceMeLambda))
    (setq regressLambda (substitute regressLambda "{***convert me***}" convertMeLambda))
    ;; Complete the final Selector SVM Lambda as a ParseLib attributed grammar token.
    (addToBeginList struct (lisp regressLambda)[0])
    (setq autoXReferentSW false)
    (setq haveSeenDataCmdsSW false)
    struct) ; end filterRegressBgm

;;*********************************************************************
;; Create a regression Lambda for the current selected row set.
;; Note: We use the persistant variables from startRule
;;*********************************************************************
(defchild esm.selector filterRegressFrm(struct expressionVector)
    regs:(n N)
    vars:(regressLambda eyLambda cvLambda )
    ;; Create the proper Selector FRM Lambda based upon the contents of the expression vector.
    ;; Create a regress lambda from the arguments.
    (setq eyLambda "(Frm xv)")
    ;; Create a conversion lambda from the expression vector.
    (setq N (length expressionVector))
    (setq cvLambda (append "(setq xxv (new Vector: Number: " N "))")) 
    (loop for n from 0 until N do
       (setq cvLambda (append cvLambda " (setq fx " (string expressionVector[n] true) ") (setq xxv[" n "] fx)"))
       ) ; end expression loop
    ;; Create a regress lambda to eliminate any void fields.
    (addToBeginList struct (autoCheck))
    ;; Create the proper expression for the regression loop.
    (setq regressLambda (append {
                                (lambda (NumVector:xv) 
                                  pvars:(Integer:ID (Train full:) Number:A Number:B Number:AvgYSq Number:Score Number:BandScore Number:ErrScore Number:Error (Number:ErrorPct 999999.0) NumVector:EY Frm History WFF Rule Genome pGenome convert run train)  
                                  (defun convert(NumVector:xv) 
                                     regs:(n N) 
                                     regs:(Number:fx) 
                                     vars:(NumVector:xxv) 
                                     {***convert me***} 
                                     xxv)
                                  (defun run(ObjVector:X) 
                                     regs:(n N) 
                                     regs:(Number:y Number:fx) 
                                     vars:(NumVector:xv NumVector:ey) 
                                     vars:(ObjVector:XX) 
                                     (onError (lambda(s) false)) 
                                     (setq N (length X)) 
                                     (setq XX (copy X)) 
                                     (loop for n from 0 until N do (setq XX[n] (convert X[n]))) 
                                     (setq X XX) 
                                     (Frm.rescale X) 
                                     (setq ey (new Vector: Number: N)) 
                                     (loop for n from 0 until N do 
                                       (setq xv X[n]) 
                                       (setq fx {***replace me***}) 
                                       (setq y B) 
                                       (*= y fx) 
                                       (+= y A) 
                                       (setq y (VALIDATE y)) 
                                       (setq ey[n] y)) 
                                     ey)  
                                  (defun train(ObjVector:X NumVector:Y ...) 
                                     regs:(n N) 
                                     regs:(Number:InvN Number:numerator Number:denominator Number:a Number:b Number:err) 
                                     regs:(Number:xmean Number:xsum Number:x) 
                                     regs:(Number:ymean Number:ysum Number:y) 
                                     regs:(Number:xxdot Number:yydot Number:xydot) 
                                     vars:(ObjVector:XX NumVector:xv) 
                                     (onError (lambda(s) false)) 
                                     (setq XX (copy X)) 
                                     (setq N (length X)) 
                                     (loop for n from 0 until N do (setq XX[n] (convert X[n]))) 
                                     (setq X XX) 
                                     (setq Frm (esm.frmRegress X Y)) 
                                     (setq EY #void)  
                                     (setq A 0.0)   
                                     (setq B 1.0)   
                                     (setq Error 0.0)   
                                     (setq AvgYSq 0.0)   
                                     (setq Score 0.0)   
                                     true) 
                                  regs:(Number:y Number:fx) 
                                  (setq xv (convert xv)) 
                                  (setq fx {***replace me***}) 
                                  (setq y B) 
                                  (*= y fx) 
                                  (+= y A) 
                                  (setq y (VALIDATE y)) 
                                  y)
                                }))
    (setq regressLambda (substitute regressLambda "{***replace me***}" eyLambda))
    (setq regressLambda (substitute regressLambda "{***convert me***}" cvLambda))
    ;; Complete the final Selector SVM Lambda as a ParseLib attributed grammar token.
    (addToBeginList struct (lisp regressLambda)[0])
    (setq autoXReferentSW false)
    (setq haveSeenDataCmdsSW false)
    struct) ; end filterRegressFrm

;;*********************************************************************
;; Create a regression Lambda for the current selected row set.
;; Note: We use the persistant variables from startRule
;;*********************************************************************
(defchild esm.selector filterRegressEnn(struct weightVector)
    regs:(h H n N)
    vars:(regressLambda )
    ;; Create the proper Selector ENN Lambda based upon the contents of the weight vector.
    (setq regressLambda (append {
                                (lambda (NumVector:xv) 
                                  pvars:(Integer:ID (Train full:) Number:A Number:B Number:AvgYSq Number:Score Number:BandScore Number:ErrScore Number:Error (Number:ErrorPct 999999.0) NumVector:EY Enn History WFF Rule Genome pGenome convert run train)  
                                  (defun run(ObjVector:X) 
                                     regs:(n N) 
                                     regs:(Number:y Number:fx) 
                                     vars:(NumVector:xv NumVector:ey) 
                                     (onError (lambda(s) false)) 
                                     (setq N (length X)) 
                                     (setq ey (new Vector: Number: N)) 
                                     (loop for n from 0 until N do 
                                       (setq xv X[n]) 
                                       (setq fx (Enn xv)) 
                                       (setq y B) 
                                       (*= y fx) 
                                       (+= y A) 
                                       (setq y (VALIDATE y)) 
                                       (setq ey[n] y)) 
                                     ey)  
                                  (defun train(ObjVector:X NumVector:Y ObjVector:weightVector ) 
                                     regs:(n N) 
                                     regs:(Number:InvN Number:numerator Number:denominator Number:a Number:b Number:err) 
                                     regs:(Number:xmean Number:xsum Number:x) 
                                     regs:(Number:ymean Number:ysum Number:y) 
                                     regs:(Number:xxdot Number:yydot Number:xydot) 
                                     vars:(NumVector:xv) 
                                     (onError (lambda(s) false)) 
                                     (setq N (length X))
                                     (setq Enn (esm.ennRegress X Y weightVector)) 
                                     (setq EY #void)  
                                     (setq A 0.0)   
                                     (setq B 1.0)   
                                     (setq Error 0.0)   
                                     (setq AvgYSq 0.0)   
                                     (setq Score 0.0)   
                                     true) 
                                  regs:(Number:y Number:fx) 
                                  (setq fx (Enn xv)) 
                                  (setq y B) 
                                  (*= y fx) 
                                  (+= y A) 
                                  (setq y (VALIDATE y)) 
                                  y)
                                }))
    ;; Complete the final Selector SVM Lambda as a ParseLib attributed grammar token.
    (addToBeginList struct (lisp regressLambda)[0])
    (setq autoXReferentSW false)
    (setq haveSeenDataCmdsSW false)
    struct) ; end filterRegressEnn








































;;**EXPORTKEY**:esm:selector:%DEFINITION
;#text#
;; ********************************************************************
;; summary:  This Selector compiler definition includes all the features
;;           of a full JavaScript compiler for the LambdaServer engine.
;;           See the following references:
;;
;;           [1]  "JavaScript Language Ref Guide", Korns Associates
;;           [2]  "JavaScript For The World Wide Web", Postscript Press
;;           [3]  "JavaScript Developer's Resource", Prentice Hall
;;
;; Notes:    Requires the browseLib, the ParseLib, and this compiler
;;           definition source must be checked into the file cabinet 
;;           under the key: |esm:selector:DEFINITION|.
;; Parms:    none
;; return:   java   The ParseLib checks in a newly updated copy of the
;;                  java compiler source and compiles the "esm.selector" compiler.
;; ********************************************************************
   
#LexicalRules#

 MAIN: user ordering :: true ::    
 MAIN: "void" << ($ASIS $ch #void vtyp: obj: Term: true Constant: true) >>    
 MAIN: "nil" << ($ASIS $ch #void vtyp: obj: Term: true Constant: true) >>    
 MAIN: DQuote NotDQuote* DQuote << ($ASIS $ch $2 String: true Term: true Constant: true) >>    
 MAIN: Quote NotQuote* Quote << ($ASIS $ch (makeQuotedSymbol $2) Symbol: true Term: true Constant: true) >>    
 MAIN: NameStart NameChar* << ($OUT $ch (symbol (append $1 $2)) Name: true Term: true) >>    
 MAIN: Digit+ Period Digit* Exponent Sign Digit+ 
                                  << ($ASIS $ch (number (append $1 $2 $3 $4 $5 $6)) Number: true Term: true) >>    
 MAIN: Digit+ Period Digit* << ($ASIS $ch (number (append $1 $2 $3)) Number: true Term: true) >>    
 MAIN: Digit+ << ($ASIS $ch (integer $1) Number: true Integer: true Term: true) >>    
 MAIN: Period Digit+    << ($ASIS $ch (number (append $1 $2)) Number: true Term: true) >>    
 MAIN: / * Any{(or (<> (refString $IN _ip) #\*) (<> (refString $IN (iadd _ip 1)) #\/))}*  * / << true >>    
 MAIN: / / NotEol* Eol? << true >>    
 MAIN: "=" "="   << ($OUT $ch (string (append $1 $2)) Operator: true) >>    
 MAIN: "+" "="   << ($OUT $ch (string (append $1 $2)) Operator: true) >>    
 MAIN: "-" "="   << ($OUT $ch (string (append $1 $2)) Operator: true) >>    
 MAIN: "/" "="   << ($OUT $ch (string (append $1 $2)) Operator: true) >>    
 MAIN: "*" "="   << ($OUT $ch (string (append $1 $2)) Operator: true) >>    
 MAIN: "%" "="   << ($OUT $ch (string (append $1 $2)) Operator: true) >>    
 MAIN: "!" "="   << ($OUT $ch (string (append $1 $2)) Operator: true) >>    
 MAIN: "&" "="   << ($OUT $ch (string (append $1 $2)) Operator: true) >>    
 MAIN: "|" "="   << ($OUT $ch (string (append $1 $2)) Operator: true) >>    
 MAIN: "<" "="   << ($OUT $ch (string (append $1 $2)) Operator: true) >>    
 MAIN: ">" "="   << ($OUT $ch (string (append $1 $2)) Operator: true) >>    
 MAIN: "&" "&"   << ($OUT $ch (string (append $1 $2)) Operator: true) >>    
 MAIN: "|" "|"   << ($OUT $ch (string (append $1 $2)) Operator: true) >>    
 MAIN: "="       << ($OUT $ch (string $1) Operator: true) >>    
 MAIN: "+"       << ($OUT $ch (string $1) Operator: true Sign: true) >>    
 MAIN: "-"       << ($OUT $ch (string $1) Operator: true Sign: true) >>    
 MAIN: "/"       << ($OUT $ch (string $1) Operator: true) >>    
 MAIN: "*"       << ($OUT $ch (string $1) Operator: true) >>    
 MAIN: "%"       << ($OUT $ch (string $1) Operator: true) >>    
 MAIN: "!"       << ($OUT $ch (string $1) Operator: true) >>    
 MAIN: "&"       << ($OUT $ch (string $1) Operator: true) >>    
 MAIN: "|"       << ($OUT $ch (string $1) Operator: true) >>    
 MAIN: "<"       << ($OUT $ch (string $1) Operator: true) >>    
 MAIN: ">"       << ($OUT $ch (string $1) Operator: true) >>    
 MAIN: "~"       << ($OUT $ch (string $1) Operator: true) >>    
 MAIN: # Letter+ , Digit+ , Digit+ << ($ASIS $ch (date (append $1 $2 $3 $4 $5 $6)) Date: true Term: true) >>    
 MAIN: # < NameStart NameChar* Whitespace*  Digit* > << ($OUT $ch (inspect (number $6)) vtyp: obj: Term: true) >>    
 MAIN: Whitespace+      << true >>    
 MAIN: Any              << ($OUT $ch (symbol (string $1)) Default: true) >>    
 MAIN: Eof :: $LIST ::

#End#

#LexicalFeatures#

Digit: [|"0"-"9"|]
Alpha: [|a-z| |A-Z|]
AlphaNum: [|a-z| |A-Z| |"0"-"9"|]
NameChar: [|a-z| |A-Z| |"0"-"9"| "_"]
Letter: [|a-z| |A-Z|]  
NameStart: [|a-z| |A-Z| "_"]  
Special: [< > = & % ! ^ "~" + / * - "|" "#"]
DQuote: [34]
NotDQuote: [|0-255| ~ 34]
Quote: [39]
NotQuote: [|0-255| ~ 39]
Whitespace: [|0-32|]
Eol: [10 13]
NotEol: [|0-255| ~ 10 13]
Period: ["."]
Exponent: [e E]
Sign: [+ -]

#End#

#SyntaxRules#

  MAIN: user ordering :: true ::
  MAIN: # "selector" # || (= $3.Charpos 8) || << true >>
  MAIN: STMTLIST Eof :: (list $1.Value) ::
  MAIN: STMTLIST Value :: (_makeError "JS 100" $2.Charpos "Invalid expression") ::
  MAIN: Value :: (_makeError "JS 101" $1.Charpos "Invalid expression") ::
  MAIN: Eof :: (list #void) ::

  ;; ********************************************************************
  ;; Start rules for compiling Selector extensions.
  ;; ********************************************************************
  FSTMTLIST: FSTATEMENT << (addToBeginList $0 $1.Value) >>
  FSTMTLIST: Semicolon << $0 >>

  FSTATEMENT: user ordering :: true ::
  FSTATEMENT: Sort{(xReferentOn)} Direction SEXPRESSION :: (sortCut $0 $2.Lisp $3.Value -1) ::
  FSTATEMENT: Sort{(xReferentOn)} SEXPRESSION :: (sortCut $0 |<=|: $2.Value -1) ::
  FSTATEMENT: Cut{(xReferentOn)} SEXPRESSION CUT :: (sortCut $0 $1.Lisp $2.Value $3.Value) ::
  FSTATEMENT: Slice{(xReferentOn)} SEXPRESSION CUT Of CUT :: (sliceCut $0 $1.Lisp $2.Value $3.Value $5.Value) ::
  FSTATEMENT: All{(xReferentOn)} Semicolon :: (setq $0.Value true) ::
  FSTATEMENT: All{(xReferentOn)} SEXPRESSION :: (filterCut $0 $2.Value $1.Value) ::
  FSTATEMENT: All{(xReferentOn)} :: (setq $0.Value true) ::
  FSTATEMENT: Run{(xReferentOff)} SEXPRESSION :: (runFilter $0 $2.Value) ::
  FSTATEMENT: Restore{(xReferentOff)} :: (setq $0.Value (list (list |ref|: XT: ''restore))) ::
  FSTATEMENT: Scale{(xReferentOn)} SEXPRESSION Semicolon :: (filterScale $0 $2.Value) ::
  FSTATEMENT: Regress{(xReferentOn)} SEXPRESSION Semicolon :: (filterRegress $0 $2.Value) ::
  FSTATEMENT: Bgmregress{(xReferentOn)} LeftParen ARGLIST Semicolon :: (filterRegressBgm $0 $3.Value) ::
  FSTATEMENT: Ennregress LeftParen WEIGHTLIST Semicolon :: (filterRegressEnn $0 (lisp $3.Value)) ::
  FSTATEMENT: Frmregress{(xReferentOn)} LeftParen ARGLIST Semicolon :: (filterRegressFrm $0 $3.Value) ::
  FSTATEMENT: Mvlregress{(xReferentOn)} LeftParen ARGLIST Semicolon :: (filterRegressMvl $0 $3.Value) ::
  FSTATEMENT: Svmregress{(xReferentOn)} LeftParen ARGLIST Semicolon :: (filterRegressSvm $0 $3.Value) ::
  FSTATEMENT: Svmregress{(xReferentOn)} LeftParen RightParen Semicolon :: (filterRegressSvm $0) ::
  FSTATEMENT: Svmregress{(xReferentOn)} Semicolon :: (filterRegressSvm $0) ::
  FSTATEMENT: Highest{(xReferentOn)} SEXPRESSION CUT :: (highestCut $0 $2.Value $3.Value) ::

  CUT: user ordering :: true ::
  CUT: Number Percent :: (foldConstants $0 |/|: $1.Value 100) ::
  CUT: Number :: $1 ::
  CUT: SEXPRESSION :: $1 ::

  FILTER: Select FSTMTLIST :: (setq $0.Value (filterFinal $2.Value)) ::
  FILTER: FSTMTLIST :: (setq $0.Value (filterFinal $1.Value)) ::
 
  STATEMENT: FILTER :: $1 ::

  SSTATEMENT: ScoreCommand{(xReferentOn)} SEXPRESSION :: (scoreCut $0 $2.Value $1.Value) ::
  SSTATEMENT: ScoreCommand{(xReferentOn)} SEXPRESSION Semicolon :: (scoreCut $0 $2.Value $1.Value) ::
  SSTATEMENT: Extract{(xReferentOn)} Colon Name Colon SEXPRESSION :: (extractColumns $0 $3.Value $5.Value) ::
  SSTATEMENT: Extract{(xReferentOn)} Colon Name Colon SEXPRESSION Semicolon :: (extractColumns $0 $3.Value $5.Value) ::
  SSTATEMENT: Extract{(xReferentOn)} SEXPRESSION :: (extractColumns $0 Object: $2.Value) ::
  SSTATEMENT: Extract{(xReferentOn)} SEXPRESSION Semicolon :: (extractColumns $0 Object: $2.Value) ::

  SCORE: user ordering :: true ::
  SCORE: Score NAME AssignmentOperator SSTATEMENT :: (setq $0.Value (appendList $3.Lisp $2.Value (list $4.Value))) ::
  SCORE: Score SSTATEMENT :: (setq $0.Value (scoreFinal $2.Value)) ::
  SCORE: Score{(begin (pushVars $0) true)} SEXPRESSION{(begin (popVars) true)} :: (setq $0.Value (scoreFinal $2.Value)) ::
  SCORE: SSTATEMENT :: (setq $0.Value (scoreFinal $1.Value)) ::

  STATEMENT: SCORE :: $1 ::
  ;; ********************************************************************
  ;; End rules for compiling Selector extensions.
  ;; ********************************************************************

  STMTLIST: SEXPRESSION << (addToBeginList $0 $1.Value) >>
  STMTLIST: Semicolon << $0 >>

  SEXPRESSION: user ordering :: true ::
  SEXPRESSION: EXPRESSION Question EXPRESSION Colon EXPRESSION 
               :: (setq $0.Value (list if: $1.Value $3.Value $5.Value)) ::
  SEXPRESSION: LeftParen Type RightParen EXPRESSION :: (setq (setq (setq $4.VType $2.Value)[SEXPRESSION:] true)[Charpos:] $0.Charpos) ::
  SEXPRESSION: EXPRESSION :: $1 ::

  EXPRESSION: user ordering :: true ::
  EXPRESSION: PHRASE Operator SEXPRESSION :: (setExpVType $0 $1 $2 $3) ::
  EXPRESSION: PHRASE Operator 
               :: (_makeError "JS 102" $2.Charpos (append "Invalid use of " $2.Value " operator")) ::
  EXPRESSION: PHRASE  :: $1 ::
  EXPRESSION: TERM Operator SEXPRESSION :: (setExpVType $0 $1 $2 $3) ::
  EXPRESSION: TERM Operator 
               :: (_makeError "JS 103" $2.Charpose (append "Invalid use of " $2.Value " operator")) ::
  EXPRESSION: TERM :: $1 ::
  EXPRESSION: STATEMENT :: $1 ::
  EXPRESSION: STATEMENT Operator SEXPRESSION :: (setExpVType $0 $1 $2 $3) ::

  STATEMENT: FUNCTION LeftParen PARMLIST LeftBrace STMTLIST RightBrace :: (funList $0 #void $3 $5) ::
  STATEMENT: FUNCTION Name LeftParen PARMLIST LeftBrace STMTLIST RightBrace :: (funList $0 $2 $4 $6) ::
  STATEMENT: FUNCTION Name LeftParen PARMLIST LeftBrace STMTLIST Value 
                    :: (_makeError "JS 104" $7.Charpos "Invalid statement") ::
  STATEMENT: FUNCTION :: (_makeError "JS 105" $1.Charpos "Invalid function declaration") ::
  STATEMENT: FRIEND Name Name LeftParen PARMLIST LeftBrace STMTLIST RightBrace :: (childList |defriend|: $0 $2 $3 $5 $7) ::
  STATEMENT: FRIEND Name LeftParen PARMLIST LeftBrace STMTLIST Value 
                    :: (_makeError "JS 106" $7.Charpos "Invalid statement") ::
  STATEMENT: FRIEND :: (_makeError "JS 107" $1.Charpos "Invalid function declaration") ::
  STATEMENT: CHILD Name Name LeftParen PARMLIST LeftBrace STMTLIST RightBrace :: (childList |defchild|: $0 $2 $3 $5 $7) ::
  STATEMENT: CHILD Name LeftParen PARMLIST LeftBrace STMTLIST Value 
                    :: (_makeError "JS 108" $7.Charpos "Invalid statement") ::
  STATEMENT: CHILD :: (_makeError "JS 109" $1.Charpos "Invalid function declaration") ::
  STATEMENT: ORPHAN Name Name LeftParen PARMLIST LeftBrace STMTLIST RightBrace 
                    :: (childList |deforphan|: $0 $2 $3 $5 $7) ::
  STATEMENT: ORPHAN Name LeftParen PARMLIST LeftBrace STMTLIST Value 
                    :: (_makeError "JS 110" $7.Charpos "Invalid statement") ::
  STATEMENT: ORPHAN :: (_makeError "JS 111" $1.Charpos "Invalid function declaration") ::
  STATEMENT: METHOD Name Name LeftParen PARMLIST LeftBrace STMTLIST RightBrace 
                    :: (childList |defmethod|: $0 $2 $3 $5 $7) ::
  STATEMENT: METHOD Name LeftParen PARMLIST LeftBrace STMTLIST Value 
                    :: (_makeError "JS 112" $7.Charpos "Invalid statement")::
  STATEMENT: METHOD :: (_makeError "JS 113" $1.Charpos "Invalid function declaration") ::
  STATEMENT: CLASS Name LeftBrace FIELDLIST :: (fieldList $0 $2.Value #void $4.Value) ::
  STATEMENT: CLASS Name Extends Name LeftBrace FIELDLIST :: (fieldList $0 $2.Value $4.Value $6.Value) ::
  STATEMENT: CLASS :: (_makeError "JS 114" $1.Charpos "Invalid class declaration") ::

  FUNCTION: Function :: (setq (pushVars $0)[Charpos:] $1.Charpos) ::
  FRIEND:   Friend :: (setq (pushVars $0)[Charpos:] $1.Charpos) ::
  ORPHAN:   Orphan :: (setq (pushVars $0)[Charpos:] $1.Charpos) ::
  METHOD:   Method :: (setq (pushVars $0)[Charpos:] $1.Charpos) ::
  CHILD:    Child :: (setq (pushVars $0)[Charpos:] $1.Charpos) ::
  CLASS:    Class :: (setq (pushVars $0)[Charpos:] $1.Charpos) ::


  QUALIFY: DotOperator Reserved QUALIFY((qualifyName $0 %0 $2 (makeQuotedSymbol $2.Value))) :: $3 ::
  QUALIFY: DotOperator Reserved :: (qualifyName $0 %0 $2 (makeQuotedSymbol $2.Value)) ::
  QUALIFY: DotOperator Name QUALIFY((qualifyName $0 %0 $2 (makeQuotedSymbol $2.Value))) :: $3 ::
  QUALIFY: DotOperator Name :: (qualifyName $0 %0 $2 (makeQuotedSymbol $2.Value)) ::
  QUALIFY: DotOperator :: (_makeError "JS 115" $1.Charpos "Invalid use of dot operator") ::
  QUALIFY: LeftBracket REFLIST QUALIFY((qualifyName $0 %0 $2 $2.Value)) :: $3 ::
  QUALIFY: LeftBracket REFLIST :: (qualifyName $0 %0 $2 $2.Value) ::
  QUALIFY: LeftBracket :: (_makeError "JS 116" $1.Charpos "Invalid use of [ operator" ) ::

  CFCALL: DotOperator Reserved LeftParen ARGLIST :: (sendList $0 $2.Value %0.Value $4.Value) ::
  CFCALL: DotOperator Reserved LeftParen 
               :: (_makeError "JS 117" $1.Charpos "Invalid function call") ::
  CFCALL: DotOperator Reserved CFCALL((qualifyName $0 %0 $2 (makeQuotedSymbol $2.Value))) :: $3 ::
  CFCALL: DotOperator Name LeftParen ARGLIST :: (sendList $0 $2.Value %0.Value $4.Value) ::
  CFCALL: DotOperator Name LeftParen 
               :: (_makeError "JS 117" $1.Charpos "Invalid function call") ::
  CFCALL: DotOperator Name CFCALL((qualifyName $0 %0 $2 (makeQuotedSymbol $2.Value))) :: $3 ::
  
    ;; ********************************************************************
  ;; Start rules for compiling Selector extensions.
  ;; ********************************************************************
  NAME: user ordering :: true ::
  NAME: Name{(isFieldName $1)} QUALIFY((xReferent $0 $1)) :: (setq $2[Charpos:] $1.Charpos) ::
  NAME: Name{(isFieldName $1)} :: (xReferent $0 $1) ::
  ;; ********************************************************************
  ;; End rules for compiling Selector extensions.
  ;; ********************************************************************

  NAME: Name QUALIFY($1) :: (setq $2[Charpos:] $1.Charpos) ::
  NAME: Name :: (setVType $1) ::

  PHRASE: user ordering :: true ::
  PHRASE: Logical SEXPRESSION :: (setq (setq $0.Value (list $1.Lisp $2.Value))[VType:] bool:) ::
  PHRASE: Logical :: (_makeError "JS 118" $1.Charpos (append "Invalid " $1.Value " operator")) ::
  PHRASE: Increment NAME :: (postfix $0 $2 $1 false) ::
  PHRASE: Increment :: (_makeError "JS 119" $1.Charpos (append "Invalid " $1.Value " operator")) ::
  PHRASE: LeftBrace STMTLIST RightBrace :: $2 ::
  PHRASE: LeftBrace :: (_makeError "JS 120" $1.Charpos "Invalid statement block") ::
  PHRASE: If SEXPRESSION SEXPRESSION Else SEXPRESSION :: (setq $0.Value (list |if|: $2.Value $3.Value $5.Value)) ::
  PHRASE: If SEXPRESSION SEXPRESSION Else 
          :: (_makeError "JS 121" $1.Charpos "Invalid else statement") ::
  PHRASE: If SEXPRESSION SEXPRESSION :: (setq $0.Value (list |if|: $2.Value $3.Value)) ::
  PHRASE: If SEXPRESSION :: (_makeError "JS 121" $1.Charpos "Invalid then statement") ::
  PHRASE: If :: (_makeError "JS 122" $1.Charpos "Invalid if statement") ::
  PHRASE: While SEXPRESSION SEXPRESSION :: (setq $0.Value (list |while|: $2.Value $3.Value)) ::
  PHRASE: While  
          :: (_makeError "JS 123" $1.Charpos "Invalid while statement") ::
  PHRASE: For LeftParen PHRASE Semicolon SEXPRESSION Semicolon PHRASE RightParen SEXPRESSION
             :: (setq $0.Value (list |begin|: $3.Value (list |while|: $5.Value $9.Value $7.Value))) ::
  PHRASE: For :: (_makeError "JS 124" $1.Charpos "Invalid for statement") ::
  PHRASE: Reg VAR(regVector setq:) :: $2 ::
  PHRASE: Reg :: (_makeError "JS 125" $1.Charpos "Invalid reg statement") ::
  PHRASE: Var VAR(varVector setq:) :: $2 ::
  PHRASE: Var :: (_makeError "JS 125" $1.Charpos "Invalid var statement") ::
  PHRASE: Pvar VAR(pvarVector define:) :: $2 ::
  PHRASE: Pvar :: (_makeError "JS 126" $1.Charpos "Invalid pvar statement") ::
  PHRASE: Cvar VAR(cvarVector setq:) :: $2 ::
  PHRASE: Cvar :: (_makeError "JS 127" $1.Charpos "Invalid cvar statement") ::
  PHRASE: Name CFCALL($1) :: $2 ::
  PHRASE: NAME Increment :: (postfix $0 $1 $2 true) ::
  PHRASE: NAME MathAssignmentOperator SEXPRESSION :: (assignMath $0 $2 $1 $3) :: 
  PHRASE: NAME AssignmentOperator SEXPRESSION :: (assignMe $0 $2 $1 $3) :: 
  PHRASE: NAME AssignmentOperator :: (_makeError "JS 128" $2.Charpos "Invalid assignment") ::
  PHRASE: Name LeftParen ARGLIST :: (argList $0 $1.Value $3.Value) ::
  PHRASE: NAME LeftParen ARGLIST :: (argList $0 $1.Value $3.Value) ::
  PHRASE: NAME LeftParen :: (_makeError "JS 129" $1.Charpos "Invalid function call") ::

  TERM: user ordering :: true ::
  TERM: Boolean :: (setq (setq $0.Value $1.Boolean)[VType:] bool:) ::
  TERM: + TERM :: (setVType $2) ::
  TERM: + Term :: (setVType $2) ::
  TERM: - Number :: (foldConstants $0 |-|: 0 $2.Value) ::
  TERM: - TERM :: (setq (setq $0.Value (list |-|: 0 $2.Value))[VType:] $2.VType) ::
  TERM: - Term :: (setq (setq $0.Value (list |-|: 0 $2.Value))[VType:] $2.VType) ::
  TERM: LeftParen Type RightParen SEXPRESSION :: (setq (setq (setq $4.VType $2.Value)[TERM:] true)[Charpos:] $0.Charpos) ::
  TERM: LeftParen SEXPRESSION RightParen :: $2 ::
  TERM: LeftParen :: (_makeError "JS 130" $1.Charpos "Invalid expression") ::
  TERM: Logical SEXPRESSION :: (setq (setq $0.Value (list $1.Lisp $2.Value))[VType:] bool:) ::
  TERM: Logical :: (_makeError "JS 131" $1.Charpos (append "Invalid " $1.Value " operator"))::
  TERM: Increment NAME :: (postfix $0 $2 $1 false) ::
  TERM: Increment :: (_makeError "JS 131" $1.Charpos (append "Invalid " $1.Value " operator")) ::
  TERM: Name CFCALL($1) :: $2 ::
  TERM: Name LeftParen ARGLIST :: (argList $0 $1.Value $3.Value) ::
  TERM: NAME LeftParen ARGLIST :: (argList $0 $1.Value $3.Value) ::
  TERM: NAME LeftParen :: (_makeError "JS 132" $1.Charpos "Invalid function call") ::
  TERM: Number Operator Number :: (foldConstants $0 $2.Lisp $1.Value $3.Value) ::
  TERM: NAME Increment :: (postfix $0 $1 $2 true) ::
  TERM: NAME :: $1 ::
  TERM: Term :: (setVType $1) ::

  VAR: user ordering :: true ::
  VAR: Type Name InitializeOperator SEXPRESSION Comma VAR(%0 %1)
         :: (addVar %0 $1.Value $2.Value (addToBeginList $0 (appendList %1 $2.Value (list $4.Value))) $6) ::
  VAR: Type Name InitializeOperator SEXPRESSION Comma
         :: (_makeError "JS 135" $5.Charpos "Invalid var statement") ::
  VAR: Type Name InitializeOperator SEXPRESSION 
         :: (addVar %0 $1.Value $2.Value (addToBeginList $0 (appendList %1 $2.Value (list $4.Value))) #void) ::
  VAR: Type Name Comma VAR(%0 %1) :: (addVar %0 $1.Value $2.Value $0 $4) ::
  VAR: Type Name Comma :: (_makeError "JS 136" $3.Charpos "Invalid var statement") ::
  VAR: Type Name :: (addVar %0 $1.Value $2.Value $0 #void) ::
  VAR: Name InitializeOperator SEXPRESSION Comma VAR(%0 %1)
         :: (addVar %0 obj: $1.Value (addToBeginList $0 (appendList %1 $1.Value (list $3.Value))) $5) ::
  VAR: Name InitializeOperator SEXPRESSION Comma
         :: (_makeError "JS 133" $4.Charpos "Invalid var statement") ::
  VAR: Name InitializeOperator SEXPRESSION 
         :: (addVar %0 obj: $1.Value (addToBeginList $0 (appendList %1 $1.Value (list $3.Value))) #void) ::
  VAR: Name Comma VAR(%0 %1) :: (addVar %0 obj: $1.Value $0 $3) ::
  VAR: Name Comma :: (_makeError "JS 134" $2.Charpos "Invalid var statement") ::
  VAR: Name Name :: (_makeError "JS 136" $1.Charpos "Invalid var type") ::
  VAR: Name :: (addVar %0 obj: $1.Value $0 #void) ::

  ARGLIST: RightParen :: (setq $0.Value #void) ::
  ARGLIST: SEXPRESSION RightParen :: (setq $0.Value (new Vector: 1 $1.Value)) ::
  ARGLIST: SEXPRESSION Comma ARGLIST :: (setq $0.Value (insert $3.Value 0 $1.Value)) ::
  ARGLIST: Value :: (_makeError "JS 137" $1.Charpos "Invalid argument list")::

  WEIGHTLIST: user ordering :: true ::
  WEIGHTLIST: RightParen :: $0 ::
  WEIGHTLIST: Pound LeftParen "num" Bar NUMLIST << (setq $0.Value (append $0.Value " #(num| " $5.Value)) >>
  WEIGHTLIST: Pound LeftParen "obj" Bar WEIGHTLIST << (setq $0.Value (append $0.Value " #(obj| " $5.Value " )")) >>
  WEIGHTLIST: Pound LeftParen WEIGHTLIST << (setq $0.Value (append $0.Value " #( " $3.Value " )")) >>
  WEIGHTLIST: Value :: (_makeError "JS 137" $1.Charpos "Invalid weight list")::

  NUMLIST: user ordering :: true ::
  NUMLIST: - Number RightParen :: (setq $0.Value (append $0.Value " -" $2.Value " )")) ::
  NUMLIST: Number RightParen :: (setq $0.Value (append $0.Value " " $1.Value " )")) ::
  NUMLIST: - Number NUMLIST :: (setq $0.Value (append $0.Value " -" $2.Value " " $3.Value)) ::
  NUMLIST: Number NUMLIST :: (setq $0.Value (append $0.Value " " $1.Value " " $2.Value)) ::
  NUMLIST: Value :: (_makeError "JS 137" $1.Charpos "Invalid weight list")::

  PARMLIST: RightParen :: (setq $0.Value #void) ::
  PARMLIST: Type Name RightParen 
               :: (addVar avarVector $1.Value $2.Value (setq $0.Value (new Vector: 1 $2.Value)) #void) ::
  PARMLIST: Type Name Comma PARMLIST 
               :: (addVar avarVector $1.Value $2.Value (setq $0.Value (insert $4.Value 0 $2.Value)) #void) ::
  PARMLIST: Name RightParen :: (addVar avarVector obj: $1.Value (setq $0.Value (new Vector: 1 $1.Value)) #void) ::
  PARMLIST: Name Comma PARMLIST :: (addVar avarVector obj: $1.Value (setq $0.Value (insert $3.Value 0 $1.Value)) #void) ::
  PARMLIST: Value :: (error (append "Invalid argument list [" $1.Charpos "] " (mid $IN $1.Charpos 120))) ::

  REFLIST: SEXPRESSION RightBracket :: (setq (setq $0.Value (new Vector: 1 $1.Value))[VType:] $1.VType) ::
  REFLIST: SEXPRESSION Comma REFLIST :: (setq $0.Value (insert $3.Value 0 $1.Value)) ::
  REFLIST: Value :: (_makeError "JS 138" $1.Charpos "Invalid index list") ::

  FIELDLIST: RightBrace :: (setq $0.Value (new Vector: 0)) ::
  FIELDLIST: Name RightBrace :: (setq $0.Value (new Vector: 1 $1.Value)) ::
  FIELDLIST: Name Semicolon RightBrace :: (setq $0.Value (new Vector: 1 $1.Value)) ::
  FIELDLIST: Name Semicolon FIELDLIST :: (setq $0.Value (insert $3.Value 0 $1.Value)) ::
  FIELDLIST: Value :: (_makeError "JS 139" $1.Charpos "Invalid field list") ::

#End# 

#SemanticPasses#

  MAIN true

#End#

#SemanticRules#

  MAIN| user ordering :: true ::
  MAIN| [ Any ] ] :: $2 ::  
  MAIN| [ Any* ] ] :: (objectToList $2) ::  
  MAIN| "begin" Any ] :: $2 ::
  
  ;; ---------------------------------------------------------------
  ;; merge filter statement All commands with field check at beginning
  ;;                        of sort command to reduce table passes.
  ;; ---------------------------------------------------------------
  MAIN| "begin" TRUNCATE [ "begin" TRUNCATE $SORT $TRUNCATE ] ] 
                                    :: (list (symbol "begin")
                                          (list 
                                             '(ref XT truncate:) 
                                             (list lambda: '(xv) '(onError (lambda(s) false)) (list and: $2 $5)))
                                          $6 $7) 
                                    ::  
  MAIN| "begin" LETTRUNCATE [ "begin" LETTRUNCATE $SORT $TRUNCATE ] ] 
                                    :: (list (symbol "begin")
                                          (list 
                                             '(ref XT truncate:) 
                                             (list (symbol "let") (ref $2 0)
                                                (list lambda: '(xv) '(onError (lambda(s) false)) 
                                                                     (list and: (ref $2 1) (ref $5 1))))) 
                                          $6 $7) 
                                    ::  

  ;; ---------------------------------------------------------------
  ;; merge filter statement All commands to reduce table passes.
  ;; ---------------------------------------------------------------
  MAIN| "begin" TRUNCATE TRUNCATE ] :: (list 
                                          '(ref XT truncate:) 
                                          (list lambda: '(xv) '(onError (lambda(s) false)) (list and: $2 $3))) 
                                    ::  
  MAIN| "begin" LETTRUNCATE LETTRUNCATE ] 
                                    :: (list 
                                          '(ref XT truncate:)
                                          (list (symbol "let") (ref $2 0)
                                             (list lambda: '(xv) '(onError (lambda(s) false)) 
                                                                  (list and: (ref $2 1) (ref $3 1))))) 
                                    ::  
  ;; ---------------------------------------------------------------
  ;; merge filter statement Omit commands to reduce table passes.
  ;; ---------------------------------------------------------------
  MAIN| "begin" OMIT OMIT ]         :: (list 
                                          '(ref XT omit) 
                                          (list lambda: '(xv) '(onError (lambda(s) false)) (list or: $2 $3))) 
                                    ::  
  MAIN| "begin" LETOMIT LETOMIT ] 
                                    :: (list 
                                          '(ref XT omit)
                                          (list (symbol "let") (ref $2 0)
                                             (list lambda: '(xv) '(onError (lambda(s) false)) 
                                                                  (list or: (ref $2 1) (ref $3 1))))) 
                                    ::  

  TRUNCATE| [ REFTRUNCATE LAMBDA ] :: $3 ::  

  LETTRUNCATE| [ REFTRUNCATE LET ] :: $3 ::  

  LETOMIT| [ REFOMIT LET ] :: $3 ::  

  REFTRUNCATE| [ "ref" "XT" "truncate" ] :: (list $2 $3 $4) ::  

  OMIT| [ REFOMIT LAMBDA ] :: $3 ::  

  REFOMIT| [ "ref" "XT" "omit" ] :: (list $2 $3 $4) ::  

  LAMBDA| [ "lambda" [ "xv" ] $E $P ]:: $7 ::  

  LET| [ "let" $V LAMBDA ] :: (new Vector: 2 $3 $4) ::  

#End#


#DelimitedStrings#
  String: {"} {"}
  Symbol: {'} {'}
  Whitespace: {/*} {*/}
  Whitespace2: {//} _eol
#End#


#SyntaxFeatures#

  Operator: [+ - * / % "&&" "||" "#"]
  Lisp:     [+ - * / % "&&" "||" "#"] [+ - * pdiv pmod and or pdiv]
  Charop:   [+ - * / % "&&" "||" ] [cadd csub cmul pdiv pmod and or]
  Chartyp:  [+ - * / % "&&" "||" ] [char char char char char bool bool]
  Boolop:   [+ - * / % "&&" "||" ] [badd bsub bmul pdiv pmod and or]
  Booltyp:  [+ - * / % "&&" "||" ] [bool bool bool bool bool bool bool]
  Intop:    [+ - * / % "&&" "||" ] [iadd isub imul pdiv pmod and or]
  Inttyp:   [+ - * / % "&&" "||" ] [int int int int int bool bool]
  Floatop:  [+ - * / % "&&" "||" ] [nadd nsub nmul pdiv pmod and or]
  Floattyp: [+ - * / % "&&" "||" ] [float float float float float bool bool]
  Operator: [== < <= > >= !=]
  Lisp:     [== < <= > >= !=] [= < <= > >= <>]
  Boolop:   [== < <= > >= !=] [bcompareEQ bcompareLT bcompareLE bcompareGT bcompareGE bcompareNE]
  Booltyp:  [== < <= > >= !=] [bool bool bool bool bool bool bool]
  Charop:   [== < <= > >= !=] [ccompareEQ ccompareLT ccompareLE ccompareGT ccompareGE ccompareNE]
  Chartyp:  [== < <= > >= !=] [bool bool bool bool bool bool bool]
  Intop:    [== < <= > >= !=] [icompareEQ icompareLT icompareLE icompareGT icompareGE icompareNE]
  Inttyp:   [== < <= > >= !=] [bool bool bool bool bool bool bool]
  Floatop:  [== < <= > >= !=] [ncompareEQ ncompareLT ncompareLE ncompareGT ncompareGE ncompareNE]
  Floattyp: [== < <= > >= !=] [bool bool bool bool bool bool bool]
  MathAssignmentOperator: [+= -= /= *= "%="]
  Lisp:     [+= -= /= *= %=] [+ - pdiv * pmod]
  Charop:   [+= -= /= *= %=] [cadd csub cmul pdiv pmod]
  Chartyp:  [+= -= /= *= %=] [char char char char char]
  Boolop:   [+= -= /= *= %=] [badd bsub bmul pdiv pmod]
  Booltyp:  [+= -= /= *= %=] [bool bool bool bool bool]
  Intop:    [+= -= /= *= %=] [iadd isub imul pdiv pmod]
  Inttyp:   [+= -= /= *= %=] [int int int int int int]
  Floatop:  [+= -= /= *= %=] [nadd nsub nmul pdiv pmod]
  Floattyp: [+= -= /= *= %=] [float float float float float]
  AssignmentOperator: [=]
  Lisp:               [=] [setq]
  InitializeOperator: [=]
  RelationOperator: [== < <= > >= !=]
  Boolean: [true false] [true false]
  Term: [true false]
  Increment: [++ --]
  Lisp:     [++ --] [+ -]
  Boolop:   [++ --] [badd bsub]
  Booltyp:  [++ --] [bool bool]
  Charop:   [++ --] [cadd csub]
  Chartyp:  [++ --] [char char]
  Intop:    [++ --] [iadd isub]
  Inttyp:   [++ --] [int int]
  Floatop:  [++ --] [nadd nsub]
  Floattyp: [++ --] [float float]
  Logical: [!]
  Lisp: [!] [not]
  For: [for]
  Function: [function]
  Friend: [friend]
  Class: [class]
  Extends: [extends]
  Child: [child]
  Orphan: [orphan]
  Method: [method]
  If: [if]
  Name: [int float char bool obj text symbol string stc dir dic vec bitvec bytvec numvec intvec fltvec objvec pcdvec matrix nummat]
  Of: [of]
  Type: [int float char bool obj text symbol string stc dir dic vec bitvec bytvec numvec intvec fltvec objvec pcdvec matrix nummat]
  Reftyp: [int float char bool obj text symbol string stc dir dic vec bitvec bytvec numvec intvec fltvec objvec pcdvec matrix nummat] [int float char bool obj char char char obj obj obj obj int char float int float obj int obj float]
  Refop: [int float char bool obj text symbol string stc dir dic vec bitvec bytvec numvec intvec fltvec objvec pcdvec matrix nummat] [ref ref ref ref ref reftext refSymbol refString ref ref ref refVector refBitVector refBytVector refNumVector refIntVector refFltVector refObjVector refPcdVector ref ref]
  Setop: [int float char bool obj text symbol string stc dir dic vec bitvec bytvec numvec intvec fltvec objvec pcdvec matrix nummat] [setq setq setq setq setq setq setq setString setq setq setq setVector setBitVector setBytVector setNumVector setIntVector setFltVector setObjVector setPcdVector setq setq]
  While: [while]
  Else: [else]
  Reg: [reg]
  Var: [var]
  Pvar: [pvar]
  Cvar: [cvar]
  Semicolon: [";"]
  Colon: [":"]
  Question: ["?"]
  LeftParen: ["("]
  RightParen: [")"]
  LeftBrace: ["{"]
  RightBrace: ["}"]
  LeftBracket: ["["]
  RightBracket: ["]"]
  Comma: [","]
  DotOperator: ["."]
  Pound: [#]
  Bar: ["|"]

  Reserved: [if then else while do reg var pvar cvar for function orphan friend child class method extends ]

  ;; ********************************************************************
  ;; Start tokens for compiling Selector extensions.
  ;; ********************************************************************

  Select: [select]
  Percent: [%]
  Sort: [sort]
  Backup: [backup]
  Direction: [up down]
  All: [all]
  Scale: [scale]
  Regress: [regress]
  Bgmregress: [bgmregress]
  Frmregress: [frmregress]
  Mvlregress: [mvlregress]
  Ennregress: [ennregress]
  Svmregress: [svmregress]
  Highest: [highest]
  Extract: [extract]
  Omit: [omit]
  Check: [check]
  Checkoff: [checkoff nocheck]
  Checkon: [checkon]
  Cut: [bottom top]
  Slice: [slice]
  Lisp: [slice bottom up top down highest all] [<= <= <= >= >= >= true]

  Score: [score]
  ScoreCommand: [average averageForAll total totalForAll maximum minimum deviation sharpe]

  Set: [Set set]
  Setnr: [Setnr setnr]

  Run: [run]
  Restore: [restore]
  
  Reserved: [all average averageForAll backup bottom check checkoff checkon down filter]
  Reserved: [nocheck omit restore run score set sort up top total totalForAll]

  ;; ********************************************************************
  ;; End tokens for compiling Selector extensions.
  ;; ********************************************************************


#End#








































;;**EXPORTKEY**:esm:selector:@@defaultLexer
(defriend esm.selector defaultLexer(inString)
;; ********************************************************************
;; summary:  This Lambda converts an input string into a vector of
;;           recognized lexemes. It is the default lexical analyzer
;;           for the ParseLib compiler generator.
;;           This Lambda may be modified, in any way, by the user.
;; Parms:    inString   The source string to be broken into lexemes.
;; return:   tokenList  The vector of recognized lexemes.
;; ********************************************************************
    pvars:(;; Persistent variables
           CH                  ;; The current input character from the input string
           INLEN               ;; The length of the input string
           IP                  ;; The input pointer for the input string
           INSTRING            ;; The string of the input characters to be parsed
           keepWhitespaceSW    ;; Switch to keep all whitespace strings
           lowerCaseSW         ;; Switch to convert all names into lower case
           oldKB               ;; The old vector of character break parsing routines
           operatorList        ;; The vector of operator symbols
           KB                  ;; The vector of character break parsing routines
           SB                  ;; The vector of string terminator pairs
           tokenDirectory      ;; Lexicon of tokens and their attributes
           tokenList           ;; The vector of lexical tokens
           TP                  ;; The output pointer for the token output vector
           ;; Methods list
           addStringDelimiters ;; Add a pair of string delimiters to the lexical analyzer
           _default            ;; Recognize this one character
           defaultTokenRule    ;; Modified default rule for adding attributes to a parsed token
           _Ignore             ;; Ignore this character parsing routine
           _Initialize         ;; Initialize the vector of character break parsing routines
           _recFraction        ;; Recognize all fractions
           _recInteger         ;; Recognize all integers
           _recName            ;; Recognize all names
           _recNumber          ;; Recognize all numbers
           _recOperators       ;; Recognize all operator symbols
           _recSpecial         ;; Recognize all special symbols
           _recString          ;; Recognize all delimited strings
           _setFeatures        ;; Give features to a recognized token
           _whiteSpace         ;; Ignore all whitespace characters
           turnFractionsOnOff  ;; Turns fraction recognition on/off
           ) ;; end of persistent variables
    vars:(token oldIP)
    ;;************************************************************************
    ;;  Define the child Lambdas for this parent.
    ;;************************************************************************
    ;; Add a named pair of string delimiters to the lexical analyzer.
    (defun addStringDelimiters(name start end)
       vars:(tmpLambda)
       ;;  Initialize the ParseLib once and only once.
       (if (= KB #void) (_Initialize))
       ;;  If this is the first delimiter pair, start a new directory.
       (setq CH start[0])
       (if (= SB[CH] #void) 
           (begin
              (setq SB[CH] (new Structure:))
              (setq KB[CH] _recString)
           )) ;; end if
       ;;  Set the character directory with this new string delimiter pair.
       (setq SB[CH][name] (new Vector: 2 start end))
       ) ;; end addStringDelimiters
    ;;  Ignore this character parsing routine.
    (defun _Ignore() (++ IP))
    ;;  Create the character break vector.
    (defun _Initialize()
        vars:(i)
        (setq KB (new Vector: 256))
        (setq SB (new Vector: 256))
        (setq operatorList #(#\= #\< #\> #\! #\^ #\~ #\+ #\/ #\* #\- #\| #\&))
        ;; Actual mapping of parse routines to break character positions.
        (loop for i from 0 until 256 do (setq KB[i] _recSpecial))
        (loop for i from 0 to 32 do (setq KB[i] _whiteSpace)) 
        (loop for i from 128 until 256 do (setq KB[i] _whiteSpace)) 
        (loop for i from (code #\a) to (code #\z) do (setq KB[i] _recName)) 
        (loop for i from (code #\A) to (code #\Z) do (setq KB[i] _recName)) 
        (loop for i from (code #\0) to (code #\9) do (setq KB[i] _recNumber)) 
        (loop for i from 0 until (length operatorList) do (setq KB[operatorList[i]] _recOperators)) 
        (setq KB[(code #\_)] _recName) 
        (setq KB[(code #\.)] _recFraction) 
        (setq oldKB (copy KB))
        ) ;; end of _Initialize
    ;;  Recognize all fractions.
    (defun _recFraction()
        vars:(oldIP result)
        (setq oldIP IP)
        (setq CH INSTRING[(++ IP)])
        ;; Recognize fraction portion of number (if any)
        (if (isCharNumeric CH)
            then
            (begin
               (setq CH INSTRING[(++ IP)])
               ;; Recognize fraction portion of number
               (while (isCharNumeric CH) do
                  (setq CH INSTRING[(++ IP)]) 
                  ) ;; end while
               (setq result (number (substring INSTRING oldIP (subi IP 1))))
               ) ; end then
            else
            (setq result (symbol ".")) 
            ) ; end recognize fraction.
        (setq tokenList[TP] result)
        (++ TP)
        ) ;; end _recFraction
    ;;  Recognize all names.
    (defun _recName()
        vars:(oldIP)
        (setq oldIP IP)
        (setq CH INSTRING[(++ IP)]) 
        (while (isCharName CH) do
           (setq CH INSTRING[(++ IP)]) 
           ) ;; end while
        (if lowerCaseSW
            (setq tokenList[TP] (symbol (downcase (substring INSTRING oldIP (subi IP 1)))))
            (setq tokenList[TP] (symbol (substring INSTRING oldIP (subi IP 1))))
            ) ; end if
        (++ TP)
        ) ;; end _recName
    ;;  Recognize all numbers.
    (defun _recNumber()
        vars:(oldIP num fraction)
        (setq oldIP IP)
        (setq CH INSTRING[(++ IP)])
        ;; Recognize integer portion of number
        (while (isCharNumeric CH) do
           (setq CH INSTRING[(++ IP)]) 
           ) ;; end while
        ;; Recognize fraction portion of number (if any)
        (if (and (= CH #\.) (isCharNumeric INSTRING[(add1 IP)]))
            (begin
               (setq fraction true)
               (setq CH INSTRING[(++ IP)])
               ;; Recognize fraction portion of number
               (while (isCharNumeric CH) do
                  (setq CH INSTRING[(++ IP)]) 
                  ) ;; end while
            )) ; end recognize fraction.
        (setq num (number (substring INSTRING oldIP (subi IP 1))))
        (if (= (integer num) num) (setq num (integer num)))
        (if (= fraction true) (setq num (number num)))
        (setq tokenList[TP] num)
        (++ TP)
        ) ;; end _recNumber
    ;;  Recognize all integers.
    (defun _recInteger()
        vars:(oldIP num)
        (setq oldIP IP)
        (setq CH INSTRING[(++ IP)])
        ;; Recognize integer portion of number
        (while (isCharNumeric CH) do
           (setq CH INSTRING[(++ IP)]) 
           ) ;; end while
        (setq num (number (substring INSTRING oldIP (subi IP 1))))
        (if (= (integer num) num) (setq num (integer num)))
        (setq tokenList[TP] num)
        (++ TP)
        ) ;; end _recInteger
    ;;  Recognize all operator symbols.
    (defun _recOperators()
        vars:(oldIP)
        (setq oldIP IP)
        (setq CH INSTRING[(++ IP)]) 
        (while (isMember CH operatorList) do
           (setq CH INSTRING[(++ IP)]) 
           ) ;; end while
        (setq tokenList[TP] (symbol (substring INSTRING oldIP (subi IP 1))))
        (++ TP)
        ) ;; end _recOperators
    ;; Recognize all special symbols.
    (defun _recSpecial() (setq tokenList[TP] (symbol (string CH))) (++ IP) (++ TP))
    ;; Recognize all delimited strings.
    (defun _recString()
        vars:(oldIP i delimPairs delimLen result              
              name this start end startLen endLen)
        (setq oldIP IP)
        ;; Check for a starting string delimiter.
        (setq delimPairs SB[CH])
        (setq delimLen (length delimPairs))
        (loop for i from 0 until delimLen do
           (setq name delimPairs[i 0])
           (setq start delimPairs[i 1][0])
           (setq startLen (length start))
           (setq this (mid INSTRING IP startLen))
           (if (= start this)
               (begin
                  (setq end delimPairs[i 1][1])
                  (setq endLen (length end))
                  (+= IP startLen)
                  (while (< IP INLEN) do
                     (if (= INSTRING[IP] end[0])
                         (begin
                            (setq this (mid INSTRING IP endLen))
                            (if (= end this)
                                (begin
                                   (+= IP endLen)
                                   (setq result (substring INSTRING oldIP (subi IP 1)))
                                   ;; Ignore all whitespace delimited strings
                                   (if (<> (left name 10) "Whitespace")
                                       (begin
                                          (setq tokenList[TP] (new Vector: 2 name result))
                                          (++ TP)
                                          )) ; end  if
                                   (return TP)              
                                   )) ; end inner if
                            )) ; end outter if
                     (++ IP)
                     ) ; end while
                  (setq result (substring INSTRING oldIP (subi IP 1)))
                  ;; Ignore all whitespace delimited strings
                  (if (or keepWhitespaceSW (<> (left name 10) "Whitespace"))
                      (begin
                         (setq tokenList[TP] (new Vector: 2 name result))
                         (++ TP)
                         )) ; end  if
                  (return TP)              
                  )) ; end if 
           ) ;; end loop
        ;; If we get here, this is not the start of a delimited string,
        ;; so invoke the old lexeme parser for this character.
        (oldKB[CH])) ;; end _recString
     ;; Give features to a recognized token
     (defun _setFeatures(token oldIP)
        vars:(parseTree treeIndex treeLen tokenAttr)
        ;; This Lambda tests compiled FSM style methods of attributing each parsed token.
        (setq tokenAttr tokenDirectory[token])
        (if (= tokenAttr #void)
            then
            ;; Create an attributed token using default rule
            (setq tokenAttr (defaultTokenRule token))
            else
            ;; Copy the attributes and set the Value from the dictionary
            (begin
               (setq tokenAttr (copy tokenAttr))
               (setq tokenAttr.Value token)
            )) ; end if
        ;; Set the displacement of the token in the source string
        (setq tokenAttr.Charpos (integer oldIP))
        (setq tokenList[(subi TP 1)] tokenAttr)
        true) ;; end of _setFeatures
    ;; Turns fraction recognition on/off.
    (defun turnFractionsOnOff(onOffSW)
        vars:(i)
        ;; Turn fractions on?
        (if onOffSW
            ;; Turn fractions on
            (begin
               (loop for i from (code #\0) to (code #\9) do
                  (if (= KB[i] oldKB[i]) (setq KB[i] _recNumber)) 
                  (setq oldKB[i] _recNumber)
                  ) ; end loop
               (if (= KB[(code #\.)] oldKB[(code #\.)]) (setq KB[(code #\.)] _recFraction))
               (setq oldKB[(code #\.)] _recFraction)
               ) ; end turn fractions on
            ;; Turn fractions off
            (begin
               (loop for i from (code #\0) to (code #\9) do
                  (if (= KB[i] oldKB[i]) (setq KB[i] _recInteger)) 
                  (setq oldKB[i] _recInteger)
                  ) ; end loop
               (if (= KB[(code #\.)] oldKB[(code #\.)]) (setq KB[(code #\.)] _recSpecial))
               (setq oldKB[(code #\.)] _recSpecial)
               ) ; end turn fractions off
            ) ; end if
        ) ;; end of turnFractionsOnOff
    ;;  Ignore all whitespace characters.
    (defun _whiteSpace()        
        vars:(oldIP i result)
        ;; Save old IP address
        (setq oldIP IP)
        ;; Loop until all whitespace chars are discovered
        (setq CH INSTRING[(++ IP)]) 
        (while (and (> CH 0) (<= CH 32)) do
           (setq CH INSTRING[(++ IP)]) 
           ) ;; end while
        ;; Return whitespace token (iff keepWhitespaceSW is true)
        (if keepWhitespaceSW
            (begin
               (setq result (substring INSTRING oldIP (subi IP 1)))
               (setq tokenList[TP] (new Vector: 2 "Whitespace" result))
               (++ TP)              
               )) ; end if
        ) ;; end _whiteSpace
    ;;************************************************************************
    ;;  Define the main code routines for this parent.
    ;;************************************************************************
    ;;  Initialize the ParseLib once and only once.
    (if (= KB #void) (_Initialize))
    ;;  Initialize the output token vector. 
    (setq tokenList (new Vector: 0))
    (setq TP 0)
    ;;  Recognize each character in the input string.
    (setq INSTRING inString)
    (setq INLEN (length INSTRING))
    (setq IP 0)
    (while (< IP INLEN) do
        (setq oldIP IP)
        ;; Retrieve the next input character
        (setq CH INSTRING[IP])
        ;; Invoke the parse routine for this input character
        (KB[CH])
        ;; If a token was recognized, give it some features
        (if (and (> TP 0) (not (isStructure (setq token tokenList[(subi TP 1)])))) (_setFeatures token oldIP))
        ) ;; end while
    ;;  Return the token list as the output
    tokenList) ;; end defaultLexer
























;;**EXPORTKEY**:esm:selector:@UserDefinedFunctions
;; ************************************************
;; esm.selector User defined functions
;; ************************************************















;;**EXPORTKEY**:esm:selector:_LEXRULE_MAIN
;; ************************************************
;; MAIN user defined Lexical Rule implementation
;; Summary: This Lambda implements the MAIN
;;          user defined rule. Each rule test is
;;          marked with a boxed comment line for
;;          ease of human understanding.
;; Note: This code was machine generated by ParseLib.
;; ************************************************
(defchild esm.selector _LEXRULE_MAIN(...)
   ;; The token object _io is a persistent variable
   ;; The token pointer _ip is a persistent variable
   ;; All getToken logic increments _ip on each token fetch.
   ;; The return value from the rule is stored in _ret.
   ;; Remember that this rule may repeat depending on _repeatSW.
   vars:(_tk0 _tk1 _tk2 _tk3 _tk4  _tk5  _i
         _tk6 _tk7 _tk8 _tk9 _tkn  _repeatSW
         _ak0 _ak1 _ak2 _ak3 _ak4  _ak5  _tkthis
         _ak6 _ak7 _ak8 _ak9 _ret  _oldIp
         _ip0 _ip1 _ip2 _ip3 _ip4  _ip5
         _ip6 _ip7 _ip8 _ip9 _tkn  _tkch
         _verboseSave       
        ) ; end temporary variables
   ;; Collect any arguments which may have been passed to this rule
   (if (> (setq _tkn (argCount)) 0)
       (begin (setq _ak0 (argFetch 0))
          (if (> _tkn 1) 
              (begin (setq _ak1 (argFetch 1))
                 (if (> _tkn 2) 
                     (begin (setq _ak2 (argFetch 2))
                        (if (> _tkn 3) 
                            (begin (setq _ak3 (argFetch 3))
                               (if (> _tkn 4) 
                                   (begin (setq _ak4 (argFetch 4))
                                      (if (> _tkn 5) 
                                          (begin (setq _ak5 (argFetch 5))
                                             (if (> _tkn 6) 
                                                 (begin (setq _ak6 (argFetch 6))
                                                    (if (> _tkn 7) 
                                                        (begin (setq _ak7 (argFetch 7))
                                                           (if (> _tkn 8) 
                                                               (begin (setq _ak8 (argFetch 8))
                                                               (if (> _tkn 9) (setq _ak9 (argFetch 9))
                                                           ))))))))))))))))))) ; end argument collection
   ;; The default structure is given the named rule attribute
   (setq _tk0 "")
   (setq _tkn 0)
   (setq _repeatSW true)
   (setq _oldIp _ip)

   (if (= _verboseLexCount.MAIN #void) 
          (setq _verboseLexCount.MAIN 1) 
          (setq _verboseLexCount.MAIN (iadd _verboseLexCount.MAIN 1)))

   (if (and (<> _verboseLexIn.MAIN #void) (> _verboseLexIn.MAIN -1))
       (begin (setq _verboseSave _verbose) (setq _verbose true))) 

   ;; Save the old token pointer and test for user defined rules
   Skip::
     (setq _indent (iadd _indent 1))
     (if _verbose (_logLine "Attempting MAIN Rule on: " input:))
     (setq _ip0 _ip)
     (setq _tkch (iadd _ip 1))
     (++ _tkn)
     (setq _repeatSW false)

     ;; *****************************************
     ;; Begin testing for each user defined rule.
     ;; *****************************************


















     ;; ====================
     ;; case: "void"
     ;; ====================
     (if (if (= (setq _tk1 (mid $IN (iadd _ip 1) (begin (setq _ip (iadd _ip 4)) 4))) "void") true (setq _ip _ip0))
       (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip)
         ;; *********************************************************
         ;; RULE: MAIN: "void" << ($ASIS $ch #void vtyp: obj: Term: true Constant: true) >>
         ;; *********************************************************
         (if true
          (begin
            (setq _ret  (_tkASIS _tkch #void vtyp: obj: Term: true Constant: true) )
            (if _verbose
                (writeRule
                     {MAIN: "void" << ($ASIS $ch #void vtyp: obj: Term: true Constant: true) >>}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
(setq _indent (isub _indent 1))
            (goto Skip:))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : "void"
     ;; ====================
     ;; case: "nil"
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (= (setq _tk1 (mid $IN (iadd _ip 1) (begin (setq _ip (iadd _ip 3)) 3))) "nil") true (setq _ip _ip0)))
       (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip)
         ;; *********************************************************
         ;; RULE: MAIN: "nil" << ($ASIS $ch #void vtyp: obj: Term: true Constant: true) >>
         ;; *********************************************************
         (if true
          (begin
            (setq _ret  (_tkASIS _tkch #void vtyp: obj: Term: true Constant: true) )
            (if _verbose
                (writeRule
                     {MAIN: "nil" << ($ASIS $ch #void vtyp: obj: Term: true Constant: true) >>}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
(setq _indent (isub _indent 1))
            (goto Skip:))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : "nil"
     ;; ====================
     ;; case: DQuote
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (and (<> (setq _tk1 $IN[(setq _ip (iadd _ip 1))]) #void) (= _LF_DQuote[_tk1] 1)) true (setq _ip _ip0)))
       (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip)
         ;; ====================
         ;; case: NotDQuote*
         ;; ====================
         (if (begin (setq _i 0) (setq _tk2 (makeString {})) (while (and (<> (setq _tkthis $IN[(setq _ip (iadd _ip 1))]) #void) (= _LF_NotDQuote[_tkthis] 1)) do (begin (setq _tk2[_i] _tkthis) (setq _i (iadd _i 1)) )) (setq _ip (isub _ip 1)) true)
           (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip)
             ;; ====================
             ;; case: DQuote
             ;; ====================
             (if (if (and (<> (setq _tk3 $IN[(setq _ip (iadd _ip 1))]) #void) (= _LF_DQuote[_tk3] 1)) true (setq _ip _ip2))
               (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip)
                 ;; *********************************************************
                 ;; RULE: MAIN: DQuote NotDQuote* DQuote << ($ASIS $ch $2 String: true Term: true Constant: true) >>
                 ;; *********************************************************
                 (if true
                  (begin
                    (setq _ret  (_tkASIS _tkch _tk2 String: true Term: true Constant: true) )
                    (if _verbose
                        (writeRule
                             {MAIN: DQuote NotDQuote* DQuote << ($ASIS $ch $2 String: true Term: true Constant: true) >>}
                             _ret  _tk0 _tk1 _tk2 _tk3 #void #void #void #void #void #void #void))
(setq _indent (isub _indent 1))
                    (goto Skip:))
                 ) ; end case : _default
               ) ; end begin
             ) ; end case : DQuote
           ) ; end begin
         ) ; end case : NotDQuote*
       ) ; end begin
     ) ; end case : DQuote
     ;; ====================
     ;; case: Quote
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (and (<> (setq _tk1 $IN[(setq _ip (iadd _ip 1))]) #void) (= _LF_Quote[_tk1] 1)) true (setq _ip _ip0)))
       (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip)
         ;; ====================
         ;; case: NotQuote*
         ;; ====================
         (if (begin (setq _i 0) (setq _tk2 (makeString {})) (while (and (<> (setq _tkthis $IN[(setq _ip (iadd _ip 1))]) #void) (= _LF_NotQuote[_tkthis] 1)) do (begin (setq _tk2[_i] _tkthis) (setq _i (iadd _i 1)) )) (setq _ip (isub _ip 1)) true)
           (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip)
             ;; ====================
             ;; case: Quote
             ;; ====================
             (if (if (and (<> (setq _tk3 $IN[(setq _ip (iadd _ip 1))]) #void) (= _LF_Quote[_tk3] 1)) true (setq _ip _ip2))
               (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip)
                 ;; *********************************************************
                 ;; RULE: MAIN: Quote NotQuote* Quote << ($ASIS $ch (makeQuotedSymbol $2) Symbol: true Term: true Constant: true) >>
                 ;; *********************************************************
                 (if true
                  (begin
                    (setq _ret  (_tkASIS _tkch (makeQuotedSymbol _tk2) Symbol: true Term: true Constant: true) )
                    (if _verbose
                        (writeRule
                             {MAIN: Quote NotQuote* Quote << ($ASIS $ch (makeQuotedSymbol $2) Symbol: true Term: true Constant: true) >>}
                             _ret  _tk0 _tk1 _tk2 _tk3 #void #void #void #void #void #void #void))
(setq _indent (isub _indent 1))
                    (goto Skip:))
                 ) ; end case : _default
               ) ; end begin
             ) ; end case : Quote
           ) ; end begin
         ) ; end case : NotQuote*
       ) ; end begin
     ) ; end case : Quote
     ;; ====================
     ;; case: NameStart
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (and (<> (setq _tk1 $IN[(setq _ip (iadd _ip 1))]) #void) (= _LF_NameStart[_tk1] 1)) true (setq _ip _ip0)))
       (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip)
         ;; ====================
         ;; case: NameChar*
         ;; ====================
         (if (begin (setq _i 0) (setq _tk2 (makeString {})) (while (and (<> (setq _tkthis $IN[(setq _ip (iadd _ip 1))]) #void) (= _LF_NameChar[_tkthis] 1)) do (begin (setq _tk2[_i] _tkthis) (setq _i (iadd _i 1)) )) (setq _ip (isub _ip 1)) true)
           (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip)
             ;; *********************************************************
             ;; RULE: MAIN: NameStart NameChar* << ($OUT $ch (symbol (append $1 $2)) Name: true Term: true) >>
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  (_tkOUT _tkch (symbol (append _tk1 _tk2)) Name: true Term: true) )
                (if _verbose
                    (writeRule
                         {MAIN: NameStart NameChar* << ($OUT $ch (symbol (append $1 $2)) Name: true Term: true) >>}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
(setq _indent (isub _indent 1))
                (goto Skip:))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : NameChar*
       ) ; end begin
     ) ; end case : NameStart
     ;; ====================
     ;; case: Digit+
     ;; ====================
     (if (begin (setq _ip _ip0)
      (begin (setq _i 0) (setq _tk1 (makeString {})) (while (and (<> (setq _tkthis $IN[(setq _ip (iadd _ip 1))]) #void) (= _LF_Digit[_tkthis] 1)) do (begin (setq _tk1[_i] _tkthis) (setq _i (iadd _i 1)) )) (setq _ip (isub _ip 1)) (if (> _i 0) true (setq _ip _ip0))))
       (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip)
         ;; ====================
         ;; case: Period
         ;; ====================
         (if (if (and (<> (setq _tk2 $IN[(setq _ip (iadd _ip 1))]) #void) (= _LF_Period[_tk2] 1)) true (setq _ip _ip1))
           (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip)
             ;; ====================
             ;; case: Digit*
             ;; ====================
             (if (begin (setq _i 0) (setq _tk3 (makeString {})) (while (and (<> (setq _tkthis $IN[(setq _ip (iadd _ip 1))]) #void) (= _LF_Digit[_tkthis] 1)) do (begin (setq _tk3[_i] _tkthis) (setq _i (iadd _i 1)) )) (setq _ip (isub _ip 1)) true)
               (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip)
                 ;; ====================
                 ;; case: Exponent
                 ;; ====================
                 (if (if (and (<> (setq _tk4 $IN[(setq _ip (iadd _ip 1))]) #void) (= _LF_Exponent[_tk4] 1)) true (setq _ip _ip3))
                   (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 _ip)
                     ;; ====================
                     ;; case: Sign
                     ;; ====================
                     (if (if (and (<> (setq _tk5 $IN[(setq _ip (iadd _ip 1))]) #void) (= _LF_Sign[_tk5] 1)) true (setq _ip _ip4))
                       (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip5 _ip)
                         ;; ====================
                         ;; case: Digit+
                         ;; ====================
                         (if (begin (setq _i 0) (setq _tk6 (makeString {})) (while (and (<> (setq _tkthis $IN[(setq _ip (iadd _ip 1))]) #void) (= _LF_Digit[_tkthis] 1)) do (begin (setq _tk6[_i] _tkthis) (setq _i (iadd _i 1)) )) (setq _ip (isub _ip 1)) (if (> _i 0) true (setq _ip _ip5)))
                           (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip6 _ip)
                             ;; *********************************************************
                             ;; RULE: MAIN: Digit+ Period Digit* Exponent Sign Digit+ << ($ASIS $ch (number (append $1 $2 $3 $4 $5 $6)) Number: true Term: true) >>
                             ;; *********************************************************
                             (if true
                              (begin
                                (setq _ret  (_tkASIS _tkch (number (append _tk1 _tk2 _tk3 _tk4 _tk5 _tk6)) Number: true Term: true) )
                                (if _verbose
                                    (writeRule
                                         {MAIN: Digit+ Period Digit* Exponent Sign Digit+ << ($ASIS $ch (number (append $1 $2 $3 $4 $5 $6)) Number: true Term: true) >>}
                                         _ret  _tk0 _tk1 _tk2 _tk3 _tk4 _tk5 _tk6 #void #void #void #void))
(setq _indent (isub _indent 1))
                                (goto Skip:))
                             ) ; end case : _default
                           ) ; end begin
                         ) ; end case : Digit+
                       ) ; end begin
                     ) ; end case : Sign
                   ) ; end begin
                 ) ; end case : Exponent
                 ;; *********************************************************
                 ;; RULE: MAIN: Digit+ Period Digit* << ($ASIS $ch (number (append $1 $2 $3)) Number: true Term: true) >>
                 ;; *********************************************************
                 (if true
                  (begin
                    (setq _ip _ip3)
                    (setq _ret  (_tkASIS _tkch (number (append _tk1 _tk2 _tk3)) Number: true Term: true) )
                    (if _verbose
                        (writeRule
                             {MAIN: Digit+ Period Digit* << ($ASIS $ch (number (append $1 $2 $3)) Number: true Term: true) >>}
                             _ret  _tk0 _tk1 _tk2 _tk3 #void #void #void #void #void #void #void))
(setq _indent (isub _indent 1))
                    (goto Skip:))
                 ) ; end case : _default
               ) ; end begin
             ) ; end case : Digit*
           ) ; end begin
         ) ; end case : Period
         ;; *********************************************************
         ;; RULE: MAIN: Digit+ << ($ASIS $ch (integer $1) Number: true Integer: true Term: true) >>
         ;; *********************************************************
         (if true
          (begin
            (setq _ip _ip1)
            (setq _ret  (_tkASIS _tkch (integer _tk1) Number: true Integer: true Term: true) )
            (if _verbose
                (writeRule
                     {MAIN: Digit+ << ($ASIS $ch (integer $1) Number: true Integer: true Term: true) >>}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
(setq _indent (isub _indent 1))
            (goto Skip:))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : Digit+
     ;; ====================
     ;; case: Period
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (and (<> (setq _tk1 $IN[(setq _ip (iadd _ip 1))]) #void) (= _LF_Period[_tk1] 1)) true (setq _ip _ip0)))
       (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip)
         ;; ====================
         ;; case: Digit+
         ;; ====================
         (if (begin (setq _i 0) (setq _tk2 (makeString {})) (while (and (<> (setq _tkthis $IN[(setq _ip (iadd _ip 1))]) #void) (= _LF_Digit[_tkthis] 1)) do (begin (setq _tk2[_i] _tkthis) (setq _i (iadd _i 1)) )) (setq _ip (isub _ip 1)) (if (> _i 0) true (setq _ip _ip1)))
           (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip)
             ;; *********************************************************
             ;; RULE: MAIN: Period Digit+ << ($ASIS $ch (number (append $1 $2)) Number: true Term: true) >>
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  (_tkASIS _tkch (number (append _tk1 _tk2)) Number: true Term: true) )
                (if _verbose
                    (writeRule
                         {MAIN: Period Digit+ << ($ASIS $ch (number (append $1 $2)) Number: true Term: true) >>}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
(setq _indent (isub _indent 1))
                (goto Skip:))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : Digit+
       ) ; end begin
     ) ; end case : Period
     ;; ====================
     ;; case: /
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (= (setq _tk1 $IN[(setq _ip (iadd _ip 1))]) #\/) true (setq _ip _ip0)))
       (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip)
         ;; ====================
         ;; case: *
         ;; ====================
         (if (if (= (setq _tk2 $IN[(setq _ip (iadd _ip 1))]) #\*) true (setq _ip _ip1))
           (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip)
             ;; ====================
             ;; case: Any{(or (<> (refString $IN _ip) #\*) (<> (refString $IN (iadd _ip 1)) #\/))}*
             ;; ====================
             (if (begin (setq _i 0) (setq _tk3 (makeString {})) (while (and (<> (setq _tkthis $IN[(setq _ip (iadd _ip 1))]) #void) (or (<> (refString _tkIN _ip) #\*) (<> (refString _tkIN (iadd _ip 1)) #\/)) ) do (begin (setq _tk3[_i] _tkthis) (setq _i (iadd _i 1)) )) (setq _ip (isub _ip 1)) true)
               (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip)
                 ;; ====================
                 ;; case: *
                 ;; ====================
                 (if (if (= (setq _tk4 $IN[(setq _ip (iadd _ip 1))]) #\*) true (setq _ip _ip3))
                   (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 _ip)
                     ;; ====================
                     ;; case: /
                     ;; ====================
                     (if (if (= (setq _tk5 $IN[(setq _ip (iadd _ip 1))]) #\/) true (setq _ip _ip4))
                       (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip5 _ip)
                         ;; *********************************************************
                         ;; RULE: MAIN: / * Any{(or (<> (refString $IN _ip) #\*) (<> (refString $IN (iadd _ip 1)) #\/))}* * / << true >>
                         ;; *********************************************************
                         (if true
                          (begin
                            (setq _ret  true )
                            (if _verbose
                                (writeRule
                                     {MAIN: / * Any{(or (<> (refString $IN _ip) #\*) (<> (refString $IN (iadd _ip 1)) #\/))}* * / << true >>}
                                     _ret  _tk0 _tk1 _tk2 _tk3 _tk4 _tk5 #void #void #void #void #void))
(setq _indent (isub _indent 1))
                            (goto Skip:))
                         ) ; end case : _default
                       ) ; end begin
                     ) ; end case : /
                   ) ; end begin
                 ) ; end case : *
               ) ; end begin
             ) ; end case : Any{(or (<> (refString $IN _ip) #\*) (<> (refString $IN (iadd _ip 1)) #\/))}*
           ) ; end begin
         ) ; end case : *
         ;; ====================
         ;; case: /
         ;; ====================
         (if (begin (setq _ip _ip1)
          (if (= (setq _tk2 $IN[(setq _ip (iadd _ip 1))]) #\/) true (setq _ip _ip1)))
           (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip)
             ;; ====================
             ;; case: NotEol*
             ;; ====================
             (if (begin (setq _i 0) (setq _tk3 (makeString {})) (while (and (<> (setq _tkthis $IN[(setq _ip (iadd _ip 1))]) #void) (= _LF_NotEol[_tkthis] 1)) do (begin (setq _tk3[_i] _tkthis) (setq _i (iadd _i 1)) )) (setq _ip (isub _ip 1)) true)
               (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip)
                 ;; ====================
                 ;; case: Eol?
                 ;; ====================
                 (if (if (and (<> (setq _tk4 $IN[(setq _ip (iadd _ip 1))]) #void) (= _LF_Eol[_tk4] 1)) true (begin (setq _ip (isub _ip 1)) (setq _tk4 {}) true))
                   (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 _ip)
                     ;; *********************************************************
                     ;; RULE: MAIN: / / NotEol* Eol? << true >>
                     ;; *********************************************************
                     (if true
                      (begin
                        (setq _ret  true )
                        (if _verbose
                            (writeRule
                                 {MAIN: / / NotEol* Eol? << true >>}
                                 _ret  _tk0 _tk1 _tk2 _tk3 _tk4 #void #void #void #void #void #void))
(setq _indent (isub _indent 1))
                        (goto Skip:))
                     ) ; end case : _default
                   ) ; end begin
                 ) ; end case : Eol?
               ) ; end begin
             ) ; end case : NotEol*
           ) ; end begin
         ) ; end case : /
       ) ; end begin
     ) ; end case : /
     ;; ====================
     ;; case: "="
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (= (setq _tk1 $IN[(setq _ip (iadd _ip 1))]) #\=) true (setq _ip _ip0)))
       (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip)
         ;; ====================
         ;; case: "="
         ;; ====================
         (if (if (= (setq _tk2 $IN[(setq _ip (iadd _ip 1))]) #\=) true (setq _ip _ip1))
           (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip)
             ;; *********************************************************
             ;; RULE: MAIN: "=" "=" << ($OUT $ch (string (append $1 $2)) Operator: true) >>
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  (_tkOUT _tkch (string (append _tk1 _tk2)) Operator: true) )
                (if _verbose
                    (writeRule
                         {MAIN: "=" "=" << ($OUT $ch (string (append $1 $2)) Operator: true) >>}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
(setq _indent (isub _indent 1))
                (goto Skip:))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : "="
         ;; *********************************************************
         ;; RULE: MAIN: "=" << ($OUT $ch (string $1) Operator: true) >>
         ;; *********************************************************
         (if true
          (begin
            (setq _ip _ip1)
            (setq _ret  (_tkOUT _tkch (string _tk1) Operator: true) )
            (if _verbose
                (writeRule
                     {MAIN: "=" << ($OUT $ch (string $1) Operator: true) >>}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
(setq _indent (isub _indent 1))
            (goto Skip:))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : "="
     ;; ====================
     ;; case: "+"
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (= (setq _tk1 $IN[(setq _ip (iadd _ip 1))]) #\+) true (setq _ip _ip0)))
       (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip)
         ;; ====================
         ;; case: "="
         ;; ====================
         (if (if (= (setq _tk2 $IN[(setq _ip (iadd _ip 1))]) #\=) true (setq _ip _ip1))
           (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip)
             ;; *********************************************************
             ;; RULE: MAIN: "+" "=" << ($OUT $ch (string (append $1 $2)) Operator: true) >>
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  (_tkOUT _tkch (string (append _tk1 _tk2)) Operator: true) )
                (if _verbose
                    (writeRule
                         {MAIN: "+" "=" << ($OUT $ch (string (append $1 $2)) Operator: true) >>}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
(setq _indent (isub _indent 1))
                (goto Skip:))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : "="
         ;; *********************************************************
         ;; RULE: MAIN: "+" << ($OUT $ch (string $1) Operator: true Sign: true) >>
         ;; *********************************************************
         (if true
          (begin
            (setq _ip _ip1)
            (setq _ret  (_tkOUT _tkch (string _tk1) Operator: true Sign: true) )
            (if _verbose
                (writeRule
                     {MAIN: "+" << ($OUT $ch (string $1) Operator: true Sign: true) >>}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
(setq _indent (isub _indent 1))
            (goto Skip:))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : "+"
     ;; ====================
     ;; case: "-"
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (= (setq _tk1 $IN[(setq _ip (iadd _ip 1))]) #\-) true (setq _ip _ip0)))
       (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip)
         ;; ====================
         ;; case: "="
         ;; ====================
         (if (if (= (setq _tk2 $IN[(setq _ip (iadd _ip 1))]) #\=) true (setq _ip _ip1))
           (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip)
             ;; *********************************************************
             ;; RULE: MAIN: "-" "=" << ($OUT $ch (string (append $1 $2)) Operator: true) >>
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  (_tkOUT _tkch (string (append _tk1 _tk2)) Operator: true) )
                (if _verbose
                    (writeRule
                         {MAIN: "-" "=" << ($OUT $ch (string (append $1 $2)) Operator: true) >>}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
(setq _indent (isub _indent 1))
                (goto Skip:))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : "="
         ;; *********************************************************
         ;; RULE: MAIN: "-" << ($OUT $ch (string $1) Operator: true Sign: true) >>
         ;; *********************************************************
         (if true
          (begin
            (setq _ip _ip1)
            (setq _ret  (_tkOUT _tkch (string _tk1) Operator: true Sign: true) )
            (if _verbose
                (writeRule
                     {MAIN: "-" << ($OUT $ch (string $1) Operator: true Sign: true) >>}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
(setq _indent (isub _indent 1))
            (goto Skip:))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : "-"
     ;; ====================
     ;; case: "/"
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (= (setq _tk1 $IN[(setq _ip (iadd _ip 1))]) #\/) true (setq _ip _ip0)))
       (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip)
         ;; ====================
         ;; case: "="
         ;; ====================
         (if (if (= (setq _tk2 $IN[(setq _ip (iadd _ip 1))]) #\=) true (setq _ip _ip1))
           (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip)
             ;; *********************************************************
             ;; RULE: MAIN: "/" "=" << ($OUT $ch (string (append $1 $2)) Operator: true) >>
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  (_tkOUT _tkch (string (append _tk1 _tk2)) Operator: true) )
                (if _verbose
                    (writeRule
                         {MAIN: "/" "=" << ($OUT $ch (string (append $1 $2)) Operator: true) >>}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
(setq _indent (isub _indent 1))
                (goto Skip:))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : "="
         ;; *********************************************************
         ;; RULE: MAIN: "/" << ($OUT $ch (string $1) Operator: true) >>
         ;; *********************************************************
         (if true
          (begin
            (setq _ip _ip1)
            (setq _ret  (_tkOUT _tkch (string _tk1) Operator: true) )
            (if _verbose
                (writeRule
                     {MAIN: "/" << ($OUT $ch (string $1) Operator: true) >>}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
(setq _indent (isub _indent 1))
            (goto Skip:))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : "/"
     ;; ====================
     ;; case: "*"
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (= (setq _tk1 $IN[(setq _ip (iadd _ip 1))]) #\*) true (setq _ip _ip0)))
       (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip)
         ;; ====================
         ;; case: "="
         ;; ====================
         (if (if (= (setq _tk2 $IN[(setq _ip (iadd _ip 1))]) #\=) true (setq _ip _ip1))
           (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip)
             ;; *********************************************************
             ;; RULE: MAIN: "*" "=" << ($OUT $ch (string (append $1 $2)) Operator: true) >>
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  (_tkOUT _tkch (string (append _tk1 _tk2)) Operator: true) )
                (if _verbose
                    (writeRule
                         {MAIN: "*" "=" << ($OUT $ch (string (append $1 $2)) Operator: true) >>}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
(setq _indent (isub _indent 1))
                (goto Skip:))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : "="
         ;; *********************************************************
         ;; RULE: MAIN: "*" << ($OUT $ch (string $1) Operator: true) >>
         ;; *********************************************************
         (if true
          (begin
            (setq _ip _ip1)
            (setq _ret  (_tkOUT _tkch (string _tk1) Operator: true) )
            (if _verbose
                (writeRule
                     {MAIN: "*" << ($OUT $ch (string $1) Operator: true) >>}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
(setq _indent (isub _indent 1))
            (goto Skip:))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : "*"
     ;; ====================
     ;; case: "%"
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (= (setq _tk1 $IN[(setq _ip (iadd _ip 1))]) #\%) true (setq _ip _ip0)))
       (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip)
         ;; ====================
         ;; case: "="
         ;; ====================
         (if (if (= (setq _tk2 $IN[(setq _ip (iadd _ip 1))]) #\=) true (setq _ip _ip1))
           (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip)
             ;; *********************************************************
             ;; RULE: MAIN: "%" "=" << ($OUT $ch (string (append $1 $2)) Operator: true) >>
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  (_tkOUT _tkch (string (append _tk1 _tk2)) Operator: true) )
                (if _verbose
                    (writeRule
                         {MAIN: "%" "=" << ($OUT $ch (string (append $1 $2)) Operator: true) >>}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
(setq _indent (isub _indent 1))
                (goto Skip:))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : "="
         ;; *********************************************************
         ;; RULE: MAIN: "%" << ($OUT $ch (string $1) Operator: true) >>
         ;; *********************************************************
         (if true
          (begin
            (setq _ip _ip1)
            (setq _ret  (_tkOUT _tkch (string _tk1) Operator: true) )
            (if _verbose
                (writeRule
                     {MAIN: "%" << ($OUT $ch (string $1) Operator: true) >>}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
(setq _indent (isub _indent 1))
            (goto Skip:))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : "%"
     ;; ====================
     ;; case: "!"
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (= (setq _tk1 $IN[(setq _ip (iadd _ip 1))]) #\!) true (setq _ip _ip0)))
       (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip)
         ;; ====================
         ;; case: "="
         ;; ====================
         (if (if (= (setq _tk2 $IN[(setq _ip (iadd _ip 1))]) #\=) true (setq _ip _ip1))
           (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip)
             ;; *********************************************************
             ;; RULE: MAIN: "!" "=" << ($OUT $ch (string (append $1 $2)) Operator: true) >>
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  (_tkOUT _tkch (string (append _tk1 _tk2)) Operator: true) )
                (if _verbose
                    (writeRule
                         {MAIN: "!" "=" << ($OUT $ch (string (append $1 $2)) Operator: true) >>}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
(setq _indent (isub _indent 1))
                (goto Skip:))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : "="
         ;; *********************************************************
         ;; RULE: MAIN: "!" << ($OUT $ch (string $1) Operator: true) >>
         ;; *********************************************************
         (if true
          (begin
            (setq _ip _ip1)
            (setq _ret  (_tkOUT _tkch (string _tk1) Operator: true) )
            (if _verbose
                (writeRule
                     {MAIN: "!" << ($OUT $ch (string $1) Operator: true) >>}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
(setq _indent (isub _indent 1))
            (goto Skip:))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : "!"
     ;; ====================
     ;; case: "&"
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (= (setq _tk1 $IN[(setq _ip (iadd _ip 1))]) #\&) true (setq _ip _ip0)))
       (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip)
         ;; ====================
         ;; case: "="
         ;; ====================
         (if (if (= (setq _tk2 $IN[(setq _ip (iadd _ip 1))]) #\=) true (setq _ip _ip1))
           (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip)
             ;; *********************************************************
             ;; RULE: MAIN: "&" "=" << ($OUT $ch (string (append $1 $2)) Operator: true) >>
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  (_tkOUT _tkch (string (append _tk1 _tk2)) Operator: true) )
                (if _verbose
                    (writeRule
                         {MAIN: "&" "=" << ($OUT $ch (string (append $1 $2)) Operator: true) >>}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
(setq _indent (isub _indent 1))
                (goto Skip:))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : "="
         ;; ====================
         ;; case: "&"
         ;; ====================
         (if (begin (setq _ip _ip1)
          (if (= (setq _tk2 $IN[(setq _ip (iadd _ip 1))]) #\&) true (setq _ip _ip1)))
           (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip)
             ;; *********************************************************
             ;; RULE: MAIN: "&" "&" << ($OUT $ch (string (append $1 $2)) Operator: true) >>
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  (_tkOUT _tkch (string (append _tk1 _tk2)) Operator: true) )
                (if _verbose
                    (writeRule
                         {MAIN: "&" "&" << ($OUT $ch (string (append $1 $2)) Operator: true) >>}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
(setq _indent (isub _indent 1))
                (goto Skip:))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : "&"
         ;; *********************************************************
         ;; RULE: MAIN: "&" << ($OUT $ch (string $1) Operator: true) >>
         ;; *********************************************************
         (if true
          (begin
            (setq _ip _ip1)
            (setq _ret  (_tkOUT _tkch (string _tk1) Operator: true) )
            (if _verbose
                (writeRule
                     {MAIN: "&" << ($OUT $ch (string $1) Operator: true) >>}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
(setq _indent (isub _indent 1))
            (goto Skip:))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : "&"
     ;; ====================
     ;; case: "|"
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (= (setq _tk1 $IN[(setq _ip (iadd _ip 1))]) #\|) true (setq _ip _ip0)))
       (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip)
         ;; ====================
         ;; case: "="
         ;; ====================
         (if (if (= (setq _tk2 $IN[(setq _ip (iadd _ip 1))]) #\=) true (setq _ip _ip1))
           (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip)
             ;; *********************************************************
             ;; RULE: MAIN: "|" "=" << ($OUT $ch (string (append $1 $2)) Operator: true) >>
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  (_tkOUT _tkch (string (append _tk1 _tk2)) Operator: true) )
                (if _verbose
                    (writeRule
                         {MAIN: "|" "=" << ($OUT $ch (string (append $1 $2)) Operator: true) >>}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
(setq _indent (isub _indent 1))
                (goto Skip:))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : "="
         ;; ====================
         ;; case: "|"
         ;; ====================
         (if (begin (setq _ip _ip1)
          (if (= (setq _tk2 $IN[(setq _ip (iadd _ip 1))]) #\|) true (setq _ip _ip1)))
           (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip)
             ;; *********************************************************
             ;; RULE: MAIN: "|" "|" << ($OUT $ch (string (append $1 $2)) Operator: true) >>
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  (_tkOUT _tkch (string (append _tk1 _tk2)) Operator: true) )
                (if _verbose
                    (writeRule
                         {MAIN: "|" "|" << ($OUT $ch (string (append $1 $2)) Operator: true) >>}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
(setq _indent (isub _indent 1))
                (goto Skip:))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : "|"
         ;; *********************************************************
         ;; RULE: MAIN: "|" << ($OUT $ch (string $1) Operator: true) >>
         ;; *********************************************************
         (if true
          (begin
            (setq _ip _ip1)
            (setq _ret  (_tkOUT _tkch (string _tk1) Operator: true) )
            (if _verbose
                (writeRule
                     {MAIN: "|" << ($OUT $ch (string $1) Operator: true) >>}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
(setq _indent (isub _indent 1))
            (goto Skip:))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : "|"
     ;; ====================
     ;; case: "<"
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (= (setq _tk1 $IN[(setq _ip (iadd _ip 1))]) #\<) true (setq _ip _ip0)))
       (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip)
         ;; ====================
         ;; case: "="
         ;; ====================
         (if (if (= (setq _tk2 $IN[(setq _ip (iadd _ip 1))]) #\=) true (setq _ip _ip1))
           (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip)
             ;; *********************************************************
             ;; RULE: MAIN: "<" "=" << ($OUT $ch (string (append $1 $2)) Operator: true) >>
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  (_tkOUT _tkch (string (append _tk1 _tk2)) Operator: true) )
                (if _verbose
                    (writeRule
                         {MAIN: "<" "=" << ($OUT $ch (string (append $1 $2)) Operator: true) >>}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
(setq _indent (isub _indent 1))
                (goto Skip:))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : "="
         ;; *********************************************************
         ;; RULE: MAIN: "<" << ($OUT $ch (string $1) Operator: true) >>
         ;; *********************************************************
         (if true
          (begin
            (setq _ip _ip1)
            (setq _ret  (_tkOUT _tkch (string _tk1) Operator: true) )
            (if _verbose
                (writeRule
                     {MAIN: "<" << ($OUT $ch (string $1) Operator: true) >>}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
(setq _indent (isub _indent 1))
            (goto Skip:))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : "<"
     ;; ====================
     ;; case: ">"
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (= (setq _tk1 $IN[(setq _ip (iadd _ip 1))]) #\>) true (setq _ip _ip0)))
       (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip)
         ;; ====================
         ;; case: "="
         ;; ====================
         (if (if (= (setq _tk2 $IN[(setq _ip (iadd _ip 1))]) #\=) true (setq _ip _ip1))
           (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip)
             ;; *********************************************************
             ;; RULE: MAIN: ">" "=" << ($OUT $ch (string (append $1 $2)) Operator: true) >>
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  (_tkOUT _tkch (string (append _tk1 _tk2)) Operator: true) )
                (if _verbose
                    (writeRule
                         {MAIN: ">" "=" << ($OUT $ch (string (append $1 $2)) Operator: true) >>}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
(setq _indent (isub _indent 1))
                (goto Skip:))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : "="
         ;; *********************************************************
         ;; RULE: MAIN: ">" << ($OUT $ch (string $1) Operator: true) >>
         ;; *********************************************************
         (if true
          (begin
            (setq _ip _ip1)
            (setq _ret  (_tkOUT _tkch (string _tk1) Operator: true) )
            (if _verbose
                (writeRule
                     {MAIN: ">" << ($OUT $ch (string $1) Operator: true) >>}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
(setq _indent (isub _indent 1))
            (goto Skip:))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : ">"
     ;; ====================
     ;; case: "~"
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (= (setq _tk1 $IN[(setq _ip (iadd _ip 1))]) #\~) true (setq _ip _ip0)))
       (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip)
         ;; *********************************************************
         ;; RULE: MAIN: "~" << ($OUT $ch (string $1) Operator: true) >>
         ;; *********************************************************
         (if true
          (begin
            (setq _ret  (_tkOUT _tkch (string _tk1) Operator: true) )
            (if _verbose
                (writeRule
                     {MAIN: "~" << ($OUT $ch (string $1) Operator: true) >>}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
(setq _indent (isub _indent 1))
            (goto Skip:))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : "~"
     ;; ====================
     ;; case: #
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (= (setq _tk1 $IN[(setq _ip (iadd _ip 1))]) #\#) true (setq _ip _ip0)))
       (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip)
         ;; ====================
         ;; case: Letter+
         ;; ====================
         (if (begin (setq _i 0) (setq _tk2 (makeString {})) (while (and (<> (setq _tkthis $IN[(setq _ip (iadd _ip 1))]) #void) (= _LF_Letter[_tkthis] 1)) do (begin (setq _tk2[_i] _tkthis) (setq _i (iadd _i 1)) )) (setq _ip (isub _ip 1)) (if (> _i 0) true (setq _ip _ip1)))
           (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip)
             ;; ====================
             ;; case: ,
             ;; ====================
             (if (if (= (setq _tk3 $IN[(setq _ip (iadd _ip 1))]) #\,) true (setq _ip _ip2))
               (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip)
                 ;; ====================
                 ;; case: Digit+
                 ;; ====================
                 (if (begin (setq _i 0) (setq _tk4 (makeString {})) (while (and (<> (setq _tkthis $IN[(setq _ip (iadd _ip 1))]) #void) (= _LF_Digit[_tkthis] 1)) do (begin (setq _tk4[_i] _tkthis) (setq _i (iadd _i 1)) )) (setq _ip (isub _ip 1)) (if (> _i 0) true (setq _ip _ip3)))
                   (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 _ip)
                     ;; ====================
                     ;; case: ,
                     ;; ====================
                     (if (if (= (setq _tk5 $IN[(setq _ip (iadd _ip 1))]) #\,) true (setq _ip _ip4))
                       (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip5 _ip)
                         ;; ====================
                         ;; case: Digit+
                         ;; ====================
                         (if (begin (setq _i 0) (setq _tk6 (makeString {})) (while (and (<> (setq _tkthis $IN[(setq _ip (iadd _ip 1))]) #void) (= _LF_Digit[_tkthis] 1)) do (begin (setq _tk6[_i] _tkthis) (setq _i (iadd _i 1)) )) (setq _ip (isub _ip 1)) (if (> _i 0) true (setq _ip _ip5)))
                           (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip6 _ip)
                             ;; *********************************************************
                             ;; RULE: MAIN: # Letter+ , Digit+ , Digit+ << ($ASIS $ch (date (append $1 $2 $3 $4 $5 $6)) Date: true Term: true) >>
                             ;; *********************************************************
                             (if true
                              (begin
                                (setq _ret  (_tkASIS _tkch (date (append _tk1 _tk2 _tk3 _tk4 _tk5 _tk6)) Date: true Term: true) )
                                (if _verbose
                                    (writeRule
                                         {MAIN: # Letter+ , Digit+ , Digit+ << ($ASIS $ch (date (append $1 $2 $3 $4 $5 $6)) Date: true Term: true) >>}
                                         _ret  _tk0 _tk1 _tk2 _tk3 _tk4 _tk5 _tk6 #void #void #void #void))
(setq _indent (isub _indent 1))
                                (goto Skip:))
                             ) ; end case : _default
                           ) ; end begin
                         ) ; end case : Digit+
                       ) ; end begin
                     ) ; end case : ,
                   ) ; end begin
                 ) ; end case : Digit+
               ) ; end begin
             ) ; end case : ,
           ) ; end begin
         ) ; end case : Letter+
         ;; ====================
         ;; case: <
         ;; ====================
         (if (begin (setq _ip _ip1)
          (if (= (setq _tk2 $IN[(setq _ip (iadd _ip 1))]) #\<) true (setq _ip _ip1)))
           (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip)
             ;; ====================
             ;; case: NameStart
             ;; ====================
             (if (if (and (<> (setq _tk3 $IN[(setq _ip (iadd _ip 1))]) #void) (= _LF_NameStart[_tk3] 1)) true (setq _ip _ip2))
               (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip)
                 ;; ====================
                 ;; case: NameChar*
                 ;; ====================
                 (if (begin (setq _i 0) (setq _tk4 (makeString {})) (while (and (<> (setq _tkthis $IN[(setq _ip (iadd _ip 1))]) #void) (= _LF_NameChar[_tkthis] 1)) do (begin (setq _tk4[_i] _tkthis) (setq _i (iadd _i 1)) )) (setq _ip (isub _ip 1)) true)
                   (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 _ip)
                     ;; ====================
                     ;; case: Whitespace*
                     ;; ====================
                     (if (begin (setq _i 0) (setq _tk5 (makeString {})) (while (and (<> (setq _tkthis $IN[(setq _ip (iadd _ip 1))]) #void) (= _LF_Whitespace[_tkthis] 1)) do (begin (setq _tk5[_i] _tkthis) (setq _i (iadd _i 1)) )) (setq _ip (isub _ip 1)) true)
                       (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip5 _ip)
                         ;; ====================
                         ;; case: Digit*
                         ;; ====================
                         (if (begin (setq _i 0) (setq _tk6 (makeString {})) (while (and (<> (setq _tkthis $IN[(setq _ip (iadd _ip 1))]) #void) (= _LF_Digit[_tkthis] 1)) do (begin (setq _tk6[_i] _tkthis) (setq _i (iadd _i 1)) )) (setq _ip (isub _ip 1)) true)
                           (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip6 _ip)
                             ;; ====================
                             ;; case: >
                             ;; ====================
                             (if (if (= (setq _tk7 $IN[(setq _ip (iadd _ip 1))]) #\>) true (setq _ip _ip6))
                               (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip7 _ip)
                                 ;; *********************************************************
                                 ;; RULE: MAIN: # < NameStart NameChar* Whitespace* Digit* > << ($OUT $ch (inspect (number $6)) vtyp: obj: Term: true) >>
                                 ;; *********************************************************
                                 (if true
                                  (begin
                                    (setq _ret  (_tkOUT _tkch (inspect (number _tk6)) vtyp: obj: Term: true) )
                                    (if _verbose
                                        (writeRule
                                             {MAIN: # < NameStart NameChar* Whitespace* Digit* > << ($OUT $ch (inspect (number $6)) vtyp: obj: Term: true) >>}
                                             _ret  _tk0 _tk1 _tk2 _tk3 _tk4 _tk5 _tk6 _tk7 #void #void #void))
(setq _indent (isub _indent 1))
                                    (goto Skip:))
                                 ) ; end case : _default
                               ) ; end begin
                             ) ; end case : >
                           ) ; end begin
                         ) ; end case : Digit*
                       ) ; end begin
                     ) ; end case : Whitespace*
                   ) ; end begin
                 ) ; end case : NameChar*
               ) ; end begin
             ) ; end case : NameStart
           ) ; end begin
         ) ; end case : <
       ) ; end begin
     ) ; end case : #
     ;; ====================
     ;; case: Whitespace+
     ;; ====================
     (if (begin (setq _ip _ip0)
      (begin (setq _i 0) (setq _tk1 (makeString {})) (while (and (<> (setq _tkthis $IN[(setq _ip (iadd _ip 1))]) #void) (= _LF_Whitespace[_tkthis] 1)) do (begin (setq _tk1[_i] _tkthis) (setq _i (iadd _i 1)) )) (setq _ip (isub _ip 1)) (if (> _i 0) true (setq _ip _ip0))))
       (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip)
         ;; *********************************************************
         ;; RULE: MAIN: Whitespace+ << true >>
         ;; *********************************************************
         (if true
          (begin
            (setq _ret  true )
            (if _verbose
                (writeRule
                     {MAIN: Whitespace+ << true >>}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
(setq _indent (isub _indent 1))
            (goto Skip:))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : Whitespace+
     ;; ====================
     ;; case: Any
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 $IN[(setq _ip (iadd _ip 1))]) #void) true (setq _ip _ip0)))
       (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip)
         ;; *********************************************************
         ;; RULE: MAIN: Any << ($OUT $ch (symbol (string $1)) Default: true) >>
         ;; *********************************************************
         (if true
          (begin
            (setq _ret  (_tkOUT _tkch (symbol (string _tk1)) Default: true) )
            (if _verbose
                (writeRule
                     {MAIN: Any << ($OUT $ch (symbol (string $1)) Default: true) >>}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
(setq _indent (isub _indent 1))
            (goto Skip:))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : Any
     ;; ====================
     ;; case: Eof
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (= (setq _tk1 $IN[(setq _ip (iadd _ip 1))]) #void) true (setq _ip _ip0)))
       (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip)
         ;; *********************************************************
         ;; RULE: MAIN: Eof :: $LIST ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ret  _tkLIST )
            (if _verbose
                (writeRule
                     {MAIN: Eof :: $LIST ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
(setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : Eof
     ;; **************************************************
     ;; DEFAULT (if we get to here we failed to recognize 
     ;; Note:       (at least on this repetition)
     ;; **************************************************
     (begin
        (if (= _tkn 1)
            ;; If we have never recognized any token on previous repetitions 
            (begin
               (setq _ip _oldIp)     
               (if _verbose (_logLine "Rejecting Rule MAIN on: " input: ))
               (setq _indent (isub _indent 1))
               (setq _ret morphFail)
               ) ; end begin then
            else
            ;; If we have recognized some tokens on previous repetitions, then return $0
            ;; Note: In Syntax Rules, we do not fail the rule if it has 
            ;;       recognized a token in one or more repetitions of the rule. 
            (begin
               (setq _ip _ip0)
               (setq _ret _tk0)
               ) ; end begin else
            ) ; end if
        ) ; end of DEFAULT

   ;; End of tests for user defined rules.

   (if (> _verboseLexIn.MAIN -1)
       (begin
          (setq _verbose _verboseSave)
          (if (>= _verboseLexCount.MAIN _verboseLexIn.MAIN) (error "Count" "in Routine MAIN"))
       ))

   ;; Repeat or return to caller is handled here.
   _ret) ;; end _LEXRULE_MAIN
























;;**EXPORTKEY**:esm:selector:_SEMRULE_LAMBDA
;; ************************************************
;; LAMBDA user defined Semantic Rule implementation
;; Summary: This Lambda implements the LAMBDA
;;          user defined rule. Each rule test is
;;          marked with a boxed comment line for
;;          ease of human understanding.
;; Note: This code was machine generated by ParseLib.
;; ************************************************
(defchild esm.selector _SEMRULE_LAMBDA(...)
   ;; The token object _io is a persistent variable
   ;; The token pointer _ip is a persistent variable
   ;; All getToken logic increments _ip on each token fetch.
   ;; The return value from the rule is stored in _ret.
   ;; Remember that this rule may repeat depending on _repeatSW.
   vars:(_tk0 _tk1 _tk2 _tk3 _tk4  _tk5 _i
         _tk6 _tk7 _tk8 _tk9 _tkn  _repeatSW
         _ak0 _ak1 _ak2 _ak3 _ak4  _ak5 _tkthis
         _ak6 _ak7 _ak8 _ak9 _ret  _oldIp
         _ip0 _ip1 _ip2 _ip3 _ip4  _ip5
         _ip6 _ip7 _ip8 _ip9 _tkn  
         _verboseSave       
        ) ; end temporary variables
   ;; Collect any arguments which may have been passed to this rule
   (if (> (setq _tkn (argCount)) 0)
       (begin (setq _ak0 (argFetch 0))
          (if (> _tkn 1) 
              (begin (setq _ak1 (argFetch 1))
                 (if (> _tkn 2) 
                     (begin (setq _ak2 (argFetch 2))
                        (if (> _tkn 3) 
                            (begin (setq _ak3 (argFetch 3))
                               (if (> _tkn 4) 
                                   (begin (setq _ak4 (argFetch 4))
                                      (if (> _tkn 5) 
                                          (begin (setq _ak5 (argFetch 5))
                                             (if (> _tkn 6) 
                                                 (begin (setq _ak6 (argFetch 6))
                                                    (if (> _tkn 7) 
                                                        (begin (setq _ak7 (argFetch 7))
                                                           (if (> _tkn 8) 
                                                               (begin (setq _ak8 (argFetch 8))
                                                               (if (> _tkn 9) (setq _ak9 (argFetch 9))
                                                           ))))))))))))))))))) ; end argument collection
   ;; The default structure is given the named rule attribute
   (setq _tk0 (new Structure: LAMBDA: true))
   (setq _tkn 0)
   (setq _oldIp (copy _ip))
   (setq _repeatSW true)

   (if (= _verboseSemCount.LAMBDA #void) 
          (setq _verboseSemCount.LAMBDA 1) 
          (setq _verboseSemCount.LAMBDA (iadd _verboseSemCount.LAMBDA 1)))

   (if (and (<> _verboseSemIn.LAMBDA #void) (> _verboseSemIn.LAMBDA -1))
       (begin (setq _verboseSave _verbose) (setq _verbose true))) 

   ;; Save the old token pointer and test for user defined rules
   Skip::         
     (setq _indent (iadd _indent 1))
     (if _verbose (_logLine "Attempting LAMBDA Rule on: " source:))
     (setq _ip0 (copy _ip))
     (++ _tkn)
     (setq _repeatSW false)

     ;; *****************************************
     ;; Begin testing for each user defined rule.
     ;; *****************************************








     ;; ====================
     ;; case: [
     ;; ====================
     (if (if (isPair (setq _tk1 (_getToken))) (_pushIp) (_lastIp))
       (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 (copy _ip))
         ;; ====================
         ;; case: Any{(= _tk2 "lambda")}
         ;; ====================
         (if (and (if (<> (setq _tk2 (_getToken)) morphFail) true (setq _ip _ip1)) (= _tk2 "lambda"))
           (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 (copy _ip))
             ;; ====================
             ;; case: [
             ;; ====================
             (if (if (isPair (setq _tk3 (_getToken))) (_pushIp) (_lastIp))
               (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 (copy _ip))
                 ;; ====================
                 ;; case: Any{(= _tk4 "xv")}
                 ;; ====================
                 (if (and (if (<> (setq _tk4 (_getToken)) morphFail) true (setq _ip _ip3)) (= _tk4 "xv"))
                   (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 (copy _ip))
                     ;; ====================
                     ;; case: ]
                     ;; ====================
                     (if (if (_eofToken) (_popIp) false)
                       (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip5 (copy _ip))
                         ;; ====================
                         ;; case: Any
                         ;; ====================
                         (if (if (<> (setq _tk6 (_getToken)) morphFail) true (setq _ip _ip5))
                           (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip6 (copy _ip))
                             ;; ====================
                             ;; case: Any
                             ;; ====================
                             (if (if (<> (setq _tk7 (_getToken)) morphFail) true (setq _ip _ip6))
                               (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip7 (copy _ip))
                                 ;; ====================
                                 ;; case: ]
                                 ;; ====================
                                 (if (if (_eofToken) (_popIp) false)
                                   (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip8 (copy _ip))
                                     ;; *********************************************************
                                     ;; RULE: LAMBDA: [ Any{(= _tk2 "lambda")} [ Any{(= _tk4 "xv")} ] Any Any ] :: $7 ::
                                     ;; *********************************************************
                                     (if true
                                      (begin
                                        (setq _ret  _tk7 )
                                        (if _verbose 
                                            (writeRule
                                                 {LAMBDA: [ Any{(= _tk2 "lambda")} [ Any{(= _tk4 "xv")} ] Any Any ] :: $7 ::}
                                                 _ret  _tk0 _tk1 _tk2 _tk3 _tk4 _tk5 _tk6 _tk7 _tk8 #void #void))
(setq _indent (isub _indent))
                                        (return _ret))
                                     ) ; end case : _default
                                   ) ; end begin
                                 ) ; end case : ]
                               ) ; end begin
                             ) ; end case : Any
                           ) ; end begin
                         ) ; end case : Any
                       ) ; end begin
                     ) ; end case : ]
                   ) ; end begin
                 ) ; end case : Any{(= _tk4 "xv")}
               ) ; end begin
             ) ; end case : [
           ) ; end begin
         ) ; end case : Any{(= _tk2 "lambda")}
       ) ; end begin
     ) ; end case : [
     ;; **************************************************
     ;; DEFAULT (if we get to here we failed to recognize)
     ;; Note:       (at least on this repetition)
     ;; **************************************************
     (begin
        (if (= _tkn 1)
            ;; If we have never recognized any token on previous repetitions 
            (begin
               (setq _ip _oldIp)     
               (if _verbose (_logLine "Rejecting Rule LAMBDA on: " source:))
               (setq _indent (isub _indent 1))
               (setq _ret morphFail)
               ) ; end begin then
            else
            ;; If we have recognized some tokens on previous repetitions, then return $0
            ;; Note: In Semantic Rules, we do not fail the rule if it has 
            ;;       recognized a token in one or more repetitions of the rule. 
            (begin
               (setq _ip _ip0)
               (setq _ret _tk0)
               ) ; end begin else
            ) ; end if
        ) ; end of DEFAULT

   ;; End of tests for user defined rules.

   (if (> _verboseSemIn.LAMBDA -1)
       (begin
          (setq _verbose _verboseSave)
          (if (>= _verboseSemCount.LAMBDA _verboseSemIn.LAMBDA) (error "Count" "in Routine LAMBDA"))
       ))
   ;; Repeat or return to caller is handled here.
   _ret) ;; end _SEMRULE_LAMBDA















;;**EXPORTKEY**:esm:selector:_SEMRULE_LET
;; ************************************************
;; LET user defined Semantic Rule implementation
;; Summary: This Lambda implements the LET
;;          user defined rule. Each rule test is
;;          marked with a boxed comment line for
;;          ease of human understanding.
;; Note: This code was machine generated by ParseLib.
;; ************************************************
(defchild esm.selector _SEMRULE_LET(...)
   ;; The token object _io is a persistent variable
   ;; The token pointer _ip is a persistent variable
   ;; All getToken logic increments _ip on each token fetch.
   ;; The return value from the rule is stored in _ret.
   ;; Remember that this rule may repeat depending on _repeatSW.
   vars:(_tk0 _tk1 _tk2 _tk3 _tk4  _tk5 _i
         _tk6 _tk7 _tk8 _tk9 _tkn  _repeatSW
         _ak0 _ak1 _ak2 _ak3 _ak4  _ak5 _tkthis
         _ak6 _ak7 _ak8 _ak9 _ret  _oldIp
         _ip0 _ip1 _ip2 _ip3 _ip4  _ip5
         _ip6 _ip7 _ip8 _ip9 _tkn  
         _verboseSave       
        ) ; end temporary variables
   ;; Collect any arguments which may have been passed to this rule
   (if (> (setq _tkn (argCount)) 0)
       (begin (setq _ak0 (argFetch 0))
          (if (> _tkn 1) 
              (begin (setq _ak1 (argFetch 1))
                 (if (> _tkn 2) 
                     (begin (setq _ak2 (argFetch 2))
                        (if (> _tkn 3) 
                            (begin (setq _ak3 (argFetch 3))
                               (if (> _tkn 4) 
                                   (begin (setq _ak4 (argFetch 4))
                                      (if (> _tkn 5) 
                                          (begin (setq _ak5 (argFetch 5))
                                             (if (> _tkn 6) 
                                                 (begin (setq _ak6 (argFetch 6))
                                                    (if (> _tkn 7) 
                                                        (begin (setq _ak7 (argFetch 7))
                                                           (if (> _tkn 8) 
                                                               (begin (setq _ak8 (argFetch 8))
                                                               (if (> _tkn 9) (setq _ak9 (argFetch 9))
                                                           ))))))))))))))))))) ; end argument collection
   ;; The default structure is given the named rule attribute
   (setq _tk0 (new Structure: LET: true))
   (setq _tkn 0)
   (setq _oldIp (copy _ip))
   (setq _repeatSW true)

   (if (= _verboseSemCount.LET #void) 
          (setq _verboseSemCount.LET 1) 
          (setq _verboseSemCount.LET (iadd _verboseSemCount.LET 1)))

   (if (and (<> _verboseSemIn.LET #void) (> _verboseSemIn.LET -1))
       (begin (setq _verboseSave _verbose) (setq _verbose true))) 

   ;; Save the old token pointer and test for user defined rules
   Skip::         
     (setq _indent (iadd _indent 1))
     (if _verbose (_logLine "Attempting LET Rule on: " source:))
     (setq _ip0 (copy _ip))
     (++ _tkn)
     (setq _repeatSW false)

     ;; *****************************************
     ;; Begin testing for each user defined rule.
     ;; *****************************************








     ;; ====================
     ;; case: [
     ;; ====================
     (if (if (isPair (setq _tk1 (_getToken))) (_pushIp) (_lastIp))
       (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 (copy _ip))
         ;; ====================
         ;; case: Any{(= _tk2 "let")}
         ;; ====================
         (if (and (if (<> (setq _tk2 (_getToken)) morphFail) true (setq _ip _ip1)) (= _tk2 "let"))
           (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 (copy _ip))
             ;; ====================
             ;; case: Any
             ;; ====================
             (if (if (<> (setq _tk3 (_getToken)) morphFail) true (setq _ip _ip2))
               (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 (copy _ip))
                 ;; ====================
                 ;; case: LAMBDA
                 ;; ====================
                 (if (if (<> (setq _tk4 (_SEMRULE_LAMBDA)) morphFail)true (setq _ip _ip3))
                   (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 (copy _ip))
                     ;; ====================
                     ;; case: ]
                     ;; ====================
                     (if (if (_eofToken) (_popIp) false)
                       (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip5 (copy _ip))
                         ;; *********************************************************
                         ;; RULE: LET: [ Any{(= _tk2 "let")} Any LAMBDA ] :: (new Vector: 2 $3 $4) ::
                         ;; *********************************************************
                         (if true
                          (begin
                            (setq _ret  (new Vector: 2 _tk3 _tk4) )
                            (if _verbose 
                                (writeRule
                                     {LET: [ Any{(= _tk2 "let")} Any LAMBDA ] :: (new Vector: 2 $3 $4) ::}
                                     _ret  _tk0 _tk1 _tk2 _tk3 _tk4 _tk5 #void #void #void #void #void))
(setq _indent (isub _indent))
                            (return _ret))
                         ) ; end case : _default
                       ) ; end begin
                     ) ; end case : ]
                   ) ; end begin
                 ) ; end case : LAMBDA
               ) ; end begin
             ) ; end case : Any
           ) ; end begin
         ) ; end case : Any{(= _tk2 "let")}
       ) ; end begin
     ) ; end case : [
     ;; **************************************************
     ;; DEFAULT (if we get to here we failed to recognize)
     ;; Note:       (at least on this repetition)
     ;; **************************************************
     (begin
        (if (= _tkn 1)
            ;; If we have never recognized any token on previous repetitions 
            (begin
               (setq _ip _oldIp)     
               (if _verbose (_logLine "Rejecting Rule LET on: " source:))
               (setq _indent (isub _indent 1))
               (setq _ret morphFail)
               ) ; end begin then
            else
            ;; If we have recognized some tokens on previous repetitions, then return $0
            ;; Note: In Semantic Rules, we do not fail the rule if it has 
            ;;       recognized a token in one or more repetitions of the rule. 
            (begin
               (setq _ip _ip0)
               (setq _ret _tk0)
               ) ; end begin else
            ) ; end if
        ) ; end of DEFAULT

   ;; End of tests for user defined rules.

   (if (> _verboseSemIn.LET -1)
       (begin
          (setq _verbose _verboseSave)
          (if (>= _verboseSemCount.LET _verboseSemIn.LET) (error "Count" "in Routine LET"))
       ))
   ;; Repeat or return to caller is handled here.
   _ret) ;; end _SEMRULE_LET















;;**EXPORTKEY**:esm:selector:_SEMRULE_LETOMIT
;; ************************************************
;; LETOMIT user defined Semantic Rule implementation
;; Summary: This Lambda implements the LETOMIT
;;          user defined rule. Each rule test is
;;          marked with a boxed comment line for
;;          ease of human understanding.
;; Note: This code was machine generated by ParseLib.
;; ************************************************
(defchild esm.selector _SEMRULE_LETOMIT(...)
   ;; The token object _io is a persistent variable
   ;; The token pointer _ip is a persistent variable
   ;; All getToken logic increments _ip on each token fetch.
   ;; The return value from the rule is stored in _ret.
   ;; Remember that this rule may repeat depending on _repeatSW.
   vars:(_tk0 _tk1 _tk2 _tk3 _tk4  _tk5 _i
         _tk6 _tk7 _tk8 _tk9 _tkn  _repeatSW
         _ak0 _ak1 _ak2 _ak3 _ak4  _ak5 _tkthis
         _ak6 _ak7 _ak8 _ak9 _ret  _oldIp
         _ip0 _ip1 _ip2 _ip3 _ip4  _ip5
         _ip6 _ip7 _ip8 _ip9 _tkn  
         _verboseSave       
        ) ; end temporary variables
   ;; Collect any arguments which may have been passed to this rule
   (if (> (setq _tkn (argCount)) 0)
       (begin (setq _ak0 (argFetch 0))
          (if (> _tkn 1) 
              (begin (setq _ak1 (argFetch 1))
                 (if (> _tkn 2) 
                     (begin (setq _ak2 (argFetch 2))
                        (if (> _tkn 3) 
                            (begin (setq _ak3 (argFetch 3))
                               (if (> _tkn 4) 
                                   (begin (setq _ak4 (argFetch 4))
                                      (if (> _tkn 5) 
                                          (begin (setq _ak5 (argFetch 5))
                                             (if (> _tkn 6) 
                                                 (begin (setq _ak6 (argFetch 6))
                                                    (if (> _tkn 7) 
                                                        (begin (setq _ak7 (argFetch 7))
                                                           (if (> _tkn 8) 
                                                               (begin (setq _ak8 (argFetch 8))
                                                               (if (> _tkn 9) (setq _ak9 (argFetch 9))
                                                           ))))))))))))))))))) ; end argument collection
   ;; The default structure is given the named rule attribute
   (setq _tk0 (new Structure: LETOMIT: true))
   (setq _tkn 0)
   (setq _oldIp (copy _ip))
   (setq _repeatSW true)

   (if (= _verboseSemCount.LETOMIT #void) 
          (setq _verboseSemCount.LETOMIT 1) 
          (setq _verboseSemCount.LETOMIT (iadd _verboseSemCount.LETOMIT 1)))

   (if (and (<> _verboseSemIn.LETOMIT #void) (> _verboseSemIn.LETOMIT -1))
       (begin (setq _verboseSave _verbose) (setq _verbose true))) 

   ;; Save the old token pointer and test for user defined rules
   Skip::         
     (setq _indent (iadd _indent 1))
     (if _verbose (_logLine "Attempting LETOMIT Rule on: " source:))
     (setq _ip0 (copy _ip))
     (++ _tkn)
     (setq _repeatSW false)

     ;; *****************************************
     ;; Begin testing for each user defined rule.
     ;; *****************************************








     ;; ====================
     ;; case: [
     ;; ====================
     (if (if (isPair (setq _tk1 (_getToken))) (_pushIp) (_lastIp))
       (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 (copy _ip))
         ;; ====================
         ;; case: REFOMIT
         ;; ====================
         (if (if (<> (setq _tk2 (_SEMRULE_REFOMIT)) morphFail)true (setq _ip _ip1))
           (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 (copy _ip))
             ;; ====================
             ;; case: LET
             ;; ====================
             (if (if (<> (setq _tk3 (_SEMRULE_LET)) morphFail)true (setq _ip _ip2))
               (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 (copy _ip))
                 ;; ====================
                 ;; case: ]
                 ;; ====================
                 (if (if (_eofToken) (_popIp) false)
                   (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 (copy _ip))
                     ;; *********************************************************
                     ;; RULE: LETOMIT: [ REFOMIT LET ] :: $3 ::
                     ;; *********************************************************
                     (if true
                      (begin
                        (setq _ret  _tk3 )
                        (if _verbose 
                            (writeRule
                                 {LETOMIT: [ REFOMIT LET ] :: $3 ::}
                                 _ret  _tk0 _tk1 _tk2 _tk3 _tk4 #void #void #void #void #void #void))
(setq _indent (isub _indent))
                        (return _ret))
                     ) ; end case : _default
                   ) ; end begin
                 ) ; end case : ]
               ) ; end begin
             ) ; end case : LET
           ) ; end begin
         ) ; end case : REFOMIT
       ) ; end begin
     ) ; end case : [
     ;; **************************************************
     ;; DEFAULT (if we get to here we failed to recognize)
     ;; Note:       (at least on this repetition)
     ;; **************************************************
     (begin
        (if (= _tkn 1)
            ;; If we have never recognized any token on previous repetitions 
            (begin
               (setq _ip _oldIp)     
               (if _verbose (_logLine "Rejecting Rule LETOMIT on: " source:))
               (setq _indent (isub _indent 1))
               (setq _ret morphFail)
               ) ; end begin then
            else
            ;; If we have recognized some tokens on previous repetitions, then return $0
            ;; Note: In Semantic Rules, we do not fail the rule if it has 
            ;;       recognized a token in one or more repetitions of the rule. 
            (begin
               (setq _ip _ip0)
               (setq _ret _tk0)
               ) ; end begin else
            ) ; end if
        ) ; end of DEFAULT

   ;; End of tests for user defined rules.

   (if (> _verboseSemIn.LETOMIT -1)
       (begin
          (setq _verbose _verboseSave)
          (if (>= _verboseSemCount.LETOMIT _verboseSemIn.LETOMIT) (error "Count" "in Routine LETOMIT"))
       ))
   ;; Repeat or return to caller is handled here.
   _ret) ;; end _SEMRULE_LETOMIT















;;**EXPORTKEY**:esm:selector:_SEMRULE_LETTRUNCATE
;; ************************************************
;; LETTRUNCATE user defined Semantic Rule implementation
;; Summary: This Lambda implements the LETTRUNCATE
;;          user defined rule. Each rule test is
;;          marked with a boxed comment line for
;;          ease of human understanding.
;; Note: This code was machine generated by ParseLib.
;; ************************************************
(defchild esm.selector _SEMRULE_LETTRUNCATE(...)
   ;; The token object _io is a persistent variable
   ;; The token pointer _ip is a persistent variable
   ;; All getToken logic increments _ip on each token fetch.
   ;; The return value from the rule is stored in _ret.
   ;; Remember that this rule may repeat depending on _repeatSW.
   vars:(_tk0 _tk1 _tk2 _tk3 _tk4  _tk5 _i
         _tk6 _tk7 _tk8 _tk9 _tkn  _repeatSW
         _ak0 _ak1 _ak2 _ak3 _ak4  _ak5 _tkthis
         _ak6 _ak7 _ak8 _ak9 _ret  _oldIp
         _ip0 _ip1 _ip2 _ip3 _ip4  _ip5
         _ip6 _ip7 _ip8 _ip9 _tkn  
         _verboseSave       
        ) ; end temporary variables
   ;; Collect any arguments which may have been passed to this rule
   (if (> (setq _tkn (argCount)) 0)
       (begin (setq _ak0 (argFetch 0))
          (if (> _tkn 1) 
              (begin (setq _ak1 (argFetch 1))
                 (if (> _tkn 2) 
                     (begin (setq _ak2 (argFetch 2))
                        (if (> _tkn 3) 
                            (begin (setq _ak3 (argFetch 3))
                               (if (> _tkn 4) 
                                   (begin (setq _ak4 (argFetch 4))
                                      (if (> _tkn 5) 
                                          (begin (setq _ak5 (argFetch 5))
                                             (if (> _tkn 6) 
                                                 (begin (setq _ak6 (argFetch 6))
                                                    (if (> _tkn 7) 
                                                        (begin (setq _ak7 (argFetch 7))
                                                           (if (> _tkn 8) 
                                                               (begin (setq _ak8 (argFetch 8))
                                                               (if (> _tkn 9) (setq _ak9 (argFetch 9))
                                                           ))))))))))))))))))) ; end argument collection
   ;; The default structure is given the named rule attribute
   (setq _tk0 (new Structure: LETTRUNCATE: true))
   (setq _tkn 0)
   (setq _oldIp (copy _ip))
   (setq _repeatSW true)

   (if (= _verboseSemCount.LETTRUNCATE #void) 
          (setq _verboseSemCount.LETTRUNCATE 1) 
          (setq _verboseSemCount.LETTRUNCATE (iadd _verboseSemCount.LETTRUNCATE 1)))

   (if (and (<> _verboseSemIn.LETTRUNCATE #void) (> _verboseSemIn.LETTRUNCATE -1))
       (begin (setq _verboseSave _verbose) (setq _verbose true))) 

   ;; Save the old token pointer and test for user defined rules
   Skip::         
     (setq _indent (iadd _indent 1))
     (if _verbose (_logLine "Attempting LETTRUNCATE Rule on: " source:))
     (setq _ip0 (copy _ip))
     (++ _tkn)
     (setq _repeatSW false)

     ;; *****************************************
     ;; Begin testing for each user defined rule.
     ;; *****************************************








     ;; ====================
     ;; case: [
     ;; ====================
     (if (if (isPair (setq _tk1 (_getToken))) (_pushIp) (_lastIp))
       (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 (copy _ip))
         ;; ====================
         ;; case: REFTRUNCATE
         ;; ====================
         (if (if (<> (setq _tk2 (_SEMRULE_REFTRUNCATE)) morphFail)true (setq _ip _ip1))
           (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 (copy _ip))
             ;; ====================
             ;; case: LET
             ;; ====================
             (if (if (<> (setq _tk3 (_SEMRULE_LET)) morphFail)true (setq _ip _ip2))
               (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 (copy _ip))
                 ;; ====================
                 ;; case: ]
                 ;; ====================
                 (if (if (_eofToken) (_popIp) false)
                   (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 (copy _ip))
                     ;; *********************************************************
                     ;; RULE: LETTRUNCATE: [ REFTRUNCATE LET ] :: $3 ::
                     ;; *********************************************************
                     (if true
                      (begin
                        (setq _ret  _tk3 )
                        (if _verbose 
                            (writeRule
                                 {LETTRUNCATE: [ REFTRUNCATE LET ] :: $3 ::}
                                 _ret  _tk0 _tk1 _tk2 _tk3 _tk4 #void #void #void #void #void #void))
(setq _indent (isub _indent))
                        (return _ret))
                     ) ; end case : _default
                   ) ; end begin
                 ) ; end case : ]
               ) ; end begin
             ) ; end case : LET
           ) ; end begin
         ) ; end case : REFTRUNCATE
       ) ; end begin
     ) ; end case : [
     ;; **************************************************
     ;; DEFAULT (if we get to here we failed to recognize)
     ;; Note:       (at least on this repetition)
     ;; **************************************************
     (begin
        (if (= _tkn 1)
            ;; If we have never recognized any token on previous repetitions 
            (begin
               (setq _ip _oldIp)     
               (if _verbose (_logLine "Rejecting Rule LETTRUNCATE on: " source:))
               (setq _indent (isub _indent 1))
               (setq _ret morphFail)
               ) ; end begin then
            else
            ;; If we have recognized some tokens on previous repetitions, then return $0
            ;; Note: In Semantic Rules, we do not fail the rule if it has 
            ;;       recognized a token in one or more repetitions of the rule. 
            (begin
               (setq _ip _ip0)
               (setq _ret _tk0)
               ) ; end begin else
            ) ; end if
        ) ; end of DEFAULT

   ;; End of tests for user defined rules.

   (if (> _verboseSemIn.LETTRUNCATE -1)
       (begin
          (setq _verbose _verboseSave)
          (if (>= _verboseSemCount.LETTRUNCATE _verboseSemIn.LETTRUNCATE) (error "Count" "in Routine LETTRUNCATE"))
       ))
   ;; Repeat or return to caller is handled here.
   _ret) ;; end _SEMRULE_LETTRUNCATE















;;**EXPORTKEY**:esm:selector:_SEMRULE_MAIN
;; ************************************************
;; MAIN user defined Semantic Rule implementation
;; Summary: This Lambda implements the MAIN
;;          user defined rule. Each rule test is
;;          marked with a boxed comment line for
;;          ease of human understanding.
;; Note: This code was machine generated by ParseLib.
;; ************************************************
(defchild esm.selector _SEMRULE_MAIN(...)
   ;; The token object _io is a persistent variable
   ;; The token pointer _ip is a persistent variable
   ;; All getToken logic increments _ip on each token fetch.
   ;; The return value from the rule is stored in _ret.
   ;; Remember that this rule may repeat depending on _repeatSW.
   vars:(_tk0 _tk1 _tk2 _tk3 _tk4  _tk5 _i
         _tk6 _tk7 _tk8 _tk9 _tkn  _repeatSW
         _ak0 _ak1 _ak2 _ak3 _ak4  _ak5 _tkthis
         _ak6 _ak7 _ak8 _ak9 _ret  _oldIp
         _ip0 _ip1 _ip2 _ip3 _ip4  _ip5
         _ip6 _ip7 _ip8 _ip9 _tkn  
         _verboseSave       
        ) ; end temporary variables
   ;; Collect any arguments which may have been passed to this rule
   (if (> (setq _tkn (argCount)) 0)
       (begin (setq _ak0 (argFetch 0))
          (if (> _tkn 1) 
              (begin (setq _ak1 (argFetch 1))
                 (if (> _tkn 2) 
                     (begin (setq _ak2 (argFetch 2))
                        (if (> _tkn 3) 
                            (begin (setq _ak3 (argFetch 3))
                               (if (> _tkn 4) 
                                   (begin (setq _ak4 (argFetch 4))
                                      (if (> _tkn 5) 
                                          (begin (setq _ak5 (argFetch 5))
                                             (if (> _tkn 6) 
                                                 (begin (setq _ak6 (argFetch 6))
                                                    (if (> _tkn 7) 
                                                        (begin (setq _ak7 (argFetch 7))
                                                           (if (> _tkn 8) 
                                                               (begin (setq _ak8 (argFetch 8))
                                                               (if (> _tkn 9) (setq _ak9 (argFetch 9))
                                                           ))))))))))))))))))) ; end argument collection
   ;; The default structure is given the named rule attribute
   (setq _tk0 (new Structure: MAIN: true))
   (setq _tkn 0)
   (setq _oldIp (copy _ip))
   (setq _repeatSW true)

   (if (= _verboseSemCount.MAIN #void) 
          (setq _verboseSemCount.MAIN 1) 
          (setq _verboseSemCount.MAIN (iadd _verboseSemCount.MAIN 1)))

   (if (and (<> _verboseSemIn.MAIN #void) (> _verboseSemIn.MAIN -1))
       (begin (setq _verboseSave _verbose) (setq _verbose true))) 

   ;; Save the old token pointer and test for user defined rules
   Skip::         
     (setq _indent (iadd _indent 1))
     (if _verbose (_logLine "Attempting MAIN Rule on: " source:))
     (setq _ip0 (copy _ip))
     (++ _tkn)
     (setq _repeatSW false)

     ;; *****************************************
     ;; Begin testing for each user defined rule.
     ;; *****************************************








     ;; ====================
     ;; case: [
     ;; ====================
     (if (if (isPair (setq _tk1 (_getToken))) (_pushIp) (_lastIp))
       (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 (copy _ip))
         ;; ====================
         ;; case: Any
         ;; ====================
         (if (if (<> (setq _tk2 (_getToken)) morphFail) true (setq _ip _ip1))
           (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 (copy _ip))
             ;; ====================
             ;; case: ]
             ;; ====================
             (if (if (_eofToken) (_popIp) false)
               (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 (copy _ip))
                 ;; ====================
                 ;; case: ]
                 ;; ====================
                 (if (if (_eofToken) (_popIp) false)
                   (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 (copy _ip))
                     ;; *********************************************************
                     ;; RULE: MAIN: [ Any ] ] :: $2 ::
                     ;; *********************************************************
                     (if true
                      (begin
                        (setq _ret  _tk2 )
                        (if _verbose 
                            (writeRule
                                 {MAIN: [ Any ] ] :: $2 ::}
                                 _ret  _tk0 _tk1 _tk2 _tk3 _tk4 #void #void #void #void #void #void))
(setq _indent (isub _indent))
                        (return _ret))
                     ) ; end case : _default
                   ) ; end begin
                 ) ; end case : ]
               ) ; end begin
             ) ; end case : ]
           ) ; end begin
         ) ; end case : Any
         ;; ====================
         ;; case: Any{(= _tk2 |Any*|:)}
         ;; ====================
         (if (begin (setq _ip _ip1)
          (and (if (<> (setq _tk2 (_getToken)) morphFail) true (setq _ip _ip1)) (= _tk2 |Any*|:)))
           (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 (copy _ip))
             ;; ====================
             ;; case: ]
             ;; ====================
             (if (if (_eofToken) (_popIp) false)
               (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 (copy _ip))
                 ;; ====================
                 ;; case: ]
                 ;; ====================
                 (if (if (_eofToken) (_popIp) false)
                   (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 (copy _ip))
                     ;; *********************************************************
                     ;; RULE: MAIN: [ Any{(= _tk2 |Any*|:)} ] ] :: (objectToList $2) ::
                     ;; *********************************************************
                     (if true
                      (begin
                        (setq _ret  (objectToList _tk2) )
                        (if _verbose 
                            (writeRule
                                 {MAIN: [ Any{(= _tk2 |Any*|:)} ] ] :: (objectToList $2) ::}
                                 _ret  _tk0 _tk1 _tk2 _tk3 _tk4 #void #void #void #void #void #void))
(setq _indent (isub _indent))
                        (return _ret))
                     ) ; end case : _default
                   ) ; end begin
                 ) ; end case : ]
               ) ; end begin
             ) ; end case : ]
           ) ; end begin
         ) ; end case : Any{(= _tk2 |Any*|:)}
       ) ; end begin
     ) ; end case : [
     ;; ====================
     ;; case: Any{(= _tk1 "begin")}
     ;; ====================
     (if (begin (setq _ip _ip0)
      (and (if (<> (setq _tk1 (_getToken)) morphFail) true (setq _ip _ip0)) (= _tk1 "begin")))
       (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 (copy _ip))
         ;; ====================
         ;; case: Any
         ;; ====================
         (if (if (<> (setq _tk2 (_getToken)) morphFail) true (setq _ip _ip1))
           (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 (copy _ip))
             ;; ====================
             ;; case: ]
             ;; ====================
             (if (if (_eofToken) (_popIp) false)
               (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 (copy _ip))
                 ;; *********************************************************
                 ;; RULE: MAIN: Any{(= _tk1 "begin")} Any ] :: $2 ::
                 ;; *********************************************************
                 (if true
                  (begin
                    (setq _ret  _tk2 )
                    (if _verbose 
                        (writeRule
                             {MAIN: Any{(= _tk1 "begin")} Any ] :: $2 ::}
                             _ret  _tk0 _tk1 _tk2 _tk3 #void #void #void #void #void #void #void))
(setq _indent (isub _indent))
                    (return _ret))
                 ) ; end case : _default
               ) ; end begin
             ) ; end case : ]
           ) ; end begin
         ) ; end case : Any
         ;; ====================
         ;; case: TRUNCATE
         ;; ====================
         (if (begin (setq _ip _ip1)
          (if (<> (setq _tk2 (_SEMRULE_TRUNCATE)) morphFail)true (setq _ip _ip1)))
           (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 (copy _ip))
             ;; ====================
             ;; case: [
             ;; ====================
             (if (if (isPair (setq _tk3 (_getToken))) (_pushIp) (_lastIp))
               (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 (copy _ip))
                 ;; ====================
                 ;; case: Any{(= _tk4 "begin")}
                 ;; ====================
                 (if (and (if (<> (setq _tk4 (_getToken)) morphFail) true (setq _ip _ip3)) (= _tk4 "begin"))
                   (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 (copy _ip))
                     ;; ====================
                     ;; case: TRUNCATE
                     ;; ====================
                     (if (if (<> (setq _tk5 (_SEMRULE_TRUNCATE)) morphFail)true (setq _ip _ip4))
                       (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip5 (copy _ip))
                         ;; ====================
                         ;; case: Any
                         ;; ====================
                         (if (if (<> (setq _tk6 (_getToken)) morphFail) true (setq _ip _ip5))
                           (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip6 (copy _ip))
                             ;; ====================
                             ;; case: Any
                             ;; ====================
                             (if (if (<> (setq _tk7 (_getToken)) morphFail) true (setq _ip _ip6))
                               (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip7 (copy _ip))
                                 ;; ====================
                                 ;; case: ]
                                 ;; ====================
                                 (if (if (_eofToken) (_popIp) false)
                                   (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip8 (copy _ip))
                                     ;; ====================
                                     ;; case: ]
                                     ;; ====================
                                     (if (if (_eofToken) (_popIp) false)
                                       (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip9 (copy _ip))
                                         ;; *********************************************************
                                         ;; RULE: MAIN: Any{(= _tk1 "begin")} TRUNCATE [ Any{(= _tk4 "begin")} TRUNCATE Any Any ] ] :: (list (symbol "begin")                                           (list                                               '(ref XT truncate:)                                               (list lambda: '(xv) '(onError (lambda(s) false)) (list and: $2 $5)))                                           $6 $7)                                      ::
                                         ;; *********************************************************
                                         (if true
                                          (begin
                                            (setq _ret  (list (symbol "begin")                                           (list                                               '(ref XT truncate:)                                               (list lambda: '(xv) '(onError (lambda(s) false)) (list and: _tk2 _tk5)))                                           _tk6 _tk7)                                      )
                                            (if _verbose 
                                                (writeRule
                                                     {MAIN: Any{(= _tk1 "begin")} TRUNCATE [ Any{(= _tk4 "begin")} TRUNCATE Any Any ] ] :: (list (symbol "begin")                                           (list                                               '(ref XT truncate:)                                               (list lambda: '(xv) '(onError (lambda(s) false)) (list and: $2 $5)))                                           $6 $7)                                      ::}
                                                     _ret  _tk0 _tk1 _tk2 _tk3 _tk4 _tk5 _tk6 _tk7 _tk8 _tk9 #void))
(setq _indent (isub _indent))
                                            (return _ret))
                                         ) ; end case : _default
                                       ) ; end begin
                                     ) ; end case : ]
                                   ) ; end begin
                                 ) ; end case : ]
                               ) ; end begin
                             ) ; end case : Any
                           ) ; end begin
                         ) ; end case : Any
                       ) ; end begin
                     ) ; end case : TRUNCATE
                   ) ; end begin
                 ) ; end case : Any{(= _tk4 "begin")}
               ) ; end begin
             ) ; end case : [
             ;; ====================
             ;; case: TRUNCATE
             ;; ====================
             (if (begin (setq _ip _ip2)
              (if (<> (setq _tk3 (_SEMRULE_TRUNCATE)) morphFail)true (setq _ip _ip2)))
               (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 (copy _ip))
                 ;; ====================
                 ;; case: ]
                 ;; ====================
                 (if (if (_eofToken) (_popIp) false)
                   (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 (copy _ip))
                     ;; *********************************************************
                     ;; RULE: MAIN: Any{(= _tk1 "begin")} TRUNCATE TRUNCATE ] :: (list                                            '(ref XT truncate:)                                            (list lambda: '(xv) '(onError (lambda(s) false)) (list and: $2 $3)))                                      ::
                     ;; *********************************************************
                     (if true
                      (begin
                        (setq _ret  (list                                            '(ref XT truncate:)                                            (list lambda: '(xv) '(onError (lambda(s) false)) (list and: _tk2 _tk3)))                                      )
                        (if _verbose 
                            (writeRule
                                 {MAIN: Any{(= _tk1 "begin")} TRUNCATE TRUNCATE ] :: (list                                            '(ref XT truncate:)                                            (list lambda: '(xv) '(onError (lambda(s) false)) (list and: $2 $3)))                                      ::}
                                 _ret  _tk0 _tk1 _tk2 _tk3 _tk4 #void #void #void #void #void #void))
(setq _indent (isub _indent))
                        (return _ret))
                     ) ; end case : _default
                   ) ; end begin
                 ) ; end case : ]
               ) ; end begin
             ) ; end case : TRUNCATE
           ) ; end begin
         ) ; end case : TRUNCATE
         ;; ====================
         ;; case: LETTRUNCATE
         ;; ====================
         (if (begin (setq _ip _ip1)
          (if (<> (setq _tk2 (_SEMRULE_LETTRUNCATE)) morphFail)true (setq _ip _ip1)))
           (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 (copy _ip))
             ;; ====================
             ;; case: [
             ;; ====================
             (if (if (isPair (setq _tk3 (_getToken))) (_pushIp) (_lastIp))
               (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 (copy _ip))
                 ;; ====================
                 ;; case: Any{(= _tk4 "begin")}
                 ;; ====================
                 (if (and (if (<> (setq _tk4 (_getToken)) morphFail) true (setq _ip _ip3)) (= _tk4 "begin"))
                   (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 (copy _ip))
                     ;; ====================
                     ;; case: LETTRUNCATE
                     ;; ====================
                     (if (if (<> (setq _tk5 (_SEMRULE_LETTRUNCATE)) morphFail)true (setq _ip _ip4))
                       (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip5 (copy _ip))
                         ;; ====================
                         ;; case: Any
                         ;; ====================
                         (if (if (<> (setq _tk6 (_getToken)) morphFail) true (setq _ip _ip5))
                           (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip6 (copy _ip))
                             ;; ====================
                             ;; case: Any
                             ;; ====================
                             (if (if (<> (setq _tk7 (_getToken)) morphFail) true (setq _ip _ip6))
                               (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip7 (copy _ip))
                                 ;; ====================
                                 ;; case: ]
                                 ;; ====================
                                 (if (if (_eofToken) (_popIp) false)
                                   (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip8 (copy _ip))
                                     ;; ====================
                                     ;; case: ]
                                     ;; ====================
                                     (if (if (_eofToken) (_popIp) false)
                                       (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip9 (copy _ip))
                                         ;; *********************************************************
                                         ;; RULE: MAIN: Any{(= _tk1 "begin")} LETTRUNCATE [ Any{(= _tk4 "begin")} LETTRUNCATE Any Any ] ] :: (list (symbol "begin")                                           (list                                               '(ref XT truncate:)                                               (list (symbol "let") (ref $2 0)                                                 (list lambda: '(xv) '(onError (lambda(s) false))                                                                       (list and: (ref $2 1) (ref $5 1)))))                                            $6 $7)                                      ::
                                         ;; *********************************************************
                                         (if true
                                          (begin
                                            (setq _ret  (list (symbol "begin")                                           (list                                               '(ref XT truncate:)                                               (list (symbol "let") (ref _tk2 0)                                                 (list lambda: '(xv) '(onError (lambda(s) false))                                                                       (list and: (ref _tk2 1) (ref _tk5 1)))))                                            _tk6 _tk7)                                      )
                                            (if _verbose 
                                                (writeRule
                                                     {MAIN: Any{(= _tk1 "begin")} LETTRUNCATE [ Any{(= _tk4 "begin")} LETTRUNCATE Any Any ] ] :: (list (symbol "begin")                                           (list                                               '(ref XT truncate:)                                               (list (symbol "let") (ref $2 0)                                                 (list lambda: '(xv) '(onError (lambda(s) false))                                                                       (list and: (ref $2 1) (ref $5 1)))))                                            $6 $7)                                      ::}
                                                     _ret  _tk0 _tk1 _tk2 _tk3 _tk4 _tk5 _tk6 _tk7 _tk8 _tk9 #void))
(setq _indent (isub _indent))
                                            (return _ret))
                                         ) ; end case : _default
                                       ) ; end begin
                                     ) ; end case : ]
                                   ) ; end begin
                                 ) ; end case : ]
                               ) ; end begin
                             ) ; end case : Any
                           ) ; end begin
                         ) ; end case : Any
                       ) ; end begin
                     ) ; end case : LETTRUNCATE
                   ) ; end begin
                 ) ; end case : Any{(= _tk4 "begin")}
               ) ; end begin
             ) ; end case : [
             ;; ====================
             ;; case: LETTRUNCATE
             ;; ====================
             (if (begin (setq _ip _ip2)
              (if (<> (setq _tk3 (_SEMRULE_LETTRUNCATE)) morphFail)true (setq _ip _ip2)))
               (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 (copy _ip))
                 ;; ====================
                 ;; case: ]
                 ;; ====================
                 (if (if (_eofToken) (_popIp) false)
                   (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 (copy _ip))
                     ;; *********************************************************
                     ;; RULE: MAIN: Any{(= _tk1 "begin")} LETTRUNCATE LETTRUNCATE ] :: (list                                            '(ref XT truncate:)                                           (list (symbol "let") (ref $2 0)                                              (list lambda: '(xv) '(onError (lambda(s) false))                                                                    (list and: (ref $2 1) (ref $3 1)))))                                      ::
                     ;; *********************************************************
                     (if true
                      (begin
                        (setq _ret  (list                                            '(ref XT truncate:)                                           (list (symbol "let") (ref _tk2 0)                                              (list lambda: '(xv) '(onError (lambda(s) false))                                                                    (list and: (ref _tk2 1) (ref _tk3 1)))))                                      )
                        (if _verbose 
                            (writeRule
                                 {MAIN: Any{(= _tk1 "begin")} LETTRUNCATE LETTRUNCATE ] :: (list                                            '(ref XT truncate:)                                           (list (symbol "let") (ref $2 0)                                              (list lambda: '(xv) '(onError (lambda(s) false))                                                                    (list and: (ref $2 1) (ref $3 1)))))                                      ::}
                                 _ret  _tk0 _tk1 _tk2 _tk3 _tk4 #void #void #void #void #void #void))
(setq _indent (isub _indent))
                        (return _ret))
                     ) ; end case : _default
                   ) ; end begin
                 ) ; end case : ]
               ) ; end begin
             ) ; end case : LETTRUNCATE
           ) ; end begin
         ) ; end case : LETTRUNCATE
         ;; ====================
         ;; case: OMIT
         ;; ====================
         (if (begin (setq _ip _ip1)
          (if (<> (setq _tk2 (_SEMRULE_OMIT)) morphFail)true (setq _ip _ip1)))
           (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 (copy _ip))
             ;; ====================
             ;; case: OMIT
             ;; ====================
             (if (if (<> (setq _tk3 (_SEMRULE_OMIT)) morphFail)true (setq _ip _ip2))
               (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 (copy _ip))
                 ;; ====================
                 ;; case: ]
                 ;; ====================
                 (if (if (_eofToken) (_popIp) false)
                   (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 (copy _ip))
                     ;; *********************************************************
                     ;; RULE: MAIN: Any{(= _tk1 "begin")} OMIT OMIT ] :: (list                                            '(ref XT omit)                                            (list lambda: '(xv) '(onError (lambda(s) false)) (list or: $2 $3)))                                      ::
                     ;; *********************************************************
                     (if true
                      (begin
                        (setq _ret  (list                                            '(ref XT omit)                                            (list lambda: '(xv) '(onError (lambda(s) false)) (list or: _tk2 _tk3)))                                      )
                        (if _verbose 
                            (writeRule
                                 {MAIN: Any{(= _tk1 "begin")} OMIT OMIT ] :: (list                                            '(ref XT omit)                                            (list lambda: '(xv) '(onError (lambda(s) false)) (list or: $2 $3)))                                      ::}
                                 _ret  _tk0 _tk1 _tk2 _tk3 _tk4 #void #void #void #void #void #void))
(setq _indent (isub _indent))
                        (return _ret))
                     ) ; end case : _default
                   ) ; end begin
                 ) ; end case : ]
               ) ; end begin
             ) ; end case : OMIT
           ) ; end begin
         ) ; end case : OMIT
         ;; ====================
         ;; case: LETOMIT
         ;; ====================
         (if (begin (setq _ip _ip1)
          (if (<> (setq _tk2 (_SEMRULE_LETOMIT)) morphFail)true (setq _ip _ip1)))
           (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 (copy _ip))
             ;; ====================
             ;; case: LETOMIT
             ;; ====================
             (if (if (<> (setq _tk3 (_SEMRULE_LETOMIT)) morphFail)true (setq _ip _ip2))
               (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 (copy _ip))
                 ;; ====================
                 ;; case: ]
                 ;; ====================
                 (if (if (_eofToken) (_popIp) false)
                   (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 (copy _ip))
                     ;; *********************************************************
                     ;; RULE: MAIN: Any{(= _tk1 "begin")} LETOMIT LETOMIT ] :: (list                                            '(ref XT omit)                                           (list (symbol "let") (ref $2 0)                                              (list lambda: '(xv) '(onError (lambda(s) false))                                                                    (list or: (ref $2 1) (ref $3 1)))))                                      ::
                     ;; *********************************************************
                     (if true
                      (begin
                        (setq _ret  (list                                            '(ref XT omit)                                           (list (symbol "let") (ref _tk2 0)                                              (list lambda: '(xv) '(onError (lambda(s) false))                                                                    (list or: (ref _tk2 1) (ref _tk3 1)))))                                      )
                        (if _verbose 
                            (writeRule
                                 {MAIN: Any{(= _tk1 "begin")} LETOMIT LETOMIT ] :: (list                                            '(ref XT omit)                                           (list (symbol "let") (ref $2 0)                                              (list lambda: '(xv) '(onError (lambda(s) false))                                                                    (list or: (ref $2 1) (ref $3 1)))))                                      ::}
                                 _ret  _tk0 _tk1 _tk2 _tk3 _tk4 #void #void #void #void #void #void))
(setq _indent (isub _indent))
                        (return _ret))
                     ) ; end case : _default
                   ) ; end begin
                 ) ; end case : ]
               ) ; end begin
             ) ; end case : LETOMIT
           ) ; end begin
         ) ; end case : LETOMIT
       ) ; end begin
     ) ; end case : Any{(= _tk1 "begin")}
     ;; **************************************************
     ;; DEFAULT (if we get to here we failed to recognize)
     ;; Note:       (at least on this repetition)
     ;; **************************************************
     (begin
        (if (= _tkn 1)
            ;; If we have never recognized any token on previous repetitions 
            (begin
               (setq _ip _oldIp)     
               (if _verbose (_logLine "Rejecting Rule MAIN on: " source:))
               (setq _indent (isub _indent 1))
               (setq _ret morphFail)
               ) ; end begin then
            else
            ;; If we have recognized some tokens on previous repetitions, then return $0
            ;; Note: In Semantic Rules, we do not fail the rule if it has 
            ;;       recognized a token in one or more repetitions of the rule. 
            (begin
               (setq _ip _ip0)
               (setq _ret _tk0)
               ) ; end begin else
            ) ; end if
        ) ; end of DEFAULT

   ;; End of tests for user defined rules.

   (if (> _verboseSemIn.MAIN -1)
       (begin
          (setq _verbose _verboseSave)
          (if (>= _verboseSemCount.MAIN _verboseSemIn.MAIN) (error "Count" "in Routine MAIN"))
       ))
   ;; Repeat or return to caller is handled here.
   _ret) ;; end _SEMRULE_MAIN















;;**EXPORTKEY**:esm:selector:_SEMRULE_OMIT
;; ************************************************
;; OMIT user defined Semantic Rule implementation
;; Summary: This Lambda implements the OMIT
;;          user defined rule. Each rule test is
;;          marked with a boxed comment line for
;;          ease of human understanding.
;; Note: This code was machine generated by ParseLib.
;; ************************************************
(defchild esm.selector _SEMRULE_OMIT(...)
   ;; The token object _io is a persistent variable
   ;; The token pointer _ip is a persistent variable
   ;; All getToken logic increments _ip on each token fetch.
   ;; The return value from the rule is stored in _ret.
   ;; Remember that this rule may repeat depending on _repeatSW.
   vars:(_tk0 _tk1 _tk2 _tk3 _tk4  _tk5 _i
         _tk6 _tk7 _tk8 _tk9 _tkn  _repeatSW
         _ak0 _ak1 _ak2 _ak3 _ak4  _ak5 _tkthis
         _ak6 _ak7 _ak8 _ak9 _ret  _oldIp
         _ip0 _ip1 _ip2 _ip3 _ip4  _ip5
         _ip6 _ip7 _ip8 _ip9 _tkn  
         _verboseSave       
        ) ; end temporary variables
   ;; Collect any arguments which may have been passed to this rule
   (if (> (setq _tkn (argCount)) 0)
       (begin (setq _ak0 (argFetch 0))
          (if (> _tkn 1) 
              (begin (setq _ak1 (argFetch 1))
                 (if (> _tkn 2) 
                     (begin (setq _ak2 (argFetch 2))
                        (if (> _tkn 3) 
                            (begin (setq _ak3 (argFetch 3))
                               (if (> _tkn 4) 
                                   (begin (setq _ak4 (argFetch 4))
                                      (if (> _tkn 5) 
                                          (begin (setq _ak5 (argFetch 5))
                                             (if (> _tkn 6) 
                                                 (begin (setq _ak6 (argFetch 6))
                                                    (if (> _tkn 7) 
                                                        (begin (setq _ak7 (argFetch 7))
                                                           (if (> _tkn 8) 
                                                               (begin (setq _ak8 (argFetch 8))
                                                               (if (> _tkn 9) (setq _ak9 (argFetch 9))
                                                           ))))))))))))))))))) ; end argument collection
   ;; The default structure is given the named rule attribute
   (setq _tk0 (new Structure: OMIT: true))
   (setq _tkn 0)
   (setq _oldIp (copy _ip))
   (setq _repeatSW true)

   (if (= _verboseSemCount.OMIT #void) 
          (setq _verboseSemCount.OMIT 1) 
          (setq _verboseSemCount.OMIT (iadd _verboseSemCount.OMIT 1)))

   (if (and (<> _verboseSemIn.OMIT #void) (> _verboseSemIn.OMIT -1))
       (begin (setq _verboseSave _verbose) (setq _verbose true))) 

   ;; Save the old token pointer and test for user defined rules
   Skip::         
     (setq _indent (iadd _indent 1))
     (if _verbose (_logLine "Attempting OMIT Rule on: " source:))
     (setq _ip0 (copy _ip))
     (++ _tkn)
     (setq _repeatSW false)

     ;; *****************************************
     ;; Begin testing for each user defined rule.
     ;; *****************************************








     ;; ====================
     ;; case: [
     ;; ====================
     (if (if (isPair (setq _tk1 (_getToken))) (_pushIp) (_lastIp))
       (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 (copy _ip))
         ;; ====================
         ;; case: REFOMIT
         ;; ====================
         (if (if (<> (setq _tk2 (_SEMRULE_REFOMIT)) morphFail)true (setq _ip _ip1))
           (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 (copy _ip))
             ;; ====================
             ;; case: LAMBDA
             ;; ====================
             (if (if (<> (setq _tk3 (_SEMRULE_LAMBDA)) morphFail)true (setq _ip _ip2))
               (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 (copy _ip))
                 ;; ====================
                 ;; case: ]
                 ;; ====================
                 (if (if (_eofToken) (_popIp) false)
                   (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 (copy _ip))
                     ;; *********************************************************
                     ;; RULE: OMIT: [ REFOMIT LAMBDA ] :: $3 ::
                     ;; *********************************************************
                     (if true
                      (begin
                        (setq _ret  _tk3 )
                        (if _verbose 
                            (writeRule
                                 {OMIT: [ REFOMIT LAMBDA ] :: $3 ::}
                                 _ret  _tk0 _tk1 _tk2 _tk3 _tk4 #void #void #void #void #void #void))
(setq _indent (isub _indent))
                        (return _ret))
                     ) ; end case : _default
                   ) ; end begin
                 ) ; end case : ]
               ) ; end begin
             ) ; end case : LAMBDA
           ) ; end begin
         ) ; end case : REFOMIT
       ) ; end begin
     ) ; end case : [
     ;; **************************************************
     ;; DEFAULT (if we get to here we failed to recognize)
     ;; Note:       (at least on this repetition)
     ;; **************************************************
     (begin
        (if (= _tkn 1)
            ;; If we have never recognized any token on previous repetitions 
            (begin
               (setq _ip _oldIp)     
               (if _verbose (_logLine "Rejecting Rule OMIT on: " source:))
               (setq _indent (isub _indent 1))
               (setq _ret morphFail)
               ) ; end begin then
            else
            ;; If we have recognized some tokens on previous repetitions, then return $0
            ;; Note: In Semantic Rules, we do not fail the rule if it has 
            ;;       recognized a token in one or more repetitions of the rule. 
            (begin
               (setq _ip _ip0)
               (setq _ret _tk0)
               ) ; end begin else
            ) ; end if
        ) ; end of DEFAULT

   ;; End of tests for user defined rules.

   (if (> _verboseSemIn.OMIT -1)
       (begin
          (setq _verbose _verboseSave)
          (if (>= _verboseSemCount.OMIT _verboseSemIn.OMIT) (error "Count" "in Routine OMIT"))
       ))
   ;; Repeat or return to caller is handled here.
   _ret) ;; end _SEMRULE_OMIT















;;**EXPORTKEY**:esm:selector:_SEMRULE_REFOMIT
;; ************************************************
;; REFOMIT user defined Semantic Rule implementation
;; Summary: This Lambda implements the REFOMIT
;;          user defined rule. Each rule test is
;;          marked with a boxed comment line for
;;          ease of human understanding.
;; Note: This code was machine generated by ParseLib.
;; ************************************************
(defchild esm.selector _SEMRULE_REFOMIT(...)
   ;; The token object _io is a persistent variable
   ;; The token pointer _ip is a persistent variable
   ;; All getToken logic increments _ip on each token fetch.
   ;; The return value from the rule is stored in _ret.
   ;; Remember that this rule may repeat depending on _repeatSW.
   vars:(_tk0 _tk1 _tk2 _tk3 _tk4  _tk5 _i
         _tk6 _tk7 _tk8 _tk9 _tkn  _repeatSW
         _ak0 _ak1 _ak2 _ak3 _ak4  _ak5 _tkthis
         _ak6 _ak7 _ak8 _ak9 _ret  _oldIp
         _ip0 _ip1 _ip2 _ip3 _ip4  _ip5
         _ip6 _ip7 _ip8 _ip9 _tkn  
         _verboseSave       
        ) ; end temporary variables
   ;; Collect any arguments which may have been passed to this rule
   (if (> (setq _tkn (argCount)) 0)
       (begin (setq _ak0 (argFetch 0))
          (if (> _tkn 1) 
              (begin (setq _ak1 (argFetch 1))
                 (if (> _tkn 2) 
                     (begin (setq _ak2 (argFetch 2))
                        (if (> _tkn 3) 
                            (begin (setq _ak3 (argFetch 3))
                               (if (> _tkn 4) 
                                   (begin (setq _ak4 (argFetch 4))
                                      (if (> _tkn 5) 
                                          (begin (setq _ak5 (argFetch 5))
                                             (if (> _tkn 6) 
                                                 (begin (setq _ak6 (argFetch 6))
                                                    (if (> _tkn 7) 
                                                        (begin (setq _ak7 (argFetch 7))
                                                           (if (> _tkn 8) 
                                                               (begin (setq _ak8 (argFetch 8))
                                                               (if (> _tkn 9) (setq _ak9 (argFetch 9))
                                                           ))))))))))))))))))) ; end argument collection
   ;; The default structure is given the named rule attribute
   (setq _tk0 (new Structure: REFOMIT: true))
   (setq _tkn 0)
   (setq _oldIp (copy _ip))
   (setq _repeatSW true)

   (if (= _verboseSemCount.REFOMIT #void) 
          (setq _verboseSemCount.REFOMIT 1) 
          (setq _verboseSemCount.REFOMIT (iadd _verboseSemCount.REFOMIT 1)))

   (if (and (<> _verboseSemIn.REFOMIT #void) (> _verboseSemIn.REFOMIT -1))
       (begin (setq _verboseSave _verbose) (setq _verbose true))) 

   ;; Save the old token pointer and test for user defined rules
   Skip::         
     (setq _indent (iadd _indent 1))
     (if _verbose (_logLine "Attempting REFOMIT Rule on: " source:))
     (setq _ip0 (copy _ip))
     (++ _tkn)
     (setq _repeatSW false)

     ;; *****************************************
     ;; Begin testing for each user defined rule.
     ;; *****************************************








     ;; ====================
     ;; case: [
     ;; ====================
     (if (if (isPair (setq _tk1 (_getToken))) (_pushIp) (_lastIp))
       (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 (copy _ip))
         ;; ====================
         ;; case: Any{(= _tk2 "ref")}
         ;; ====================
         (if (and (if (<> (setq _tk2 (_getToken)) morphFail) true (setq _ip _ip1)) (= _tk2 "ref"))
           (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 (copy _ip))
             ;; ====================
             ;; case: Any{(= _tk3 "XT")}
             ;; ====================
             (if (and (if (<> (setq _tk3 (_getToken)) morphFail) true (setq _ip _ip2)) (= _tk3 "XT"))
               (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 (copy _ip))
                 ;; ====================
                 ;; case: Any{(= _tk4 "omit")}
                 ;; ====================
                 (if (and (if (<> (setq _tk4 (_getToken)) morphFail) true (setq _ip _ip3)) (= _tk4 "omit"))
                   (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 (copy _ip))
                     ;; ====================
                     ;; case: ]
                     ;; ====================
                     (if (if (_eofToken) (_popIp) false)
                       (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip5 (copy _ip))
                         ;; *********************************************************
                         ;; RULE: REFOMIT: [ Any{(= _tk2 "ref")} Any{(= _tk3 "XT")} Any{(= _tk4 "omit")} ] :: (list $2 $3 $4) ::
                         ;; *********************************************************
                         (if true
                          (begin
                            (setq _ret  (list _tk2 _tk3 _tk4) )
                            (if _verbose 
                                (writeRule
                                     {REFOMIT: [ Any{(= _tk2 "ref")} Any{(= _tk3 "XT")} Any{(= _tk4 "omit")} ] :: (list $2 $3 $4) ::}
                                     _ret  _tk0 _tk1 _tk2 _tk3 _tk4 _tk5 #void #void #void #void #void))
(setq _indent (isub _indent))
                            (return _ret))
                         ) ; end case : _default
                       ) ; end begin
                     ) ; end case : ]
                   ) ; end begin
                 ) ; end case : Any{(= _tk4 "omit")}
               ) ; end begin
             ) ; end case : Any{(= _tk3 "XT")}
           ) ; end begin
         ) ; end case : Any{(= _tk2 "ref")}
       ) ; end begin
     ) ; end case : [
     ;; **************************************************
     ;; DEFAULT (if we get to here we failed to recognize)
     ;; Note:       (at least on this repetition)
     ;; **************************************************
     (begin
        (if (= _tkn 1)
            ;; If we have never recognized any token on previous repetitions 
            (begin
               (setq _ip _oldIp)     
               (if _verbose (_logLine "Rejecting Rule REFOMIT on: " source:))
               (setq _indent (isub _indent 1))
               (setq _ret morphFail)
               ) ; end begin then
            else
            ;; If we have recognized some tokens on previous repetitions, then return $0
            ;; Note: In Semantic Rules, we do not fail the rule if it has 
            ;;       recognized a token in one or more repetitions of the rule. 
            (begin
               (setq _ip _ip0)
               (setq _ret _tk0)
               ) ; end begin else
            ) ; end if
        ) ; end of DEFAULT

   ;; End of tests for user defined rules.

   (if (> _verboseSemIn.REFOMIT -1)
       (begin
          (setq _verbose _verboseSave)
          (if (>= _verboseSemCount.REFOMIT _verboseSemIn.REFOMIT) (error "Count" "in Routine REFOMIT"))
       ))
   ;; Repeat or return to caller is handled here.
   _ret) ;; end _SEMRULE_REFOMIT















;;**EXPORTKEY**:esm:selector:_SEMRULE_REFTRUNCATE
;; ************************************************
;; REFTRUNCATE user defined Semantic Rule implementation
;; Summary: This Lambda implements the REFTRUNCATE
;;          user defined rule. Each rule test is
;;          marked with a boxed comment line for
;;          ease of human understanding.
;; Note: This code was machine generated by ParseLib.
;; ************************************************
(defchild esm.selector _SEMRULE_REFTRUNCATE(...)
   ;; The token object _io is a persistent variable
   ;; The token pointer _ip is a persistent variable
   ;; All getToken logic increments _ip on each token fetch.
   ;; The return value from the rule is stored in _ret.
   ;; Remember that this rule may repeat depending on _repeatSW.
   vars:(_tk0 _tk1 _tk2 _tk3 _tk4  _tk5 _i
         _tk6 _tk7 _tk8 _tk9 _tkn  _repeatSW
         _ak0 _ak1 _ak2 _ak3 _ak4  _ak5 _tkthis
         _ak6 _ak7 _ak8 _ak9 _ret  _oldIp
         _ip0 _ip1 _ip2 _ip3 _ip4  _ip5
         _ip6 _ip7 _ip8 _ip9 _tkn  
         _verboseSave       
        ) ; end temporary variables
   ;; Collect any arguments which may have been passed to this rule
   (if (> (setq _tkn (argCount)) 0)
       (begin (setq _ak0 (argFetch 0))
          (if (> _tkn 1) 
              (begin (setq _ak1 (argFetch 1))
                 (if (> _tkn 2) 
                     (begin (setq _ak2 (argFetch 2))
                        (if (> _tkn 3) 
                            (begin (setq _ak3 (argFetch 3))
                               (if (> _tkn 4) 
                                   (begin (setq _ak4 (argFetch 4))
                                      (if (> _tkn 5) 
                                          (begin (setq _ak5 (argFetch 5))
                                             (if (> _tkn 6) 
                                                 (begin (setq _ak6 (argFetch 6))
                                                    (if (> _tkn 7) 
                                                        (begin (setq _ak7 (argFetch 7))
                                                           (if (> _tkn 8) 
                                                               (begin (setq _ak8 (argFetch 8))
                                                               (if (> _tkn 9) (setq _ak9 (argFetch 9))
                                                           ))))))))))))))))))) ; end argument collection
   ;; The default structure is given the named rule attribute
   (setq _tk0 (new Structure: REFTRUNCATE: true))
   (setq _tkn 0)
   (setq _oldIp (copy _ip))
   (setq _repeatSW true)

   (if (= _verboseSemCount.REFTRUNCATE #void) 
          (setq _verboseSemCount.REFTRUNCATE 1) 
          (setq _verboseSemCount.REFTRUNCATE (iadd _verboseSemCount.REFTRUNCATE 1)))

   (if (and (<> _verboseSemIn.REFTRUNCATE #void) (> _verboseSemIn.REFTRUNCATE -1))
       (begin (setq _verboseSave _verbose) (setq _verbose true))) 

   ;; Save the old token pointer and test for user defined rules
   Skip::         
     (setq _indent (iadd _indent 1))
     (if _verbose (_logLine "Attempting REFTRUNCATE Rule on: " source:))
     (setq _ip0 (copy _ip))
     (++ _tkn)
     (setq _repeatSW false)

     ;; *****************************************
     ;; Begin testing for each user defined rule.
     ;; *****************************************








     ;; ====================
     ;; case: [
     ;; ====================
     (if (if (isPair (setq _tk1 (_getToken))) (_pushIp) (_lastIp))
       (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 (copy _ip))
         ;; ====================
         ;; case: Any{(= _tk2 "ref")}
         ;; ====================
         (if (and (if (<> (setq _tk2 (_getToken)) morphFail) true (setq _ip _ip1)) (= _tk2 "ref"))
           (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 (copy _ip))
             ;; ====================
             ;; case: Any{(= _tk3 "XT")}
             ;; ====================
             (if (and (if (<> (setq _tk3 (_getToken)) morphFail) true (setq _ip _ip2)) (= _tk3 "XT"))
               (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 (copy _ip))
                 ;; ====================
                 ;; case: Any{(= _tk4 "truncate")}
                 ;; ====================
                 (if (and (if (<> (setq _tk4 (_getToken)) morphFail) true (setq _ip _ip3)) (= _tk4 "truncate"))
                   (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 (copy _ip))
                     ;; ====================
                     ;; case: ]
                     ;; ====================
                     (if (if (_eofToken) (_popIp) false)
                       (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip5 (copy _ip))
                         ;; *********************************************************
                         ;; RULE: REFTRUNCATE: [ Any{(= _tk2 "ref")} Any{(= _tk3 "XT")} Any{(= _tk4 "truncate")} ] :: (list $2 $3 $4) ::
                         ;; *********************************************************
                         (if true
                          (begin
                            (setq _ret  (list _tk2 _tk3 _tk4) )
                            (if _verbose 
                                (writeRule
                                     {REFTRUNCATE: [ Any{(= _tk2 "ref")} Any{(= _tk3 "XT")} Any{(= _tk4 "truncate")} ] :: (list $2 $3 $4) ::}
                                     _ret  _tk0 _tk1 _tk2 _tk3 _tk4 _tk5 #void #void #void #void #void))
(setq _indent (isub _indent))
                            (return _ret))
                         ) ; end case : _default
                       ) ; end begin
                     ) ; end case : ]
                   ) ; end begin
                 ) ; end case : Any{(= _tk4 "truncate")}
               ) ; end begin
             ) ; end case : Any{(= _tk3 "XT")}
           ) ; end begin
         ) ; end case : Any{(= _tk2 "ref")}
       ) ; end begin
     ) ; end case : [
     ;; **************************************************
     ;; DEFAULT (if we get to here we failed to recognize)
     ;; Note:       (at least on this repetition)
     ;; **************************************************
     (begin
        (if (= _tkn 1)
            ;; If we have never recognized any token on previous repetitions 
            (begin
               (setq _ip _oldIp)     
               (if _verbose (_logLine "Rejecting Rule REFTRUNCATE on: " source:))
               (setq _indent (isub _indent 1))
               (setq _ret morphFail)
               ) ; end begin then
            else
            ;; If we have recognized some tokens on previous repetitions, then return $0
            ;; Note: In Semantic Rules, we do not fail the rule if it has 
            ;;       recognized a token in one or more repetitions of the rule. 
            (begin
               (setq _ip _ip0)
               (setq _ret _tk0)
               ) ; end begin else
            ) ; end if
        ) ; end of DEFAULT

   ;; End of tests for user defined rules.

   (if (> _verboseSemIn.REFTRUNCATE -1)
       (begin
          (setq _verbose _verboseSave)
          (if (>= _verboseSemCount.REFTRUNCATE _verboseSemIn.REFTRUNCATE) (error "Count" "in Routine REFTRUNCATE"))
       ))
   ;; Repeat or return to caller is handled here.
   _ret) ;; end _SEMRULE_REFTRUNCATE















;;**EXPORTKEY**:esm:selector:_SEMRULE_TRUNCATE
;; ************************************************
;; TRUNCATE user defined Semantic Rule implementation
;; Summary: This Lambda implements the TRUNCATE
;;          user defined rule. Each rule test is
;;          marked with a boxed comment line for
;;          ease of human understanding.
;; Note: This code was machine generated by ParseLib.
;; ************************************************
(defchild esm.selector _SEMRULE_TRUNCATE(...)
   ;; The token object _io is a persistent variable
   ;; The token pointer _ip is a persistent variable
   ;; All getToken logic increments _ip on each token fetch.
   ;; The return value from the rule is stored in _ret.
   ;; Remember that this rule may repeat depending on _repeatSW.
   vars:(_tk0 _tk1 _tk2 _tk3 _tk4  _tk5 _i
         _tk6 _tk7 _tk8 _tk9 _tkn  _repeatSW
         _ak0 _ak1 _ak2 _ak3 _ak4  _ak5 _tkthis
         _ak6 _ak7 _ak8 _ak9 _ret  _oldIp
         _ip0 _ip1 _ip2 _ip3 _ip4  _ip5
         _ip6 _ip7 _ip8 _ip9 _tkn  
         _verboseSave       
        ) ; end temporary variables
   ;; Collect any arguments which may have been passed to this rule
   (if (> (setq _tkn (argCount)) 0)
       (begin (setq _ak0 (argFetch 0))
          (if (> _tkn 1) 
              (begin (setq _ak1 (argFetch 1))
                 (if (> _tkn 2) 
                     (begin (setq _ak2 (argFetch 2))
                        (if (> _tkn 3) 
                            (begin (setq _ak3 (argFetch 3))
                               (if (> _tkn 4) 
                                   (begin (setq _ak4 (argFetch 4))
                                      (if (> _tkn 5) 
                                          (begin (setq _ak5 (argFetch 5))
                                             (if (> _tkn 6) 
                                                 (begin (setq _ak6 (argFetch 6))
                                                    (if (> _tkn 7) 
                                                        (begin (setq _ak7 (argFetch 7))
                                                           (if (> _tkn 8) 
                                                               (begin (setq _ak8 (argFetch 8))
                                                               (if (> _tkn 9) (setq _ak9 (argFetch 9))
                                                           ))))))))))))))))))) ; end argument collection
   ;; The default structure is given the named rule attribute
   (setq _tk0 (new Structure: TRUNCATE: true))
   (setq _tkn 0)
   (setq _oldIp (copy _ip))
   (setq _repeatSW true)

   (if (= _verboseSemCount.TRUNCATE #void) 
          (setq _verboseSemCount.TRUNCATE 1) 
          (setq _verboseSemCount.TRUNCATE (iadd _verboseSemCount.TRUNCATE 1)))

   (if (and (<> _verboseSemIn.TRUNCATE #void) (> _verboseSemIn.TRUNCATE -1))
       (begin (setq _verboseSave _verbose) (setq _verbose true))) 

   ;; Save the old token pointer and test for user defined rules
   Skip::         
     (setq _indent (iadd _indent 1))
     (if _verbose (_logLine "Attempting TRUNCATE Rule on: " source:))
     (setq _ip0 (copy _ip))
     (++ _tkn)
     (setq _repeatSW false)

     ;; *****************************************
     ;; Begin testing for each user defined rule.
     ;; *****************************************








     ;; ====================
     ;; case: [
     ;; ====================
     (if (if (isPair (setq _tk1 (_getToken))) (_pushIp) (_lastIp))
       (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 (copy _ip))
         ;; ====================
         ;; case: REFTRUNCATE
         ;; ====================
         (if (if (<> (setq _tk2 (_SEMRULE_REFTRUNCATE)) morphFail)true (setq _ip _ip1))
           (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 (copy _ip))
             ;; ====================
             ;; case: LAMBDA
             ;; ====================
             (if (if (<> (setq _tk3 (_SEMRULE_LAMBDA)) morphFail)true (setq _ip _ip2))
               (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 (copy _ip))
                 ;; ====================
                 ;; case: ]
                 ;; ====================
                 (if (if (_eofToken) (_popIp) false)
                   (begin (setq _ruleCount (+ _ruleCount 1)) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 (copy _ip))
                     ;; *********************************************************
                     ;; RULE: TRUNCATE: [ REFTRUNCATE LAMBDA ] :: $3 ::
                     ;; *********************************************************
                     (if true
                      (begin
                        (setq _ret  _tk3 )
                        (if _verbose 
                            (writeRule
                                 {TRUNCATE: [ REFTRUNCATE LAMBDA ] :: $3 ::}
                                 _ret  _tk0 _tk1 _tk2 _tk3 _tk4 #void #void #void #void #void #void))
(setq _indent (isub _indent))
                        (return _ret))
                     ) ; end case : _default
                   ) ; end begin
                 ) ; end case : ]
               ) ; end begin
             ) ; end case : LAMBDA
           ) ; end begin
         ) ; end case : REFTRUNCATE
       ) ; end begin
     ) ; end case : [
     ;; **************************************************
     ;; DEFAULT (if we get to here we failed to recognize)
     ;; Note:       (at least on this repetition)
     ;; **************************************************
     (begin
        (if (= _tkn 1)
            ;; If we have never recognized any token on previous repetitions 
            (begin
               (setq _ip _oldIp)     
               (if _verbose (_logLine "Rejecting Rule TRUNCATE on: " source:))
               (setq _indent (isub _indent 1))
               (setq _ret morphFail)
               ) ; end begin then
            else
            ;; If we have recognized some tokens on previous repetitions, then return $0
            ;; Note: In Semantic Rules, we do not fail the rule if it has 
            ;;       recognized a token in one or more repetitions of the rule. 
            (begin
               (setq _ip _ip0)
               (setq _ret _tk0)
               ) ; end begin else
            ) ; end if
        ) ; end of DEFAULT

   ;; End of tests for user defined rules.

   (if (> _verboseSemIn.TRUNCATE -1)
       (begin
          (setq _verbose _verboseSave)
          (if (>= _verboseSemCount.TRUNCATE _verboseSemIn.TRUNCATE) (error "Count" "in Routine TRUNCATE"))
       ))
   ;; Repeat or return to caller is handled here.
   _ret) ;; end _SEMRULE_TRUNCATE















;;**EXPORTKEY**:esm:selector:_SYNRULE_ARGLIST
;; ************************************************
;; ARGLIST user defined Syntax Rule implementation
;; Summary: This Lambda implements the ARGLIST
;;          user defined rule. Each rule test is
;;          marked with a boxed comment line for
;;          ease of human understanding.
;; Note: This code was machine generated by ParseLib.
;; ************************************************
(defchild esm.selector _SYNRULE_ARGLIST(...)
   ;; The token object _io is a persistent variable
   ;; The token pointer _ip is a persistent variable
   ;; All getToken logic increments _ip on each token fetch.
   ;; The return value from the rule is stored in _ret.
   ;; Remember that this rule may repeat depending on _repeatSW.
   vars:(_tk0 _tk1 _tk2 _tk3 _tk4  _tk5  _i
         _tk6 _tk7 _tk8 _tk9 _tkn  _repeatSW
         _ak0 _ak1 _ak2 _ak3 _ak4  _ak5  _tkthis
         _ak6 _ak7 _ak8 _ak9 _ret  _oldIp
         _ip0 _ip1 _ip2 _ip3 _ip4  _ip5
         _ip6 _ip7 _ip8 _ip9 _tkn  
         _verboseSave       
        ) ; end temporary variables
   ;; Collect any arguments which may have been passed to this rule
   (if (> (setq _tkn (argCount)) 0)
       (begin (setq _ak0 (argFetch 0))
          (if (> _tkn 1) 
              (begin (setq _ak1 (argFetch 1))
                 (if (> _tkn 2) 
                     (begin (setq _ak2 (argFetch 2))
                        (if (> _tkn 3) 
                            (begin (setq _ak3 (argFetch 3))
                               (if (> _tkn 4) 
                                   (begin (setq _ak4 (argFetch 4))
                                      (if (> _tkn 5) 
                                          (begin (setq _ak5 (argFetch 5))
                                             (if (> _tkn 6) 
                                                 (begin (setq _ak6 (argFetch 6))
                                                    (if (> _tkn 7) 
                                                        (begin (setq _ak7 (argFetch 7))
                                                           (if (> _tkn 8) 
                                                               (begin (setq _ak8 (argFetch 8))
                                                               (if (> _tkn 9) (setq _ak9 (argFetch 9))
                                                           ))))))))))))))))))) ; end argument collection
   ;; The default structure is given the named rule attribute
   (setq _tk0 (new Structure: ARGLIST: true  ))
   (setq _tkn 0)
   (setq _oldIp _ip)
   (setq _repeatSW true)

   (if (= _verboseSynCount.ARGLIST #void) 
          (setq _verboseSynCount.ARGLIST 1) 
          (setq _verboseSynCount.ARGLIST (iadd _verboseSynCount.ARGLIST 1)))

   (if (and (<> _verboseSynIn.ARGLIST #void) (> _verboseSynIn.ARGLIST -1))
       (begin (setq _verboseSave _verbose) (setq _verbose true))) 

   ;; Save the old token pointer and test for user defined rules
   Skip::
     (setq _indent (iadd _indent 1))
     (if _verbose (_logLine "Attempting ARGLIST Rule on: " source:))
     (setq _ip0 _ip)
     (++ _tkn)
     (setq _repeatSW false)

     ;; *****************************************
     ;; Begin testing for each user defined rule.
     ;; *****************************************

     ;; ====================
     ;; case: RightParen
     ;; ====================
     (if (if (<> (setq _tk1 (_getToken))[RightParen:] #void) true (setq _ip _ip0))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; *********************************************************
         ;; RULE: ARGLIST: RightParen :: (setq $0.Value #void) ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ret  (setq _tk0.Value #void) )
            (if _verbose 
                (writeRule
                     {ARGLIST: RightParen :: (setq $0.Value #void) ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : RightParen
     ;; ====================
     ;; case: SEXPRESSION
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_SYNRULE_SEXPRESSION)) morphFail)true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: Comma
         ;; ====================
         (if (if (<> (setq _tk2 (_getToken))[Comma:] #void) true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: ARGLIST
             ;; ====================
             (if (if (<> (setq _tk3 (_SYNRULE_ARGLIST)) morphFail)true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; *********************************************************
                 ;; RULE: ARGLIST: SEXPRESSION Comma ARGLIST :: (setq $0.Value (insert $3.Value 0 $1.Value)) ::
                 ;; *********************************************************
                 (if true
                  (begin
                    (setq _ret  (setq _tk0.Value (insert _tk3.Value 0 _tk1.Value)) )
                    (if _verbose 
                        (writeRule
                             {ARGLIST: SEXPRESSION Comma ARGLIST :: (setq $0.Value (insert $3.Value 0 $1.Value)) ::}
                             _ret  _tk0 _tk1 _tk2 _tk3 #void #void #void #void #void #void #void))
                    (setq _indent (isub _indent 1))
                    (return _ret))
                 ) ; end case : _default
               ) ; end begin
             ) ; end case : ARGLIST
           ) ; end begin
         ) ; end case : Comma
         ;; ====================
         ;; case: RightParen
         ;; ====================
         (if (begin (setq _ip _ip1)
          (if (<> (setq _tk2 (_getToken))[RightParen:] #void) true (setq _ip _ip1)))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; *********************************************************
             ;; RULE: ARGLIST: SEXPRESSION RightParen :: (setq $0.Value (new Vector: 1 $1.Value)) ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  (setq _tk0.Value (new Vector: 1 _tk1.Value)) )
                (if _verbose 
                    (writeRule
                         {ARGLIST: SEXPRESSION RightParen :: (setq $0.Value (new Vector: 1 $1.Value)) ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : RightParen
       ) ; end begin
     ) ; end case : SEXPRESSION
     ;; ====================
     ;; case: Value
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_getToken))[Value:] #void) true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; *********************************************************
         ;; RULE: ARGLIST: Value :: (_makeError "JS 137" $1.Charpos "Invalid argument list")::
         ;; *********************************************************
         (if true
          (begin
            (setq _ret  (_makeError "JS 137" _tk1.Charpos "Invalid argument list"))
            (if _verbose 
                (writeRule
                     {ARGLIST: Value :: (_makeError "JS 137" $1.Charpos "Invalid argument list")::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : Value
     ;; **************************************************
     ;; DEFAULT (if we get to here we failed to recognize)
     ;; Note:       (at least on this repetition)
     ;; **************************************************
     (begin
        (if (= _tkn 1)
            ;; If we have never recognized any token on previous repetitions 
            (begin
               (setq _ip _oldIp)     
               (if _verbose (_logLine "Rejecting Rule ARGLIST on: " source:))
               (setq _indent (isub _indent 1))
               (setq _ret morphFail)
               ) ; end begin then
            else
            ;; If we have recognized some tokens on previous repetitions, then return $0
            ;; Note: In Syntax Rules, we do not fail the rule if it has 
            ;;       recognized a token in one or more repetitions of the rule. 
            (begin
               (setq _ip _ip0)
               (setq _ret _tk0)
               ) ; end begin else
            ) ; end if
        ) ; end of DEFAULT

   ;; End of tests for user defined rules.
   
   (if (> _verboseSynIn.ARGLIST -1)
       (begin
          (setq _verbose _verboseSave)
          (if (>= _verboseSynCount.ARGLIST _verboseSynIn.ARGLIST) (error "Count" "in Routine ARGLIST"))
       ))

   ;; Repeat or return to caller is handled here.
   _ret) ;; end _SYNRULE_ARGLIST
























;;**EXPORTKEY**:esm:selector:_SYNRULE_CFCALL
;; ************************************************
;; CFCALL user defined Syntax Rule implementation
;; Summary: This Lambda implements the CFCALL
;;          user defined rule. Each rule test is
;;          marked with a boxed comment line for
;;          ease of human understanding.
;; Note: This code was machine generated by ParseLib.
;; ************************************************
(defchild esm.selector _SYNRULE_CFCALL(...)
   ;; The token object _io is a persistent variable
   ;; The token pointer _ip is a persistent variable
   ;; All getToken logic increments _ip on each token fetch.
   ;; The return value from the rule is stored in _ret.
   ;; Remember that this rule may repeat depending on _repeatSW.
   vars:(_tk0 _tk1 _tk2 _tk3 _tk4  _tk5  _i
         _tk6 _tk7 _tk8 _tk9 _tkn  _repeatSW
         _ak0 _ak1 _ak2 _ak3 _ak4  _ak5  _tkthis
         _ak6 _ak7 _ak8 _ak9 _ret  _oldIp
         _ip0 _ip1 _ip2 _ip3 _ip4  _ip5
         _ip6 _ip7 _ip8 _ip9 _tkn  
         _verboseSave       
        ) ; end temporary variables
   ;; Collect any arguments which may have been passed to this rule
   (if (> (setq _tkn (argCount)) 0)
       (begin (setq _ak0 (argFetch 0))
          (if (> _tkn 1) 
              (begin (setq _ak1 (argFetch 1))
                 (if (> _tkn 2) 
                     (begin (setq _ak2 (argFetch 2))
                        (if (> _tkn 3) 
                            (begin (setq _ak3 (argFetch 3))
                               (if (> _tkn 4) 
                                   (begin (setq _ak4 (argFetch 4))
                                      (if (> _tkn 5) 
                                          (begin (setq _ak5 (argFetch 5))
                                             (if (> _tkn 6) 
                                                 (begin (setq _ak6 (argFetch 6))
                                                    (if (> _tkn 7) 
                                                        (begin (setq _ak7 (argFetch 7))
                                                           (if (> _tkn 8) 
                                                               (begin (setq _ak8 (argFetch 8))
                                                               (if (> _tkn 9) (setq _ak9 (argFetch 9))
                                                           ))))))))))))))))))) ; end argument collection
   ;; The default structure is given the named rule attribute
   (setq _tk0 (new Structure: CFCALL: true  ))
   (setq _tkn 0)
   (setq _oldIp _ip)
   (setq _repeatSW true)

   (if (= _verboseSynCount.CFCALL #void) 
          (setq _verboseSynCount.CFCALL 1) 
          (setq _verboseSynCount.CFCALL (iadd _verboseSynCount.CFCALL 1)))

   (if (and (<> _verboseSynIn.CFCALL #void) (> _verboseSynIn.CFCALL -1))
       (begin (setq _verboseSave _verbose) (setq _verbose true))) 

   ;; Save the old token pointer and test for user defined rules
   Skip::
     (setq _indent (iadd _indent 1))
     (if _verbose (_logLine "Attempting CFCALL Rule on: " source:))
     (setq _ip0 _ip)
     (++ _tkn)
     (setq _repeatSW false)

     ;; *****************************************
     ;; Begin testing for each user defined rule.
     ;; *****************************************

     ;; ====================
     ;; case: DotOperator
     ;; ====================
     (if (if (<> (setq _tk1 (_getToken))[DotOperator:] #void) true (setq _ip _ip0))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: Name
         ;; ====================
         (if (if (<> (setq _tk2 (_getToken))[Name:] #void) true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: CFCALL((qualifyName $0 %0 $2 (makeQuotedSymbol $2.Value)))
             ;; ====================
             (if (if (<> (setq _tk3 (_SYNRULE_CFCALL (qualifyName _tk0 _ak0 _tk2 (makeQuotedSymbol _tk2.Value)))) morphFail)true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; *********************************************************
                 ;; RULE: CFCALL: DotOperator Name CFCALL((qualifyName $0 %0 $2 (makeQuotedSymbol $2.Value))) :: $3 ::
                 ;; *********************************************************
                 (if true
                  (begin
                    (setq _ret  _tk3 )
                    (if _verbose 
                        (writeRule
                             {CFCALL: DotOperator Name CFCALL((qualifyName $0 %0 $2 (makeQuotedSymbol $2.Value))) :: $3 ::}
                             _ret  _tk0 _tk1 _tk2 _tk3 #void #void #void #void #void #void #void))
                    (setq _indent (isub _indent 1))
                    (return _ret))
                 ) ; end case : _default
               ) ; end begin
             ) ; end case : CFCALL((qualifyName $0 %0 $2 (makeQuotedSymbol $2.Value)))
             ;; ====================
             ;; case: LeftParen
             ;; ====================
             (if (begin (setq _ip _ip2)
              (if (<> (setq _tk3 (_getToken))[LeftParen:] #void) true (setq _ip _ip2)))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; ====================
                 ;; case: ARGLIST
                 ;; ====================
                 (if (if (<> (setq _tk4 (_SYNRULE_ARGLIST)) morphFail)true (setq _ip _ip3))
                   (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 _ip) 
                     ;; *********************************************************
                     ;; RULE: CFCALL: DotOperator Name LeftParen ARGLIST :: (sendList $0 $2.Value %0.Value $4.Value) ::
                     ;; *********************************************************
                     (if true
                      (begin
                        (setq _ret  (sendList _tk0 _tk2.Value _ak0.Value _tk4.Value) )
                        (if _verbose 
                            (writeRule
                                 {CFCALL: DotOperator Name LeftParen ARGLIST :: (sendList $0 $2.Value %0.Value $4.Value) ::}
                                 _ret  _tk0 _tk1 _tk2 _tk3 _tk4 #void #void #void #void #void #void))
                        (setq _indent (isub _indent 1))
                        (return _ret))
                     ) ; end case : _default
                   ) ; end begin
                 ) ; end case : ARGLIST
                 ;; *********************************************************
                 ;; RULE: CFCALL: DotOperator Name LeftParen :: (_makeError "JS 117" $1.Charpos "Invalid function call") ::
                 ;; *********************************************************
                 (if true
                  (begin
                    (setq _ip _ip3)
                    (setq _ret  (_makeError "JS 117" _tk1.Charpos "Invalid function call") )
                    (if _verbose 
                        (writeRule
                             {CFCALL: DotOperator Name LeftParen :: (_makeError "JS 117" $1.Charpos "Invalid function call") ::}
                             _ret  _tk0 _tk1 _tk2 _tk3 #void #void #void #void #void #void #void))
                    (setq _indent (isub _indent 1))
                    (return _ret))
                 ) ; end case : _default
               ) ; end begin
             ) ; end case : LeftParen
           ) ; end begin
         ) ; end case : Name
         ;; ====================
         ;; case: Reserved
         ;; ====================
         (if (begin (setq _ip _ip1)
          (if (<> (setq _tk2 (_getToken))[Reserved:] #void) true (setq _ip _ip1)))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: CFCALL((qualifyName $0 %0 $2 (makeQuotedSymbol $2.Value)))
             ;; ====================
             (if (if (<> (setq _tk3 (_SYNRULE_CFCALL (qualifyName _tk0 _ak0 _tk2 (makeQuotedSymbol _tk2.Value)))) morphFail)true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; *********************************************************
                 ;; RULE: CFCALL: DotOperator Reserved CFCALL((qualifyName $0 %0 $2 (makeQuotedSymbol $2.Value))) :: $3 ::
                 ;; *********************************************************
                 (if true
                  (begin
                    (setq _ret  _tk3 )
                    (if _verbose 
                        (writeRule
                             {CFCALL: DotOperator Reserved CFCALL((qualifyName $0 %0 $2 (makeQuotedSymbol $2.Value))) :: $3 ::}
                             _ret  _tk0 _tk1 _tk2 _tk3 #void #void #void #void #void #void #void))
                    (setq _indent (isub _indent 1))
                    (return _ret))
                 ) ; end case : _default
               ) ; end begin
             ) ; end case : CFCALL((qualifyName $0 %0 $2 (makeQuotedSymbol $2.Value)))
             ;; ====================
             ;; case: LeftParen
             ;; ====================
             (if (begin (setq _ip _ip2)
              (if (<> (setq _tk3 (_getToken))[LeftParen:] #void) true (setq _ip _ip2)))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; ====================
                 ;; case: ARGLIST
                 ;; ====================
                 (if (if (<> (setq _tk4 (_SYNRULE_ARGLIST)) morphFail)true (setq _ip _ip3))
                   (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 _ip) 
                     ;; *********************************************************
                     ;; RULE: CFCALL: DotOperator Reserved LeftParen ARGLIST :: (sendList $0 $2.Value %0.Value $4.Value) ::
                     ;; *********************************************************
                     (if true
                      (begin
                        (setq _ret  (sendList _tk0 _tk2.Value _ak0.Value _tk4.Value) )
                        (if _verbose 
                            (writeRule
                                 {CFCALL: DotOperator Reserved LeftParen ARGLIST :: (sendList $0 $2.Value %0.Value $4.Value) ::}
                                 _ret  _tk0 _tk1 _tk2 _tk3 _tk4 #void #void #void #void #void #void))
                        (setq _indent (isub _indent 1))
                        (return _ret))
                     ) ; end case : _default
                   ) ; end begin
                 ) ; end case : ARGLIST
                 ;; *********************************************************
                 ;; RULE: CFCALL: DotOperator Reserved LeftParen :: (_makeError "JS 117" $1.Charpos "Invalid function call") ::
                 ;; *********************************************************
                 (if true
                  (begin
                    (setq _ip _ip3)
                    (setq _ret  (_makeError "JS 117" _tk1.Charpos "Invalid function call") )
                    (if _verbose 
                        (writeRule
                             {CFCALL: DotOperator Reserved LeftParen :: (_makeError "JS 117" $1.Charpos "Invalid function call") ::}
                             _ret  _tk0 _tk1 _tk2 _tk3 #void #void #void #void #void #void #void))
                    (setq _indent (isub _indent 1))
                    (return _ret))
                 ) ; end case : _default
               ) ; end begin
             ) ; end case : LeftParen
           ) ; end begin
         ) ; end case : Reserved
       ) ; end begin
     ) ; end case : DotOperator
     ;; **************************************************
     ;; DEFAULT (if we get to here we failed to recognize)
     ;; Note:       (at least on this repetition)
     ;; **************************************************
     (begin
        (if (= _tkn 1)
            ;; If we have never recognized any token on previous repetitions 
            (begin
               (setq _ip _oldIp)     
               (if _verbose (_logLine "Rejecting Rule CFCALL on: " source:))
               (setq _indent (isub _indent 1))
               (setq _ret morphFail)
               ) ; end begin then
            else
            ;; If we have recognized some tokens on previous repetitions, then return $0
            ;; Note: In Syntax Rules, we do not fail the rule if it has 
            ;;       recognized a token in one or more repetitions of the rule. 
            (begin
               (setq _ip _ip0)
               (setq _ret _tk0)
               ) ; end begin else
            ) ; end if
        ) ; end of DEFAULT

   ;; End of tests for user defined rules.
   
   (if (> _verboseSynIn.CFCALL -1)
       (begin
          (setq _verbose _verboseSave)
          (if (>= _verboseSynCount.CFCALL _verboseSynIn.CFCALL) (error "Count" "in Routine CFCALL"))
       ))

   ;; Repeat or return to caller is handled here.
   _ret) ;; end _SYNRULE_CFCALL
























;;**EXPORTKEY**:esm:selector:_SYNRULE_CHILD
;; ************************************************
;; CHILD user defined Syntax Rule implementation
;; Summary: This Lambda implements the CHILD
;;          user defined rule. Each rule test is
;;          marked with a boxed comment line for
;;          ease of human understanding.
;; Note: This code was machine generated by ParseLib.
;; ************************************************
(defchild esm.selector _SYNRULE_CHILD(...)
   ;; The token object _io is a persistent variable
   ;; The token pointer _ip is a persistent variable
   ;; All getToken logic increments _ip on each token fetch.
   ;; The return value from the rule is stored in _ret.
   ;; Remember that this rule may repeat depending on _repeatSW.
   vars:(_tk0 _tk1 _tk2 _tk3 _tk4  _tk5  _i
         _tk6 _tk7 _tk8 _tk9 _tkn  _repeatSW
         _ak0 _ak1 _ak2 _ak3 _ak4  _ak5  _tkthis
         _ak6 _ak7 _ak8 _ak9 _ret  _oldIp
         _ip0 _ip1 _ip2 _ip3 _ip4  _ip5
         _ip6 _ip7 _ip8 _ip9 _tkn  
         _verboseSave       
        ) ; end temporary variables
   ;; Collect any arguments which may have been passed to this rule
   (if (> (setq _tkn (argCount)) 0)
       (begin (setq _ak0 (argFetch 0))
          (if (> _tkn 1) 
              (begin (setq _ak1 (argFetch 1))
                 (if (> _tkn 2) 
                     (begin (setq _ak2 (argFetch 2))
                        (if (> _tkn 3) 
                            (begin (setq _ak3 (argFetch 3))
                               (if (> _tkn 4) 
                                   (begin (setq _ak4 (argFetch 4))
                                      (if (> _tkn 5) 
                                          (begin (setq _ak5 (argFetch 5))
                                             (if (> _tkn 6) 
                                                 (begin (setq _ak6 (argFetch 6))
                                                    (if (> _tkn 7) 
                                                        (begin (setq _ak7 (argFetch 7))
                                                           (if (> _tkn 8) 
                                                               (begin (setq _ak8 (argFetch 8))
                                                               (if (> _tkn 9) (setq _ak9 (argFetch 9))
                                                           ))))))))))))))))))) ; end argument collection
   ;; The default structure is given the named rule attribute
   (setq _tk0 (new Structure: CHILD: true  ))
   (setq _tkn 0)
   (setq _oldIp _ip)
   (setq _repeatSW true)

   (if (= _verboseSynCount.CHILD #void) 
          (setq _verboseSynCount.CHILD 1) 
          (setq _verboseSynCount.CHILD (iadd _verboseSynCount.CHILD 1)))

   (if (and (<> _verboseSynIn.CHILD #void) (> _verboseSynIn.CHILD -1))
       (begin (setq _verboseSave _verbose) (setq _verbose true))) 

   ;; Save the old token pointer and test for user defined rules
   Skip::
     (setq _indent (iadd _indent 1))
     (if _verbose (_logLine "Attempting CHILD Rule on: " source:))
     (setq _ip0 _ip)
     (++ _tkn)
     (setq _repeatSW false)

     ;; *****************************************
     ;; Begin testing for each user defined rule.
     ;; *****************************************

     ;; ====================
     ;; case: Child
     ;; ====================
     (if (if (<> (setq _tk1 (_getToken))[Child:] #void) true (setq _ip _ip0))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; *********************************************************
         ;; RULE: CHILD: Child :: (setq (pushVars $0)[Charpos:] $1.Charpos) ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ret  (setq (pushVars _tk0)[Charpos:] _tk1.Charpos) )
            (if _verbose 
                (writeRule
                     {CHILD: Child :: (setq (pushVars $0)[Charpos:] $1.Charpos) ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : Child
     ;; **************************************************
     ;; DEFAULT (if we get to here we failed to recognize)
     ;; Note:       (at least on this repetition)
     ;; **************************************************
     (begin
        (if (= _tkn 1)
            ;; If we have never recognized any token on previous repetitions 
            (begin
               (setq _ip _oldIp)     
               (if _verbose (_logLine "Rejecting Rule CHILD on: " source:))
               (setq _indent (isub _indent 1))
               (setq _ret morphFail)
               ) ; end begin then
            else
            ;; If we have recognized some tokens on previous repetitions, then return $0
            ;; Note: In Syntax Rules, we do not fail the rule if it has 
            ;;       recognized a token in one or more repetitions of the rule. 
            (begin
               (setq _ip _ip0)
               (setq _ret _tk0)
               ) ; end begin else
            ) ; end if
        ) ; end of DEFAULT

   ;; End of tests for user defined rules.
   
   (if (> _verboseSynIn.CHILD -1)
       (begin
          (setq _verbose _verboseSave)
          (if (>= _verboseSynCount.CHILD _verboseSynIn.CHILD) (error "Count" "in Routine CHILD"))
       ))

   ;; Repeat or return to caller is handled here.
   _ret) ;; end _SYNRULE_CHILD
























;;**EXPORTKEY**:esm:selector:_SYNRULE_CLASS
;; ************************************************
;; CLASS user defined Syntax Rule implementation
;; Summary: This Lambda implements the CLASS
;;          user defined rule. Each rule test is
;;          marked with a boxed comment line for
;;          ease of human understanding.
;; Note: This code was machine generated by ParseLib.
;; ************************************************
(defchild esm.selector _SYNRULE_CLASS(...)
   ;; The token object _io is a persistent variable
   ;; The token pointer _ip is a persistent variable
   ;; All getToken logic increments _ip on each token fetch.
   ;; The return value from the rule is stored in _ret.
   ;; Remember that this rule may repeat depending on _repeatSW.
   vars:(_tk0 _tk1 _tk2 _tk3 _tk4  _tk5  _i
         _tk6 _tk7 _tk8 _tk9 _tkn  _repeatSW
         _ak0 _ak1 _ak2 _ak3 _ak4  _ak5  _tkthis
         _ak6 _ak7 _ak8 _ak9 _ret  _oldIp
         _ip0 _ip1 _ip2 _ip3 _ip4  _ip5
         _ip6 _ip7 _ip8 _ip9 _tkn  
         _verboseSave       
        ) ; end temporary variables
   ;; Collect any arguments which may have been passed to this rule
   (if (> (setq _tkn (argCount)) 0)
       (begin (setq _ak0 (argFetch 0))
          (if (> _tkn 1) 
              (begin (setq _ak1 (argFetch 1))
                 (if (> _tkn 2) 
                     (begin (setq _ak2 (argFetch 2))
                        (if (> _tkn 3) 
                            (begin (setq _ak3 (argFetch 3))
                               (if (> _tkn 4) 
                                   (begin (setq _ak4 (argFetch 4))
                                      (if (> _tkn 5) 
                                          (begin (setq _ak5 (argFetch 5))
                                             (if (> _tkn 6) 
                                                 (begin (setq _ak6 (argFetch 6))
                                                    (if (> _tkn 7) 
                                                        (begin (setq _ak7 (argFetch 7))
                                                           (if (> _tkn 8) 
                                                               (begin (setq _ak8 (argFetch 8))
                                                               (if (> _tkn 9) (setq _ak9 (argFetch 9))
                                                           ))))))))))))))))))) ; end argument collection
   ;; The default structure is given the named rule attribute
   (setq _tk0 (new Structure: CLASS: true  ))
   (setq _tkn 0)
   (setq _oldIp _ip)
   (setq _repeatSW true)

   (if (= _verboseSynCount.CLASS #void) 
          (setq _verboseSynCount.CLASS 1) 
          (setq _verboseSynCount.CLASS (iadd _verboseSynCount.CLASS 1)))

   (if (and (<> _verboseSynIn.CLASS #void) (> _verboseSynIn.CLASS -1))
       (begin (setq _verboseSave _verbose) (setq _verbose true))) 

   ;; Save the old token pointer and test for user defined rules
   Skip::
     (setq _indent (iadd _indent 1))
     (if _verbose (_logLine "Attempting CLASS Rule on: " source:))
     (setq _ip0 _ip)
     (++ _tkn)
     (setq _repeatSW false)

     ;; *****************************************
     ;; Begin testing for each user defined rule.
     ;; *****************************************

     ;; ====================
     ;; case: Class
     ;; ====================
     (if (if (<> (setq _tk1 (_getToken))[Class:] #void) true (setq _ip _ip0))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; *********************************************************
         ;; RULE: CLASS: Class :: (setq (pushVars $0)[Charpos:] $1.Charpos) ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ret  (setq (pushVars _tk0)[Charpos:] _tk1.Charpos) )
            (if _verbose 
                (writeRule
                     {CLASS: Class :: (setq (pushVars $0)[Charpos:] $1.Charpos) ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : Class
     ;; **************************************************
     ;; DEFAULT (if we get to here we failed to recognize)
     ;; Note:       (at least on this repetition)
     ;; **************************************************
     (begin
        (if (= _tkn 1)
            ;; If we have never recognized any token on previous repetitions 
            (begin
               (setq _ip _oldIp)     
               (if _verbose (_logLine "Rejecting Rule CLASS on: " source:))
               (setq _indent (isub _indent 1))
               (setq _ret morphFail)
               ) ; end begin then
            else
            ;; If we have recognized some tokens on previous repetitions, then return $0
            ;; Note: In Syntax Rules, we do not fail the rule if it has 
            ;;       recognized a token in one or more repetitions of the rule. 
            (begin
               (setq _ip _ip0)
               (setq _ret _tk0)
               ) ; end begin else
            ) ; end if
        ) ; end of DEFAULT

   ;; End of tests for user defined rules.
   
   (if (> _verboseSynIn.CLASS -1)
       (begin
          (setq _verbose _verboseSave)
          (if (>= _verboseSynCount.CLASS _verboseSynIn.CLASS) (error "Count" "in Routine CLASS"))
       ))

   ;; Repeat or return to caller is handled here.
   _ret) ;; end _SYNRULE_CLASS
























;;**EXPORTKEY**:esm:selector:_SYNRULE_CUT
;; ************************************************
;; CUT user defined Syntax Rule implementation
;; Summary: This Lambda implements the CUT
;;          user defined rule. Each rule test is
;;          marked with a boxed comment line for
;;          ease of human understanding.
;; Note: This code was machine generated by ParseLib.
;; ************************************************
(defchild esm.selector _SYNRULE_CUT(...)
   ;; The token object _io is a persistent variable
   ;; The token pointer _ip is a persistent variable
   ;; All getToken logic increments _ip on each token fetch.
   ;; The return value from the rule is stored in _ret.
   ;; Remember that this rule may repeat depending on _repeatSW.
   vars:(_tk0 _tk1 _tk2 _tk3 _tk4  _tk5  _i
         _tk6 _tk7 _tk8 _tk9 _tkn  _repeatSW
         _ak0 _ak1 _ak2 _ak3 _ak4  _ak5  _tkthis
         _ak6 _ak7 _ak8 _ak9 _ret  _oldIp
         _ip0 _ip1 _ip2 _ip3 _ip4  _ip5
         _ip6 _ip7 _ip8 _ip9 _tkn  
         _verboseSave       
        ) ; end temporary variables
   ;; Collect any arguments which may have been passed to this rule
   (if (> (setq _tkn (argCount)) 0)
       (begin (setq _ak0 (argFetch 0))
          (if (> _tkn 1) 
              (begin (setq _ak1 (argFetch 1))
                 (if (> _tkn 2) 
                     (begin (setq _ak2 (argFetch 2))
                        (if (> _tkn 3) 
                            (begin (setq _ak3 (argFetch 3))
                               (if (> _tkn 4) 
                                   (begin (setq _ak4 (argFetch 4))
                                      (if (> _tkn 5) 
                                          (begin (setq _ak5 (argFetch 5))
                                             (if (> _tkn 6) 
                                                 (begin (setq _ak6 (argFetch 6))
                                                    (if (> _tkn 7) 
                                                        (begin (setq _ak7 (argFetch 7))
                                                           (if (> _tkn 8) 
                                                               (begin (setq _ak8 (argFetch 8))
                                                               (if (> _tkn 9) (setq _ak9 (argFetch 9))
                                                           ))))))))))))))))))) ; end argument collection
   ;; The default structure is given the named rule attribute
   (setq _tk0 (new Structure: CUT: true  ))
   (setq _tkn 0)
   (setq _oldIp _ip)
   (setq _repeatSW true)

   (if (= _verboseSynCount.CUT #void) 
          (setq _verboseSynCount.CUT 1) 
          (setq _verboseSynCount.CUT (iadd _verboseSynCount.CUT 1)))

   (if (and (<> _verboseSynIn.CUT #void) (> _verboseSynIn.CUT -1))
       (begin (setq _verboseSave _verbose) (setq _verbose true))) 

   ;; Save the old token pointer and test for user defined rules
   Skip::
     (setq _indent (iadd _indent 1))
     (if _verbose (_logLine "Attempting CUT Rule on: " source:))
     (setq _ip0 _ip)
     (++ _tkn)
     (setq _repeatSW false)

     ;; *****************************************
     ;; Begin testing for each user defined rule.
     ;; *****************************************

     ;; ====================
     ;; case: Number
     ;; ====================
     (if (if (<> (setq _tk1 (_getToken))[Number:] #void) true (setq _ip _ip0))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: Percent
         ;; ====================
         (if (if (<> (setq _tk2 (_getToken))[Percent:] #void) true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; *********************************************************
             ;; RULE: CUT: Number Percent :: (foldConstants $0 |/|: $1.Value 100) ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  (foldConstants _tk0 |/|: _tk1.Value 100) )
                (if _verbose 
                    (writeRule
                         {CUT: Number Percent :: (foldConstants $0 |/|: $1.Value 100) ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : Percent
         ;; *********************************************************
         ;; RULE: CUT: Number :: $1 ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ip _ip1)
            (setq _ret  _tk1 )
            (if _verbose 
                (writeRule
                     {CUT: Number :: $1 ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : Number
     ;; ====================
     ;; case: SEXPRESSION
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_SYNRULE_SEXPRESSION)) morphFail)true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; *********************************************************
         ;; RULE: CUT: SEXPRESSION :: $1 ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ret  _tk1 )
            (if _verbose 
                (writeRule
                     {CUT: SEXPRESSION :: $1 ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : SEXPRESSION
     ;; **************************************************
     ;; DEFAULT (if we get to here we failed to recognize)
     ;; Note:       (at least on this repetition)
     ;; **************************************************
     (begin
        (if (= _tkn 1)
            ;; If we have never recognized any token on previous repetitions 
            (begin
               (setq _ip _oldIp)     
               (if _verbose (_logLine "Rejecting Rule CUT on: " source:))
               (setq _indent (isub _indent 1))
               (setq _ret morphFail)
               ) ; end begin then
            else
            ;; If we have recognized some tokens on previous repetitions, then return $0
            ;; Note: In Syntax Rules, we do not fail the rule if it has 
            ;;       recognized a token in one or more repetitions of the rule. 
            (begin
               (setq _ip _ip0)
               (setq _ret _tk0)
               ) ; end begin else
            ) ; end if
        ) ; end of DEFAULT

   ;; End of tests for user defined rules.
   
   (if (> _verboseSynIn.CUT -1)
       (begin
          (setq _verbose _verboseSave)
          (if (>= _verboseSynCount.CUT _verboseSynIn.CUT) (error "Count" "in Routine CUT"))
       ))

   ;; Repeat or return to caller is handled here.
   _ret) ;; end _SYNRULE_CUT
























;;**EXPORTKEY**:esm:selector:_SYNRULE_EXPRESSION
;; ************************************************
;; EXPRESSION user defined Syntax Rule implementation
;; Summary: This Lambda implements the EXPRESSION
;;          user defined rule. Each rule test is
;;          marked with a boxed comment line for
;;          ease of human understanding.
;; Note: This code was machine generated by ParseLib.
;; ************************************************
(defchild esm.selector _SYNRULE_EXPRESSION(...)
   ;; The token object _io is a persistent variable
   ;; The token pointer _ip is a persistent variable
   ;; All getToken logic increments _ip on each token fetch.
   ;; The return value from the rule is stored in _ret.
   ;; Remember that this rule may repeat depending on _repeatSW.
   vars:(_tk0 _tk1 _tk2 _tk3 _tk4  _tk5  _i
         _tk6 _tk7 _tk8 _tk9 _tkn  _repeatSW
         _ak0 _ak1 _ak2 _ak3 _ak4  _ak5  _tkthis
         _ak6 _ak7 _ak8 _ak9 _ret  _oldIp
         _ip0 _ip1 _ip2 _ip3 _ip4  _ip5
         _ip6 _ip7 _ip8 _ip9 _tkn  
         _verboseSave       
        ) ; end temporary variables
   ;; Collect any arguments which may have been passed to this rule
   (if (> (setq _tkn (argCount)) 0)
       (begin (setq _ak0 (argFetch 0))
          (if (> _tkn 1) 
              (begin (setq _ak1 (argFetch 1))
                 (if (> _tkn 2) 
                     (begin (setq _ak2 (argFetch 2))
                        (if (> _tkn 3) 
                            (begin (setq _ak3 (argFetch 3))
                               (if (> _tkn 4) 
                                   (begin (setq _ak4 (argFetch 4))
                                      (if (> _tkn 5) 
                                          (begin (setq _ak5 (argFetch 5))
                                             (if (> _tkn 6) 
                                                 (begin (setq _ak6 (argFetch 6))
                                                    (if (> _tkn 7) 
                                                        (begin (setq _ak7 (argFetch 7))
                                                           (if (> _tkn 8) 
                                                               (begin (setq _ak8 (argFetch 8))
                                                               (if (> _tkn 9) (setq _ak9 (argFetch 9))
                                                           ))))))))))))))))))) ; end argument collection
   ;; The default structure is given the named rule attribute
   (setq _tk0 (new Structure: EXPRESSION: true  ))
   (setq _tkn 0)
   (setq _oldIp _ip)
   (setq _repeatSW true)

   (if (= _verboseSynCount.EXPRESSION #void) 
          (setq _verboseSynCount.EXPRESSION 1) 
          (setq _verboseSynCount.EXPRESSION (iadd _verboseSynCount.EXPRESSION 1)))

   (if (and (<> _verboseSynIn.EXPRESSION #void) (> _verboseSynIn.EXPRESSION -1))
       (begin (setq _verboseSave _verbose) (setq _verbose true))) 

   ;; Save the old token pointer and test for user defined rules
   Skip::
     (setq _indent (iadd _indent 1))
     (if _verbose (_logLine "Attempting EXPRESSION Rule on: " source:))
     (setq _ip0 _ip)
     (++ _tkn)
     (setq _repeatSW false)

     ;; *****************************************
     ;; Begin testing for each user defined rule.
     ;; *****************************************

     ;; ====================
     ;; case: PHRASE
     ;; ====================
     (if (if (<> (setq _tk1 (_SYNRULE_PHRASE)) morphFail)true (setq _ip _ip0))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: Operator
         ;; ====================
         (if (if (<> (setq _tk2 (_getToken))[Operator:] #void) true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: SEXPRESSION
             ;; ====================
             (if (if (<> (setq _tk3 (_SYNRULE_SEXPRESSION)) morphFail)true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; *********************************************************
                 ;; RULE: EXPRESSION: PHRASE Operator SEXPRESSION :: (setExpVType $0 $1 $2 $3) ::
                 ;; *********************************************************
                 (if true
                  (begin
                    (setq _ret  (setExpVType _tk0 _tk1 _tk2 _tk3) )
                    (if _verbose 
                        (writeRule
                             {EXPRESSION: PHRASE Operator SEXPRESSION :: (setExpVType $0 $1 $2 $3) ::}
                             _ret  _tk0 _tk1 _tk2 _tk3 #void #void #void #void #void #void #void))
                    (setq _indent (isub _indent 1))
                    (return _ret))
                 ) ; end case : _default
               ) ; end begin
             ) ; end case : SEXPRESSION
             ;; *********************************************************
             ;; RULE: EXPRESSION: PHRASE Operator :: (_makeError "JS 102" $2.Charpos (append "Invalid use of " $2.Value " operator")) ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ip _ip2)
                (setq _ret  (_makeError "JS 102" _tk2.Charpos (append "Invalid use of " _tk2.Value " operator")) )
                (if _verbose 
                    (writeRule
                         {EXPRESSION: PHRASE Operator :: (_makeError "JS 102" $2.Charpos (append "Invalid use of " $2.Value " operator")) ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : Operator
         ;; *********************************************************
         ;; RULE: EXPRESSION: PHRASE :: $1 ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ip _ip1)
            (setq _ret  _tk1 )
            (if _verbose 
                (writeRule
                     {EXPRESSION: PHRASE :: $1 ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : PHRASE
     ;; ====================
     ;; case: TERM
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_SYNRULE_TERM)) morphFail)true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: Operator
         ;; ====================
         (if (if (<> (setq _tk2 (_getToken))[Operator:] #void) true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: SEXPRESSION
             ;; ====================
             (if (if (<> (setq _tk3 (_SYNRULE_SEXPRESSION)) morphFail)true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; *********************************************************
                 ;; RULE: EXPRESSION: TERM Operator SEXPRESSION :: (setExpVType $0 $1 $2 $3) ::
                 ;; *********************************************************
                 (if true
                  (begin
                    (setq _ret  (setExpVType _tk0 _tk1 _tk2 _tk3) )
                    (if _verbose 
                        (writeRule
                             {EXPRESSION: TERM Operator SEXPRESSION :: (setExpVType $0 $1 $2 $3) ::}
                             _ret  _tk0 _tk1 _tk2 _tk3 #void #void #void #void #void #void #void))
                    (setq _indent (isub _indent 1))
                    (return _ret))
                 ) ; end case : _default
               ) ; end begin
             ) ; end case : SEXPRESSION
             ;; *********************************************************
             ;; RULE: EXPRESSION: TERM Operator :: (_makeError "JS 103" $2.Charpose (append "Invalid use of " $2.Value " operator")) ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ip _ip2)
                (setq _ret  (_makeError "JS 103" _tk2.Charpose (append "Invalid use of " _tk2.Value " operator")) )
                (if _verbose 
                    (writeRule
                         {EXPRESSION: TERM Operator :: (_makeError "JS 103" $2.Charpose (append "Invalid use of " $2.Value " operator")) ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : Operator
         ;; *********************************************************
         ;; RULE: EXPRESSION: TERM :: $1 ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ip _ip1)
            (setq _ret  _tk1 )
            (if _verbose 
                (writeRule
                     {EXPRESSION: TERM :: $1 ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : TERM
     ;; ====================
     ;; case: STATEMENT
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_SYNRULE_STATEMENT)) morphFail)true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; *********************************************************
         ;; RULE: EXPRESSION: STATEMENT :: $1 ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ret  _tk1 )
            (if _verbose 
                (writeRule
                     {EXPRESSION: STATEMENT :: $1 ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
         ;; ====================
         ;; case: Operator
         ;; ====================
         (if (begin (setq _ip _ip1)
          (if (<> (setq _tk2 (_getToken))[Operator:] #void) true (setq _ip _ip1)))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: SEXPRESSION
             ;; ====================
             (if (if (<> (setq _tk3 (_SYNRULE_SEXPRESSION)) morphFail)true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; *********************************************************
                 ;; RULE: EXPRESSION: STATEMENT Operator SEXPRESSION :: (setExpVType $0 $1 $2 $3) ::
                 ;; *********************************************************
                 (if true
                  (begin
                    (setq _ret  (setExpVType _tk0 _tk1 _tk2 _tk3) )
                    (if _verbose 
                        (writeRule
                             {EXPRESSION: STATEMENT Operator SEXPRESSION :: (setExpVType $0 $1 $2 $3) ::}
                             _ret  _tk0 _tk1 _tk2 _tk3 #void #void #void #void #void #void #void))
                    (setq _indent (isub _indent 1))
                    (return _ret))
                 ) ; end case : _default
               ) ; end begin
             ) ; end case : SEXPRESSION
           ) ; end begin
         ) ; end case : Operator
       ) ; end begin
     ) ; end case : STATEMENT
     ;; **************************************************
     ;; DEFAULT (if we get to here we failed to recognize)
     ;; Note:       (at least on this repetition)
     ;; **************************************************
     (begin
        (if (= _tkn 1)
            ;; If we have never recognized any token on previous repetitions 
            (begin
               (setq _ip _oldIp)     
               (if _verbose (_logLine "Rejecting Rule EXPRESSION on: " source:))
               (setq _indent (isub _indent 1))
               (setq _ret morphFail)
               ) ; end begin then
            else
            ;; If we have recognized some tokens on previous repetitions, then return $0
            ;; Note: In Syntax Rules, we do not fail the rule if it has 
            ;;       recognized a token in one or more repetitions of the rule. 
            (begin
               (setq _ip _ip0)
               (setq _ret _tk0)
               ) ; end begin else
            ) ; end if
        ) ; end of DEFAULT

   ;; End of tests for user defined rules.
   
   (if (> _verboseSynIn.EXPRESSION -1)
       (begin
          (setq _verbose _verboseSave)
          (if (>= _verboseSynCount.EXPRESSION _verboseSynIn.EXPRESSION) (error "Count" "in Routine EXPRESSION"))
       ))

   ;; Repeat or return to caller is handled here.
   _ret) ;; end _SYNRULE_EXPRESSION
























;;**EXPORTKEY**:esm:selector:_SYNRULE_FIELDLIST
;; ************************************************
;; FIELDLIST user defined Syntax Rule implementation
;; Summary: This Lambda implements the FIELDLIST
;;          user defined rule. Each rule test is
;;          marked with a boxed comment line for
;;          ease of human understanding.
;; Note: This code was machine generated by ParseLib.
;; ************************************************
(defchild esm.selector _SYNRULE_FIELDLIST(...)
   ;; The token object _io is a persistent variable
   ;; The token pointer _ip is a persistent variable
   ;; All getToken logic increments _ip on each token fetch.
   ;; The return value from the rule is stored in _ret.
   ;; Remember that this rule may repeat depending on _repeatSW.
   vars:(_tk0 _tk1 _tk2 _tk3 _tk4  _tk5  _i
         _tk6 _tk7 _tk8 _tk9 _tkn  _repeatSW
         _ak0 _ak1 _ak2 _ak3 _ak4  _ak5  _tkthis
         _ak6 _ak7 _ak8 _ak9 _ret  _oldIp
         _ip0 _ip1 _ip2 _ip3 _ip4  _ip5
         _ip6 _ip7 _ip8 _ip9 _tkn  
         _verboseSave       
        ) ; end temporary variables
   ;; Collect any arguments which may have been passed to this rule
   (if (> (setq _tkn (argCount)) 0)
       (begin (setq _ak0 (argFetch 0))
          (if (> _tkn 1) 
              (begin (setq _ak1 (argFetch 1))
                 (if (> _tkn 2) 
                     (begin (setq _ak2 (argFetch 2))
                        (if (> _tkn 3) 
                            (begin (setq _ak3 (argFetch 3))
                               (if (> _tkn 4) 
                                   (begin (setq _ak4 (argFetch 4))
                                      (if (> _tkn 5) 
                                          (begin (setq _ak5 (argFetch 5))
                                             (if (> _tkn 6) 
                                                 (begin (setq _ak6 (argFetch 6))
                                                    (if (> _tkn 7) 
                                                        (begin (setq _ak7 (argFetch 7))
                                                           (if (> _tkn 8) 
                                                               (begin (setq _ak8 (argFetch 8))
                                                               (if (> _tkn 9) (setq _ak9 (argFetch 9))
                                                           ))))))))))))))))))) ; end argument collection
   ;; The default structure is given the named rule attribute
   (setq _tk0 (new Structure: FIELDLIST: true  ))
   (setq _tkn 0)
   (setq _oldIp _ip)
   (setq _repeatSW true)

   (if (= _verboseSynCount.FIELDLIST #void) 
          (setq _verboseSynCount.FIELDLIST 1) 
          (setq _verboseSynCount.FIELDLIST (iadd _verboseSynCount.FIELDLIST 1)))

   (if (and (<> _verboseSynIn.FIELDLIST #void) (> _verboseSynIn.FIELDLIST -1))
       (begin (setq _verboseSave _verbose) (setq _verbose true))) 

   ;; Save the old token pointer and test for user defined rules
   Skip::
     (setq _indent (iadd _indent 1))
     (if _verbose (_logLine "Attempting FIELDLIST Rule on: " source:))
     (setq _ip0 _ip)
     (++ _tkn)
     (setq _repeatSW false)

     ;; *****************************************
     ;; Begin testing for each user defined rule.
     ;; *****************************************

     ;; ====================
     ;; case: Name
     ;; ====================
     (if (if (<> (setq _tk1 (_getToken))[Name:] #void) true (setq _ip _ip0))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: RightBrace
         ;; ====================
         (if (if (<> (setq _tk2 (_getToken))[RightBrace:] #void) true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; *********************************************************
             ;; RULE: FIELDLIST: Name RightBrace :: (setq $0.Value (new Vector: 1 $1.Value)) ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  (setq _tk0.Value (new Vector: 1 _tk1.Value)) )
                (if _verbose 
                    (writeRule
                         {FIELDLIST: Name RightBrace :: (setq $0.Value (new Vector: 1 $1.Value)) ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : RightBrace
         ;; ====================
         ;; case: Semicolon
         ;; ====================
         (if (begin (setq _ip _ip1)
          (if (<> (setq _tk2 (_getToken))[Semicolon:] #void) true (setq _ip _ip1)))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: FIELDLIST
             ;; ====================
             (if (if (<> (setq _tk3 (_SYNRULE_FIELDLIST)) morphFail)true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; *********************************************************
                 ;; RULE: FIELDLIST: Name Semicolon FIELDLIST :: (setq $0.Value (insert $3.Value 0 $1.Value)) ::
                 ;; *********************************************************
                 (if true
                  (begin
                    (setq _ret  (setq _tk0.Value (insert _tk3.Value 0 _tk1.Value)) )
                    (if _verbose 
                        (writeRule
                             {FIELDLIST: Name Semicolon FIELDLIST :: (setq $0.Value (insert $3.Value 0 $1.Value)) ::}
                             _ret  _tk0 _tk1 _tk2 _tk3 #void #void #void #void #void #void #void))
                    (setq _indent (isub _indent 1))
                    (return _ret))
                 ) ; end case : _default
               ) ; end begin
             ) ; end case : FIELDLIST
             ;; ====================
             ;; case: RightBrace
             ;; ====================
             (if (begin (setq _ip _ip2)
              (if (<> (setq _tk3 (_getToken))[RightBrace:] #void) true (setq _ip _ip2)))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; *********************************************************
                 ;; RULE: FIELDLIST: Name Semicolon RightBrace :: (setq $0.Value (new Vector: 1 $1.Value)) ::
                 ;; *********************************************************
                 (if true
                  (begin
                    (setq _ret  (setq _tk0.Value (new Vector: 1 _tk1.Value)) )
                    (if _verbose 
                        (writeRule
                             {FIELDLIST: Name Semicolon RightBrace :: (setq $0.Value (new Vector: 1 $1.Value)) ::}
                             _ret  _tk0 _tk1 _tk2 _tk3 #void #void #void #void #void #void #void))
                    (setq _indent (isub _indent 1))
                    (return _ret))
                 ) ; end case : _default
               ) ; end begin
             ) ; end case : RightBrace
           ) ; end begin
         ) ; end case : Semicolon
       ) ; end begin
     ) ; end case : Name
     ;; ====================
     ;; case: RightBrace
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_getToken))[RightBrace:] #void) true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; *********************************************************
         ;; RULE: FIELDLIST: RightBrace :: (setq $0.Value (new Vector: 0)) ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ret  (setq _tk0.Value (new Vector: 0)) )
            (if _verbose 
                (writeRule
                     {FIELDLIST: RightBrace :: (setq $0.Value (new Vector: 0)) ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : RightBrace
     ;; ====================
     ;; case: Value
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_getToken))[Value:] #void) true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; *********************************************************
         ;; RULE: FIELDLIST: Value :: (_makeError "JS 139" $1.Charpos "Invalid field list") ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ret  (_makeError "JS 139" _tk1.Charpos "Invalid field list") )
            (if _verbose 
                (writeRule
                     {FIELDLIST: Value :: (_makeError "JS 139" $1.Charpos "Invalid field list") ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : Value
     ;; **************************************************
     ;; DEFAULT (if we get to here we failed to recognize)
     ;; Note:       (at least on this repetition)
     ;; **************************************************
     (begin
        (if (= _tkn 1)
            ;; If we have never recognized any token on previous repetitions 
            (begin
               (setq _ip _oldIp)     
               (if _verbose (_logLine "Rejecting Rule FIELDLIST on: " source:))
               (setq _indent (isub _indent 1))
               (setq _ret morphFail)
               ) ; end begin then
            else
            ;; If we have recognized some tokens on previous repetitions, then return $0
            ;; Note: In Syntax Rules, we do not fail the rule if it has 
            ;;       recognized a token in one or more repetitions of the rule. 
            (begin
               (setq _ip _ip0)
               (setq _ret _tk0)
               ) ; end begin else
            ) ; end if
        ) ; end of DEFAULT

   ;; End of tests for user defined rules.
   
   (if (> _verboseSynIn.FIELDLIST -1)
       (begin
          (setq _verbose _verboseSave)
          (if (>= _verboseSynCount.FIELDLIST _verboseSynIn.FIELDLIST) (error "Count" "in Routine FIELDLIST"))
       ))

   ;; Repeat or return to caller is handled here.
   _ret) ;; end _SYNRULE_FIELDLIST
























;;**EXPORTKEY**:esm:selector:_SYNRULE_FILTER
;; ************************************************
;; FILTER user defined Syntax Rule implementation
;; Summary: This Lambda implements the FILTER
;;          user defined rule. Each rule test is
;;          marked with a boxed comment line for
;;          ease of human understanding.
;; Note: This code was machine generated by ParseLib.
;; ************************************************
(defchild esm.selector _SYNRULE_FILTER(...)
   ;; The token object _io is a persistent variable
   ;; The token pointer _ip is a persistent variable
   ;; All getToken logic increments _ip on each token fetch.
   ;; The return value from the rule is stored in _ret.
   ;; Remember that this rule may repeat depending on _repeatSW.
   vars:(_tk0 _tk1 _tk2 _tk3 _tk4  _tk5  _i
         _tk6 _tk7 _tk8 _tk9 _tkn  _repeatSW
         _ak0 _ak1 _ak2 _ak3 _ak4  _ak5  _tkthis
         _ak6 _ak7 _ak8 _ak9 _ret  _oldIp
         _ip0 _ip1 _ip2 _ip3 _ip4  _ip5
         _ip6 _ip7 _ip8 _ip9 _tkn  
         _verboseSave       
        ) ; end temporary variables
   ;; Collect any arguments which may have been passed to this rule
   (if (> (setq _tkn (argCount)) 0)
       (begin (setq _ak0 (argFetch 0))
          (if (> _tkn 1) 
              (begin (setq _ak1 (argFetch 1))
                 (if (> _tkn 2) 
                     (begin (setq _ak2 (argFetch 2))
                        (if (> _tkn 3) 
                            (begin (setq _ak3 (argFetch 3))
                               (if (> _tkn 4) 
                                   (begin (setq _ak4 (argFetch 4))
                                      (if (> _tkn 5) 
                                          (begin (setq _ak5 (argFetch 5))
                                             (if (> _tkn 6) 
                                                 (begin (setq _ak6 (argFetch 6))
                                                    (if (> _tkn 7) 
                                                        (begin (setq _ak7 (argFetch 7))
                                                           (if (> _tkn 8) 
                                                               (begin (setq _ak8 (argFetch 8))
                                                               (if (> _tkn 9) (setq _ak9 (argFetch 9))
                                                           ))))))))))))))))))) ; end argument collection
   ;; The default structure is given the named rule attribute
   (setq _tk0 (new Structure: FILTER: true  ))
   (setq _tkn 0)
   (setq _oldIp _ip)
   (setq _repeatSW true)

   (if (= _verboseSynCount.FILTER #void) 
          (setq _verboseSynCount.FILTER 1) 
          (setq _verboseSynCount.FILTER (iadd _verboseSynCount.FILTER 1)))

   (if (and (<> _verboseSynIn.FILTER #void) (> _verboseSynIn.FILTER -1))
       (begin (setq _verboseSave _verbose) (setq _verbose true))) 

   ;; Save the old token pointer and test for user defined rules
   Skip::
     (setq _indent (iadd _indent 1))
     (if _verbose (_logLine "Attempting FILTER Rule on: " source:))
     (setq _ip0 _ip)
     (++ _tkn)
     (setq _repeatSW false)

     ;; *****************************************
     ;; Begin testing for each user defined rule.
     ;; *****************************************

     ;; ====================
     ;; case: FSTMTLIST
     ;; ====================
     (if (if (<> (setq _tk1 (_SYNRULE_FSTMTLIST)) morphFail)true (setq _ip _ip0))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; *********************************************************
         ;; RULE: FILTER: FSTMTLIST :: (setq $0.Value (filterFinal $1.Value)) ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ret  (setq _tk0.Value (filterFinal _tk1.Value)) )
            (if _verbose 
                (writeRule
                     {FILTER: FSTMTLIST :: (setq $0.Value (filterFinal $1.Value)) ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : FSTMTLIST
     ;; ====================
     ;; case: Select
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_getToken))[Select:] #void) true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: FSTMTLIST
         ;; ====================
         (if (if (<> (setq _tk2 (_SYNRULE_FSTMTLIST)) morphFail)true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; *********************************************************
             ;; RULE: FILTER: Select FSTMTLIST :: (setq $0.Value (filterFinal $2.Value)) ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  (setq _tk0.Value (filterFinal _tk2.Value)) )
                (if _verbose 
                    (writeRule
                         {FILTER: Select FSTMTLIST :: (setq $0.Value (filterFinal $2.Value)) ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : FSTMTLIST
       ) ; end begin
     ) ; end case : Select
     ;; **************************************************
     ;; DEFAULT (if we get to here we failed to recognize)
     ;; Note:       (at least on this repetition)
     ;; **************************************************
     (begin
        (if (= _tkn 1)
            ;; If we have never recognized any token on previous repetitions 
            (begin
               (setq _ip _oldIp)     
               (if _verbose (_logLine "Rejecting Rule FILTER on: " source:))
               (setq _indent (isub _indent 1))
               (setq _ret morphFail)
               ) ; end begin then
            else
            ;; If we have recognized some tokens on previous repetitions, then return $0
            ;; Note: In Syntax Rules, we do not fail the rule if it has 
            ;;       recognized a token in one or more repetitions of the rule. 
            (begin
               (setq _ip _ip0)
               (setq _ret _tk0)
               ) ; end begin else
            ) ; end if
        ) ; end of DEFAULT

   ;; End of tests for user defined rules.
   
   (if (> _verboseSynIn.FILTER -1)
       (begin
          (setq _verbose _verboseSave)
          (if (>= _verboseSynCount.FILTER _verboseSynIn.FILTER) (error "Count" "in Routine FILTER"))
       ))

   ;; Repeat or return to caller is handled here.
   _ret) ;; end _SYNRULE_FILTER
























;;**EXPORTKEY**:esm:selector:_SYNRULE_FRIEND
;; ************************************************
;; FRIEND user defined Syntax Rule implementation
;; Summary: This Lambda implements the FRIEND
;;          user defined rule. Each rule test is
;;          marked with a boxed comment line for
;;          ease of human understanding.
;; Note: This code was machine generated by ParseLib.
;; ************************************************
(defchild esm.selector _SYNRULE_FRIEND(...)
   ;; The token object _io is a persistent variable
   ;; The token pointer _ip is a persistent variable
   ;; All getToken logic increments _ip on each token fetch.
   ;; The return value from the rule is stored in _ret.
   ;; Remember that this rule may repeat depending on _repeatSW.
   vars:(_tk0 _tk1 _tk2 _tk3 _tk4  _tk5  _i
         _tk6 _tk7 _tk8 _tk9 _tkn  _repeatSW
         _ak0 _ak1 _ak2 _ak3 _ak4  _ak5  _tkthis
         _ak6 _ak7 _ak8 _ak9 _ret  _oldIp
         _ip0 _ip1 _ip2 _ip3 _ip4  _ip5
         _ip6 _ip7 _ip8 _ip9 _tkn  
         _verboseSave       
        ) ; end temporary variables
   ;; Collect any arguments which may have been passed to this rule
   (if (> (setq _tkn (argCount)) 0)
       (begin (setq _ak0 (argFetch 0))
          (if (> _tkn 1) 
              (begin (setq _ak1 (argFetch 1))
                 (if (> _tkn 2) 
                     (begin (setq _ak2 (argFetch 2))
                        (if (> _tkn 3) 
                            (begin (setq _ak3 (argFetch 3))
                               (if (> _tkn 4) 
                                   (begin (setq _ak4 (argFetch 4))
                                      (if (> _tkn 5) 
                                          (begin (setq _ak5 (argFetch 5))
                                             (if (> _tkn 6) 
                                                 (begin (setq _ak6 (argFetch 6))
                                                    (if (> _tkn 7) 
                                                        (begin (setq _ak7 (argFetch 7))
                                                           (if (> _tkn 8) 
                                                               (begin (setq _ak8 (argFetch 8))
                                                               (if (> _tkn 9) (setq _ak9 (argFetch 9))
                                                           ))))))))))))))))))) ; end argument collection
   ;; The default structure is given the named rule attribute
   (setq _tk0 (new Structure: FRIEND: true  ))
   (setq _tkn 0)
   (setq _oldIp _ip)
   (setq _repeatSW true)

   (if (= _verboseSynCount.FRIEND #void) 
          (setq _verboseSynCount.FRIEND 1) 
          (setq _verboseSynCount.FRIEND (iadd _verboseSynCount.FRIEND 1)))

   (if (and (<> _verboseSynIn.FRIEND #void) (> _verboseSynIn.FRIEND -1))
       (begin (setq _verboseSave _verbose) (setq _verbose true))) 

   ;; Save the old token pointer and test for user defined rules
   Skip::
     (setq _indent (iadd _indent 1))
     (if _verbose (_logLine "Attempting FRIEND Rule on: " source:))
     (setq _ip0 _ip)
     (++ _tkn)
     (setq _repeatSW false)

     ;; *****************************************
     ;; Begin testing for each user defined rule.
     ;; *****************************************

     ;; ====================
     ;; case: Friend
     ;; ====================
     (if (if (<> (setq _tk1 (_getToken))[Friend:] #void) true (setq _ip _ip0))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; *********************************************************
         ;; RULE: FRIEND: Friend :: (setq (pushVars $0)[Charpos:] $1.Charpos) ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ret  (setq (pushVars _tk0)[Charpos:] _tk1.Charpos) )
            (if _verbose 
                (writeRule
                     {FRIEND: Friend :: (setq (pushVars $0)[Charpos:] $1.Charpos) ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : Friend
     ;; **************************************************
     ;; DEFAULT (if we get to here we failed to recognize)
     ;; Note:       (at least on this repetition)
     ;; **************************************************
     (begin
        (if (= _tkn 1)
            ;; If we have never recognized any token on previous repetitions 
            (begin
               (setq _ip _oldIp)     
               (if _verbose (_logLine "Rejecting Rule FRIEND on: " source:))
               (setq _indent (isub _indent 1))
               (setq _ret morphFail)
               ) ; end begin then
            else
            ;; If we have recognized some tokens on previous repetitions, then return $0
            ;; Note: In Syntax Rules, we do not fail the rule if it has 
            ;;       recognized a token in one or more repetitions of the rule. 
            (begin
               (setq _ip _ip0)
               (setq _ret _tk0)
               ) ; end begin else
            ) ; end if
        ) ; end of DEFAULT

   ;; End of tests for user defined rules.
   
   (if (> _verboseSynIn.FRIEND -1)
       (begin
          (setq _verbose _verboseSave)
          (if (>= _verboseSynCount.FRIEND _verboseSynIn.FRIEND) (error "Count" "in Routine FRIEND"))
       ))

   ;; Repeat or return to caller is handled here.
   _ret) ;; end _SYNRULE_FRIEND
























;;**EXPORTKEY**:esm:selector:_SYNRULE_FSTATEMENT
;; ************************************************
;; FSTATEMENT user defined Syntax Rule implementation
;; Summary: This Lambda implements the FSTATEMENT
;;          user defined rule. Each rule test is
;;          marked with a boxed comment line for
;;          ease of human understanding.
;; Note: This code was machine generated by ParseLib.
;; ************************************************
(defchild esm.selector _SYNRULE_FSTATEMENT(...)
   ;; The token object _io is a persistent variable
   ;; The token pointer _ip is a persistent variable
   ;; All getToken logic increments _ip on each token fetch.
   ;; The return value from the rule is stored in _ret.
   ;; Remember that this rule may repeat depending on _repeatSW.
   vars:(_tk0 _tk1 _tk2 _tk3 _tk4  _tk5  _i
         _tk6 _tk7 _tk8 _tk9 _tkn  _repeatSW
         _ak0 _ak1 _ak2 _ak3 _ak4  _ak5  _tkthis
         _ak6 _ak7 _ak8 _ak9 _ret  _oldIp
         _ip0 _ip1 _ip2 _ip3 _ip4  _ip5
         _ip6 _ip7 _ip8 _ip9 _tkn  
         _verboseSave       
        ) ; end temporary variables
   ;; Collect any arguments which may have been passed to this rule
   (if (> (setq _tkn (argCount)) 0)
       (begin (setq _ak0 (argFetch 0))
          (if (> _tkn 1) 
              (begin (setq _ak1 (argFetch 1))
                 (if (> _tkn 2) 
                     (begin (setq _ak2 (argFetch 2))
                        (if (> _tkn 3) 
                            (begin (setq _ak3 (argFetch 3))
                               (if (> _tkn 4) 
                                   (begin (setq _ak4 (argFetch 4))
                                      (if (> _tkn 5) 
                                          (begin (setq _ak5 (argFetch 5))
                                             (if (> _tkn 6) 
                                                 (begin (setq _ak6 (argFetch 6))
                                                    (if (> _tkn 7) 
                                                        (begin (setq _ak7 (argFetch 7))
                                                           (if (> _tkn 8) 
                                                               (begin (setq _ak8 (argFetch 8))
                                                               (if (> _tkn 9) (setq _ak9 (argFetch 9))
                                                           ))))))))))))))))))) ; end argument collection
   ;; The default structure is given the named rule attribute
   (setq _tk0 (new Structure: FSTATEMENT: true  ))
   (setq _tkn 0)
   (setq _oldIp _ip)
   (setq _repeatSW true)

   (if (= _verboseSynCount.FSTATEMENT #void) 
          (setq _verboseSynCount.FSTATEMENT 1) 
          (setq _verboseSynCount.FSTATEMENT (iadd _verboseSynCount.FSTATEMENT 1)))

   (if (and (<> _verboseSynIn.FSTATEMENT #void) (> _verboseSynIn.FSTATEMENT -1))
       (begin (setq _verboseSave _verbose) (setq _verbose true))) 

   ;; Save the old token pointer and test for user defined rules
   Skip::
     (setq _indent (iadd _indent 1))
     (if _verbose (_logLine "Attempting FSTATEMENT Rule on: " source:))
     (setq _ip0 _ip)
     (++ _tkn)
     (setq _repeatSW false)

     ;; *****************************************
     ;; Begin testing for each user defined rule.
     ;; *****************************************

     ;; ====================
     ;; case: Sort{(xReferentOn)}
     ;; ====================
     (if (and (if (<> (setq _tk1 (_getToken))[Sort:] #void) true (setq _ip _ip0)) (xReferentOn))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: Direction
         ;; ====================
         (if (if (<> (setq _tk2 (_getToken))[Direction:] #void) true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: SEXPRESSION
             ;; ====================
             (if (if (<> (setq _tk3 (_SYNRULE_SEXPRESSION)) morphFail)true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; *********************************************************
                 ;; RULE: FSTATEMENT: Sort{(xReferentOn)} Direction SEXPRESSION :: (sortCut $0 $2.Lisp $3.Value -1) ::
                 ;; *********************************************************
                 (if true
                  (begin
                    (setq _ret  (sortCut _tk0 _tk2.Lisp _tk3.Value -1) )
                    (if _verbose 
                        (writeRule
                             {FSTATEMENT: Sort{(xReferentOn)} Direction SEXPRESSION :: (sortCut $0 $2.Lisp $3.Value -1) ::}
                             _ret  _tk0 _tk1 _tk2 _tk3 #void #void #void #void #void #void #void))
                    (setq _indent (isub _indent 1))
                    (return _ret))
                 ) ; end case : _default
               ) ; end begin
             ) ; end case : SEXPRESSION
           ) ; end begin
         ) ; end case : Direction
         ;; ====================
         ;; case: SEXPRESSION
         ;; ====================
         (if (begin (setq _ip _ip1)
          (if (<> (setq _tk2 (_SYNRULE_SEXPRESSION)) morphFail)true (setq _ip _ip1)))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; *********************************************************
             ;; RULE: FSTATEMENT: Sort{(xReferentOn)} SEXPRESSION :: (sortCut $0 |<=|: $2.Value -1) ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  (sortCut _tk0 |<=|: _tk2.Value -1) )
                (if _verbose 
                    (writeRule
                         {FSTATEMENT: Sort{(xReferentOn)} SEXPRESSION :: (sortCut $0 |<=|: $2.Value -1) ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : SEXPRESSION
       ) ; end begin
     ) ; end case : Sort{(xReferentOn)}
     ;; ====================
     ;; case: Cut{(xReferentOn)}
     ;; ====================
     (if (begin (setq _ip _ip0)
      (and (if (<> (setq _tk1 (_getToken))[Cut:] #void) true (setq _ip _ip0)) (xReferentOn)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: SEXPRESSION
         ;; ====================
         (if (if (<> (setq _tk2 (_SYNRULE_SEXPRESSION)) morphFail)true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: CUT
             ;; ====================
             (if (if (<> (setq _tk3 (_SYNRULE_CUT)) morphFail)true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; *********************************************************
                 ;; RULE: FSTATEMENT: Cut{(xReferentOn)} SEXPRESSION CUT :: (sortCut $0 $1.Lisp $2.Value $3.Value) ::
                 ;; *********************************************************
                 (if true
                  (begin
                    (setq _ret  (sortCut _tk0 _tk1.Lisp _tk2.Value _tk3.Value) )
                    (if _verbose 
                        (writeRule
                             {FSTATEMENT: Cut{(xReferentOn)} SEXPRESSION CUT :: (sortCut $0 $1.Lisp $2.Value $3.Value) ::}
                             _ret  _tk0 _tk1 _tk2 _tk3 #void #void #void #void #void #void #void))
                    (setq _indent (isub _indent 1))
                    (return _ret))
                 ) ; end case : _default
               ) ; end begin
             ) ; end case : CUT
           ) ; end begin
         ) ; end case : SEXPRESSION
       ) ; end begin
     ) ; end case : Cut{(xReferentOn)}
     ;; ====================
     ;; case: Slice{(xReferentOn)}
     ;; ====================
     (if (begin (setq _ip _ip0)
      (and (if (<> (setq _tk1 (_getToken))[Slice:] #void) true (setq _ip _ip0)) (xReferentOn)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: SEXPRESSION
         ;; ====================
         (if (if (<> (setq _tk2 (_SYNRULE_SEXPRESSION)) morphFail)true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: CUT
             ;; ====================
             (if (if (<> (setq _tk3 (_SYNRULE_CUT)) morphFail)true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; ====================
                 ;; case: Of
                 ;; ====================
                 (if (if (<> (setq _tk4 (_getToken))[Of:] #void) true (setq _ip _ip3))
                   (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 _ip) 
                     ;; ====================
                     ;; case: CUT
                     ;; ====================
                     (if (if (<> (setq _tk5 (_SYNRULE_CUT)) morphFail)true (setq _ip _ip4))
                       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip5 _ip) 
                         ;; *********************************************************
                         ;; RULE: FSTATEMENT: Slice{(xReferentOn)} SEXPRESSION CUT Of CUT :: (sliceCut $0 $1.Lisp $2.Value $3.Value $5.Value) ::
                         ;; *********************************************************
                         (if true
                          (begin
                            (setq _ret  (sliceCut _tk0 _tk1.Lisp _tk2.Value _tk3.Value _tk5.Value) )
                            (if _verbose 
                                (writeRule
                                     {FSTATEMENT: Slice{(xReferentOn)} SEXPRESSION CUT Of CUT :: (sliceCut $0 $1.Lisp $2.Value $3.Value $5.Value) ::}
                                     _ret  _tk0 _tk1 _tk2 _tk3 _tk4 _tk5 #void #void #void #void #void))
                            (setq _indent (isub _indent 1))
                            (return _ret))
                         ) ; end case : _default
                       ) ; end begin
                     ) ; end case : CUT
                   ) ; end begin
                 ) ; end case : Of
               ) ; end begin
             ) ; end case : CUT
           ) ; end begin
         ) ; end case : SEXPRESSION
       ) ; end begin
     ) ; end case : Slice{(xReferentOn)}
     ;; ====================
     ;; case: All{(xReferentOn)}
     ;; ====================
     (if (begin (setq _ip _ip0)
      (and (if (<> (setq _tk1 (_getToken))[All:] #void) true (setq _ip _ip0)) (xReferentOn)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: Semicolon
         ;; ====================
         (if (if (<> (setq _tk2 (_getToken))[Semicolon:] #void) true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; *********************************************************
             ;; RULE: FSTATEMENT: All{(xReferentOn)} Semicolon :: (setq $0.Value true) ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  (setq _tk0.Value true) )
                (if _verbose 
                    (writeRule
                         {FSTATEMENT: All{(xReferentOn)} Semicolon :: (setq $0.Value true) ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : Semicolon
         ;; ====================
         ;; case: SEXPRESSION
         ;; ====================
         (if (begin (setq _ip _ip1)
          (if (<> (setq _tk2 (_SYNRULE_SEXPRESSION)) morphFail)true (setq _ip _ip1)))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; *********************************************************
             ;; RULE: FSTATEMENT: All{(xReferentOn)} SEXPRESSION :: (filterCut $0 $2.Value $1.Value) ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  (filterCut _tk0 _tk2.Value _tk1.Value) )
                (if _verbose 
                    (writeRule
                         {FSTATEMENT: All{(xReferentOn)} SEXPRESSION :: (filterCut $0 $2.Value $1.Value) ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : SEXPRESSION
         ;; *********************************************************
         ;; RULE: FSTATEMENT: All{(xReferentOn)} :: (setq $0.Value true) ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ip _ip1)
            (setq _ret  (setq _tk0.Value true) )
            (if _verbose 
                (writeRule
                     {FSTATEMENT: All{(xReferentOn)} :: (setq $0.Value true) ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : All{(xReferentOn)}
     ;; ====================
     ;; case: Run{(xReferentOff)}
     ;; ====================
     (if (begin (setq _ip _ip0)
      (and (if (<> (setq _tk1 (_getToken))[Run:] #void) true (setq _ip _ip0)) (xReferentOff)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: SEXPRESSION
         ;; ====================
         (if (if (<> (setq _tk2 (_SYNRULE_SEXPRESSION)) morphFail)true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; *********************************************************
             ;; RULE: FSTATEMENT: Run{(xReferentOff)} SEXPRESSION :: (runFilter $0 $2.Value) ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  (runFilter _tk0 _tk2.Value) )
                (if _verbose 
                    (writeRule
                         {FSTATEMENT: Run{(xReferentOff)} SEXPRESSION :: (runFilter $0 $2.Value) ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : SEXPRESSION
       ) ; end begin
     ) ; end case : Run{(xReferentOff)}
     ;; ====================
     ;; case: Restore{(xReferentOff)}
     ;; ====================
     (if (begin (setq _ip _ip0)
      (and (if (<> (setq _tk1 (_getToken))[Restore:] #void) true (setq _ip _ip0)) (xReferentOff)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; *********************************************************
         ;; RULE: FSTATEMENT: Restore{(xReferentOff)} :: (setq $0.Value (list (list |ref|: XT: ''restore))) ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ret  (setq _tk0.Value (list (list |ref|: XT: ''restore))) )
            (if _verbose 
                (writeRule
                     {FSTATEMENT: Restore{(xReferentOff)} :: (setq $0.Value (list (list |ref|: XT: ''restore))) ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : Restore{(xReferentOff)}
     ;; ====================
     ;; case: Scale{(xReferentOn)}
     ;; ====================
     (if (begin (setq _ip _ip0)
      (and (if (<> (setq _tk1 (_getToken))[Scale:] #void) true (setq _ip _ip0)) (xReferentOn)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: SEXPRESSION
         ;; ====================
         (if (if (<> (setq _tk2 (_SYNRULE_SEXPRESSION)) morphFail)true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: Semicolon
             ;; ====================
             (if (if (<> (setq _tk3 (_getToken))[Semicolon:] #void) true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; *********************************************************
                 ;; RULE: FSTATEMENT: Scale{(xReferentOn)} SEXPRESSION Semicolon :: (filterScale $0 $2.Value) ::
                 ;; *********************************************************
                 (if true
                  (begin
                    (setq _ret  (filterScale _tk0 _tk2.Value) )
                    (if _verbose 
                        (writeRule
                             {FSTATEMENT: Scale{(xReferentOn)} SEXPRESSION Semicolon :: (filterScale $0 $2.Value) ::}
                             _ret  _tk0 _tk1 _tk2 _tk3 #void #void #void #void #void #void #void))
                    (setq _indent (isub _indent 1))
                    (return _ret))
                 ) ; end case : _default
               ) ; end begin
             ) ; end case : Semicolon
           ) ; end begin
         ) ; end case : SEXPRESSION
       ) ; end begin
     ) ; end case : Scale{(xReferentOn)}
     ;; ====================
     ;; case: Regress{(xReferentOn)}
     ;; ====================
     (if (begin (setq _ip _ip0)
      (and (if (<> (setq _tk1 (_getToken))[Regress:] #void) true (setq _ip _ip0)) (xReferentOn)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: SEXPRESSION
         ;; ====================
         (if (if (<> (setq _tk2 (_SYNRULE_SEXPRESSION)) morphFail)true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: Semicolon
             ;; ====================
             (if (if (<> (setq _tk3 (_getToken))[Semicolon:] #void) true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; *********************************************************
                 ;; RULE: FSTATEMENT: Regress{(xReferentOn)} SEXPRESSION Semicolon :: (filterRegress $0 $2.Value) ::
                 ;; *********************************************************
                 (if true
                  (begin
                    (setq _ret  (filterRegress _tk0 _tk2.Value) )
                    (if _verbose 
                        (writeRule
                             {FSTATEMENT: Regress{(xReferentOn)} SEXPRESSION Semicolon :: (filterRegress $0 $2.Value) ::}
                             _ret  _tk0 _tk1 _tk2 _tk3 #void #void #void #void #void #void #void))
                    (setq _indent (isub _indent 1))
                    (return _ret))
                 ) ; end case : _default
               ) ; end begin
             ) ; end case : Semicolon
           ) ; end begin
         ) ; end case : SEXPRESSION
       ) ; end begin
     ) ; end case : Regress{(xReferentOn)}
     ;; ====================
     ;; case: Bgmregress{(xReferentOn)}
     ;; ====================
     (if (begin (setq _ip _ip0)
      (and (if (<> (setq _tk1 (_getToken))[Bgmregress:] #void) true (setq _ip _ip0)) (xReferentOn)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: LeftParen
         ;; ====================
         (if (if (<> (setq _tk2 (_getToken))[LeftParen:] #void) true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: ARGLIST
             ;; ====================
             (if (if (<> (setq _tk3 (_SYNRULE_ARGLIST)) morphFail)true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; ====================
                 ;; case: Semicolon
                 ;; ====================
                 (if (if (<> (setq _tk4 (_getToken))[Semicolon:] #void) true (setq _ip _ip3))
                   (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 _ip) 
                     ;; *********************************************************
                     ;; RULE: FSTATEMENT: Bgmregress{(xReferentOn)} LeftParen ARGLIST Semicolon :: (filterRegressBgm $0 $3.Value) ::
                     ;; *********************************************************
                     (if true
                      (begin
                        (setq _ret  (filterRegressBgm _tk0 _tk3.Value) )
                        (if _verbose 
                            (writeRule
                                 {FSTATEMENT: Bgmregress{(xReferentOn)} LeftParen ARGLIST Semicolon :: (filterRegressBgm $0 $3.Value) ::}
                                 _ret  _tk0 _tk1 _tk2 _tk3 _tk4 #void #void #void #void #void #void))
                        (setq _indent (isub _indent 1))
                        (return _ret))
                     ) ; end case : _default
                   ) ; end begin
                 ) ; end case : Semicolon
               ) ; end begin
             ) ; end case : ARGLIST
           ) ; end begin
         ) ; end case : LeftParen
       ) ; end begin
     ) ; end case : Bgmregress{(xReferentOn)}
     ;; ====================
     ;; case: Ennregress
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_getToken))[Ennregress:] #void) true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: LeftParen
         ;; ====================
         (if (if (<> (setq _tk2 (_getToken))[LeftParen:] #void) true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: WEIGHTLIST
             ;; ====================
             (if (if (<> (setq _tk3 (_SYNRULE_WEIGHTLIST)) morphFail)true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; ====================
                 ;; case: Semicolon
                 ;; ====================
                 (if (if (<> (setq _tk4 (_getToken))[Semicolon:] #void) true (setq _ip _ip3))
                   (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 _ip) 
                     ;; *********************************************************
                     ;; RULE: FSTATEMENT: Ennregress LeftParen WEIGHTLIST Semicolon :: (filterRegressEnn $0 (lisp $3.Value)) ::
                     ;; *********************************************************
                     (if true
                      (begin
                        (setq _ret  (filterRegressEnn _tk0 (lisp _tk3.Value)) )
                        (if _verbose 
                            (writeRule
                                 {FSTATEMENT: Ennregress LeftParen WEIGHTLIST Semicolon :: (filterRegressEnn $0 (lisp $3.Value)) ::}
                                 _ret  _tk0 _tk1 _tk2 _tk3 _tk4 #void #void #void #void #void #void))
                        (setq _indent (isub _indent 1))
                        (return _ret))
                     ) ; end case : _default
                   ) ; end begin
                 ) ; end case : Semicolon
               ) ; end begin
             ) ; end case : WEIGHTLIST
           ) ; end begin
         ) ; end case : LeftParen
       ) ; end begin
     ) ; end case : Ennregress
     ;; ====================
     ;; case: Frmregress{(xReferentOn)}
     ;; ====================
     (if (begin (setq _ip _ip0)
      (and (if (<> (setq _tk1 (_getToken))[Frmregress:] #void) true (setq _ip _ip0)) (xReferentOn)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: LeftParen
         ;; ====================
         (if (if (<> (setq _tk2 (_getToken))[LeftParen:] #void) true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: ARGLIST
             ;; ====================
             (if (if (<> (setq _tk3 (_SYNRULE_ARGLIST)) morphFail)true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; ====================
                 ;; case: Semicolon
                 ;; ====================
                 (if (if (<> (setq _tk4 (_getToken))[Semicolon:] #void) true (setq _ip _ip3))
                   (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 _ip) 
                     ;; *********************************************************
                     ;; RULE: FSTATEMENT: Frmregress{(xReferentOn)} LeftParen ARGLIST Semicolon :: (filterRegressFrm $0 $3.Value) ::
                     ;; *********************************************************
                     (if true
                      (begin
                        (setq _ret  (filterRegressFrm _tk0 _tk3.Value) )
                        (if _verbose 
                            (writeRule
                                 {FSTATEMENT: Frmregress{(xReferentOn)} LeftParen ARGLIST Semicolon :: (filterRegressFrm $0 $3.Value) ::}
                                 _ret  _tk0 _tk1 _tk2 _tk3 _tk4 #void #void #void #void #void #void))
                        (setq _indent (isub _indent 1))
                        (return _ret))
                     ) ; end case : _default
                   ) ; end begin
                 ) ; end case : Semicolon
               ) ; end begin
             ) ; end case : ARGLIST
           ) ; end begin
         ) ; end case : LeftParen
       ) ; end begin
     ) ; end case : Frmregress{(xReferentOn)}
     ;; ====================
     ;; case: Mvlregress{(xReferentOn)}
     ;; ====================
     (if (begin (setq _ip _ip0)
      (and (if (<> (setq _tk1 (_getToken))[Mvlregress:] #void) true (setq _ip _ip0)) (xReferentOn)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: LeftParen
         ;; ====================
         (if (if (<> (setq _tk2 (_getToken))[LeftParen:] #void) true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: ARGLIST
             ;; ====================
             (if (if (<> (setq _tk3 (_SYNRULE_ARGLIST)) morphFail)true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; ====================
                 ;; case: Semicolon
                 ;; ====================
                 (if (if (<> (setq _tk4 (_getToken))[Semicolon:] #void) true (setq _ip _ip3))
                   (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 _ip) 
                     ;; *********************************************************
                     ;; RULE: FSTATEMENT: Mvlregress{(xReferentOn)} LeftParen ARGLIST Semicolon :: (filterRegressMvl $0 $3.Value) ::
                     ;; *********************************************************
                     (if true
                      (begin
                        (setq _ret  (filterRegressMvl _tk0 _tk3.Value) )
                        (if _verbose 
                            (writeRule
                                 {FSTATEMENT: Mvlregress{(xReferentOn)} LeftParen ARGLIST Semicolon :: (filterRegressMvl $0 $3.Value) ::}
                                 _ret  _tk0 _tk1 _tk2 _tk3 _tk4 #void #void #void #void #void #void))
                        (setq _indent (isub _indent 1))
                        (return _ret))
                     ) ; end case : _default
                   ) ; end begin
                 ) ; end case : Semicolon
               ) ; end begin
             ) ; end case : ARGLIST
           ) ; end begin
         ) ; end case : LeftParen
       ) ; end begin
     ) ; end case : Mvlregress{(xReferentOn)}
     ;; ====================
     ;; case: Svmregress{(xReferentOn)}
     ;; ====================
     (if (begin (setq _ip _ip0)
      (and (if (<> (setq _tk1 (_getToken))[Svmregress:] #void) true (setq _ip _ip0)) (xReferentOn)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: LeftParen
         ;; ====================
         (if (if (<> (setq _tk2 (_getToken))[LeftParen:] #void) true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: ARGLIST
             ;; ====================
             (if (if (<> (setq _tk3 (_SYNRULE_ARGLIST)) morphFail)true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; ====================
                 ;; case: Semicolon
                 ;; ====================
                 (if (if (<> (setq _tk4 (_getToken))[Semicolon:] #void) true (setq _ip _ip3))
                   (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 _ip) 
                     ;; *********************************************************
                     ;; RULE: FSTATEMENT: Svmregress{(xReferentOn)} LeftParen ARGLIST Semicolon :: (filterRegressSvm $0 $3.Value) ::
                     ;; *********************************************************
                     (if true
                      (begin
                        (setq _ret  (filterRegressSvm _tk0 _tk3.Value) )
                        (if _verbose 
                            (writeRule
                                 {FSTATEMENT: Svmregress{(xReferentOn)} LeftParen ARGLIST Semicolon :: (filterRegressSvm $0 $3.Value) ::}
                                 _ret  _tk0 _tk1 _tk2 _tk3 _tk4 #void #void #void #void #void #void))
                        (setq _indent (isub _indent 1))
                        (return _ret))
                     ) ; end case : _default
                   ) ; end begin
                 ) ; end case : Semicolon
               ) ; end begin
             ) ; end case : ARGLIST
             ;; ====================
             ;; case: RightParen
             ;; ====================
             (if (begin (setq _ip _ip2)
              (if (<> (setq _tk3 (_getToken))[RightParen:] #void) true (setq _ip _ip2)))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; ====================
                 ;; case: Semicolon
                 ;; ====================
                 (if (if (<> (setq _tk4 (_getToken))[Semicolon:] #void) true (setq _ip _ip3))
                   (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 _ip) 
                     ;; *********************************************************
                     ;; RULE: FSTATEMENT: Svmregress{(xReferentOn)} LeftParen RightParen Semicolon :: (filterRegressSvm $0) ::
                     ;; *********************************************************
                     (if true
                      (begin
                        (setq _ret  (filterRegressSvm _tk0) )
                        (if _verbose 
                            (writeRule
                                 {FSTATEMENT: Svmregress{(xReferentOn)} LeftParen RightParen Semicolon :: (filterRegressSvm $0) ::}
                                 _ret  _tk0 _tk1 _tk2 _tk3 _tk4 #void #void #void #void #void #void))
                        (setq _indent (isub _indent 1))
                        (return _ret))
                     ) ; end case : _default
                   ) ; end begin
                 ) ; end case : Semicolon
               ) ; end begin
             ) ; end case : RightParen
           ) ; end begin
         ) ; end case : LeftParen
         ;; ====================
         ;; case: Semicolon
         ;; ====================
         (if (begin (setq _ip _ip1)
          (if (<> (setq _tk2 (_getToken))[Semicolon:] #void) true (setq _ip _ip1)))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; *********************************************************
             ;; RULE: FSTATEMENT: Svmregress{(xReferentOn)} Semicolon :: (filterRegressSvm $0) ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  (filterRegressSvm _tk0) )
                (if _verbose 
                    (writeRule
                         {FSTATEMENT: Svmregress{(xReferentOn)} Semicolon :: (filterRegressSvm $0) ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : Semicolon
       ) ; end begin
     ) ; end case : Svmregress{(xReferentOn)}
     ;; ====================
     ;; case: Highest{(xReferentOn)}
     ;; ====================
     (if (begin (setq _ip _ip0)
      (and (if (<> (setq _tk1 (_getToken))[Highest:] #void) true (setq _ip _ip0)) (xReferentOn)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: SEXPRESSION
         ;; ====================
         (if (if (<> (setq _tk2 (_SYNRULE_SEXPRESSION)) morphFail)true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: CUT
             ;; ====================
             (if (if (<> (setq _tk3 (_SYNRULE_CUT)) morphFail)true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; *********************************************************
                 ;; RULE: FSTATEMENT: Highest{(xReferentOn)} SEXPRESSION CUT :: (highestCut $0 $2.Value $3.Value) ::
                 ;; *********************************************************
                 (if true
                  (begin
                    (setq _ret  (highestCut _tk0 _tk2.Value _tk3.Value) )
                    (if _verbose 
                        (writeRule
                             {FSTATEMENT: Highest{(xReferentOn)} SEXPRESSION CUT :: (highestCut $0 $2.Value $3.Value) ::}
                             _ret  _tk0 _tk1 _tk2 _tk3 #void #void #void #void #void #void #void))
                    (setq _indent (isub _indent 1))
                    (return _ret))
                 ) ; end case : _default
               ) ; end begin
             ) ; end case : CUT
           ) ; end begin
         ) ; end case : SEXPRESSION
       ) ; end begin
     ) ; end case : Highest{(xReferentOn)}
     ;; **************************************************
     ;; DEFAULT (if we get to here we failed to recognize)
     ;; Note:       (at least on this repetition)
     ;; **************************************************
     (begin
        (if (= _tkn 1)
            ;; If we have never recognized any token on previous repetitions 
            (begin
               (setq _ip _oldIp)     
               (if _verbose (_logLine "Rejecting Rule FSTATEMENT on: " source:))
               (setq _indent (isub _indent 1))
               (setq _ret morphFail)
               ) ; end begin then
            else
            ;; If we have recognized some tokens on previous repetitions, then return $0
            ;; Note: In Syntax Rules, we do not fail the rule if it has 
            ;;       recognized a token in one or more repetitions of the rule. 
            (begin
               (setq _ip _ip0)
               (setq _ret _tk0)
               ) ; end begin else
            ) ; end if
        ) ; end of DEFAULT

   ;; End of tests for user defined rules.
   
   (if (> _verboseSynIn.FSTATEMENT -1)
       (begin
          (setq _verbose _verboseSave)
          (if (>= _verboseSynCount.FSTATEMENT _verboseSynIn.FSTATEMENT) (error "Count" "in Routine FSTATEMENT"))
       ))

   ;; Repeat or return to caller is handled here.
   _ret) ;; end _SYNRULE_FSTATEMENT
























;;**EXPORTKEY**:esm:selector:_SYNRULE_FSTMTLIST
;; ************************************************
;; FSTMTLIST user defined Syntax Rule implementation
;; Summary: This Lambda implements the FSTMTLIST
;;          user defined rule. Each rule test is
;;          marked with a boxed comment line for
;;          ease of human understanding.
;; Note: This code was machine generated by ParseLib.
;; ************************************************
(defchild esm.selector _SYNRULE_FSTMTLIST(...)
   ;; The token object _io is a persistent variable
   ;; The token pointer _ip is a persistent variable
   ;; All getToken logic increments _ip on each token fetch.
   ;; The return value from the rule is stored in _ret.
   ;; Remember that this rule may repeat depending on _repeatSW.
   vars:(_tk0 _tk1 _tk2 _tk3 _tk4  _tk5  _i
         _tk6 _tk7 _tk8 _tk9 _tkn  _repeatSW
         _ak0 _ak1 _ak2 _ak3 _ak4  _ak5  _tkthis
         _ak6 _ak7 _ak8 _ak9 _ret  _oldIp
         _ip0 _ip1 _ip2 _ip3 _ip4  _ip5
         _ip6 _ip7 _ip8 _ip9 _tkn  
         _verboseSave       
        ) ; end temporary variables
   ;; Collect any arguments which may have been passed to this rule
   (if (> (setq _tkn (argCount)) 0)
       (begin (setq _ak0 (argFetch 0))
          (if (> _tkn 1) 
              (begin (setq _ak1 (argFetch 1))
                 (if (> _tkn 2) 
                     (begin (setq _ak2 (argFetch 2))
                        (if (> _tkn 3) 
                            (begin (setq _ak3 (argFetch 3))
                               (if (> _tkn 4) 
                                   (begin (setq _ak4 (argFetch 4))
                                      (if (> _tkn 5) 
                                          (begin (setq _ak5 (argFetch 5))
                                             (if (> _tkn 6) 
                                                 (begin (setq _ak6 (argFetch 6))
                                                    (if (> _tkn 7) 
                                                        (begin (setq _ak7 (argFetch 7))
                                                           (if (> _tkn 8) 
                                                               (begin (setq _ak8 (argFetch 8))
                                                               (if (> _tkn 9) (setq _ak9 (argFetch 9))
                                                           ))))))))))))))))))) ; end argument collection
   ;; The default structure is given the named rule attribute
   (setq _tk0 (new Structure: FSTMTLIST: true  ))
   (setq _tkn 0)
   (setq _oldIp _ip)
   (setq _repeatSW true)

   (if (= _verboseSynCount.FSTMTLIST #void) 
          (setq _verboseSynCount.FSTMTLIST 1) 
          (setq _verboseSynCount.FSTMTLIST (iadd _verboseSynCount.FSTMTLIST 1)))

   (if (and (<> _verboseSynIn.FSTMTLIST #void) (> _verboseSynIn.FSTMTLIST -1))
       (begin (setq _verboseSave _verbose) (setq _verbose true))) 

   ;; Save the old token pointer and test for user defined rules
   Skip::
     (setq _indent (iadd _indent 1))
     (if _verbose (_logLine "Attempting FSTMTLIST Rule on: " source:))
     (setq _ip0 _ip)
     (++ _tkn)
     (setq _repeatSW false)

     ;; *****************************************
     ;; Begin testing for each user defined rule.
     ;; *****************************************

     ;; ====================
     ;; case: FSTATEMENT
     ;; ====================
     (if (if (<> (setq _tk1 (_SYNRULE_FSTATEMENT)) morphFail)true (setq _ip _ip0))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; *********************************************************
         ;; RULE: FSTMTLIST: FSTATEMENT << (addToBeginList $0 $1.Value) >>
         ;; *********************************************************
         (if true
          (begin
            (setq _ret  (addToBeginList _tk0 _tk1.Value) )
            (if _verbose 
                (writeRule
                     {FSTMTLIST: FSTATEMENT << (addToBeginList $0 $1.Value) >>}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (goto Skip:))

         ) ; end case : _default
       ) ; end begin
     ) ; end case : FSTATEMENT
     ;; ====================
     ;; case: Semicolon
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_getToken))[Semicolon:] #void) true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; *********************************************************
         ;; RULE: FSTMTLIST: Semicolon << $0 >>
         ;; *********************************************************
         (if true
          (begin
            (setq _ret  _tk0 )
            (if _verbose 
                (writeRule
                     {FSTMTLIST: Semicolon << $0 >>}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (goto Skip:))

         ) ; end case : _default
       ) ; end begin
     ) ; end case : Semicolon
     ;; **************************************************
     ;; DEFAULT (if we get to here we failed to recognize)
     ;; Note:       (at least on this repetition)
     ;; **************************************************
     (begin
        (if (= _tkn 1)
            ;; If we have never recognized any token on previous repetitions 
            (begin
               (setq _ip _oldIp)     
               (if _verbose (_logLine "Rejecting Rule FSTMTLIST on: " source:))
               (setq _indent (isub _indent 1))
               (setq _ret morphFail)
               ) ; end begin then
            else
            ;; If we have recognized some tokens on previous repetitions, then return $0
            ;; Note: In Syntax Rules, we do not fail the rule if it has 
            ;;       recognized a token in one or more repetitions of the rule. 
            (begin
               (setq _ip _ip0)
               (setq _ret _tk0)
               ) ; end begin else
            ) ; end if
        ) ; end of DEFAULT

   ;; End of tests for user defined rules.
   
   (if (> _verboseSynIn.FSTMTLIST -1)
       (begin
          (setq _verbose _verboseSave)
          (if (>= _verboseSynCount.FSTMTLIST _verboseSynIn.FSTMTLIST) (error "Count" "in Routine FSTMTLIST"))
       ))

   ;; Repeat or return to caller is handled here.
   _ret) ;; end _SYNRULE_FSTMTLIST
























;;**EXPORTKEY**:esm:selector:_SYNRULE_FUNCTION
;; ************************************************
;; FUNCTION user defined Syntax Rule implementation
;; Summary: This Lambda implements the FUNCTION
;;          user defined rule. Each rule test is
;;          marked with a boxed comment line for
;;          ease of human understanding.
;; Note: This code was machine generated by ParseLib.
;; ************************************************
(defchild esm.selector _SYNRULE_FUNCTION(...)
   ;; The token object _io is a persistent variable
   ;; The token pointer _ip is a persistent variable
   ;; All getToken logic increments _ip on each token fetch.
   ;; The return value from the rule is stored in _ret.
   ;; Remember that this rule may repeat depending on _repeatSW.
   vars:(_tk0 _tk1 _tk2 _tk3 _tk4  _tk5  _i
         _tk6 _tk7 _tk8 _tk9 _tkn  _repeatSW
         _ak0 _ak1 _ak2 _ak3 _ak4  _ak5  _tkthis
         _ak6 _ak7 _ak8 _ak9 _ret  _oldIp
         _ip0 _ip1 _ip2 _ip3 _ip4  _ip5
         _ip6 _ip7 _ip8 _ip9 _tkn  
         _verboseSave       
        ) ; end temporary variables
   ;; Collect any arguments which may have been passed to this rule
   (if (> (setq _tkn (argCount)) 0)
       (begin (setq _ak0 (argFetch 0))
          (if (> _tkn 1) 
              (begin (setq _ak1 (argFetch 1))
                 (if (> _tkn 2) 
                     (begin (setq _ak2 (argFetch 2))
                        (if (> _tkn 3) 
                            (begin (setq _ak3 (argFetch 3))
                               (if (> _tkn 4) 
                                   (begin (setq _ak4 (argFetch 4))
                                      (if (> _tkn 5) 
                                          (begin (setq _ak5 (argFetch 5))
                                             (if (> _tkn 6) 
                                                 (begin (setq _ak6 (argFetch 6))
                                                    (if (> _tkn 7) 
                                                        (begin (setq _ak7 (argFetch 7))
                                                           (if (> _tkn 8) 
                                                               (begin (setq _ak8 (argFetch 8))
                                                               (if (> _tkn 9) (setq _ak9 (argFetch 9))
                                                           ))))))))))))))))))) ; end argument collection
   ;; The default structure is given the named rule attribute
   (setq _tk0 (new Structure: FUNCTION: true  ))
   (setq _tkn 0)
   (setq _oldIp _ip)
   (setq _repeatSW true)

   (if (= _verboseSynCount.FUNCTION #void) 
          (setq _verboseSynCount.FUNCTION 1) 
          (setq _verboseSynCount.FUNCTION (iadd _verboseSynCount.FUNCTION 1)))

   (if (and (<> _verboseSynIn.FUNCTION #void) (> _verboseSynIn.FUNCTION -1))
       (begin (setq _verboseSave _verbose) (setq _verbose true))) 

   ;; Save the old token pointer and test for user defined rules
   Skip::
     (setq _indent (iadd _indent 1))
     (if _verbose (_logLine "Attempting FUNCTION Rule on: " source:))
     (setq _ip0 _ip)
     (++ _tkn)
     (setq _repeatSW false)

     ;; *****************************************
     ;; Begin testing for each user defined rule.
     ;; *****************************************

     ;; ====================
     ;; case: Function
     ;; ====================
     (if (if (<> (setq _tk1 (_getToken))[Function:] #void) true (setq _ip _ip0))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; *********************************************************
         ;; RULE: FUNCTION: Function :: (setq (pushVars $0)[Charpos:] $1.Charpos) ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ret  (setq (pushVars _tk0)[Charpos:] _tk1.Charpos) )
            (if _verbose 
                (writeRule
                     {FUNCTION: Function :: (setq (pushVars $0)[Charpos:] $1.Charpos) ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : Function
     ;; **************************************************
     ;; DEFAULT (if we get to here we failed to recognize)
     ;; Note:       (at least on this repetition)
     ;; **************************************************
     (begin
        (if (= _tkn 1)
            ;; If we have never recognized any token on previous repetitions 
            (begin
               (setq _ip _oldIp)     
               (if _verbose (_logLine "Rejecting Rule FUNCTION on: " source:))
               (setq _indent (isub _indent 1))
               (setq _ret morphFail)
               ) ; end begin then
            else
            ;; If we have recognized some tokens on previous repetitions, then return $0
            ;; Note: In Syntax Rules, we do not fail the rule if it has 
            ;;       recognized a token in one or more repetitions of the rule. 
            (begin
               (setq _ip _ip0)
               (setq _ret _tk0)
               ) ; end begin else
            ) ; end if
        ) ; end of DEFAULT

   ;; End of tests for user defined rules.
   
   (if (> _verboseSynIn.FUNCTION -1)
       (begin
          (setq _verbose _verboseSave)
          (if (>= _verboseSynCount.FUNCTION _verboseSynIn.FUNCTION) (error "Count" "in Routine FUNCTION"))
       ))

   ;; Repeat or return to caller is handled here.
   _ret) ;; end _SYNRULE_FUNCTION
























;;**EXPORTKEY**:esm:selector:_SYNRULE_MAIN
;; ************************************************
;; MAIN user defined Syntax Rule implementation
;; Summary: This Lambda implements the MAIN
;;          user defined rule. Each rule test is
;;          marked with a boxed comment line for
;;          ease of human understanding.
;; Note: This code was machine generated by ParseLib.
;; ************************************************
(defchild esm.selector _SYNRULE_MAIN(...)
   ;; The token object _io is a persistent variable
   ;; The token pointer _ip is a persistent variable
   ;; All getToken logic increments _ip on each token fetch.
   ;; The return value from the rule is stored in _ret.
   ;; Remember that this rule may repeat depending on _repeatSW.
   vars:(_tk0 _tk1 _tk2 _tk3 _tk4  _tk5  _i
         _tk6 _tk7 _tk8 _tk9 _tkn  _repeatSW
         _ak0 _ak1 _ak2 _ak3 _ak4  _ak5  _tkthis
         _ak6 _ak7 _ak8 _ak9 _ret  _oldIp
         _ip0 _ip1 _ip2 _ip3 _ip4  _ip5
         _ip6 _ip7 _ip8 _ip9 _tkn  
         _verboseSave       
        ) ; end temporary variables
   ;; Collect any arguments which may have been passed to this rule
   (if (> (setq _tkn (argCount)) 0)
       (begin (setq _ak0 (argFetch 0))
          (if (> _tkn 1) 
              (begin (setq _ak1 (argFetch 1))
                 (if (> _tkn 2) 
                     (begin (setq _ak2 (argFetch 2))
                        (if (> _tkn 3) 
                            (begin (setq _ak3 (argFetch 3))
                               (if (> _tkn 4) 
                                   (begin (setq _ak4 (argFetch 4))
                                      (if (> _tkn 5) 
                                          (begin (setq _ak5 (argFetch 5))
                                             (if (> _tkn 6) 
                                                 (begin (setq _ak6 (argFetch 6))
                                                    (if (> _tkn 7) 
                                                        (begin (setq _ak7 (argFetch 7))
                                                           (if (> _tkn 8) 
                                                               (begin (setq _ak8 (argFetch 8))
                                                               (if (> _tkn 9) (setq _ak9 (argFetch 9))
                                                           ))))))))))))))))))) ; end argument collection
   ;; The default structure is given the named rule attribute
   (setq _tk0 (new Structure: MAIN: true  ))
   (setq _tkn 0)
   (setq _oldIp _ip)
   (setq _repeatSW true)

   (if (= _verboseSynCount.MAIN #void) 
          (setq _verboseSynCount.MAIN 1) 
          (setq _verboseSynCount.MAIN (iadd _verboseSynCount.MAIN 1)))

   (if (and (<> _verboseSynIn.MAIN #void) (> _verboseSynIn.MAIN -1))
       (begin (setq _verboseSave _verbose) (setq _verbose true))) 

   ;; Save the old token pointer and test for user defined rules
   Skip::
     (setq _indent (iadd _indent 1))
     (if _verbose (_logLine "Attempting MAIN Rule on: " source:))
     (setq _ip0 _ip)
     (++ _tkn)
     (setq _repeatSW false)

     ;; *****************************************
     ;; Begin testing for each user defined rule.
     ;; *****************************************

     ;; ====================
     ;; case: #
     ;; ====================
     (if (if (= (setq _tk1 (_getToken))[Value:] |#|:) true (setq _ip _ip0))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: "selector"
         ;; ====================
         (if (if (= (setq _tk2 (_getToken))[Value:] "selector") true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: #
             ;; ====================
             (if (if (= (setq _tk3 (_getToken))[Value:] |#|:) true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; *********************************************************
                 ;; RULE: MAIN: # "selector" # || (= $3.Charpos 8) || << true >>
                 ;; *********************************************************
                 (if  (= _tk3.Charpos 8) 
                  (begin
                    (setq _ret  true )
                    (if _verbose 
                        (writeRule
                             {MAIN: # "selector" # || (= $3.Charpos 8) || << true >>}
                             _ret  _tk0 _tk1 _tk2 _tk3 #void #void #void #void #void #void #void))
                    (setq _indent (isub _indent 1))
                    (goto Skip:))

                 ) ; end case : _default
               ) ; end begin
             ) ; end case : #
           ) ; end begin
         ) ; end case : "selector"
       ) ; end begin
     ) ; end case : #
     ;; ====================
     ;; case: STMTLIST
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_SYNRULE_STMTLIST)) morphFail)true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: Eof
         ;; ====================
         (if (if (_eofToken)true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; *********************************************************
             ;; RULE: MAIN: STMTLIST Eof :: (list $1.Value) ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  (list _tk1.Value) )
                (if _verbose 
                    (writeRule
                         {MAIN: STMTLIST Eof :: (list $1.Value) ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : Eof
         ;; ====================
         ;; case: Value
         ;; ====================
         (if (begin (setq _ip _ip1)
          (if (<> (setq _tk2 (_getToken))[Value:] #void) true (setq _ip _ip1)))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; *********************************************************
             ;; RULE: MAIN: STMTLIST Value :: (_makeError "JS 100" $2.Charpos "Invalid expression") ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  (_makeError "JS 100" _tk2.Charpos "Invalid expression") )
                (if _verbose 
                    (writeRule
                         {MAIN: STMTLIST Value :: (_makeError "JS 100" $2.Charpos "Invalid expression") ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : Value
       ) ; end begin
     ) ; end case : STMTLIST
     ;; ====================
     ;; case: Value
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_getToken))[Value:] #void) true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; *********************************************************
         ;; RULE: MAIN: Value :: (_makeError "JS 101" $1.Charpos "Invalid expression") ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ret  (_makeError "JS 101" _tk1.Charpos "Invalid expression") )
            (if _verbose 
                (writeRule
                     {MAIN: Value :: (_makeError "JS 101" $1.Charpos "Invalid expression") ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : Value
     ;; ====================
     ;; case: Eof
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (_eofToken)true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; *********************************************************
         ;; RULE: MAIN: Eof :: (list #void) ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ret  (list #void) )
            (if _verbose 
                (writeRule
                     {MAIN: Eof :: (list #void) ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : Eof
     ;; **************************************************
     ;; DEFAULT (if we get to here we failed to recognize)
     ;; Note:       (at least on this repetition)
     ;; **************************************************
     (begin
        (if (= _tkn 1)
            ;; If we have never recognized any token on previous repetitions 
            (begin
               (setq _ip _oldIp)     
               (if _verbose (_logLine "Rejecting Rule MAIN on: " source:))
               (setq _indent (isub _indent 1))
               (setq _ret morphFail)
               ) ; end begin then
            else
            ;; If we have recognized some tokens on previous repetitions, then return $0
            ;; Note: In Syntax Rules, we do not fail the rule if it has 
            ;;       recognized a token in one or more repetitions of the rule. 
            (begin
               (setq _ip _ip0)
               (setq _ret _tk0)
               ) ; end begin else
            ) ; end if
        ) ; end of DEFAULT

   ;; End of tests for user defined rules.
   
   (if (> _verboseSynIn.MAIN -1)
       (begin
          (setq _verbose _verboseSave)
          (if (>= _verboseSynCount.MAIN _verboseSynIn.MAIN) (error "Count" "in Routine MAIN"))
       ))

   ;; Repeat or return to caller is handled here.
   _ret) ;; end _SYNRULE_MAIN
























;;**EXPORTKEY**:esm:selector:_SYNRULE_METHOD
;; ************************************************
;; METHOD user defined Syntax Rule implementation
;; Summary: This Lambda implements the METHOD
;;          user defined rule. Each rule test is
;;          marked with a boxed comment line for
;;          ease of human understanding.
;; Note: This code was machine generated by ParseLib.
;; ************************************************
(defchild esm.selector _SYNRULE_METHOD(...)
   ;; The token object _io is a persistent variable
   ;; The token pointer _ip is a persistent variable
   ;; All getToken logic increments _ip on each token fetch.
   ;; The return value from the rule is stored in _ret.
   ;; Remember that this rule may repeat depending on _repeatSW.
   vars:(_tk0 _tk1 _tk2 _tk3 _tk4  _tk5  _i
         _tk6 _tk7 _tk8 _tk9 _tkn  _repeatSW
         _ak0 _ak1 _ak2 _ak3 _ak4  _ak5  _tkthis
         _ak6 _ak7 _ak8 _ak9 _ret  _oldIp
         _ip0 _ip1 _ip2 _ip3 _ip4  _ip5
         _ip6 _ip7 _ip8 _ip9 _tkn  
         _verboseSave       
        ) ; end temporary variables
   ;; Collect any arguments which may have been passed to this rule
   (if (> (setq _tkn (argCount)) 0)
       (begin (setq _ak0 (argFetch 0))
          (if (> _tkn 1) 
              (begin (setq _ak1 (argFetch 1))
                 (if (> _tkn 2) 
                     (begin (setq _ak2 (argFetch 2))
                        (if (> _tkn 3) 
                            (begin (setq _ak3 (argFetch 3))
                               (if (> _tkn 4) 
                                   (begin (setq _ak4 (argFetch 4))
                                      (if (> _tkn 5) 
                                          (begin (setq _ak5 (argFetch 5))
                                             (if (> _tkn 6) 
                                                 (begin (setq _ak6 (argFetch 6))
                                                    (if (> _tkn 7) 
                                                        (begin (setq _ak7 (argFetch 7))
                                                           (if (> _tkn 8) 
                                                               (begin (setq _ak8 (argFetch 8))
                                                               (if (> _tkn 9) (setq _ak9 (argFetch 9))
                                                           ))))))))))))))))))) ; end argument collection
   ;; The default structure is given the named rule attribute
   (setq _tk0 (new Structure: METHOD: true  ))
   (setq _tkn 0)
   (setq _oldIp _ip)
   (setq _repeatSW true)

   (if (= _verboseSynCount.METHOD #void) 
          (setq _verboseSynCount.METHOD 1) 
          (setq _verboseSynCount.METHOD (iadd _verboseSynCount.METHOD 1)))

   (if (and (<> _verboseSynIn.METHOD #void) (> _verboseSynIn.METHOD -1))
       (begin (setq _verboseSave _verbose) (setq _verbose true))) 

   ;; Save the old token pointer and test for user defined rules
   Skip::
     (setq _indent (iadd _indent 1))
     (if _verbose (_logLine "Attempting METHOD Rule on: " source:))
     (setq _ip0 _ip)
     (++ _tkn)
     (setq _repeatSW false)

     ;; *****************************************
     ;; Begin testing for each user defined rule.
     ;; *****************************************

     ;; ====================
     ;; case: Method
     ;; ====================
     (if (if (<> (setq _tk1 (_getToken))[Method:] #void) true (setq _ip _ip0))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; *********************************************************
         ;; RULE: METHOD: Method :: (setq (pushVars $0)[Charpos:] $1.Charpos) ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ret  (setq (pushVars _tk0)[Charpos:] _tk1.Charpos) )
            (if _verbose 
                (writeRule
                     {METHOD: Method :: (setq (pushVars $0)[Charpos:] $1.Charpos) ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : Method
     ;; **************************************************
     ;; DEFAULT (if we get to here we failed to recognize)
     ;; Note:       (at least on this repetition)
     ;; **************************************************
     (begin
        (if (= _tkn 1)
            ;; If we have never recognized any token on previous repetitions 
            (begin
               (setq _ip _oldIp)     
               (if _verbose (_logLine "Rejecting Rule METHOD on: " source:))
               (setq _indent (isub _indent 1))
               (setq _ret morphFail)
               ) ; end begin then
            else
            ;; If we have recognized some tokens on previous repetitions, then return $0
            ;; Note: In Syntax Rules, we do not fail the rule if it has 
            ;;       recognized a token in one or more repetitions of the rule. 
            (begin
               (setq _ip _ip0)
               (setq _ret _tk0)
               ) ; end begin else
            ) ; end if
        ) ; end of DEFAULT

   ;; End of tests for user defined rules.
   
   (if (> _verboseSynIn.METHOD -1)
       (begin
          (setq _verbose _verboseSave)
          (if (>= _verboseSynCount.METHOD _verboseSynIn.METHOD) (error "Count" "in Routine METHOD"))
       ))

   ;; Repeat or return to caller is handled here.
   _ret) ;; end _SYNRULE_METHOD
























;;**EXPORTKEY**:esm:selector:_SYNRULE_NAME
;; ************************************************
;; NAME user defined Syntax Rule implementation
;; Summary: This Lambda implements the NAME
;;          user defined rule. Each rule test is
;;          marked with a boxed comment line for
;;          ease of human understanding.
;; Note: This code was machine generated by ParseLib.
;; ************************************************
(defchild esm.selector _SYNRULE_NAME(...)
   ;; The token object _io is a persistent variable
   ;; The token pointer _ip is a persistent variable
   ;; All getToken logic increments _ip on each token fetch.
   ;; The return value from the rule is stored in _ret.
   ;; Remember that this rule may repeat depending on _repeatSW.
   vars:(_tk0 _tk1 _tk2 _tk3 _tk4  _tk5  _i
         _tk6 _tk7 _tk8 _tk9 _tkn  _repeatSW
         _ak0 _ak1 _ak2 _ak3 _ak4  _ak5  _tkthis
         _ak6 _ak7 _ak8 _ak9 _ret  _oldIp
         _ip0 _ip1 _ip2 _ip3 _ip4  _ip5
         _ip6 _ip7 _ip8 _ip9 _tkn  
         _verboseSave       
        ) ; end temporary variables
   ;; Collect any arguments which may have been passed to this rule
   (if (> (setq _tkn (argCount)) 0)
       (begin (setq _ak0 (argFetch 0))
          (if (> _tkn 1) 
              (begin (setq _ak1 (argFetch 1))
                 (if (> _tkn 2) 
                     (begin (setq _ak2 (argFetch 2))
                        (if (> _tkn 3) 
                            (begin (setq _ak3 (argFetch 3))
                               (if (> _tkn 4) 
                                   (begin (setq _ak4 (argFetch 4))
                                      (if (> _tkn 5) 
                                          (begin (setq _ak5 (argFetch 5))
                                             (if (> _tkn 6) 
                                                 (begin (setq _ak6 (argFetch 6))
                                                    (if (> _tkn 7) 
                                                        (begin (setq _ak7 (argFetch 7))
                                                           (if (> _tkn 8) 
                                                               (begin (setq _ak8 (argFetch 8))
                                                               (if (> _tkn 9) (setq _ak9 (argFetch 9))
                                                           ))))))))))))))))))) ; end argument collection
   ;; The default structure is given the named rule attribute
   (setq _tk0 (new Structure: NAME: true  ))
   (setq _tkn 0)
   (setq _oldIp _ip)
   (setq _repeatSW true)

   (if (= _verboseSynCount.NAME #void) 
          (setq _verboseSynCount.NAME 1) 
          (setq _verboseSynCount.NAME (iadd _verboseSynCount.NAME 1)))

   (if (and (<> _verboseSynIn.NAME #void) (> _verboseSynIn.NAME -1))
       (begin (setq _verboseSave _verbose) (setq _verbose true))) 

   ;; Save the old token pointer and test for user defined rules
   Skip::
     (setq _indent (iadd _indent 1))
     (if _verbose (_logLine "Attempting NAME Rule on: " source:))
     (setq _ip0 _ip)
     (++ _tkn)
     (setq _repeatSW false)

     ;; *****************************************
     ;; Begin testing for each user defined rule.
     ;; *****************************************

     ;; ====================
     ;; case: Name{(isFieldName $1)}
     ;; ====================
     (if (and (if (<> (setq _tk1 (_getToken))[Name:] #void) true (setq _ip _ip0)) (isFieldName _tk1))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: QUALIFY((xReferent $0 $1))
         ;; ====================
         (if (if (<> (setq _tk2 (_SYNRULE_QUALIFY (xReferent _tk0 _tk1))) morphFail)true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; *********************************************************
             ;; RULE: NAME: Name{(isFieldName $1)} QUALIFY((xReferent $0 $1)) :: (setq $2[Charpos:] $1.Charpos) ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  (setq _tk2[Charpos:] _tk1.Charpos) )
                (if _verbose 
                    (writeRule
                         {NAME: Name{(isFieldName $1)} QUALIFY((xReferent $0 $1)) :: (setq $2[Charpos:] $1.Charpos) ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : QUALIFY((xReferent $0 $1))
         ;; *********************************************************
         ;; RULE: NAME: Name{(isFieldName $1)} :: (xReferent $0 $1) ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ip _ip1)
            (setq _ret  (xReferent _tk0 _tk1) )
            (if _verbose 
                (writeRule
                     {NAME: Name{(isFieldName $1)} :: (xReferent $0 $1) ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : Name{(isFieldName $1)}
     ;; ====================
     ;; case: Name
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_getToken))[Name:] #void) true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: QUALIFY($1)
         ;; ====================
         (if (if (<> (setq _tk2 (_SYNRULE_QUALIFY _tk1)) morphFail)true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; *********************************************************
             ;; RULE: NAME: Name QUALIFY($1) :: (setq $2[Charpos:] $1.Charpos) ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  (setq _tk2[Charpos:] _tk1.Charpos) )
                (if _verbose 
                    (writeRule
                         {NAME: Name QUALIFY($1) :: (setq $2[Charpos:] $1.Charpos) ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : QUALIFY($1)
         ;; *********************************************************
         ;; RULE: NAME: Name :: (setVType $1) ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ip _ip1)
            (setq _ret  (setVType _tk1) )
            (if _verbose 
                (writeRule
                     {NAME: Name :: (setVType $1) ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : Name
     ;; **************************************************
     ;; DEFAULT (if we get to here we failed to recognize)
     ;; Note:       (at least on this repetition)
     ;; **************************************************
     (begin
        (if (= _tkn 1)
            ;; If we have never recognized any token on previous repetitions 
            (begin
               (setq _ip _oldIp)     
               (if _verbose (_logLine "Rejecting Rule NAME on: " source:))
               (setq _indent (isub _indent 1))
               (setq _ret morphFail)
               ) ; end begin then
            else
            ;; If we have recognized some tokens on previous repetitions, then return $0
            ;; Note: In Syntax Rules, we do not fail the rule if it has 
            ;;       recognized a token in one or more repetitions of the rule. 
            (begin
               (setq _ip _ip0)
               (setq _ret _tk0)
               ) ; end begin else
            ) ; end if
        ) ; end of DEFAULT

   ;; End of tests for user defined rules.
   
   (if (> _verboseSynIn.NAME -1)
       (begin
          (setq _verbose _verboseSave)
          (if (>= _verboseSynCount.NAME _verboseSynIn.NAME) (error "Count" "in Routine NAME"))
       ))

   ;; Repeat or return to caller is handled here.
   _ret) ;; end _SYNRULE_NAME
























;;**EXPORTKEY**:esm:selector:_SYNRULE_NUMLIST
;; ************************************************
;; NUMLIST user defined Syntax Rule implementation
;; Summary: This Lambda implements the NUMLIST
;;          user defined rule. Each rule test is
;;          marked with a boxed comment line for
;;          ease of human understanding.
;; Note: This code was machine generated by ParseLib.
;; ************************************************
(defchild esm.selector _SYNRULE_NUMLIST(...)
   ;; The token object _io is a persistent variable
   ;; The token pointer _ip is a persistent variable
   ;; All getToken logic increments _ip on each token fetch.
   ;; The return value from the rule is stored in _ret.
   ;; Remember that this rule may repeat depending on _repeatSW.
   vars:(_tk0 _tk1 _tk2 _tk3 _tk4  _tk5  _i
         _tk6 _tk7 _tk8 _tk9 _tkn  _repeatSW
         _ak0 _ak1 _ak2 _ak3 _ak4  _ak5  _tkthis
         _ak6 _ak7 _ak8 _ak9 _ret  _oldIp
         _ip0 _ip1 _ip2 _ip3 _ip4  _ip5
         _ip6 _ip7 _ip8 _ip9 _tkn  
         _verboseSave       
        ) ; end temporary variables
   ;; Collect any arguments which may have been passed to this rule
   (if (> (setq _tkn (argCount)) 0)
       (begin (setq _ak0 (argFetch 0))
          (if (> _tkn 1) 
              (begin (setq _ak1 (argFetch 1))
                 (if (> _tkn 2) 
                     (begin (setq _ak2 (argFetch 2))
                        (if (> _tkn 3) 
                            (begin (setq _ak3 (argFetch 3))
                               (if (> _tkn 4) 
                                   (begin (setq _ak4 (argFetch 4))
                                      (if (> _tkn 5) 
                                          (begin (setq _ak5 (argFetch 5))
                                             (if (> _tkn 6) 
                                                 (begin (setq _ak6 (argFetch 6))
                                                    (if (> _tkn 7) 
                                                        (begin (setq _ak7 (argFetch 7))
                                                           (if (> _tkn 8) 
                                                               (begin (setq _ak8 (argFetch 8))
                                                               (if (> _tkn 9) (setq _ak9 (argFetch 9))
                                                           ))))))))))))))))))) ; end argument collection
   ;; The default structure is given the named rule attribute
   (setq _tk0 (new Structure: NUMLIST: true  ))
   (setq _tkn 0)
   (setq _oldIp _ip)
   (setq _repeatSW true)

   (if (= _verboseSynCount.NUMLIST #void) 
          (setq _verboseSynCount.NUMLIST 1) 
          (setq _verboseSynCount.NUMLIST (iadd _verboseSynCount.NUMLIST 1)))

   (if (and (<> _verboseSynIn.NUMLIST #void) (> _verboseSynIn.NUMLIST -1))
       (begin (setq _verboseSave _verbose) (setq _verbose true))) 

   ;; Save the old token pointer and test for user defined rules
   Skip::
     (setq _indent (iadd _indent 1))
     (if _verbose (_logLine "Attempting NUMLIST Rule on: " source:))
     (setq _ip0 _ip)
     (++ _tkn)
     (setq _repeatSW false)

     ;; *****************************************
     ;; Begin testing for each user defined rule.
     ;; *****************************************

     ;; ====================
     ;; case: -
     ;; ====================
     (if (if (= (setq _tk1 (_getToken))[Value:] |-|:) true (setq _ip _ip0))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: Number
         ;; ====================
         (if (if (<> (setq _tk2 (_getToken))[Number:] #void) true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: RightParen
             ;; ====================
             (if (if (<> (setq _tk3 (_getToken))[RightParen:] #void) true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; *********************************************************
                 ;; RULE: NUMLIST: - Number RightParen :: (setq $0.Value (append $0.Value " -" $2.Value " )")) ::
                 ;; *********************************************************
                 (if true
                  (begin
                    (setq _ret  (setq _tk0.Value (append _tk0.Value " -" _tk2.Value " )")) )
                    (if _verbose 
                        (writeRule
                             {NUMLIST: - Number RightParen :: (setq $0.Value (append $0.Value " -" $2.Value " )")) ::}
                             _ret  _tk0 _tk1 _tk2 _tk3 #void #void #void #void #void #void #void))
                    (setq _indent (isub _indent 1))
                    (return _ret))
                 ) ; end case : _default
               ) ; end begin
             ) ; end case : RightParen
             ;; ====================
             ;; case: NUMLIST
             ;; ====================
             (if (begin (setq _ip _ip2)
              (if (<> (setq _tk3 (_SYNRULE_NUMLIST)) morphFail)true (setq _ip _ip2)))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; *********************************************************
                 ;; RULE: NUMLIST: - Number NUMLIST :: (setq $0.Value (append $0.Value " -" $2.Value " " $3.Value)) ::
                 ;; *********************************************************
                 (if true
                  (begin
                    (setq _ret  (setq _tk0.Value (append _tk0.Value " -" _tk2.Value " " _tk3.Value)) )
                    (if _verbose 
                        (writeRule
                             {NUMLIST: - Number NUMLIST :: (setq $0.Value (append $0.Value " -" $2.Value " " $3.Value)) ::}
                             _ret  _tk0 _tk1 _tk2 _tk3 #void #void #void #void #void #void #void))
                    (setq _indent (isub _indent 1))
                    (return _ret))
                 ) ; end case : _default
               ) ; end begin
             ) ; end case : NUMLIST
           ) ; end begin
         ) ; end case : Number
       ) ; end begin
     ) ; end case : -
     ;; ====================
     ;; case: Number
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_getToken))[Number:] #void) true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: RightParen
         ;; ====================
         (if (if (<> (setq _tk2 (_getToken))[RightParen:] #void) true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; *********************************************************
             ;; RULE: NUMLIST: Number RightParen :: (setq $0.Value (append $0.Value " " $1.Value " )")) ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  (setq _tk0.Value (append _tk0.Value " " _tk1.Value " )")) )
                (if _verbose 
                    (writeRule
                         {NUMLIST: Number RightParen :: (setq $0.Value (append $0.Value " " $1.Value " )")) ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : RightParen
         ;; ====================
         ;; case: NUMLIST
         ;; ====================
         (if (begin (setq _ip _ip1)
          (if (<> (setq _tk2 (_SYNRULE_NUMLIST)) morphFail)true (setq _ip _ip1)))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; *********************************************************
             ;; RULE: NUMLIST: Number NUMLIST :: (setq $0.Value (append $0.Value " " $1.Value " " $2.Value)) ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  (setq _tk0.Value (append _tk0.Value " " _tk1.Value " " _tk2.Value)) )
                (if _verbose 
                    (writeRule
                         {NUMLIST: Number NUMLIST :: (setq $0.Value (append $0.Value " " $1.Value " " $2.Value)) ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : NUMLIST
       ) ; end begin
     ) ; end case : Number
     ;; ====================
     ;; case: Value
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_getToken))[Value:] #void) true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; *********************************************************
         ;; RULE: NUMLIST: Value :: (_makeError "JS 137" $1.Charpos "Invalid weight list")::
         ;; *********************************************************
         (if true
          (begin
            (setq _ret  (_makeError "JS 137" _tk1.Charpos "Invalid weight list"))
            (if _verbose 
                (writeRule
                     {NUMLIST: Value :: (_makeError "JS 137" $1.Charpos "Invalid weight list")::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : Value
     ;; **************************************************
     ;; DEFAULT (if we get to here we failed to recognize)
     ;; Note:       (at least on this repetition)
     ;; **************************************************
     (begin
        (if (= _tkn 1)
            ;; If we have never recognized any token on previous repetitions 
            (begin
               (setq _ip _oldIp)     
               (if _verbose (_logLine "Rejecting Rule NUMLIST on: " source:))
               (setq _indent (isub _indent 1))
               (setq _ret morphFail)
               ) ; end begin then
            else
            ;; If we have recognized some tokens on previous repetitions, then return $0
            ;; Note: In Syntax Rules, we do not fail the rule if it has 
            ;;       recognized a token in one or more repetitions of the rule. 
            (begin
               (setq _ip _ip0)
               (setq _ret _tk0)
               ) ; end begin else
            ) ; end if
        ) ; end of DEFAULT

   ;; End of tests for user defined rules.
   
   (if (> _verboseSynIn.NUMLIST -1)
       (begin
          (setq _verbose _verboseSave)
          (if (>= _verboseSynCount.NUMLIST _verboseSynIn.NUMLIST) (error "Count" "in Routine NUMLIST"))
       ))

   ;; Repeat or return to caller is handled here.
   _ret) ;; end _SYNRULE_NUMLIST
























;;**EXPORTKEY**:esm:selector:_SYNRULE_ORPHAN
;; ************************************************
;; ORPHAN user defined Syntax Rule implementation
;; Summary: This Lambda implements the ORPHAN
;;          user defined rule. Each rule test is
;;          marked with a boxed comment line for
;;          ease of human understanding.
;; Note: This code was machine generated by ParseLib.
;; ************************************************
(defchild esm.selector _SYNRULE_ORPHAN(...)
   ;; The token object _io is a persistent variable
   ;; The token pointer _ip is a persistent variable
   ;; All getToken logic increments _ip on each token fetch.
   ;; The return value from the rule is stored in _ret.
   ;; Remember that this rule may repeat depending on _repeatSW.
   vars:(_tk0 _tk1 _tk2 _tk3 _tk4  _tk5  _i
         _tk6 _tk7 _tk8 _tk9 _tkn  _repeatSW
         _ak0 _ak1 _ak2 _ak3 _ak4  _ak5  _tkthis
         _ak6 _ak7 _ak8 _ak9 _ret  _oldIp
         _ip0 _ip1 _ip2 _ip3 _ip4  _ip5
         _ip6 _ip7 _ip8 _ip9 _tkn  
         _verboseSave       
        ) ; end temporary variables
   ;; Collect any arguments which may have been passed to this rule
   (if (> (setq _tkn (argCount)) 0)
       (begin (setq _ak0 (argFetch 0))
          (if (> _tkn 1) 
              (begin (setq _ak1 (argFetch 1))
                 (if (> _tkn 2) 
                     (begin (setq _ak2 (argFetch 2))
                        (if (> _tkn 3) 
                            (begin (setq _ak3 (argFetch 3))
                               (if (> _tkn 4) 
                                   (begin (setq _ak4 (argFetch 4))
                                      (if (> _tkn 5) 
                                          (begin (setq _ak5 (argFetch 5))
                                             (if (> _tkn 6) 
                                                 (begin (setq _ak6 (argFetch 6))
                                                    (if (> _tkn 7) 
                                                        (begin (setq _ak7 (argFetch 7))
                                                           (if (> _tkn 8) 
                                                               (begin (setq _ak8 (argFetch 8))
                                                               (if (> _tkn 9) (setq _ak9 (argFetch 9))
                                                           ))))))))))))))))))) ; end argument collection
   ;; The default structure is given the named rule attribute
   (setq _tk0 (new Structure: ORPHAN: true  ))
   (setq _tkn 0)
   (setq _oldIp _ip)
   (setq _repeatSW true)

   (if (= _verboseSynCount.ORPHAN #void) 
          (setq _verboseSynCount.ORPHAN 1) 
          (setq _verboseSynCount.ORPHAN (iadd _verboseSynCount.ORPHAN 1)))

   (if (and (<> _verboseSynIn.ORPHAN #void) (> _verboseSynIn.ORPHAN -1))
       (begin (setq _verboseSave _verbose) (setq _verbose true))) 

   ;; Save the old token pointer and test for user defined rules
   Skip::
     (setq _indent (iadd _indent 1))
     (if _verbose (_logLine "Attempting ORPHAN Rule on: " source:))
     (setq _ip0 _ip)
     (++ _tkn)
     (setq _repeatSW false)

     ;; *****************************************
     ;; Begin testing for each user defined rule.
     ;; *****************************************

     ;; ====================
     ;; case: Orphan
     ;; ====================
     (if (if (<> (setq _tk1 (_getToken))[Orphan:] #void) true (setq _ip _ip0))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; *********************************************************
         ;; RULE: ORPHAN: Orphan :: (setq (pushVars $0)[Charpos:] $1.Charpos) ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ret  (setq (pushVars _tk0)[Charpos:] _tk1.Charpos) )
            (if _verbose 
                (writeRule
                     {ORPHAN: Orphan :: (setq (pushVars $0)[Charpos:] $1.Charpos) ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : Orphan
     ;; **************************************************
     ;; DEFAULT (if we get to here we failed to recognize)
     ;; Note:       (at least on this repetition)
     ;; **************************************************
     (begin
        (if (= _tkn 1)
            ;; If we have never recognized any token on previous repetitions 
            (begin
               (setq _ip _oldIp)     
               (if _verbose (_logLine "Rejecting Rule ORPHAN on: " source:))
               (setq _indent (isub _indent 1))
               (setq _ret morphFail)
               ) ; end begin then
            else
            ;; If we have recognized some tokens on previous repetitions, then return $0
            ;; Note: In Syntax Rules, we do not fail the rule if it has 
            ;;       recognized a token in one or more repetitions of the rule. 
            (begin
               (setq _ip _ip0)
               (setq _ret _tk0)
               ) ; end begin else
            ) ; end if
        ) ; end of DEFAULT

   ;; End of tests for user defined rules.
   
   (if (> _verboseSynIn.ORPHAN -1)
       (begin
          (setq _verbose _verboseSave)
          (if (>= _verboseSynCount.ORPHAN _verboseSynIn.ORPHAN) (error "Count" "in Routine ORPHAN"))
       ))

   ;; Repeat or return to caller is handled here.
   _ret) ;; end _SYNRULE_ORPHAN
























;;**EXPORTKEY**:esm:selector:_SYNRULE_PARMLIST
;; ************************************************
;; PARMLIST user defined Syntax Rule implementation
;; Summary: This Lambda implements the PARMLIST
;;          user defined rule. Each rule test is
;;          marked with a boxed comment line for
;;          ease of human understanding.
;; Note: This code was machine generated by ParseLib.
;; ************************************************
(defchild esm.selector _SYNRULE_PARMLIST(...)
   ;; The token object _io is a persistent variable
   ;; The token pointer _ip is a persistent variable
   ;; All getToken logic increments _ip on each token fetch.
   ;; The return value from the rule is stored in _ret.
   ;; Remember that this rule may repeat depending on _repeatSW.
   vars:(_tk0 _tk1 _tk2 _tk3 _tk4  _tk5  _i
         _tk6 _tk7 _tk8 _tk9 _tkn  _repeatSW
         _ak0 _ak1 _ak2 _ak3 _ak4  _ak5  _tkthis
         _ak6 _ak7 _ak8 _ak9 _ret  _oldIp
         _ip0 _ip1 _ip2 _ip3 _ip4  _ip5
         _ip6 _ip7 _ip8 _ip9 _tkn  
         _verboseSave       
        ) ; end temporary variables
   ;; Collect any arguments which may have been passed to this rule
   (if (> (setq _tkn (argCount)) 0)
       (begin (setq _ak0 (argFetch 0))
          (if (> _tkn 1) 
              (begin (setq _ak1 (argFetch 1))
                 (if (> _tkn 2) 
                     (begin (setq _ak2 (argFetch 2))
                        (if (> _tkn 3) 
                            (begin (setq _ak3 (argFetch 3))
                               (if (> _tkn 4) 
                                   (begin (setq _ak4 (argFetch 4))
                                      (if (> _tkn 5) 
                                          (begin (setq _ak5 (argFetch 5))
                                             (if (> _tkn 6) 
                                                 (begin (setq _ak6 (argFetch 6))
                                                    (if (> _tkn 7) 
                                                        (begin (setq _ak7 (argFetch 7))
                                                           (if (> _tkn 8) 
                                                               (begin (setq _ak8 (argFetch 8))
                                                               (if (> _tkn 9) (setq _ak9 (argFetch 9))
                                                           ))))))))))))))))))) ; end argument collection
   ;; The default structure is given the named rule attribute
   (setq _tk0 (new Structure: PARMLIST: true  ))
   (setq _tkn 0)
   (setq _oldIp _ip)
   (setq _repeatSW true)

   (if (= _verboseSynCount.PARMLIST #void) 
          (setq _verboseSynCount.PARMLIST 1) 
          (setq _verboseSynCount.PARMLIST (iadd _verboseSynCount.PARMLIST 1)))

   (if (and (<> _verboseSynIn.PARMLIST #void) (> _verboseSynIn.PARMLIST -1))
       (begin (setq _verboseSave _verbose) (setq _verbose true))) 

   ;; Save the old token pointer and test for user defined rules
   Skip::
     (setq _indent (iadd _indent 1))
     (if _verbose (_logLine "Attempting PARMLIST Rule on: " source:))
     (setq _ip0 _ip)
     (++ _tkn)
     (setq _repeatSW false)

     ;; *****************************************
     ;; Begin testing for each user defined rule.
     ;; *****************************************

     ;; ====================
     ;; case: Name
     ;; ====================
     (if (if (<> (setq _tk1 (_getToken))[Name:] #void) true (setq _ip _ip0))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: Comma
         ;; ====================
         (if (if (<> (setq _tk2 (_getToken))[Comma:] #void) true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: PARMLIST
             ;; ====================
             (if (if (<> (setq _tk3 (_SYNRULE_PARMLIST)) morphFail)true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; *********************************************************
                 ;; RULE: PARMLIST: Name Comma PARMLIST :: (addVar avarVector obj: $1.Value (setq $0.Value (insert $3.Value 0 $1.Value)) #void) ::
                 ;; *********************************************************
                 (if true
                  (begin
                    (setq _ret  (addVar avarVector obj: _tk1.Value (setq _tk0.Value (insert _tk3.Value 0 _tk1.Value)) #void) )
                    (if _verbose 
                        (writeRule
                             {PARMLIST: Name Comma PARMLIST :: (addVar avarVector obj: $1.Value (setq $0.Value (insert $3.Value 0 $1.Value)) #void) ::}
                             _ret  _tk0 _tk1 _tk2 _tk3 #void #void #void #void #void #void #void))
                    (setq _indent (isub _indent 1))
                    (return _ret))
                 ) ; end case : _default
               ) ; end begin
             ) ; end case : PARMLIST
           ) ; end begin
         ) ; end case : Comma
         ;; ====================
         ;; case: RightParen
         ;; ====================
         (if (begin (setq _ip _ip1)
          (if (<> (setq _tk2 (_getToken))[RightParen:] #void) true (setq _ip _ip1)))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; *********************************************************
             ;; RULE: PARMLIST: Name RightParen :: (addVar avarVector obj: $1.Value (setq $0.Value (new Vector: 1 $1.Value)) #void) ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  (addVar avarVector obj: _tk1.Value (setq _tk0.Value (new Vector: 1 _tk1.Value)) #void) )
                (if _verbose 
                    (writeRule
                         {PARMLIST: Name RightParen :: (addVar avarVector obj: $1.Value (setq $0.Value (new Vector: 1 $1.Value)) #void) ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : RightParen
       ) ; end begin
     ) ; end case : Name
     ;; ====================
     ;; case: RightParen
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_getToken))[RightParen:] #void) true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; *********************************************************
         ;; RULE: PARMLIST: RightParen :: (setq $0.Value #void) ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ret  (setq _tk0.Value #void) )
            (if _verbose 
                (writeRule
                     {PARMLIST: RightParen :: (setq $0.Value #void) ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : RightParen
     ;; ====================
     ;; case: Type
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_getToken))[Type:] #void) true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: Name
         ;; ====================
         (if (if (<> (setq _tk2 (_getToken))[Name:] #void) true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: Comma
             ;; ====================
             (if (if (<> (setq _tk3 (_getToken))[Comma:] #void) true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; ====================
                 ;; case: PARMLIST
                 ;; ====================
                 (if (if (<> (setq _tk4 (_SYNRULE_PARMLIST)) morphFail)true (setq _ip _ip3))
                   (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 _ip) 
                     ;; *********************************************************
                     ;; RULE: PARMLIST: Type Name Comma PARMLIST :: (addVar avarVector $1.Value $2.Value (setq $0.Value (insert $4.Value 0 $2.Value)) #void) ::
                     ;; *********************************************************
                     (if true
                      (begin
                        (setq _ret  (addVar avarVector _tk1.Value _tk2.Value (setq _tk0.Value (insert _tk4.Value 0 _tk2.Value)) #void) )
                        (if _verbose 
                            (writeRule
                                 {PARMLIST: Type Name Comma PARMLIST :: (addVar avarVector $1.Value $2.Value (setq $0.Value (insert $4.Value 0 $2.Value)) #void) ::}
                                 _ret  _tk0 _tk1 _tk2 _tk3 _tk4 #void #void #void #void #void #void))
                        (setq _indent (isub _indent 1))
                        (return _ret))
                     ) ; end case : _default
                   ) ; end begin
                 ) ; end case : PARMLIST
               ) ; end begin
             ) ; end case : Comma
             ;; ====================
             ;; case: RightParen
             ;; ====================
             (if (begin (setq _ip _ip2)
              (if (<> (setq _tk3 (_getToken))[RightParen:] #void) true (setq _ip _ip2)))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; *********************************************************
                 ;; RULE: PARMLIST: Type Name RightParen :: (addVar avarVector $1.Value $2.Value (setq $0.Value (new Vector: 1 $2.Value)) #void) ::
                 ;; *********************************************************
                 (if true
                  (begin
                    (setq _ret  (addVar avarVector _tk1.Value _tk2.Value (setq _tk0.Value (new Vector: 1 _tk2.Value)) #void) )
                    (if _verbose 
                        (writeRule
                             {PARMLIST: Type Name RightParen :: (addVar avarVector $1.Value $2.Value (setq $0.Value (new Vector: 1 $2.Value)) #void) ::}
                             _ret  _tk0 _tk1 _tk2 _tk3 #void #void #void #void #void #void #void))
                    (setq _indent (isub _indent 1))
                    (return _ret))
                 ) ; end case : _default
               ) ; end begin
             ) ; end case : RightParen
           ) ; end begin
         ) ; end case : Name
       ) ; end begin
     ) ; end case : Type
     ;; ====================
     ;; case: Value
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_getToken))[Value:] #void) true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; *********************************************************
         ;; RULE: PARMLIST: Value :: (error (append "Invalid argument list [" $1.Charpos "] " (mid $IN $1.Charpos 120))) ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ret  (error (append "Invalid argument list [" _tk1.Charpos "] " (mid _tkIN _tk1.Charpos 120))) )
            (if _verbose 
                (writeRule
                     {PARMLIST: Value :: (error (append "Invalid argument list [" $1.Charpos "] " (mid $IN $1.Charpos 120))) ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : Value
     ;; **************************************************
     ;; DEFAULT (if we get to here we failed to recognize)
     ;; Note:       (at least on this repetition)
     ;; **************************************************
     (begin
        (if (= _tkn 1)
            ;; If we have never recognized any token on previous repetitions 
            (begin
               (setq _ip _oldIp)     
               (if _verbose (_logLine "Rejecting Rule PARMLIST on: " source:))
               (setq _indent (isub _indent 1))
               (setq _ret morphFail)
               ) ; end begin then
            else
            ;; If we have recognized some tokens on previous repetitions, then return $0
            ;; Note: In Syntax Rules, we do not fail the rule if it has 
            ;;       recognized a token in one or more repetitions of the rule. 
            (begin
               (setq _ip _ip0)
               (setq _ret _tk0)
               ) ; end begin else
            ) ; end if
        ) ; end of DEFAULT

   ;; End of tests for user defined rules.
   
   (if (> _verboseSynIn.PARMLIST -1)
       (begin
          (setq _verbose _verboseSave)
          (if (>= _verboseSynCount.PARMLIST _verboseSynIn.PARMLIST) (error "Count" "in Routine PARMLIST"))
       ))

   ;; Repeat or return to caller is handled here.
   _ret) ;; end _SYNRULE_PARMLIST
























;;**EXPORTKEY**:esm:selector:_SYNRULE_PHRASE
;; ************************************************
;; PHRASE user defined Syntax Rule implementation
;; Summary: This Lambda implements the PHRASE
;;          user defined rule. Each rule test is
;;          marked with a boxed comment line for
;;          ease of human understanding.
;; Note: This code was machine generated by ParseLib.
;; ************************************************
(defchild esm.selector _SYNRULE_PHRASE(...)
   ;; The token object _io is a persistent variable
   ;; The token pointer _ip is a persistent variable
   ;; All getToken logic increments _ip on each token fetch.
   ;; The return value from the rule is stored in _ret.
   ;; Remember that this rule may repeat depending on _repeatSW.
   vars:(_tk0 _tk1 _tk2 _tk3 _tk4  _tk5  _i
         _tk6 _tk7 _tk8 _tk9 _tkn  _repeatSW
         _ak0 _ak1 _ak2 _ak3 _ak4  _ak5  _tkthis
         _ak6 _ak7 _ak8 _ak9 _ret  _oldIp
         _ip0 _ip1 _ip2 _ip3 _ip4  _ip5
         _ip6 _ip7 _ip8 _ip9 _tkn  
         _verboseSave       
        ) ; end temporary variables
   ;; Collect any arguments which may have been passed to this rule
   (if (> (setq _tkn (argCount)) 0)
       (begin (setq _ak0 (argFetch 0))
          (if (> _tkn 1) 
              (begin (setq _ak1 (argFetch 1))
                 (if (> _tkn 2) 
                     (begin (setq _ak2 (argFetch 2))
                        (if (> _tkn 3) 
                            (begin (setq _ak3 (argFetch 3))
                               (if (> _tkn 4) 
                                   (begin (setq _ak4 (argFetch 4))
                                      (if (> _tkn 5) 
                                          (begin (setq _ak5 (argFetch 5))
                                             (if (> _tkn 6) 
                                                 (begin (setq _ak6 (argFetch 6))
                                                    (if (> _tkn 7) 
                                                        (begin (setq _ak7 (argFetch 7))
                                                           (if (> _tkn 8) 
                                                               (begin (setq _ak8 (argFetch 8))
                                                               (if (> _tkn 9) (setq _ak9 (argFetch 9))
                                                           ))))))))))))))))))) ; end argument collection
   ;; The default structure is given the named rule attribute
   (setq _tk0 (new Structure: PHRASE: true  ))
   (setq _tkn 0)
   (setq _oldIp _ip)
   (setq _repeatSW true)

   (if (= _verboseSynCount.PHRASE #void) 
          (setq _verboseSynCount.PHRASE 1) 
          (setq _verboseSynCount.PHRASE (iadd _verboseSynCount.PHRASE 1)))

   (if (and (<> _verboseSynIn.PHRASE #void) (> _verboseSynIn.PHRASE -1))
       (begin (setq _verboseSave _verbose) (setq _verbose true))) 

   ;; Save the old token pointer and test for user defined rules
   Skip::
     (setq _indent (iadd _indent 1))
     (if _verbose (_logLine "Attempting PHRASE Rule on: " source:))
     (setq _ip0 _ip)
     (++ _tkn)
     (setq _repeatSW false)

     ;; *****************************************
     ;; Begin testing for each user defined rule.
     ;; *****************************************

     ;; ====================
     ;; case: Logical
     ;; ====================
     (if (if (<> (setq _tk1 (_getToken))[Logical:] #void) true (setq _ip _ip0))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: SEXPRESSION
         ;; ====================
         (if (if (<> (setq _tk2 (_SYNRULE_SEXPRESSION)) morphFail)true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; *********************************************************
             ;; RULE: PHRASE: Logical SEXPRESSION :: (setq (setq $0.Value (list $1.Lisp $2.Value))[VType:] bool:) ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  (setq (setq _tk0.Value (list _tk1.Lisp _tk2.Value))[VType:] bool:) )
                (if _verbose 
                    (writeRule
                         {PHRASE: Logical SEXPRESSION :: (setq (setq $0.Value (list $1.Lisp $2.Value))[VType:] bool:) ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : SEXPRESSION
         ;; *********************************************************
         ;; RULE: PHRASE: Logical :: (_makeError "JS 118" $1.Charpos (append "Invalid " $1.Value " operator")) ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ip _ip1)
            (setq _ret  (_makeError "JS 118" _tk1.Charpos (append "Invalid " _tk1.Value " operator")) )
            (if _verbose 
                (writeRule
                     {PHRASE: Logical :: (_makeError "JS 118" $1.Charpos (append "Invalid " $1.Value " operator")) ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : Logical
     ;; ====================
     ;; case: Increment
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_getToken))[Increment:] #void) true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: NAME
         ;; ====================
         (if (if (<> (setq _tk2 (_SYNRULE_NAME)) morphFail)true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; *********************************************************
             ;; RULE: PHRASE: Increment NAME :: (postfix $0 $2 $1 false) ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  (postfix _tk0 _tk2 _tk1 false) )
                (if _verbose 
                    (writeRule
                         {PHRASE: Increment NAME :: (postfix $0 $2 $1 false) ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : NAME
         ;; *********************************************************
         ;; RULE: PHRASE: Increment :: (_makeError "JS 119" $1.Charpos (append "Invalid " $1.Value " operator")) ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ip _ip1)
            (setq _ret  (_makeError "JS 119" _tk1.Charpos (append "Invalid " _tk1.Value " operator")) )
            (if _verbose 
                (writeRule
                     {PHRASE: Increment :: (_makeError "JS 119" $1.Charpos (append "Invalid " $1.Value " operator")) ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : Increment
     ;; ====================
     ;; case: LeftBrace
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_getToken))[LeftBrace:] #void) true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: STMTLIST
         ;; ====================
         (if (if (<> (setq _tk2 (_SYNRULE_STMTLIST)) morphFail)true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: RightBrace
             ;; ====================
             (if (if (<> (setq _tk3 (_getToken))[RightBrace:] #void) true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; *********************************************************
                 ;; RULE: PHRASE: LeftBrace STMTLIST RightBrace :: $2 ::
                 ;; *********************************************************
                 (if true
                  (begin
                    (setq _ret  _tk2 )
                    (if _verbose 
                        (writeRule
                             {PHRASE: LeftBrace STMTLIST RightBrace :: $2 ::}
                             _ret  _tk0 _tk1 _tk2 _tk3 #void #void #void #void #void #void #void))
                    (setq _indent (isub _indent 1))
                    (return _ret))
                 ) ; end case : _default
               ) ; end begin
             ) ; end case : RightBrace
           ) ; end begin
         ) ; end case : STMTLIST
         ;; *********************************************************
         ;; RULE: PHRASE: LeftBrace :: (_makeError "JS 120" $1.Charpos "Invalid statement block") ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ip _ip1)
            (setq _ret  (_makeError "JS 120" _tk1.Charpos "Invalid statement block") )
            (if _verbose 
                (writeRule
                     {PHRASE: LeftBrace :: (_makeError "JS 120" $1.Charpos "Invalid statement block") ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : LeftBrace
     ;; ====================
     ;; case: If
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_getToken))[If:] #void) true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: SEXPRESSION
         ;; ====================
         (if (if (<> (setq _tk2 (_SYNRULE_SEXPRESSION)) morphFail)true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: SEXPRESSION
             ;; ====================
             (if (if (<> (setq _tk3 (_SYNRULE_SEXPRESSION)) morphFail)true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; ====================
                 ;; case: Else
                 ;; ====================
                 (if (if (<> (setq _tk4 (_getToken))[Else:] #void) true (setq _ip _ip3))
                   (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 _ip) 
                     ;; ====================
                     ;; case: SEXPRESSION
                     ;; ====================
                     (if (if (<> (setq _tk5 (_SYNRULE_SEXPRESSION)) morphFail)true (setq _ip _ip4))
                       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip5 _ip) 
                         ;; *********************************************************
                         ;; RULE: PHRASE: If SEXPRESSION SEXPRESSION Else SEXPRESSION :: (setq $0.Value (list |if|: $2.Value $3.Value $5.Value)) ::
                         ;; *********************************************************
                         (if true
                          (begin
                            (setq _ret  (setq _tk0.Value (list |if|: _tk2.Value _tk3.Value _tk5.Value)) )
                            (if _verbose 
                                (writeRule
                                     {PHRASE: If SEXPRESSION SEXPRESSION Else SEXPRESSION :: (setq $0.Value (list |if|: $2.Value $3.Value $5.Value)) ::}
                                     _ret  _tk0 _tk1 _tk2 _tk3 _tk4 _tk5 #void #void #void #void #void))
                            (setq _indent (isub _indent 1))
                            (return _ret))
                         ) ; end case : _default
                       ) ; end begin
                     ) ; end case : SEXPRESSION
                     ;; *********************************************************
                     ;; RULE: PHRASE: If SEXPRESSION SEXPRESSION Else :: (_makeError "JS 121" $1.Charpos "Invalid else statement") ::
                     ;; *********************************************************
                     (if true
                      (begin
                        (setq _ip _ip4)
                        (setq _ret  (_makeError "JS 121" _tk1.Charpos "Invalid else statement") )
                        (if _verbose 
                            (writeRule
                                 {PHRASE: If SEXPRESSION SEXPRESSION Else :: (_makeError "JS 121" $1.Charpos "Invalid else statement") ::}
                                 _ret  _tk0 _tk1 _tk2 _tk3 _tk4 #void #void #void #void #void #void))
                        (setq _indent (isub _indent 1))
                        (return _ret))
                     ) ; end case : _default
                   ) ; end begin
                 ) ; end case : Else
                 ;; *********************************************************
                 ;; RULE: PHRASE: If SEXPRESSION SEXPRESSION :: (setq $0.Value (list |if|: $2.Value $3.Value)) ::
                 ;; *********************************************************
                 (if true
                  (begin
                    (setq _ip _ip3)
                    (setq _ret  (setq _tk0.Value (list |if|: _tk2.Value _tk3.Value)) )
                    (if _verbose 
                        (writeRule
                             {PHRASE: If SEXPRESSION SEXPRESSION :: (setq $0.Value (list |if|: $2.Value $3.Value)) ::}
                             _ret  _tk0 _tk1 _tk2 _tk3 #void #void #void #void #void #void #void))
                    (setq _indent (isub _indent 1))
                    (return _ret))
                 ) ; end case : _default
               ) ; end begin
             ) ; end case : SEXPRESSION
             ;; *********************************************************
             ;; RULE: PHRASE: If SEXPRESSION :: (_makeError "JS 121" $1.Charpos "Invalid then statement") ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ip _ip2)
                (setq _ret  (_makeError "JS 121" _tk1.Charpos "Invalid then statement") )
                (if _verbose 
                    (writeRule
                         {PHRASE: If SEXPRESSION :: (_makeError "JS 121" $1.Charpos "Invalid then statement") ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : SEXPRESSION
         ;; *********************************************************
         ;; RULE: PHRASE: If :: (_makeError "JS 122" $1.Charpos "Invalid if statement") ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ip _ip1)
            (setq _ret  (_makeError "JS 122" _tk1.Charpos "Invalid if statement") )
            (if _verbose 
                (writeRule
                     {PHRASE: If :: (_makeError "JS 122" $1.Charpos "Invalid if statement") ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : If
     ;; ====================
     ;; case: While
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_getToken))[While:] #void) true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: SEXPRESSION
         ;; ====================
         (if (if (<> (setq _tk2 (_SYNRULE_SEXPRESSION)) morphFail)true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: SEXPRESSION
             ;; ====================
             (if (if (<> (setq _tk3 (_SYNRULE_SEXPRESSION)) morphFail)true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; *********************************************************
                 ;; RULE: PHRASE: While SEXPRESSION SEXPRESSION :: (setq $0.Value (list |while|: $2.Value $3.Value)) ::
                 ;; *********************************************************
                 (if true
                  (begin
                    (setq _ret  (setq _tk0.Value (list |while|: _tk2.Value _tk3.Value)) )
                    (if _verbose 
                        (writeRule
                             {PHRASE: While SEXPRESSION SEXPRESSION :: (setq $0.Value (list |while|: $2.Value $3.Value)) ::}
                             _ret  _tk0 _tk1 _tk2 _tk3 #void #void #void #void #void #void #void))
                    (setq _indent (isub _indent 1))
                    (return _ret))
                 ) ; end case : _default
               ) ; end begin
             ) ; end case : SEXPRESSION
           ) ; end begin
         ) ; end case : SEXPRESSION
         ;; *********************************************************
         ;; RULE: PHRASE: While :: (_makeError "JS 123" $1.Charpos "Invalid while statement") ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ip _ip1)
            (setq _ret  (_makeError "JS 123" _tk1.Charpos "Invalid while statement") )
            (if _verbose 
                (writeRule
                     {PHRASE: While :: (_makeError "JS 123" $1.Charpos "Invalid while statement") ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : While
     ;; ====================
     ;; case: For
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_getToken))[For:] #void) true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: LeftParen
         ;; ====================
         (if (if (<> (setq _tk2 (_getToken))[LeftParen:] #void) true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: PHRASE
             ;; ====================
             (if (if (<> (setq _tk3 (_SYNRULE_PHRASE)) morphFail)true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; ====================
                 ;; case: Semicolon
                 ;; ====================
                 (if (if (<> (setq _tk4 (_getToken))[Semicolon:] #void) true (setq _ip _ip3))
                   (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 _ip) 
                     ;; ====================
                     ;; case: SEXPRESSION
                     ;; ====================
                     (if (if (<> (setq _tk5 (_SYNRULE_SEXPRESSION)) morphFail)true (setq _ip _ip4))
                       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip5 _ip) 
                         ;; ====================
                         ;; case: Semicolon
                         ;; ====================
                         (if (if (<> (setq _tk6 (_getToken))[Semicolon:] #void) true (setq _ip _ip5))
                           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip6 _ip) 
                             ;; ====================
                             ;; case: PHRASE
                             ;; ====================
                             (if (if (<> (setq _tk7 (_SYNRULE_PHRASE)) morphFail)true (setq _ip _ip6))
                               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip7 _ip) 
                                 ;; ====================
                                 ;; case: RightParen
                                 ;; ====================
                                 (if (if (<> (setq _tk8 (_getToken))[RightParen:] #void) true (setq _ip _ip7))
                                   (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip8 _ip) 
                                     ;; ====================
                                     ;; case: SEXPRESSION
                                     ;; ====================
                                     (if (if (<> (setq _tk9 (_SYNRULE_SEXPRESSION)) morphFail)true (setq _ip _ip8))
                                       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip9 _ip) 
                                         ;; *********************************************************
                                         ;; RULE: PHRASE: For LeftParen PHRASE Semicolon SEXPRESSION Semicolon PHRASE RightParen SEXPRESSION :: (setq $0.Value (list |begin|: $3.Value (list |while|: $5.Value $9.Value $7.Value))) ::
                                         ;; *********************************************************
                                         (if true
                                          (begin
                                            (setq _ret  (setq _tk0.Value (list |begin|: _tk3.Value (list |while|: _tk5.Value _tk9.Value _tk7.Value))) )
                                            (if _verbose 
                                                (writeRule
                                                     {PHRASE: For LeftParen PHRASE Semicolon SEXPRESSION Semicolon PHRASE RightParen SEXPRESSION :: (setq $0.Value (list |begin|: $3.Value (list |while|: $5.Value $9.Value $7.Value))) ::}
                                                     _ret  _tk0 _tk1 _tk2 _tk3 _tk4 _tk5 _tk6 _tk7 _tk8 _tk9 #void))
                                            (setq _indent (isub _indent 1))
                                            (return _ret))
                                         ) ; end case : _default
                                       ) ; end begin
                                     ) ; end case : SEXPRESSION
                                   ) ; end begin
                                 ) ; end case : RightParen
                               ) ; end begin
                             ) ; end case : PHRASE
                           ) ; end begin
                         ) ; end case : Semicolon
                       ) ; end begin
                     ) ; end case : SEXPRESSION
                   ) ; end begin
                 ) ; end case : Semicolon
               ) ; end begin
             ) ; end case : PHRASE
           ) ; end begin
         ) ; end case : LeftParen
         ;; *********************************************************
         ;; RULE: PHRASE: For :: (_makeError "JS 124" $1.Charpos "Invalid for statement") ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ip _ip1)
            (setq _ret  (_makeError "JS 124" _tk1.Charpos "Invalid for statement") )
            (if _verbose 
                (writeRule
                     {PHRASE: For :: (_makeError "JS 124" $1.Charpos "Invalid for statement") ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : For
     ;; ====================
     ;; case: Reg
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_getToken))[Reg:] #void) true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: VAR(regVector setq:)
         ;; ====================
         (if (if (<> (setq _tk2 (_SYNRULE_VAR regVector setq:)) morphFail)true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; *********************************************************
             ;; RULE: PHRASE: Reg VAR(regVector setq:) :: $2 ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  _tk2 )
                (if _verbose 
                    (writeRule
                         {PHRASE: Reg VAR(regVector setq:) :: $2 ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : VAR(regVector setq:)
         ;; *********************************************************
         ;; RULE: PHRASE: Reg :: (_makeError "JS 125" $1.Charpos "Invalid reg statement") ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ip _ip1)
            (setq _ret  (_makeError "JS 125" _tk1.Charpos "Invalid reg statement") )
            (if _verbose 
                (writeRule
                     {PHRASE: Reg :: (_makeError "JS 125" $1.Charpos "Invalid reg statement") ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : Reg
     ;; ====================
     ;; case: Var
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_getToken))[Var:] #void) true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: VAR(varVector setq:)
         ;; ====================
         (if (if (<> (setq _tk2 (_SYNRULE_VAR varVector setq:)) morphFail)true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; *********************************************************
             ;; RULE: PHRASE: Var VAR(varVector setq:) :: $2 ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  _tk2 )
                (if _verbose 
                    (writeRule
                         {PHRASE: Var VAR(varVector setq:) :: $2 ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : VAR(varVector setq:)
         ;; *********************************************************
         ;; RULE: PHRASE: Var :: (_makeError "JS 125" $1.Charpos "Invalid var statement") ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ip _ip1)
            (setq _ret  (_makeError "JS 125" _tk1.Charpos "Invalid var statement") )
            (if _verbose 
                (writeRule
                     {PHRASE: Var :: (_makeError "JS 125" $1.Charpos "Invalid var statement") ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : Var
     ;; ====================
     ;; case: Pvar
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_getToken))[Pvar:] #void) true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: VAR(pvarVector define:)
         ;; ====================
         (if (if (<> (setq _tk2 (_SYNRULE_VAR pvarVector define:)) morphFail)true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; *********************************************************
             ;; RULE: PHRASE: Pvar VAR(pvarVector define:) :: $2 ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  _tk2 )
                (if _verbose 
                    (writeRule
                         {PHRASE: Pvar VAR(pvarVector define:) :: $2 ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : VAR(pvarVector define:)
         ;; *********************************************************
         ;; RULE: PHRASE: Pvar :: (_makeError "JS 126" $1.Charpos "Invalid pvar statement") ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ip _ip1)
            (setq _ret  (_makeError "JS 126" _tk1.Charpos "Invalid pvar statement") )
            (if _verbose 
                (writeRule
                     {PHRASE: Pvar :: (_makeError "JS 126" $1.Charpos "Invalid pvar statement") ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : Pvar
     ;; ====================
     ;; case: Cvar
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_getToken))[Cvar:] #void) true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: VAR(cvarVector setq:)
         ;; ====================
         (if (if (<> (setq _tk2 (_SYNRULE_VAR cvarVector setq:)) morphFail)true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; *********************************************************
             ;; RULE: PHRASE: Cvar VAR(cvarVector setq:) :: $2 ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  _tk2 )
                (if _verbose 
                    (writeRule
                         {PHRASE: Cvar VAR(cvarVector setq:) :: $2 ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : VAR(cvarVector setq:)
         ;; *********************************************************
         ;; RULE: PHRASE: Cvar :: (_makeError "JS 127" $1.Charpos "Invalid cvar statement") ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ip _ip1)
            (setq _ret  (_makeError "JS 127" _tk1.Charpos "Invalid cvar statement") )
            (if _verbose 
                (writeRule
                     {PHRASE: Cvar :: (_makeError "JS 127" $1.Charpos "Invalid cvar statement") ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : Cvar
     ;; ====================
     ;; case: Name
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_getToken))[Name:] #void) true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: CFCALL($1)
         ;; ====================
         (if (if (<> (setq _tk2 (_SYNRULE_CFCALL _tk1)) morphFail)true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; *********************************************************
             ;; RULE: PHRASE: Name CFCALL($1) :: $2 ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  _tk2 )
                (if _verbose 
                    (writeRule
                         {PHRASE: Name CFCALL($1) :: $2 ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : CFCALL($1)
         ;; ====================
         ;; case: LeftParen
         ;; ====================
         (if (begin (setq _ip _ip1)
          (if (<> (setq _tk2 (_getToken))[LeftParen:] #void) true (setq _ip _ip1)))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: ARGLIST
             ;; ====================
             (if (if (<> (setq _tk3 (_SYNRULE_ARGLIST)) morphFail)true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; *********************************************************
                 ;; RULE: PHRASE: Name LeftParen ARGLIST :: (argList $0 $1.Value $3.Value) ::
                 ;; *********************************************************
                 (if true
                  (begin
                    (setq _ret  (argList _tk0 _tk1.Value _tk3.Value) )
                    (if _verbose 
                        (writeRule
                             {PHRASE: Name LeftParen ARGLIST :: (argList $0 $1.Value $3.Value) ::}
                             _ret  _tk0 _tk1 _tk2 _tk3 #void #void #void #void #void #void #void))
                    (setq _indent (isub _indent 1))
                    (return _ret))
                 ) ; end case : _default
               ) ; end begin
             ) ; end case : ARGLIST
           ) ; end begin
         ) ; end case : LeftParen
       ) ; end begin
     ) ; end case : Name
     ;; ====================
     ;; case: NAME
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_SYNRULE_NAME)) morphFail)true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: Increment
         ;; ====================
         (if (if (<> (setq _tk2 (_getToken))[Increment:] #void) true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; *********************************************************
             ;; RULE: PHRASE: NAME Increment :: (postfix $0 $1 $2 true) ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  (postfix _tk0 _tk1 _tk2 true) )
                (if _verbose 
                    (writeRule
                         {PHRASE: NAME Increment :: (postfix $0 $1 $2 true) ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : Increment
         ;; ====================
         ;; case: MathAssignmentOperator
         ;; ====================
         (if (begin (setq _ip _ip1)
          (if (<> (setq _tk2 (_getToken))[MathAssignmentOperator:] #void) true (setq _ip _ip1)))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: SEXPRESSION
             ;; ====================
             (if (if (<> (setq _tk3 (_SYNRULE_SEXPRESSION)) morphFail)true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; *********************************************************
                 ;; RULE: PHRASE: NAME MathAssignmentOperator SEXPRESSION :: (assignMath $0 $2 $1 $3) ::
                 ;; *********************************************************
                 (if true
                  (begin
                    (setq _ret  (assignMath _tk0 _tk2 _tk1 _tk3) )
                    (if _verbose 
                        (writeRule
                             {PHRASE: NAME MathAssignmentOperator SEXPRESSION :: (assignMath $0 $2 $1 $3) ::}
                             _ret  _tk0 _tk1 _tk2 _tk3 #void #void #void #void #void #void #void))
                    (setq _indent (isub _indent 1))
                    (return _ret))
                 ) ; end case : _default
               ) ; end begin
             ) ; end case : SEXPRESSION
           ) ; end begin
         ) ; end case : MathAssignmentOperator
         ;; ====================
         ;; case: AssignmentOperator
         ;; ====================
         (if (begin (setq _ip _ip1)
          (if (<> (setq _tk2 (_getToken))[AssignmentOperator:] #void) true (setq _ip _ip1)))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: SEXPRESSION
             ;; ====================
             (if (if (<> (setq _tk3 (_SYNRULE_SEXPRESSION)) morphFail)true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; *********************************************************
                 ;; RULE: PHRASE: NAME AssignmentOperator SEXPRESSION :: (assignMe $0 $2 $1 $3) ::
                 ;; *********************************************************
                 (if true
                  (begin
                    (setq _ret  (assignMe _tk0 _tk2 _tk1 _tk3) )
                    (if _verbose 
                        (writeRule
                             {PHRASE: NAME AssignmentOperator SEXPRESSION :: (assignMe $0 $2 $1 $3) ::}
                             _ret  _tk0 _tk1 _tk2 _tk3 #void #void #void #void #void #void #void))
                    (setq _indent (isub _indent 1))
                    (return _ret))
                 ) ; end case : _default
               ) ; end begin
             ) ; end case : SEXPRESSION
             ;; *********************************************************
             ;; RULE: PHRASE: NAME AssignmentOperator :: (_makeError "JS 128" $2.Charpos "Invalid assignment") ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ip _ip2)
                (setq _ret  (_makeError "JS 128" _tk2.Charpos "Invalid assignment") )
                (if _verbose 
                    (writeRule
                         {PHRASE: NAME AssignmentOperator :: (_makeError "JS 128" $2.Charpos "Invalid assignment") ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : AssignmentOperator
         ;; ====================
         ;; case: LeftParen
         ;; ====================
         (if (begin (setq _ip _ip1)
          (if (<> (setq _tk2 (_getToken))[LeftParen:] #void) true (setq _ip _ip1)))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: ARGLIST
             ;; ====================
             (if (if (<> (setq _tk3 (_SYNRULE_ARGLIST)) morphFail)true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; *********************************************************
                 ;; RULE: PHRASE: NAME LeftParen ARGLIST :: (argList $0 $1.Value $3.Value) ::
                 ;; *********************************************************
                 (if true
                  (begin
                    (setq _ret  (argList _tk0 _tk1.Value _tk3.Value) )
                    (if _verbose 
                        (writeRule
                             {PHRASE: NAME LeftParen ARGLIST :: (argList $0 $1.Value $3.Value) ::}
                             _ret  _tk0 _tk1 _tk2 _tk3 #void #void #void #void #void #void #void))
                    (setq _indent (isub _indent 1))
                    (return _ret))
                 ) ; end case : _default
               ) ; end begin
             ) ; end case : ARGLIST
             ;; *********************************************************
             ;; RULE: PHRASE: NAME LeftParen :: (_makeError "JS 129" $1.Charpos "Invalid function call") ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ip _ip2)
                (setq _ret  (_makeError "JS 129" _tk1.Charpos "Invalid function call") )
                (if _verbose 
                    (writeRule
                         {PHRASE: NAME LeftParen :: (_makeError "JS 129" $1.Charpos "Invalid function call") ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : LeftParen
       ) ; end begin
     ) ; end case : NAME
     ;; **************************************************
     ;; DEFAULT (if we get to here we failed to recognize)
     ;; Note:       (at least on this repetition)
     ;; **************************************************
     (begin
        (if (= _tkn 1)
            ;; If we have never recognized any token on previous repetitions 
            (begin
               (setq _ip _oldIp)     
               (if _verbose (_logLine "Rejecting Rule PHRASE on: " source:))
               (setq _indent (isub _indent 1))
               (setq _ret morphFail)
               ) ; end begin then
            else
            ;; If we have recognized some tokens on previous repetitions, then return $0
            ;; Note: In Syntax Rules, we do not fail the rule if it has 
            ;;       recognized a token in one or more repetitions of the rule. 
            (begin
               (setq _ip _ip0)
               (setq _ret _tk0)
               ) ; end begin else
            ) ; end if
        ) ; end of DEFAULT

   ;; End of tests for user defined rules.
   
   (if (> _verboseSynIn.PHRASE -1)
       (begin
          (setq _verbose _verboseSave)
          (if (>= _verboseSynCount.PHRASE _verboseSynIn.PHRASE) (error "Count" "in Routine PHRASE"))
       ))

   ;; Repeat or return to caller is handled here.
   _ret) ;; end _SYNRULE_PHRASE
























;;**EXPORTKEY**:esm:selector:_SYNRULE_QUALIFY
;; ************************************************
;; QUALIFY user defined Syntax Rule implementation
;; Summary: This Lambda implements the QUALIFY
;;          user defined rule. Each rule test is
;;          marked with a boxed comment line for
;;          ease of human understanding.
;; Note: This code was machine generated by ParseLib.
;; ************************************************
(defchild esm.selector _SYNRULE_QUALIFY(...)
   ;; The token object _io is a persistent variable
   ;; The token pointer _ip is a persistent variable
   ;; All getToken logic increments _ip on each token fetch.
   ;; The return value from the rule is stored in _ret.
   ;; Remember that this rule may repeat depending on _repeatSW.
   vars:(_tk0 _tk1 _tk2 _tk3 _tk4  _tk5  _i
         _tk6 _tk7 _tk8 _tk9 _tkn  _repeatSW
         _ak0 _ak1 _ak2 _ak3 _ak4  _ak5  _tkthis
         _ak6 _ak7 _ak8 _ak9 _ret  _oldIp
         _ip0 _ip1 _ip2 _ip3 _ip4  _ip5
         _ip6 _ip7 _ip8 _ip9 _tkn  
         _verboseSave       
        ) ; end temporary variables
   ;; Collect any arguments which may have been passed to this rule
   (if (> (setq _tkn (argCount)) 0)
       (begin (setq _ak0 (argFetch 0))
          (if (> _tkn 1) 
              (begin (setq _ak1 (argFetch 1))
                 (if (> _tkn 2) 
                     (begin (setq _ak2 (argFetch 2))
                        (if (> _tkn 3) 
                            (begin (setq _ak3 (argFetch 3))
                               (if (> _tkn 4) 
                                   (begin (setq _ak4 (argFetch 4))
                                      (if (> _tkn 5) 
                                          (begin (setq _ak5 (argFetch 5))
                                             (if (> _tkn 6) 
                                                 (begin (setq _ak6 (argFetch 6))
                                                    (if (> _tkn 7) 
                                                        (begin (setq _ak7 (argFetch 7))
                                                           (if (> _tkn 8) 
                                                               (begin (setq _ak8 (argFetch 8))
                                                               (if (> _tkn 9) (setq _ak9 (argFetch 9))
                                                           ))))))))))))))))))) ; end argument collection
   ;; The default structure is given the named rule attribute
   (setq _tk0 (new Structure: QUALIFY: true  ))
   (setq _tkn 0)
   (setq _oldIp _ip)
   (setq _repeatSW true)

   (if (= _verboseSynCount.QUALIFY #void) 
          (setq _verboseSynCount.QUALIFY 1) 
          (setq _verboseSynCount.QUALIFY (iadd _verboseSynCount.QUALIFY 1)))

   (if (and (<> _verboseSynIn.QUALIFY #void) (> _verboseSynIn.QUALIFY -1))
       (begin (setq _verboseSave _verbose) (setq _verbose true))) 

   ;; Save the old token pointer and test for user defined rules
   Skip::
     (setq _indent (iadd _indent 1))
     (if _verbose (_logLine "Attempting QUALIFY Rule on: " source:))
     (setq _ip0 _ip)
     (++ _tkn)
     (setq _repeatSW false)

     ;; *****************************************
     ;; Begin testing for each user defined rule.
     ;; *****************************************

     ;; ====================
     ;; case: DotOperator
     ;; ====================
     (if (if (<> (setq _tk1 (_getToken))[DotOperator:] #void) true (setq _ip _ip0))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: Name
         ;; ====================
         (if (if (<> (setq _tk2 (_getToken))[Name:] #void) true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: QUALIFY((qualifyName $0 %0 $2 (makeQuotedSymbol $2.Value)))
             ;; ====================
             (if (if (<> (setq _tk3 (_SYNRULE_QUALIFY (qualifyName _tk0 _ak0 _tk2 (makeQuotedSymbol _tk2.Value)))) morphFail)true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; *********************************************************
                 ;; RULE: QUALIFY: DotOperator Name QUALIFY((qualifyName $0 %0 $2 (makeQuotedSymbol $2.Value))) :: $3 ::
                 ;; *********************************************************
                 (if true
                  (begin
                    (setq _ret  _tk3 )
                    (if _verbose 
                        (writeRule
                             {QUALIFY: DotOperator Name QUALIFY((qualifyName $0 %0 $2 (makeQuotedSymbol $2.Value))) :: $3 ::}
                             _ret  _tk0 _tk1 _tk2 _tk3 #void #void #void #void #void #void #void))
                    (setq _indent (isub _indent 1))
                    (return _ret))
                 ) ; end case : _default
               ) ; end begin
             ) ; end case : QUALIFY((qualifyName $0 %0 $2 (makeQuotedSymbol $2.Value)))
             ;; *********************************************************
             ;; RULE: QUALIFY: DotOperator Name :: (qualifyName $0 %0 $2 (makeQuotedSymbol $2.Value)) ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ip _ip2)
                (setq _ret  (qualifyName _tk0 _ak0 _tk2 (makeQuotedSymbol _tk2.Value)) )
                (if _verbose 
                    (writeRule
                         {QUALIFY: DotOperator Name :: (qualifyName $0 %0 $2 (makeQuotedSymbol $2.Value)) ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : Name
         ;; ====================
         ;; case: Reserved
         ;; ====================
         (if (begin (setq _ip _ip1)
          (if (<> (setq _tk2 (_getToken))[Reserved:] #void) true (setq _ip _ip1)))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: QUALIFY((qualifyName $0 %0 $2 (makeQuotedSymbol $2.Value)))
             ;; ====================
             (if (if (<> (setq _tk3 (_SYNRULE_QUALIFY (qualifyName _tk0 _ak0 _tk2 (makeQuotedSymbol _tk2.Value)))) morphFail)true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; *********************************************************
                 ;; RULE: QUALIFY: DotOperator Reserved QUALIFY((qualifyName $0 %0 $2 (makeQuotedSymbol $2.Value))) :: $3 ::
                 ;; *********************************************************
                 (if true
                  (begin
                    (setq _ret  _tk3 )
                    (if _verbose 
                        (writeRule
                             {QUALIFY: DotOperator Reserved QUALIFY((qualifyName $0 %0 $2 (makeQuotedSymbol $2.Value))) :: $3 ::}
                             _ret  _tk0 _tk1 _tk2 _tk3 #void #void #void #void #void #void #void))
                    (setq _indent (isub _indent 1))
                    (return _ret))
                 ) ; end case : _default
               ) ; end begin
             ) ; end case : QUALIFY((qualifyName $0 %0 $2 (makeQuotedSymbol $2.Value)))
             ;; *********************************************************
             ;; RULE: QUALIFY: DotOperator Reserved :: (qualifyName $0 %0 $2 (makeQuotedSymbol $2.Value)) ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ip _ip2)
                (setq _ret  (qualifyName _tk0 _ak0 _tk2 (makeQuotedSymbol _tk2.Value)) )
                (if _verbose 
                    (writeRule
                         {QUALIFY: DotOperator Reserved :: (qualifyName $0 %0 $2 (makeQuotedSymbol $2.Value)) ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : Reserved
         ;; *********************************************************
         ;; RULE: QUALIFY: DotOperator :: (_makeError "JS 115" $1.Charpos "Invalid use of dot operator") ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ip _ip1)
            (setq _ret  (_makeError "JS 115" _tk1.Charpos "Invalid use of dot operator") )
            (if _verbose 
                (writeRule
                     {QUALIFY: DotOperator :: (_makeError "JS 115" $1.Charpos "Invalid use of dot operator") ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : DotOperator
     ;; ====================
     ;; case: LeftBracket
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_getToken))[LeftBracket:] #void) true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: REFLIST
         ;; ====================
         (if (if (<> (setq _tk2 (_SYNRULE_REFLIST)) morphFail)true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: QUALIFY((qualifyName $0 %0 $2 $2.Value))
             ;; ====================
             (if (if (<> (setq _tk3 (_SYNRULE_QUALIFY (qualifyName _tk0 _ak0 _tk2 _tk2.Value))) morphFail)true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; *********************************************************
                 ;; RULE: QUALIFY: LeftBracket REFLIST QUALIFY((qualifyName $0 %0 $2 $2.Value)) :: $3 ::
                 ;; *********************************************************
                 (if true
                  (begin
                    (setq _ret  _tk3 )
                    (if _verbose 
                        (writeRule
                             {QUALIFY: LeftBracket REFLIST QUALIFY((qualifyName $0 %0 $2 $2.Value)) :: $3 ::}
                             _ret  _tk0 _tk1 _tk2 _tk3 #void #void #void #void #void #void #void))
                    (setq _indent (isub _indent 1))
                    (return _ret))
                 ) ; end case : _default
               ) ; end begin
             ) ; end case : QUALIFY((qualifyName $0 %0 $2 $2.Value))
             ;; *********************************************************
             ;; RULE: QUALIFY: LeftBracket REFLIST :: (qualifyName $0 %0 $2 $2.Value) ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ip _ip2)
                (setq _ret  (qualifyName _tk0 _ak0 _tk2 _tk2.Value) )
                (if _verbose 
                    (writeRule
                         {QUALIFY: LeftBracket REFLIST :: (qualifyName $0 %0 $2 $2.Value) ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : REFLIST
         ;; *********************************************************
         ;; RULE: QUALIFY: LeftBracket :: (_makeError "JS 116" $1.Charpos "Invalid use of [ operator" ) ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ip _ip1)
            (setq _ret  (_makeError "JS 116" _tk1.Charpos "Invalid use of [ operator" ) )
            (if _verbose 
                (writeRule
                     {QUALIFY: LeftBracket :: (_makeError "JS 116" $1.Charpos "Invalid use of [ operator" ) ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : LeftBracket
     ;; **************************************************
     ;; DEFAULT (if we get to here we failed to recognize)
     ;; Note:       (at least on this repetition)
     ;; **************************************************
     (begin
        (if (= _tkn 1)
            ;; If we have never recognized any token on previous repetitions 
            (begin
               (setq _ip _oldIp)     
               (if _verbose (_logLine "Rejecting Rule QUALIFY on: " source:))
               (setq _indent (isub _indent 1))
               (setq _ret morphFail)
               ) ; end begin then
            else
            ;; If we have recognized some tokens on previous repetitions, then return $0
            ;; Note: In Syntax Rules, we do not fail the rule if it has 
            ;;       recognized a token in one or more repetitions of the rule. 
            (begin
               (setq _ip _ip0)
               (setq _ret _tk0)
               ) ; end begin else
            ) ; end if
        ) ; end of DEFAULT

   ;; End of tests for user defined rules.
   
   (if (> _verboseSynIn.QUALIFY -1)
       (begin
          (setq _verbose _verboseSave)
          (if (>= _verboseSynCount.QUALIFY _verboseSynIn.QUALIFY) (error "Count" "in Routine QUALIFY"))
       ))

   ;; Repeat or return to caller is handled here.
   _ret) ;; end _SYNRULE_QUALIFY
























;;**EXPORTKEY**:esm:selector:_SYNRULE_REFLIST
;; ************************************************
;; REFLIST user defined Syntax Rule implementation
;; Summary: This Lambda implements the REFLIST
;;          user defined rule. Each rule test is
;;          marked with a boxed comment line for
;;          ease of human understanding.
;; Note: This code was machine generated by ParseLib.
;; ************************************************
(defchild esm.selector _SYNRULE_REFLIST(...)
   ;; The token object _io is a persistent variable
   ;; The token pointer _ip is a persistent variable
   ;; All getToken logic increments _ip on each token fetch.
   ;; The return value from the rule is stored in _ret.
   ;; Remember that this rule may repeat depending on _repeatSW.
   vars:(_tk0 _tk1 _tk2 _tk3 _tk4  _tk5  _i
         _tk6 _tk7 _tk8 _tk9 _tkn  _repeatSW
         _ak0 _ak1 _ak2 _ak3 _ak4  _ak5  _tkthis
         _ak6 _ak7 _ak8 _ak9 _ret  _oldIp
         _ip0 _ip1 _ip2 _ip3 _ip4  _ip5
         _ip6 _ip7 _ip8 _ip9 _tkn  
         _verboseSave       
        ) ; end temporary variables
   ;; Collect any arguments which may have been passed to this rule
   (if (> (setq _tkn (argCount)) 0)
       (begin (setq _ak0 (argFetch 0))
          (if (> _tkn 1) 
              (begin (setq _ak1 (argFetch 1))
                 (if (> _tkn 2) 
                     (begin (setq _ak2 (argFetch 2))
                        (if (> _tkn 3) 
                            (begin (setq _ak3 (argFetch 3))
                               (if (> _tkn 4) 
                                   (begin (setq _ak4 (argFetch 4))
                                      (if (> _tkn 5) 
                                          (begin (setq _ak5 (argFetch 5))
                                             (if (> _tkn 6) 
                                                 (begin (setq _ak6 (argFetch 6))
                                                    (if (> _tkn 7) 
                                                        (begin (setq _ak7 (argFetch 7))
                                                           (if (> _tkn 8) 
                                                               (begin (setq _ak8 (argFetch 8))
                                                               (if (> _tkn 9) (setq _ak9 (argFetch 9))
                                                           ))))))))))))))))))) ; end argument collection
   ;; The default structure is given the named rule attribute
   (setq _tk0 (new Structure: REFLIST: true  ))
   (setq _tkn 0)
   (setq _oldIp _ip)
   (setq _repeatSW true)

   (if (= _verboseSynCount.REFLIST #void) 
          (setq _verboseSynCount.REFLIST 1) 
          (setq _verboseSynCount.REFLIST (iadd _verboseSynCount.REFLIST 1)))

   (if (and (<> _verboseSynIn.REFLIST #void) (> _verboseSynIn.REFLIST -1))
       (begin (setq _verboseSave _verbose) (setq _verbose true))) 

   ;; Save the old token pointer and test for user defined rules
   Skip::
     (setq _indent (iadd _indent 1))
     (if _verbose (_logLine "Attempting REFLIST Rule on: " source:))
     (setq _ip0 _ip)
     (++ _tkn)
     (setq _repeatSW false)

     ;; *****************************************
     ;; Begin testing for each user defined rule.
     ;; *****************************************

     ;; ====================
     ;; case: SEXPRESSION
     ;; ====================
     (if (if (<> (setq _tk1 (_SYNRULE_SEXPRESSION)) morphFail)true (setq _ip _ip0))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: Comma
         ;; ====================
         (if (if (<> (setq _tk2 (_getToken))[Comma:] #void) true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: REFLIST
             ;; ====================
             (if (if (<> (setq _tk3 (_SYNRULE_REFLIST)) morphFail)true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; *********************************************************
                 ;; RULE: REFLIST: SEXPRESSION Comma REFLIST :: (setq $0.Value (insert $3.Value 0 $1.Value)) ::
                 ;; *********************************************************
                 (if true
                  (begin
                    (setq _ret  (setq _tk0.Value (insert _tk3.Value 0 _tk1.Value)) )
                    (if _verbose 
                        (writeRule
                             {REFLIST: SEXPRESSION Comma REFLIST :: (setq $0.Value (insert $3.Value 0 $1.Value)) ::}
                             _ret  _tk0 _tk1 _tk2 _tk3 #void #void #void #void #void #void #void))
                    (setq _indent (isub _indent 1))
                    (return _ret))
                 ) ; end case : _default
               ) ; end begin
             ) ; end case : REFLIST
           ) ; end begin
         ) ; end case : Comma
         ;; ====================
         ;; case: RightBracket
         ;; ====================
         (if (begin (setq _ip _ip1)
          (if (<> (setq _tk2 (_getToken))[RightBracket:] #void) true (setq _ip _ip1)))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; *********************************************************
             ;; RULE: REFLIST: SEXPRESSION RightBracket :: (setq (setq $0.Value (new Vector: 1 $1.Value))[VType:] $1.VType) ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  (setq (setq _tk0.Value (new Vector: 1 _tk1.Value))[VType:] _tk1.VType) )
                (if _verbose 
                    (writeRule
                         {REFLIST: SEXPRESSION RightBracket :: (setq (setq $0.Value (new Vector: 1 $1.Value))[VType:] $1.VType) ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : RightBracket
       ) ; end begin
     ) ; end case : SEXPRESSION
     ;; ====================
     ;; case: Value
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_getToken))[Value:] #void) true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; *********************************************************
         ;; RULE: REFLIST: Value :: (_makeError "JS 138" $1.Charpos "Invalid index list") ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ret  (_makeError "JS 138" _tk1.Charpos "Invalid index list") )
            (if _verbose 
                (writeRule
                     {REFLIST: Value :: (_makeError "JS 138" $1.Charpos "Invalid index list") ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : Value
     ;; **************************************************
     ;; DEFAULT (if we get to here we failed to recognize)
     ;; Note:       (at least on this repetition)
     ;; **************************************************
     (begin
        (if (= _tkn 1)
            ;; If we have never recognized any token on previous repetitions 
            (begin
               (setq _ip _oldIp)     
               (if _verbose (_logLine "Rejecting Rule REFLIST on: " source:))
               (setq _indent (isub _indent 1))
               (setq _ret morphFail)
               ) ; end begin then
            else
            ;; If we have recognized some tokens on previous repetitions, then return $0
            ;; Note: In Syntax Rules, we do not fail the rule if it has 
            ;;       recognized a token in one or more repetitions of the rule. 
            (begin
               (setq _ip _ip0)
               (setq _ret _tk0)
               ) ; end begin else
            ) ; end if
        ) ; end of DEFAULT

   ;; End of tests for user defined rules.
   
   (if (> _verboseSynIn.REFLIST -1)
       (begin
          (setq _verbose _verboseSave)
          (if (>= _verboseSynCount.REFLIST _verboseSynIn.REFLIST) (error "Count" "in Routine REFLIST"))
       ))

   ;; Repeat or return to caller is handled here.
   _ret) ;; end _SYNRULE_REFLIST
























;;**EXPORTKEY**:esm:selector:_SYNRULE_SCORE
;; ************************************************
;; SCORE user defined Syntax Rule implementation
;; Summary: This Lambda implements the SCORE
;;          user defined rule. Each rule test is
;;          marked with a boxed comment line for
;;          ease of human understanding.
;; Note: This code was machine generated by ParseLib.
;; ************************************************
(defchild esm.selector _SYNRULE_SCORE(...)
   ;; The token object _io is a persistent variable
   ;; The token pointer _ip is a persistent variable
   ;; All getToken logic increments _ip on each token fetch.
   ;; The return value from the rule is stored in _ret.
   ;; Remember that this rule may repeat depending on _repeatSW.
   vars:(_tk0 _tk1 _tk2 _tk3 _tk4  _tk5  _i
         _tk6 _tk7 _tk8 _tk9 _tkn  _repeatSW
         _ak0 _ak1 _ak2 _ak3 _ak4  _ak5  _tkthis
         _ak6 _ak7 _ak8 _ak9 _ret  _oldIp
         _ip0 _ip1 _ip2 _ip3 _ip4  _ip5
         _ip6 _ip7 _ip8 _ip9 _tkn  
         _verboseSave       
        ) ; end temporary variables
   ;; Collect any arguments which may have been passed to this rule
   (if (> (setq _tkn (argCount)) 0)
       (begin (setq _ak0 (argFetch 0))
          (if (> _tkn 1) 
              (begin (setq _ak1 (argFetch 1))
                 (if (> _tkn 2) 
                     (begin (setq _ak2 (argFetch 2))
                        (if (> _tkn 3) 
                            (begin (setq _ak3 (argFetch 3))
                               (if (> _tkn 4) 
                                   (begin (setq _ak4 (argFetch 4))
                                      (if (> _tkn 5) 
                                          (begin (setq _ak5 (argFetch 5))
                                             (if (> _tkn 6) 
                                                 (begin (setq _ak6 (argFetch 6))
                                                    (if (> _tkn 7) 
                                                        (begin (setq _ak7 (argFetch 7))
                                                           (if (> _tkn 8) 
                                                               (begin (setq _ak8 (argFetch 8))
                                                               (if (> _tkn 9) (setq _ak9 (argFetch 9))
                                                           ))))))))))))))))))) ; end argument collection
   ;; The default structure is given the named rule attribute
   (setq _tk0 (new Structure: SCORE: true  ))
   (setq _tkn 0)
   (setq _oldIp _ip)
   (setq _repeatSW true)

   (if (= _verboseSynCount.SCORE #void) 
          (setq _verboseSynCount.SCORE 1) 
          (setq _verboseSynCount.SCORE (iadd _verboseSynCount.SCORE 1)))

   (if (and (<> _verboseSynIn.SCORE #void) (> _verboseSynIn.SCORE -1))
       (begin (setq _verboseSave _verbose) (setq _verbose true))) 

   ;; Save the old token pointer and test for user defined rules
   Skip::
     (setq _indent (iadd _indent 1))
     (if _verbose (_logLine "Attempting SCORE Rule on: " source:))
     (setq _ip0 _ip)
     (++ _tkn)
     (setq _repeatSW false)

     ;; *****************************************
     ;; Begin testing for each user defined rule.
     ;; *****************************************

     ;; ====================
     ;; case: Score
     ;; ====================
     (if (if (<> (setq _tk1 (_getToken))[Score:] #void) true (setq _ip _ip0))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: NAME
         ;; ====================
         (if (if (<> (setq _tk2 (_SYNRULE_NAME)) morphFail)true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: AssignmentOperator
             ;; ====================
             (if (if (<> (setq _tk3 (_getToken))[AssignmentOperator:] #void) true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; ====================
                 ;; case: SSTATEMENT
                 ;; ====================
                 (if (if (<> (setq _tk4 (_SYNRULE_SSTATEMENT)) morphFail)true (setq _ip _ip3))
                   (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 _ip) 
                     ;; *********************************************************
                     ;; RULE: SCORE: Score NAME AssignmentOperator SSTATEMENT :: (setq $0.Value (appendList $3.Lisp $2.Value (list $4.Value))) ::
                     ;; *********************************************************
                     (if true
                      (begin
                        (setq _ret  (setq _tk0.Value (appendList _tk3.Lisp _tk2.Value (list _tk4.Value))) )
                        (if _verbose 
                            (writeRule
                                 {SCORE: Score NAME AssignmentOperator SSTATEMENT :: (setq $0.Value (appendList $3.Lisp $2.Value (list $4.Value))) ::}
                                 _ret  _tk0 _tk1 _tk2 _tk3 _tk4 #void #void #void #void #void #void))
                        (setq _indent (isub _indent 1))
                        (return _ret))
                     ) ; end case : _default
                   ) ; end begin
                 ) ; end case : SSTATEMENT
               ) ; end begin
             ) ; end case : AssignmentOperator
           ) ; end begin
         ) ; end case : NAME
         ;; ====================
         ;; case: SSTATEMENT
         ;; ====================
         (if (begin (setq _ip _ip1)
          (if (<> (setq _tk2 (_SYNRULE_SSTATEMENT)) morphFail)true (setq _ip _ip1)))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; *********************************************************
             ;; RULE: SCORE: Score SSTATEMENT :: (setq $0.Value (scoreFinal $2.Value)) ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  (setq _tk0.Value (scoreFinal _tk2.Value)) )
                (if _verbose 
                    (writeRule
                         {SCORE: Score SSTATEMENT :: (setq $0.Value (scoreFinal $2.Value)) ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : SSTATEMENT
       ) ; end begin
     ) ; end case : Score
     ;; ====================
     ;; case: Score{(begin (pushVars $0) true)}
     ;; ====================
     (if (begin (setq _ip _ip0)
      (and (if (<> (setq _tk1 (_getToken))[Score:] #void) true (setq _ip _ip0)) (begin (pushVars _tk0) true)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: SEXPRESSION{(begin (popVars) true)}
         ;; ====================
         (if (and (if (<> (setq _tk2 (_SYNRULE_SEXPRESSION)) morphFail)true (setq _ip _ip1)) (begin (popVars) true))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; *********************************************************
             ;; RULE: SCORE: Score{(begin (pushVars $0) true)} SEXPRESSION{(begin (popVars) true)} :: (setq $0.Value (scoreFinal $2.Value)) ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  (setq _tk0.Value (scoreFinal _tk2.Value)) )
                (if _verbose 
                    (writeRule
                         {SCORE: Score{(begin (pushVars $0) true)} SEXPRESSION{(begin (popVars) true)} :: (setq $0.Value (scoreFinal $2.Value)) ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : SEXPRESSION{(begin (popVars) true)}
       ) ; end begin
     ) ; end case : Score{(begin (pushVars $0) true)}
     ;; ====================
     ;; case: SSTATEMENT
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_SYNRULE_SSTATEMENT)) morphFail)true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; *********************************************************
         ;; RULE: SCORE: SSTATEMENT :: (setq $0.Value (scoreFinal $1.Value)) ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ret  (setq _tk0.Value (scoreFinal _tk1.Value)) )
            (if _verbose 
                (writeRule
                     {SCORE: SSTATEMENT :: (setq $0.Value (scoreFinal $1.Value)) ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : SSTATEMENT
     ;; **************************************************
     ;; DEFAULT (if we get to here we failed to recognize)
     ;; Note:       (at least on this repetition)
     ;; **************************************************
     (begin
        (if (= _tkn 1)
            ;; If we have never recognized any token on previous repetitions 
            (begin
               (setq _ip _oldIp)     
               (if _verbose (_logLine "Rejecting Rule SCORE on: " source:))
               (setq _indent (isub _indent 1))
               (setq _ret morphFail)
               ) ; end begin then
            else
            ;; If we have recognized some tokens on previous repetitions, then return $0
            ;; Note: In Syntax Rules, we do not fail the rule if it has 
            ;;       recognized a token in one or more repetitions of the rule. 
            (begin
               (setq _ip _ip0)
               (setq _ret _tk0)
               ) ; end begin else
            ) ; end if
        ) ; end of DEFAULT

   ;; End of tests for user defined rules.
   
   (if (> _verboseSynIn.SCORE -1)
       (begin
          (setq _verbose _verboseSave)
          (if (>= _verboseSynCount.SCORE _verboseSynIn.SCORE) (error "Count" "in Routine SCORE"))
       ))

   ;; Repeat or return to caller is handled here.
   _ret) ;; end _SYNRULE_SCORE
























;;**EXPORTKEY**:esm:selector:_SYNRULE_SEXPRESSION
;; ************************************************
;; SEXPRESSION user defined Syntax Rule implementation
;; Summary: This Lambda implements the SEXPRESSION
;;          user defined rule. Each rule test is
;;          marked with a boxed comment line for
;;          ease of human understanding.
;; Note: This code was machine generated by ParseLib.
;; ************************************************
(defchild esm.selector _SYNRULE_SEXPRESSION(...)
   ;; The token object _io is a persistent variable
   ;; The token pointer _ip is a persistent variable
   ;; All getToken logic increments _ip on each token fetch.
   ;; The return value from the rule is stored in _ret.
   ;; Remember that this rule may repeat depending on _repeatSW.
   vars:(_tk0 _tk1 _tk2 _tk3 _tk4  _tk5  _i
         _tk6 _tk7 _tk8 _tk9 _tkn  _repeatSW
         _ak0 _ak1 _ak2 _ak3 _ak4  _ak5  _tkthis
         _ak6 _ak7 _ak8 _ak9 _ret  _oldIp
         _ip0 _ip1 _ip2 _ip3 _ip4  _ip5
         _ip6 _ip7 _ip8 _ip9 _tkn  
         _verboseSave       
        ) ; end temporary variables
   ;; Collect any arguments which may have been passed to this rule
   (if (> (setq _tkn (argCount)) 0)
       (begin (setq _ak0 (argFetch 0))
          (if (> _tkn 1) 
              (begin (setq _ak1 (argFetch 1))
                 (if (> _tkn 2) 
                     (begin (setq _ak2 (argFetch 2))
                        (if (> _tkn 3) 
                            (begin (setq _ak3 (argFetch 3))
                               (if (> _tkn 4) 
                                   (begin (setq _ak4 (argFetch 4))
                                      (if (> _tkn 5) 
                                          (begin (setq _ak5 (argFetch 5))
                                             (if (> _tkn 6) 
                                                 (begin (setq _ak6 (argFetch 6))
                                                    (if (> _tkn 7) 
                                                        (begin (setq _ak7 (argFetch 7))
                                                           (if (> _tkn 8) 
                                                               (begin (setq _ak8 (argFetch 8))
                                                               (if (> _tkn 9) (setq _ak9 (argFetch 9))
                                                           ))))))))))))))))))) ; end argument collection
   ;; The default structure is given the named rule attribute
   (setq _tk0 (new Structure: SEXPRESSION: true  ))
   (setq _tkn 0)
   (setq _oldIp _ip)
   (setq _repeatSW true)

   (if (= _verboseSynCount.SEXPRESSION #void) 
          (setq _verboseSynCount.SEXPRESSION 1) 
          (setq _verboseSynCount.SEXPRESSION (iadd _verboseSynCount.SEXPRESSION 1)))

   (if (and (<> _verboseSynIn.SEXPRESSION #void) (> _verboseSynIn.SEXPRESSION -1))
       (begin (setq _verboseSave _verbose) (setq _verbose true))) 

   ;; Save the old token pointer and test for user defined rules
   Skip::
     (setq _indent (iadd _indent 1))
     (if _verbose (_logLine "Attempting SEXPRESSION Rule on: " source:))
     (setq _ip0 _ip)
     (++ _tkn)
     (setq _repeatSW false)

     ;; *****************************************
     ;; Begin testing for each user defined rule.
     ;; *****************************************

     ;; ====================
     ;; case: EXPRESSION
     ;; ====================
     (if (if (<> (setq _tk1 (_SYNRULE_EXPRESSION)) morphFail)true (setq _ip _ip0))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: Question
         ;; ====================
         (if (if (<> (setq _tk2 (_getToken))[Question:] #void) true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: EXPRESSION
             ;; ====================
             (if (if (<> (setq _tk3 (_SYNRULE_EXPRESSION)) morphFail)true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; ====================
                 ;; case: Colon
                 ;; ====================
                 (if (if (<> (setq _tk4 (_getToken))[Colon:] #void) true (setq _ip _ip3))
                   (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 _ip) 
                     ;; ====================
                     ;; case: EXPRESSION
                     ;; ====================
                     (if (if (<> (setq _tk5 (_SYNRULE_EXPRESSION)) morphFail)true (setq _ip _ip4))
                       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip5 _ip) 
                         ;; *********************************************************
                         ;; RULE: SEXPRESSION: EXPRESSION Question EXPRESSION Colon EXPRESSION :: (setq $0.Value (list if: $1.Value $3.Value $5.Value)) ::
                         ;; *********************************************************
                         (if true
                          (begin
                            (setq _ret  (setq _tk0.Value (list if: _tk1.Value _tk3.Value _tk5.Value)) )
                            (if _verbose 
                                (writeRule
                                     {SEXPRESSION: EXPRESSION Question EXPRESSION Colon EXPRESSION :: (setq $0.Value (list if: $1.Value $3.Value $5.Value)) ::}
                                     _ret  _tk0 _tk1 _tk2 _tk3 _tk4 _tk5 #void #void #void #void #void))
                            (setq _indent (isub _indent 1))
                            (return _ret))
                         ) ; end case : _default
                       ) ; end begin
                     ) ; end case : EXPRESSION
                   ) ; end begin
                 ) ; end case : Colon
               ) ; end begin
             ) ; end case : EXPRESSION
           ) ; end begin
         ) ; end case : Question
         ;; *********************************************************
         ;; RULE: SEXPRESSION: EXPRESSION :: $1 ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ip _ip1)
            (setq _ret  _tk1 )
            (if _verbose 
                (writeRule
                     {SEXPRESSION: EXPRESSION :: $1 ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : EXPRESSION
     ;; ====================
     ;; case: LeftParen
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_getToken))[LeftParen:] #void) true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: Type
         ;; ====================
         (if (if (<> (setq _tk2 (_getToken))[Type:] #void) true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: RightParen
             ;; ====================
             (if (if (<> (setq _tk3 (_getToken))[RightParen:] #void) true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; ====================
                 ;; case: EXPRESSION
                 ;; ====================
                 (if (if (<> (setq _tk4 (_SYNRULE_EXPRESSION)) morphFail)true (setq _ip _ip3))
                   (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 _ip) 
                     ;; *********************************************************
                     ;; RULE: SEXPRESSION: LeftParen Type RightParen EXPRESSION :: (setq (setq (setq $4.VType $2.Value)[SEXPRESSION:] true)[Charpos:] $0.Charpos) ::
                     ;; *********************************************************
                     (if true
                      (begin
                        (setq _ret  (setq (setq (setq _tk4.VType _tk2.Value)[SEXPRESSION:] true)[Charpos:] _tk0.Charpos) )
                        (if _verbose 
                            (writeRule
                                 {SEXPRESSION: LeftParen Type RightParen EXPRESSION :: (setq (setq (setq $4.VType $2.Value)[SEXPRESSION:] true)[Charpos:] $0.Charpos) ::}
                                 _ret  _tk0 _tk1 _tk2 _tk3 _tk4 #void #void #void #void #void #void))
                        (setq _indent (isub _indent 1))
                        (return _ret))
                     ) ; end case : _default
                   ) ; end begin
                 ) ; end case : EXPRESSION
               ) ; end begin
             ) ; end case : RightParen
           ) ; end begin
         ) ; end case : Type
       ) ; end begin
     ) ; end case : LeftParen
     ;; **************************************************
     ;; DEFAULT (if we get to here we failed to recognize)
     ;; Note:       (at least on this repetition)
     ;; **************************************************
     (begin
        (if (= _tkn 1)
            ;; If we have never recognized any token on previous repetitions 
            (begin
               (setq _ip _oldIp)     
               (if _verbose (_logLine "Rejecting Rule SEXPRESSION on: " source:))
               (setq _indent (isub _indent 1))
               (setq _ret morphFail)
               ) ; end begin then
            else
            ;; If we have recognized some tokens on previous repetitions, then return $0
            ;; Note: In Syntax Rules, we do not fail the rule if it has 
            ;;       recognized a token in one or more repetitions of the rule. 
            (begin
               (setq _ip _ip0)
               (setq _ret _tk0)
               ) ; end begin else
            ) ; end if
        ) ; end of DEFAULT

   ;; End of tests for user defined rules.
   
   (if (> _verboseSynIn.SEXPRESSION -1)
       (begin
          (setq _verbose _verboseSave)
          (if (>= _verboseSynCount.SEXPRESSION _verboseSynIn.SEXPRESSION) (error "Count" "in Routine SEXPRESSION"))
       ))

   ;; Repeat or return to caller is handled here.
   _ret) ;; end _SYNRULE_SEXPRESSION
























;;**EXPORTKEY**:esm:selector:_SYNRULE_SSTATEMENT
;; ************************************************
;; SSTATEMENT user defined Syntax Rule implementation
;; Summary: This Lambda implements the SSTATEMENT
;;          user defined rule. Each rule test is
;;          marked with a boxed comment line for
;;          ease of human understanding.
;; Note: This code was machine generated by ParseLib.
;; ************************************************
(defchild esm.selector _SYNRULE_SSTATEMENT(...)
   ;; The token object _io is a persistent variable
   ;; The token pointer _ip is a persistent variable
   ;; All getToken logic increments _ip on each token fetch.
   ;; The return value from the rule is stored in _ret.
   ;; Remember that this rule may repeat depending on _repeatSW.
   vars:(_tk0 _tk1 _tk2 _tk3 _tk4  _tk5  _i
         _tk6 _tk7 _tk8 _tk9 _tkn  _repeatSW
         _ak0 _ak1 _ak2 _ak3 _ak4  _ak5  _tkthis
         _ak6 _ak7 _ak8 _ak9 _ret  _oldIp
         _ip0 _ip1 _ip2 _ip3 _ip4  _ip5
         _ip6 _ip7 _ip8 _ip9 _tkn  
         _verboseSave       
        ) ; end temporary variables
   ;; Collect any arguments which may have been passed to this rule
   (if (> (setq _tkn (argCount)) 0)
       (begin (setq _ak0 (argFetch 0))
          (if (> _tkn 1) 
              (begin (setq _ak1 (argFetch 1))
                 (if (> _tkn 2) 
                     (begin (setq _ak2 (argFetch 2))
                        (if (> _tkn 3) 
                            (begin (setq _ak3 (argFetch 3))
                               (if (> _tkn 4) 
                                   (begin (setq _ak4 (argFetch 4))
                                      (if (> _tkn 5) 
                                          (begin (setq _ak5 (argFetch 5))
                                             (if (> _tkn 6) 
                                                 (begin (setq _ak6 (argFetch 6))
                                                    (if (> _tkn 7) 
                                                        (begin (setq _ak7 (argFetch 7))
                                                           (if (> _tkn 8) 
                                                               (begin (setq _ak8 (argFetch 8))
                                                               (if (> _tkn 9) (setq _ak9 (argFetch 9))
                                                           ))))))))))))))))))) ; end argument collection
   ;; The default structure is given the named rule attribute
   (setq _tk0 (new Structure: SSTATEMENT: true  ))
   (setq _tkn 0)
   (setq _oldIp _ip)
   (setq _repeatSW true)

   (if (= _verboseSynCount.SSTATEMENT #void) 
          (setq _verboseSynCount.SSTATEMENT 1) 
          (setq _verboseSynCount.SSTATEMENT (iadd _verboseSynCount.SSTATEMENT 1)))

   (if (and (<> _verboseSynIn.SSTATEMENT #void) (> _verboseSynIn.SSTATEMENT -1))
       (begin (setq _verboseSave _verbose) (setq _verbose true))) 

   ;; Save the old token pointer and test for user defined rules
   Skip::
     (setq _indent (iadd _indent 1))
     (if _verbose (_logLine "Attempting SSTATEMENT Rule on: " source:))
     (setq _ip0 _ip)
     (++ _tkn)
     (setq _repeatSW false)

     ;; *****************************************
     ;; Begin testing for each user defined rule.
     ;; *****************************************

     ;; ====================
     ;; case: Extract{(xReferentOn)}
     ;; ====================
     (if (and (if (<> (setq _tk1 (_getToken))[Extract:] #void) true (setq _ip _ip0)) (xReferentOn))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: Colon
         ;; ====================
         (if (if (<> (setq _tk2 (_getToken))[Colon:] #void) true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: Name
             ;; ====================
             (if (if (<> (setq _tk3 (_getToken))[Name:] #void) true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; ====================
                 ;; case: Colon
                 ;; ====================
                 (if (if (<> (setq _tk4 (_getToken))[Colon:] #void) true (setq _ip _ip3))
                   (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 _ip) 
                     ;; ====================
                     ;; case: SEXPRESSION
                     ;; ====================
                     (if (if (<> (setq _tk5 (_SYNRULE_SEXPRESSION)) morphFail)true (setq _ip _ip4))
                       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip5 _ip) 
                         ;; ====================
                         ;; case: Semicolon
                         ;; ====================
                         (if (if (<> (setq _tk6 (_getToken))[Semicolon:] #void) true (setq _ip _ip5))
                           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip6 _ip) 
                             ;; *********************************************************
                             ;; RULE: SSTATEMENT: Extract{(xReferentOn)} Colon Name Colon SEXPRESSION Semicolon :: (extractColumns $0 $3.Value $5.Value) ::
                             ;; *********************************************************
                             (if true
                              (begin
                                (setq _ret  (extractColumns _tk0 _tk3.Value _tk5.Value) )
                                (if _verbose 
                                    (writeRule
                                         {SSTATEMENT: Extract{(xReferentOn)} Colon Name Colon SEXPRESSION Semicolon :: (extractColumns $0 $3.Value $5.Value) ::}
                                         _ret  _tk0 _tk1 _tk2 _tk3 _tk4 _tk5 _tk6 #void #void #void #void))
                                (setq _indent (isub _indent 1))
                                (return _ret))
                             ) ; end case : _default
                           ) ; end begin
                         ) ; end case : Semicolon
                         ;; *********************************************************
                         ;; RULE: SSTATEMENT: Extract{(xReferentOn)} Colon Name Colon SEXPRESSION :: (extractColumns $0 $3.Value $5.Value) ::
                         ;; *********************************************************
                         (if true
                          (begin
                            (setq _ip _ip5)
                            (setq _ret  (extractColumns _tk0 _tk3.Value _tk5.Value) )
                            (if _verbose 
                                (writeRule
                                     {SSTATEMENT: Extract{(xReferentOn)} Colon Name Colon SEXPRESSION :: (extractColumns $0 $3.Value $5.Value) ::}
                                     _ret  _tk0 _tk1 _tk2 _tk3 _tk4 _tk5 #void #void #void #void #void))
                            (setq _indent (isub _indent 1))
                            (return _ret))
                         ) ; end case : _default
                       ) ; end begin
                     ) ; end case : SEXPRESSION
                   ) ; end begin
                 ) ; end case : Colon
               ) ; end begin
             ) ; end case : Name
           ) ; end begin
         ) ; end case : Colon
         ;; ====================
         ;; case: SEXPRESSION
         ;; ====================
         (if (begin (setq _ip _ip1)
          (if (<> (setq _tk2 (_SYNRULE_SEXPRESSION)) morphFail)true (setq _ip _ip1)))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: Semicolon
             ;; ====================
             (if (if (<> (setq _tk3 (_getToken))[Semicolon:] #void) true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; *********************************************************
                 ;; RULE: SSTATEMENT: Extract{(xReferentOn)} SEXPRESSION Semicolon :: (extractColumns $0 Object: $2.Value) ::
                 ;; *********************************************************
                 (if true
                  (begin
                    (setq _ret  (extractColumns _tk0 Object: _tk2.Value) )
                    (if _verbose 
                        (writeRule
                             {SSTATEMENT: Extract{(xReferentOn)} SEXPRESSION Semicolon :: (extractColumns $0 Object: $2.Value) ::}
                             _ret  _tk0 _tk1 _tk2 _tk3 #void #void #void #void #void #void #void))
                    (setq _indent (isub _indent 1))
                    (return _ret))
                 ) ; end case : _default
               ) ; end begin
             ) ; end case : Semicolon
             ;; *********************************************************
             ;; RULE: SSTATEMENT: Extract{(xReferentOn)} SEXPRESSION :: (extractColumns $0 Object: $2.Value) ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ip _ip2)
                (setq _ret  (extractColumns _tk0 Object: _tk2.Value) )
                (if _verbose 
                    (writeRule
                         {SSTATEMENT: Extract{(xReferentOn)} SEXPRESSION :: (extractColumns $0 Object: $2.Value) ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : SEXPRESSION
       ) ; end begin
     ) ; end case : Extract{(xReferentOn)}
     ;; ====================
     ;; case: ScoreCommand{(xReferentOn)}
     ;; ====================
     (if (begin (setq _ip _ip0)
      (and (if (<> (setq _tk1 (_getToken))[ScoreCommand:] #void) true (setq _ip _ip0)) (xReferentOn)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: SEXPRESSION
         ;; ====================
         (if (if (<> (setq _tk2 (_SYNRULE_SEXPRESSION)) morphFail)true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: Semicolon
             ;; ====================
             (if (if (<> (setq _tk3 (_getToken))[Semicolon:] #void) true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; *********************************************************
                 ;; RULE: SSTATEMENT: ScoreCommand{(xReferentOn)} SEXPRESSION Semicolon :: (scoreCut $0 $2.Value $1.Value) ::
                 ;; *********************************************************
                 (if true
                  (begin
                    (setq _ret  (scoreCut _tk0 _tk2.Value _tk1.Value) )
                    (if _verbose 
                        (writeRule
                             {SSTATEMENT: ScoreCommand{(xReferentOn)} SEXPRESSION Semicolon :: (scoreCut $0 $2.Value $1.Value) ::}
                             _ret  _tk0 _tk1 _tk2 _tk3 #void #void #void #void #void #void #void))
                    (setq _indent (isub _indent 1))
                    (return _ret))
                 ) ; end case : _default
               ) ; end begin
             ) ; end case : Semicolon
             ;; *********************************************************
             ;; RULE: SSTATEMENT: ScoreCommand{(xReferentOn)} SEXPRESSION :: (scoreCut $0 $2.Value $1.Value) ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ip _ip2)
                (setq _ret  (scoreCut _tk0 _tk2.Value _tk1.Value) )
                (if _verbose 
                    (writeRule
                         {SSTATEMENT: ScoreCommand{(xReferentOn)} SEXPRESSION :: (scoreCut $0 $2.Value $1.Value) ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : SEXPRESSION
       ) ; end begin
     ) ; end case : ScoreCommand{(xReferentOn)}
     ;; **************************************************
     ;; DEFAULT (if we get to here we failed to recognize)
     ;; Note:       (at least on this repetition)
     ;; **************************************************
     (begin
        (if (= _tkn 1)
            ;; If we have never recognized any token on previous repetitions 
            (begin
               (setq _ip _oldIp)     
               (if _verbose (_logLine "Rejecting Rule SSTATEMENT on: " source:))
               (setq _indent (isub _indent 1))
               (setq _ret morphFail)
               ) ; end begin then
            else
            ;; If we have recognized some tokens on previous repetitions, then return $0
            ;; Note: In Syntax Rules, we do not fail the rule if it has 
            ;;       recognized a token in one or more repetitions of the rule. 
            (begin
               (setq _ip _ip0)
               (setq _ret _tk0)
               ) ; end begin else
            ) ; end if
        ) ; end of DEFAULT

   ;; End of tests for user defined rules.
   
   (if (> _verboseSynIn.SSTATEMENT -1)
       (begin
          (setq _verbose _verboseSave)
          (if (>= _verboseSynCount.SSTATEMENT _verboseSynIn.SSTATEMENT) (error "Count" "in Routine SSTATEMENT"))
       ))

   ;; Repeat or return to caller is handled here.
   _ret) ;; end _SYNRULE_SSTATEMENT
























;;**EXPORTKEY**:esm:selector:_SYNRULE_STATEMENT
;; ************************************************
;; STATEMENT user defined Syntax Rule implementation
;; Summary: This Lambda implements the STATEMENT
;;          user defined rule. Each rule test is
;;          marked with a boxed comment line for
;;          ease of human understanding.
;; Note: This code was machine generated by ParseLib.
;; ************************************************
(defchild esm.selector _SYNRULE_STATEMENT(...)
   ;; The token object _io is a persistent variable
   ;; The token pointer _ip is a persistent variable
   ;; All getToken logic increments _ip on each token fetch.
   ;; The return value from the rule is stored in _ret.
   ;; Remember that this rule may repeat depending on _repeatSW.
   vars:(_tk0 _tk1 _tk2 _tk3 _tk4  _tk5  _i
         _tk6 _tk7 _tk8 _tk9 _tkn  _repeatSW
         _ak0 _ak1 _ak2 _ak3 _ak4  _ak5  _tkthis
         _ak6 _ak7 _ak8 _ak9 _ret  _oldIp
         _ip0 _ip1 _ip2 _ip3 _ip4  _ip5
         _ip6 _ip7 _ip8 _ip9 _tkn  
         _verboseSave       
        ) ; end temporary variables
   ;; Collect any arguments which may have been passed to this rule
   (if (> (setq _tkn (argCount)) 0)
       (begin (setq _ak0 (argFetch 0))
          (if (> _tkn 1) 
              (begin (setq _ak1 (argFetch 1))
                 (if (> _tkn 2) 
                     (begin (setq _ak2 (argFetch 2))
                        (if (> _tkn 3) 
                            (begin (setq _ak3 (argFetch 3))
                               (if (> _tkn 4) 
                                   (begin (setq _ak4 (argFetch 4))
                                      (if (> _tkn 5) 
                                          (begin (setq _ak5 (argFetch 5))
                                             (if (> _tkn 6) 
                                                 (begin (setq _ak6 (argFetch 6))
                                                    (if (> _tkn 7) 
                                                        (begin (setq _ak7 (argFetch 7))
                                                           (if (> _tkn 8) 
                                                               (begin (setq _ak8 (argFetch 8))
                                                               (if (> _tkn 9) (setq _ak9 (argFetch 9))
                                                           ))))))))))))))))))) ; end argument collection
   ;; The default structure is given the named rule attribute
   (setq _tk0 (new Structure: STATEMENT: true  ))
   (setq _tkn 0)
   (setq _oldIp _ip)
   (setq _repeatSW true)

   (if (= _verboseSynCount.STATEMENT #void) 
          (setq _verboseSynCount.STATEMENT 1) 
          (setq _verboseSynCount.STATEMENT (iadd _verboseSynCount.STATEMENT 1)))

   (if (and (<> _verboseSynIn.STATEMENT #void) (> _verboseSynIn.STATEMENT -1))
       (begin (setq _verboseSave _verbose) (setq _verbose true))) 

   ;; Save the old token pointer and test for user defined rules
   Skip::
     (setq _indent (iadd _indent 1))
     (if _verbose (_logLine "Attempting STATEMENT Rule on: " source:))
     (setq _ip0 _ip)
     (++ _tkn)
     (setq _repeatSW false)

     ;; *****************************************
     ;; Begin testing for each user defined rule.
     ;; *****************************************

     ;; ====================
     ;; case: CHILD
     ;; ====================
     (if (if (<> (setq _tk1 (_SYNRULE_CHILD)) morphFail)true (setq _ip _ip0))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: Name
         ;; ====================
         (if (if (<> (setq _tk2 (_getToken))[Name:] #void) true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: LeftParen
             ;; ====================
             (if (if (<> (setq _tk3 (_getToken))[LeftParen:] #void) true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; ====================
                 ;; case: PARMLIST
                 ;; ====================
                 (if (if (<> (setq _tk4 (_SYNRULE_PARMLIST)) morphFail)true (setq _ip _ip3))
                   (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 _ip) 
                     ;; ====================
                     ;; case: LeftBrace
                     ;; ====================
                     (if (if (<> (setq _tk5 (_getToken))[LeftBrace:] #void) true (setq _ip _ip4))
                       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip5 _ip) 
                         ;; ====================
                         ;; case: STMTLIST
                         ;; ====================
                         (if (if (<> (setq _tk6 (_SYNRULE_STMTLIST)) morphFail)true (setq _ip _ip5))
                           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip6 _ip) 
                             ;; ====================
                             ;; case: Value
                             ;; ====================
                             (if (if (<> (setq _tk7 (_getToken))[Value:] #void) true (setq _ip _ip6))
                               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip7 _ip) 
                                 ;; *********************************************************
                                 ;; RULE: STATEMENT: CHILD Name LeftParen PARMLIST LeftBrace STMTLIST Value :: (_makeError "JS 108" $7.Charpos "Invalid statement") ::
                                 ;; *********************************************************
                                 (if true
                                  (begin
                                    (setq _ret  (_makeError "JS 108" _tk7.Charpos "Invalid statement") )
                                    (if _verbose 
                                        (writeRule
                                             {STATEMENT: CHILD Name LeftParen PARMLIST LeftBrace STMTLIST Value :: (_makeError "JS 108" $7.Charpos "Invalid statement") ::}
                                             _ret  _tk0 _tk1 _tk2 _tk3 _tk4 _tk5 _tk6 _tk7 #void #void #void))
                                    (setq _indent (isub _indent 1))
                                    (return _ret))
                                 ) ; end case : _default
                               ) ; end begin
                             ) ; end case : Value
                           ) ; end begin
                         ) ; end case : STMTLIST
                       ) ; end begin
                     ) ; end case : LeftBrace
                   ) ; end begin
                 ) ; end case : PARMLIST
               ) ; end begin
             ) ; end case : LeftParen
             ;; ====================
             ;; case: Name
             ;; ====================
             (if (begin (setq _ip _ip2)
              (if (<> (setq _tk3 (_getToken))[Name:] #void) true (setq _ip _ip2)))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; ====================
                 ;; case: LeftParen
                 ;; ====================
                 (if (if (<> (setq _tk4 (_getToken))[LeftParen:] #void) true (setq _ip _ip3))
                   (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 _ip) 
                     ;; ====================
                     ;; case: PARMLIST
                     ;; ====================
                     (if (if (<> (setq _tk5 (_SYNRULE_PARMLIST)) morphFail)true (setq _ip _ip4))
                       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip5 _ip) 
                         ;; ====================
                         ;; case: LeftBrace
                         ;; ====================
                         (if (if (<> (setq _tk6 (_getToken))[LeftBrace:] #void) true (setq _ip _ip5))
                           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip6 _ip) 
                             ;; ====================
                             ;; case: STMTLIST
                             ;; ====================
                             (if (if (<> (setq _tk7 (_SYNRULE_STMTLIST)) morphFail)true (setq _ip _ip6))
                               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip7 _ip) 
                                 ;; ====================
                                 ;; case: RightBrace
                                 ;; ====================
                                 (if (if (<> (setq _tk8 (_getToken))[RightBrace:] #void) true (setq _ip _ip7))
                                   (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip8 _ip) 
                                     ;; *********************************************************
                                     ;; RULE: STATEMENT: CHILD Name Name LeftParen PARMLIST LeftBrace STMTLIST RightBrace :: (childList |defchild|: $0 $2 $3 $5 $7) ::
                                     ;; *********************************************************
                                     (if true
                                      (begin
                                        (setq _ret  (childList |defchild|: _tk0 _tk2 _tk3 _tk5 _tk7) )
                                        (if _verbose 
                                            (writeRule
                                                 {STATEMENT: CHILD Name Name LeftParen PARMLIST LeftBrace STMTLIST RightBrace :: (childList |defchild|: $0 $2 $3 $5 $7) ::}
                                                 _ret  _tk0 _tk1 _tk2 _tk3 _tk4 _tk5 _tk6 _tk7 _tk8 #void #void))
                                        (setq _indent (isub _indent 1))
                                        (return _ret))
                                     ) ; end case : _default
                                   ) ; end begin
                                 ) ; end case : RightBrace
                               ) ; end begin
                             ) ; end case : STMTLIST
                           ) ; end begin
                         ) ; end case : LeftBrace
                       ) ; end begin
                     ) ; end case : PARMLIST
                   ) ; end begin
                 ) ; end case : LeftParen
               ) ; end begin
             ) ; end case : Name
           ) ; end begin
         ) ; end case : Name
         ;; *********************************************************
         ;; RULE: STATEMENT: CHILD :: (_makeError "JS 109" $1.Charpos "Invalid function declaration") ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ip _ip1)
            (setq _ret  (_makeError "JS 109" _tk1.Charpos "Invalid function declaration") )
            (if _verbose 
                (writeRule
                     {STATEMENT: CHILD :: (_makeError "JS 109" $1.Charpos "Invalid function declaration") ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : CHILD
     ;; ====================
     ;; case: CLASS
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_SYNRULE_CLASS)) morphFail)true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: Name
         ;; ====================
         (if (if (<> (setq _tk2 (_getToken))[Name:] #void) true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: Extends
             ;; ====================
             (if (if (<> (setq _tk3 (_getToken))[Extends:] #void) true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; ====================
                 ;; case: Name
                 ;; ====================
                 (if (if (<> (setq _tk4 (_getToken))[Name:] #void) true (setq _ip _ip3))
                   (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 _ip) 
                     ;; ====================
                     ;; case: LeftBrace
                     ;; ====================
                     (if (if (<> (setq _tk5 (_getToken))[LeftBrace:] #void) true (setq _ip _ip4))
                       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip5 _ip) 
                         ;; ====================
                         ;; case: FIELDLIST
                         ;; ====================
                         (if (if (<> (setq _tk6 (_SYNRULE_FIELDLIST)) morphFail)true (setq _ip _ip5))
                           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip6 _ip) 
                             ;; *********************************************************
                             ;; RULE: STATEMENT: CLASS Name Extends Name LeftBrace FIELDLIST :: (fieldList $0 $2.Value $4.Value $6.Value) ::
                             ;; *********************************************************
                             (if true
                              (begin
                                (setq _ret  (fieldList _tk0 _tk2.Value _tk4.Value _tk6.Value) )
                                (if _verbose 
                                    (writeRule
                                         {STATEMENT: CLASS Name Extends Name LeftBrace FIELDLIST :: (fieldList $0 $2.Value $4.Value $6.Value) ::}
                                         _ret  _tk0 _tk1 _tk2 _tk3 _tk4 _tk5 _tk6 #void #void #void #void))
                                (setq _indent (isub _indent 1))
                                (return _ret))
                             ) ; end case : _default
                           ) ; end begin
                         ) ; end case : FIELDLIST
                       ) ; end begin
                     ) ; end case : LeftBrace
                   ) ; end begin
                 ) ; end case : Name
               ) ; end begin
             ) ; end case : Extends
             ;; ====================
             ;; case: LeftBrace
             ;; ====================
             (if (begin (setq _ip _ip2)
              (if (<> (setq _tk3 (_getToken))[LeftBrace:] #void) true (setq _ip _ip2)))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; ====================
                 ;; case: FIELDLIST
                 ;; ====================
                 (if (if (<> (setq _tk4 (_SYNRULE_FIELDLIST)) morphFail)true (setq _ip _ip3))
                   (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 _ip) 
                     ;; *********************************************************
                     ;; RULE: STATEMENT: CLASS Name LeftBrace FIELDLIST :: (fieldList $0 $2.Value #void $4.Value) ::
                     ;; *********************************************************
                     (if true
                      (begin
                        (setq _ret  (fieldList _tk0 _tk2.Value #void _tk4.Value) )
                        (if _verbose 
                            (writeRule
                                 {STATEMENT: CLASS Name LeftBrace FIELDLIST :: (fieldList $0 $2.Value #void $4.Value) ::}
                                 _ret  _tk0 _tk1 _tk2 _tk3 _tk4 #void #void #void #void #void #void))
                        (setq _indent (isub _indent 1))
                        (return _ret))
                     ) ; end case : _default
                   ) ; end begin
                 ) ; end case : FIELDLIST
               ) ; end begin
             ) ; end case : LeftBrace
           ) ; end begin
         ) ; end case : Name
         ;; *********************************************************
         ;; RULE: STATEMENT: CLASS :: (_makeError "JS 114" $1.Charpos "Invalid class declaration") ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ip _ip1)
            (setq _ret  (_makeError "JS 114" _tk1.Charpos "Invalid class declaration") )
            (if _verbose 
                (writeRule
                     {STATEMENT: CLASS :: (_makeError "JS 114" $1.Charpos "Invalid class declaration") ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : CLASS
     ;; ====================
     ;; case: FILTER
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_SYNRULE_FILTER)) morphFail)true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; *********************************************************
         ;; RULE: STATEMENT: FILTER :: $1 ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ret  _tk1 )
            (if _verbose 
                (writeRule
                     {STATEMENT: FILTER :: $1 ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : FILTER
     ;; ====================
     ;; case: FRIEND
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_SYNRULE_FRIEND)) morphFail)true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: Name
         ;; ====================
         (if (if (<> (setq _tk2 (_getToken))[Name:] #void) true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: LeftParen
             ;; ====================
             (if (if (<> (setq _tk3 (_getToken))[LeftParen:] #void) true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; ====================
                 ;; case: PARMLIST
                 ;; ====================
                 (if (if (<> (setq _tk4 (_SYNRULE_PARMLIST)) morphFail)true (setq _ip _ip3))
                   (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 _ip) 
                     ;; ====================
                     ;; case: LeftBrace
                     ;; ====================
                     (if (if (<> (setq _tk5 (_getToken))[LeftBrace:] #void) true (setq _ip _ip4))
                       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip5 _ip) 
                         ;; ====================
                         ;; case: STMTLIST
                         ;; ====================
                         (if (if (<> (setq _tk6 (_SYNRULE_STMTLIST)) morphFail)true (setq _ip _ip5))
                           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip6 _ip) 
                             ;; ====================
                             ;; case: Value
                             ;; ====================
                             (if (if (<> (setq _tk7 (_getToken))[Value:] #void) true (setq _ip _ip6))
                               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip7 _ip) 
                                 ;; *********************************************************
                                 ;; RULE: STATEMENT: FRIEND Name LeftParen PARMLIST LeftBrace STMTLIST Value :: (_makeError "JS 106" $7.Charpos "Invalid statement") ::
                                 ;; *********************************************************
                                 (if true
                                  (begin
                                    (setq _ret  (_makeError "JS 106" _tk7.Charpos "Invalid statement") )
                                    (if _verbose 
                                        (writeRule
                                             {STATEMENT: FRIEND Name LeftParen PARMLIST LeftBrace STMTLIST Value :: (_makeError "JS 106" $7.Charpos "Invalid statement") ::}
                                             _ret  _tk0 _tk1 _tk2 _tk3 _tk4 _tk5 _tk6 _tk7 #void #void #void))
                                    (setq _indent (isub _indent 1))
                                    (return _ret))
                                 ) ; end case : _default
                               ) ; end begin
                             ) ; end case : Value
                           ) ; end begin
                         ) ; end case : STMTLIST
                       ) ; end begin
                     ) ; end case : LeftBrace
                   ) ; end begin
                 ) ; end case : PARMLIST
               ) ; end begin
             ) ; end case : LeftParen
             ;; ====================
             ;; case: Name
             ;; ====================
             (if (begin (setq _ip _ip2)
              (if (<> (setq _tk3 (_getToken))[Name:] #void) true (setq _ip _ip2)))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; ====================
                 ;; case: LeftParen
                 ;; ====================
                 (if (if (<> (setq _tk4 (_getToken))[LeftParen:] #void) true (setq _ip _ip3))
                   (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 _ip) 
                     ;; ====================
                     ;; case: PARMLIST
                     ;; ====================
                     (if (if (<> (setq _tk5 (_SYNRULE_PARMLIST)) morphFail)true (setq _ip _ip4))
                       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip5 _ip) 
                         ;; ====================
                         ;; case: LeftBrace
                         ;; ====================
                         (if (if (<> (setq _tk6 (_getToken))[LeftBrace:] #void) true (setq _ip _ip5))
                           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip6 _ip) 
                             ;; ====================
                             ;; case: STMTLIST
                             ;; ====================
                             (if (if (<> (setq _tk7 (_SYNRULE_STMTLIST)) morphFail)true (setq _ip _ip6))
                               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip7 _ip) 
                                 ;; ====================
                                 ;; case: RightBrace
                                 ;; ====================
                                 (if (if (<> (setq _tk8 (_getToken))[RightBrace:] #void) true (setq _ip _ip7))
                                   (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip8 _ip) 
                                     ;; *********************************************************
                                     ;; RULE: STATEMENT: FRIEND Name Name LeftParen PARMLIST LeftBrace STMTLIST RightBrace :: (childList |defriend|: $0 $2 $3 $5 $7) ::
                                     ;; *********************************************************
                                     (if true
                                      (begin
                                        (setq _ret  (childList |defriend|: _tk0 _tk2 _tk3 _tk5 _tk7) )
                                        (if _verbose 
                                            (writeRule
                                                 {STATEMENT: FRIEND Name Name LeftParen PARMLIST LeftBrace STMTLIST RightBrace :: (childList |defriend|: $0 $2 $3 $5 $7) ::}
                                                 _ret  _tk0 _tk1 _tk2 _tk3 _tk4 _tk5 _tk6 _tk7 _tk8 #void #void))
                                        (setq _indent (isub _indent 1))
                                        (return _ret))
                                     ) ; end case : _default
                                   ) ; end begin
                                 ) ; end case : RightBrace
                               ) ; end begin
                             ) ; end case : STMTLIST
                           ) ; end begin
                         ) ; end case : LeftBrace
                       ) ; end begin
                     ) ; end case : PARMLIST
                   ) ; end begin
                 ) ; end case : LeftParen
               ) ; end begin
             ) ; end case : Name
           ) ; end begin
         ) ; end case : Name
         ;; *********************************************************
         ;; RULE: STATEMENT: FRIEND :: (_makeError "JS 107" $1.Charpos "Invalid function declaration") ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ip _ip1)
            (setq _ret  (_makeError "JS 107" _tk1.Charpos "Invalid function declaration") )
            (if _verbose 
                (writeRule
                     {STATEMENT: FRIEND :: (_makeError "JS 107" $1.Charpos "Invalid function declaration") ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : FRIEND
     ;; ====================
     ;; case: FUNCTION
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_SYNRULE_FUNCTION)) morphFail)true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: LeftParen
         ;; ====================
         (if (if (<> (setq _tk2 (_getToken))[LeftParen:] #void) true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: PARMLIST
             ;; ====================
             (if (if (<> (setq _tk3 (_SYNRULE_PARMLIST)) morphFail)true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; ====================
                 ;; case: LeftBrace
                 ;; ====================
                 (if (if (<> (setq _tk4 (_getToken))[LeftBrace:] #void) true (setq _ip _ip3))
                   (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 _ip) 
                     ;; ====================
                     ;; case: STMTLIST
                     ;; ====================
                     (if (if (<> (setq _tk5 (_SYNRULE_STMTLIST)) morphFail)true (setq _ip _ip4))
                       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip5 _ip) 
                         ;; ====================
                         ;; case: RightBrace
                         ;; ====================
                         (if (if (<> (setq _tk6 (_getToken))[RightBrace:] #void) true (setq _ip _ip5))
                           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip6 _ip) 
                             ;; *********************************************************
                             ;; RULE: STATEMENT: FUNCTION LeftParen PARMLIST LeftBrace STMTLIST RightBrace :: (funList $0 #void $3 $5) ::
                             ;; *********************************************************
                             (if true
                              (begin
                                (setq _ret  (funList _tk0 #void _tk3 _tk5) )
                                (if _verbose 
                                    (writeRule
                                         {STATEMENT: FUNCTION LeftParen PARMLIST LeftBrace STMTLIST RightBrace :: (funList $0 #void $3 $5) ::}
                                         _ret  _tk0 _tk1 _tk2 _tk3 _tk4 _tk5 _tk6 #void #void #void #void))
                                (setq _indent (isub _indent 1))
                                (return _ret))
                             ) ; end case : _default
                           ) ; end begin
                         ) ; end case : RightBrace
                       ) ; end begin
                     ) ; end case : STMTLIST
                   ) ; end begin
                 ) ; end case : LeftBrace
               ) ; end begin
             ) ; end case : PARMLIST
           ) ; end begin
         ) ; end case : LeftParen
         ;; ====================
         ;; case: Name
         ;; ====================
         (if (begin (setq _ip _ip1)
          (if (<> (setq _tk2 (_getToken))[Name:] #void) true (setq _ip _ip1)))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: LeftParen
             ;; ====================
             (if (if (<> (setq _tk3 (_getToken))[LeftParen:] #void) true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; ====================
                 ;; case: PARMLIST
                 ;; ====================
                 (if (if (<> (setq _tk4 (_SYNRULE_PARMLIST)) morphFail)true (setq _ip _ip3))
                   (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 _ip) 
                     ;; ====================
                     ;; case: LeftBrace
                     ;; ====================
                     (if (if (<> (setq _tk5 (_getToken))[LeftBrace:] #void) true (setq _ip _ip4))
                       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip5 _ip) 
                         ;; ====================
                         ;; case: STMTLIST
                         ;; ====================
                         (if (if (<> (setq _tk6 (_SYNRULE_STMTLIST)) morphFail)true (setq _ip _ip5))
                           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip6 _ip) 
                             ;; ====================
                             ;; case: RightBrace
                             ;; ====================
                             (if (if (<> (setq _tk7 (_getToken))[RightBrace:] #void) true (setq _ip _ip6))
                               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip7 _ip) 
                                 ;; *********************************************************
                                 ;; RULE: STATEMENT: FUNCTION Name LeftParen PARMLIST LeftBrace STMTLIST RightBrace :: (funList $0 $2 $4 $6) ::
                                 ;; *********************************************************
                                 (if true
                                  (begin
                                    (setq _ret  (funList _tk0 _tk2 _tk4 _tk6) )
                                    (if _verbose 
                                        (writeRule
                                             {STATEMENT: FUNCTION Name LeftParen PARMLIST LeftBrace STMTLIST RightBrace :: (funList $0 $2 $4 $6) ::}
                                             _ret  _tk0 _tk1 _tk2 _tk3 _tk4 _tk5 _tk6 _tk7 #void #void #void))
                                    (setq _indent (isub _indent 1))
                                    (return _ret))
                                 ) ; end case : _default
                               ) ; end begin
                             ) ; end case : RightBrace
                             ;; ====================
                             ;; case: Value
                             ;; ====================
                             (if (begin (setq _ip _ip6)
                              (if (<> (setq _tk7 (_getToken))[Value:] #void) true (setq _ip _ip6)))
                               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip7 _ip) 
                                 ;; *********************************************************
                                 ;; RULE: STATEMENT: FUNCTION Name LeftParen PARMLIST LeftBrace STMTLIST Value :: (_makeError "JS 104" $7.Charpos "Invalid statement") ::
                                 ;; *********************************************************
                                 (if true
                                  (begin
                                    (setq _ret  (_makeError "JS 104" _tk7.Charpos "Invalid statement") )
                                    (if _verbose 
                                        (writeRule
                                             {STATEMENT: FUNCTION Name LeftParen PARMLIST LeftBrace STMTLIST Value :: (_makeError "JS 104" $7.Charpos "Invalid statement") ::}
                                             _ret  _tk0 _tk1 _tk2 _tk3 _tk4 _tk5 _tk6 _tk7 #void #void #void))
                                    (setq _indent (isub _indent 1))
                                    (return _ret))
                                 ) ; end case : _default
                               ) ; end begin
                             ) ; end case : Value
                           ) ; end begin
                         ) ; end case : STMTLIST
                       ) ; end begin
                     ) ; end case : LeftBrace
                   ) ; end begin
                 ) ; end case : PARMLIST
               ) ; end begin
             ) ; end case : LeftParen
           ) ; end begin
         ) ; end case : Name
         ;; *********************************************************
         ;; RULE: STATEMENT: FUNCTION :: (_makeError "JS 105" $1.Charpos "Invalid function declaration") ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ip _ip1)
            (setq _ret  (_makeError "JS 105" _tk1.Charpos "Invalid function declaration") )
            (if _verbose 
                (writeRule
                     {STATEMENT: FUNCTION :: (_makeError "JS 105" $1.Charpos "Invalid function declaration") ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : FUNCTION
     ;; ====================
     ;; case: METHOD
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_SYNRULE_METHOD)) morphFail)true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: Name
         ;; ====================
         (if (if (<> (setq _tk2 (_getToken))[Name:] #void) true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: LeftParen
             ;; ====================
             (if (if (<> (setq _tk3 (_getToken))[LeftParen:] #void) true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; ====================
                 ;; case: PARMLIST
                 ;; ====================
                 (if (if (<> (setq _tk4 (_SYNRULE_PARMLIST)) morphFail)true (setq _ip _ip3))
                   (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 _ip) 
                     ;; ====================
                     ;; case: LeftBrace
                     ;; ====================
                     (if (if (<> (setq _tk5 (_getToken))[LeftBrace:] #void) true (setq _ip _ip4))
                       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip5 _ip) 
                         ;; ====================
                         ;; case: STMTLIST
                         ;; ====================
                         (if (if (<> (setq _tk6 (_SYNRULE_STMTLIST)) morphFail)true (setq _ip _ip5))
                           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip6 _ip) 
                             ;; ====================
                             ;; case: Value
                             ;; ====================
                             (if (if (<> (setq _tk7 (_getToken))[Value:] #void) true (setq _ip _ip6))
                               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip7 _ip) 
                                 ;; *********************************************************
                                 ;; RULE: STATEMENT: METHOD Name LeftParen PARMLIST LeftBrace STMTLIST Value :: (_makeError "JS 112" $7.Charpos "Invalid statement")::
                                 ;; *********************************************************
                                 (if true
                                  (begin
                                    (setq _ret  (_makeError "JS 112" _tk7.Charpos "Invalid statement"))
                                    (if _verbose 
                                        (writeRule
                                             {STATEMENT: METHOD Name LeftParen PARMLIST LeftBrace STMTLIST Value :: (_makeError "JS 112" $7.Charpos "Invalid statement")::}
                                             _ret  _tk0 _tk1 _tk2 _tk3 _tk4 _tk5 _tk6 _tk7 #void #void #void))
                                    (setq _indent (isub _indent 1))
                                    (return _ret))
                                 ) ; end case : _default
                               ) ; end begin
                             ) ; end case : Value
                           ) ; end begin
                         ) ; end case : STMTLIST
                       ) ; end begin
                     ) ; end case : LeftBrace
                   ) ; end begin
                 ) ; end case : PARMLIST
               ) ; end begin
             ) ; end case : LeftParen
             ;; ====================
             ;; case: Name
             ;; ====================
             (if (begin (setq _ip _ip2)
              (if (<> (setq _tk3 (_getToken))[Name:] #void) true (setq _ip _ip2)))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; ====================
                 ;; case: LeftParen
                 ;; ====================
                 (if (if (<> (setq _tk4 (_getToken))[LeftParen:] #void) true (setq _ip _ip3))
                   (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 _ip) 
                     ;; ====================
                     ;; case: PARMLIST
                     ;; ====================
                     (if (if (<> (setq _tk5 (_SYNRULE_PARMLIST)) morphFail)true (setq _ip _ip4))
                       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip5 _ip) 
                         ;; ====================
                         ;; case: LeftBrace
                         ;; ====================
                         (if (if (<> (setq _tk6 (_getToken))[LeftBrace:] #void) true (setq _ip _ip5))
                           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip6 _ip) 
                             ;; ====================
                             ;; case: STMTLIST
                             ;; ====================
                             (if (if (<> (setq _tk7 (_SYNRULE_STMTLIST)) morphFail)true (setq _ip _ip6))
                               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip7 _ip) 
                                 ;; ====================
                                 ;; case: RightBrace
                                 ;; ====================
                                 (if (if (<> (setq _tk8 (_getToken))[RightBrace:] #void) true (setq _ip _ip7))
                                   (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip8 _ip) 
                                     ;; *********************************************************
                                     ;; RULE: STATEMENT: METHOD Name Name LeftParen PARMLIST LeftBrace STMTLIST RightBrace :: (childList |defmethod|: $0 $2 $3 $5 $7) ::
                                     ;; *********************************************************
                                     (if true
                                      (begin
                                        (setq _ret  (childList |defmethod|: _tk0 _tk2 _tk3 _tk5 _tk7) )
                                        (if _verbose 
                                            (writeRule
                                                 {STATEMENT: METHOD Name Name LeftParen PARMLIST LeftBrace STMTLIST RightBrace :: (childList |defmethod|: $0 $2 $3 $5 $7) ::}
                                                 _ret  _tk0 _tk1 _tk2 _tk3 _tk4 _tk5 _tk6 _tk7 _tk8 #void #void))
                                        (setq _indent (isub _indent 1))
                                        (return _ret))
                                     ) ; end case : _default
                                   ) ; end begin
                                 ) ; end case : RightBrace
                               ) ; end begin
                             ) ; end case : STMTLIST
                           ) ; end begin
                         ) ; end case : LeftBrace
                       ) ; end begin
                     ) ; end case : PARMLIST
                   ) ; end begin
                 ) ; end case : LeftParen
               ) ; end begin
             ) ; end case : Name
           ) ; end begin
         ) ; end case : Name
         ;; *********************************************************
         ;; RULE: STATEMENT: METHOD :: (_makeError "JS 113" $1.Charpos "Invalid function declaration") ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ip _ip1)
            (setq _ret  (_makeError "JS 113" _tk1.Charpos "Invalid function declaration") )
            (if _verbose 
                (writeRule
                     {STATEMENT: METHOD :: (_makeError "JS 113" $1.Charpos "Invalid function declaration") ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : METHOD
     ;; ====================
     ;; case: ORPHAN
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_SYNRULE_ORPHAN)) morphFail)true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: Name
         ;; ====================
         (if (if (<> (setq _tk2 (_getToken))[Name:] #void) true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: LeftParen
             ;; ====================
             (if (if (<> (setq _tk3 (_getToken))[LeftParen:] #void) true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; ====================
                 ;; case: PARMLIST
                 ;; ====================
                 (if (if (<> (setq _tk4 (_SYNRULE_PARMLIST)) morphFail)true (setq _ip _ip3))
                   (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 _ip) 
                     ;; ====================
                     ;; case: LeftBrace
                     ;; ====================
                     (if (if (<> (setq _tk5 (_getToken))[LeftBrace:] #void) true (setq _ip _ip4))
                       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip5 _ip) 
                         ;; ====================
                         ;; case: STMTLIST
                         ;; ====================
                         (if (if (<> (setq _tk6 (_SYNRULE_STMTLIST)) morphFail)true (setq _ip _ip5))
                           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip6 _ip) 
                             ;; ====================
                             ;; case: Value
                             ;; ====================
                             (if (if (<> (setq _tk7 (_getToken))[Value:] #void) true (setq _ip _ip6))
                               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip7 _ip) 
                                 ;; *********************************************************
                                 ;; RULE: STATEMENT: ORPHAN Name LeftParen PARMLIST LeftBrace STMTLIST Value :: (_makeError "JS 110" $7.Charpos "Invalid statement") ::
                                 ;; *********************************************************
                                 (if true
                                  (begin
                                    (setq _ret  (_makeError "JS 110" _tk7.Charpos "Invalid statement") )
                                    (if _verbose 
                                        (writeRule
                                             {STATEMENT: ORPHAN Name LeftParen PARMLIST LeftBrace STMTLIST Value :: (_makeError "JS 110" $7.Charpos "Invalid statement") ::}
                                             _ret  _tk0 _tk1 _tk2 _tk3 _tk4 _tk5 _tk6 _tk7 #void #void #void))
                                    (setq _indent (isub _indent 1))
                                    (return _ret))
                                 ) ; end case : _default
                               ) ; end begin
                             ) ; end case : Value
                           ) ; end begin
                         ) ; end case : STMTLIST
                       ) ; end begin
                     ) ; end case : LeftBrace
                   ) ; end begin
                 ) ; end case : PARMLIST
               ) ; end begin
             ) ; end case : LeftParen
             ;; ====================
             ;; case: Name
             ;; ====================
             (if (begin (setq _ip _ip2)
              (if (<> (setq _tk3 (_getToken))[Name:] #void) true (setq _ip _ip2)))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; ====================
                 ;; case: LeftParen
                 ;; ====================
                 (if (if (<> (setq _tk4 (_getToken))[LeftParen:] #void) true (setq _ip _ip3))
                   (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 _ip) 
                     ;; ====================
                     ;; case: PARMLIST
                     ;; ====================
                     (if (if (<> (setq _tk5 (_SYNRULE_PARMLIST)) morphFail)true (setq _ip _ip4))
                       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip5 _ip) 
                         ;; ====================
                         ;; case: LeftBrace
                         ;; ====================
                         (if (if (<> (setq _tk6 (_getToken))[LeftBrace:] #void) true (setq _ip _ip5))
                           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip6 _ip) 
                             ;; ====================
                             ;; case: STMTLIST
                             ;; ====================
                             (if (if (<> (setq _tk7 (_SYNRULE_STMTLIST)) morphFail)true (setq _ip _ip6))
                               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip7 _ip) 
                                 ;; ====================
                                 ;; case: RightBrace
                                 ;; ====================
                                 (if (if (<> (setq _tk8 (_getToken))[RightBrace:] #void) true (setq _ip _ip7))
                                   (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip8 _ip) 
                                     ;; *********************************************************
                                     ;; RULE: STATEMENT: ORPHAN Name Name LeftParen PARMLIST LeftBrace STMTLIST RightBrace :: (childList |deforphan|: $0 $2 $3 $5 $7) ::
                                     ;; *********************************************************
                                     (if true
                                      (begin
                                        (setq _ret  (childList |deforphan|: _tk0 _tk2 _tk3 _tk5 _tk7) )
                                        (if _verbose 
                                            (writeRule
                                                 {STATEMENT: ORPHAN Name Name LeftParen PARMLIST LeftBrace STMTLIST RightBrace :: (childList |deforphan|: $0 $2 $3 $5 $7) ::}
                                                 _ret  _tk0 _tk1 _tk2 _tk3 _tk4 _tk5 _tk6 _tk7 _tk8 #void #void))
                                        (setq _indent (isub _indent 1))
                                        (return _ret))
                                     ) ; end case : _default
                                   ) ; end begin
                                 ) ; end case : RightBrace
                               ) ; end begin
                             ) ; end case : STMTLIST
                           ) ; end begin
                         ) ; end case : LeftBrace
                       ) ; end begin
                     ) ; end case : PARMLIST
                   ) ; end begin
                 ) ; end case : LeftParen
               ) ; end begin
             ) ; end case : Name
           ) ; end begin
         ) ; end case : Name
         ;; *********************************************************
         ;; RULE: STATEMENT: ORPHAN :: (_makeError "JS 111" $1.Charpos "Invalid function declaration") ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ip _ip1)
            (setq _ret  (_makeError "JS 111" _tk1.Charpos "Invalid function declaration") )
            (if _verbose 
                (writeRule
                     {STATEMENT: ORPHAN :: (_makeError "JS 111" $1.Charpos "Invalid function declaration") ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : ORPHAN
     ;; ====================
     ;; case: SCORE
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_SYNRULE_SCORE)) morphFail)true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; *********************************************************
         ;; RULE: STATEMENT: SCORE :: $1 ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ret  _tk1 )
            (if _verbose 
                (writeRule
                     {STATEMENT: SCORE :: $1 ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : SCORE
     ;; **************************************************
     ;; DEFAULT (if we get to here we failed to recognize)
     ;; Note:       (at least on this repetition)
     ;; **************************************************
     (begin
        (if (= _tkn 1)
            ;; If we have never recognized any token on previous repetitions 
            (begin
               (setq _ip _oldIp)     
               (if _verbose (_logLine "Rejecting Rule STATEMENT on: " source:))
               (setq _indent (isub _indent 1))
               (setq _ret morphFail)
               ) ; end begin then
            else
            ;; If we have recognized some tokens on previous repetitions, then return $0
            ;; Note: In Syntax Rules, we do not fail the rule if it has 
            ;;       recognized a token in one or more repetitions of the rule. 
            (begin
               (setq _ip _ip0)
               (setq _ret _tk0)
               ) ; end begin else
            ) ; end if
        ) ; end of DEFAULT

   ;; End of tests for user defined rules.
   
   (if (> _verboseSynIn.STATEMENT -1)
       (begin
          (setq _verbose _verboseSave)
          (if (>= _verboseSynCount.STATEMENT _verboseSynIn.STATEMENT) (error "Count" "in Routine STATEMENT"))
       ))

   ;; Repeat or return to caller is handled here.
   _ret) ;; end _SYNRULE_STATEMENT
























;;**EXPORTKEY**:esm:selector:_SYNRULE_STMTLIST
;; ************************************************
;; STMTLIST user defined Syntax Rule implementation
;; Summary: This Lambda implements the STMTLIST
;;          user defined rule. Each rule test is
;;          marked with a boxed comment line for
;;          ease of human understanding.
;; Note: This code was machine generated by ParseLib.
;; ************************************************
(defchild esm.selector _SYNRULE_STMTLIST(...)
   ;; The token object _io is a persistent variable
   ;; The token pointer _ip is a persistent variable
   ;; All getToken logic increments _ip on each token fetch.
   ;; The return value from the rule is stored in _ret.
   ;; Remember that this rule may repeat depending on _repeatSW.
   vars:(_tk0 _tk1 _tk2 _tk3 _tk4  _tk5  _i
         _tk6 _tk7 _tk8 _tk9 _tkn  _repeatSW
         _ak0 _ak1 _ak2 _ak3 _ak4  _ak5  _tkthis
         _ak6 _ak7 _ak8 _ak9 _ret  _oldIp
         _ip0 _ip1 _ip2 _ip3 _ip4  _ip5
         _ip6 _ip7 _ip8 _ip9 _tkn  
         _verboseSave       
        ) ; end temporary variables
   ;; Collect any arguments which may have been passed to this rule
   (if (> (setq _tkn (argCount)) 0)
       (begin (setq _ak0 (argFetch 0))
          (if (> _tkn 1) 
              (begin (setq _ak1 (argFetch 1))
                 (if (> _tkn 2) 
                     (begin (setq _ak2 (argFetch 2))
                        (if (> _tkn 3) 
                            (begin (setq _ak3 (argFetch 3))
                               (if (> _tkn 4) 
                                   (begin (setq _ak4 (argFetch 4))
                                      (if (> _tkn 5) 
                                          (begin (setq _ak5 (argFetch 5))
                                             (if (> _tkn 6) 
                                                 (begin (setq _ak6 (argFetch 6))
                                                    (if (> _tkn 7) 
                                                        (begin (setq _ak7 (argFetch 7))
                                                           (if (> _tkn 8) 
                                                               (begin (setq _ak8 (argFetch 8))
                                                               (if (> _tkn 9) (setq _ak9 (argFetch 9))
                                                           ))))))))))))))))))) ; end argument collection
   ;; The default structure is given the named rule attribute
   (setq _tk0 (new Structure: STMTLIST: true  ))
   (setq _tkn 0)
   (setq _oldIp _ip)
   (setq _repeatSW true)

   (if (= _verboseSynCount.STMTLIST #void) 
          (setq _verboseSynCount.STMTLIST 1) 
          (setq _verboseSynCount.STMTLIST (iadd _verboseSynCount.STMTLIST 1)))

   (if (and (<> _verboseSynIn.STMTLIST #void) (> _verboseSynIn.STMTLIST -1))
       (begin (setq _verboseSave _verbose) (setq _verbose true))) 

   ;; Save the old token pointer and test for user defined rules
   Skip::
     (setq _indent (iadd _indent 1))
     (if _verbose (_logLine "Attempting STMTLIST Rule on: " source:))
     (setq _ip0 _ip)
     (++ _tkn)
     (setq _repeatSW false)

     ;; *****************************************
     ;; Begin testing for each user defined rule.
     ;; *****************************************

     ;; ====================
     ;; case: SEXPRESSION
     ;; ====================
     (if (if (<> (setq _tk1 (_SYNRULE_SEXPRESSION)) morphFail)true (setq _ip _ip0))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; *********************************************************
         ;; RULE: STMTLIST: SEXPRESSION << (addToBeginList $0 $1.Value) >>
         ;; *********************************************************
         (if true
          (begin
            (setq _ret  (addToBeginList _tk0 _tk1.Value) )
            (if _verbose 
                (writeRule
                     {STMTLIST: SEXPRESSION << (addToBeginList $0 $1.Value) >>}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (goto Skip:))

         ) ; end case : _default
       ) ; end begin
     ) ; end case : SEXPRESSION
     ;; ====================
     ;; case: Semicolon
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_getToken))[Semicolon:] #void) true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; *********************************************************
         ;; RULE: STMTLIST: Semicolon << $0 >>
         ;; *********************************************************
         (if true
          (begin
            (setq _ret  _tk0 )
            (if _verbose 
                (writeRule
                     {STMTLIST: Semicolon << $0 >>}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (goto Skip:))

         ) ; end case : _default
       ) ; end begin
     ) ; end case : Semicolon
     ;; **************************************************
     ;; DEFAULT (if we get to here we failed to recognize)
     ;; Note:       (at least on this repetition)
     ;; **************************************************
     (begin
        (if (= _tkn 1)
            ;; If we have never recognized any token on previous repetitions 
            (begin
               (setq _ip _oldIp)     
               (if _verbose (_logLine "Rejecting Rule STMTLIST on: " source:))
               (setq _indent (isub _indent 1))
               (setq _ret morphFail)
               ) ; end begin then
            else
            ;; If we have recognized some tokens on previous repetitions, then return $0
            ;; Note: In Syntax Rules, we do not fail the rule if it has 
            ;;       recognized a token in one or more repetitions of the rule. 
            (begin
               (setq _ip _ip0)
               (setq _ret _tk0)
               ) ; end begin else
            ) ; end if
        ) ; end of DEFAULT

   ;; End of tests for user defined rules.
   
   (if (> _verboseSynIn.STMTLIST -1)
       (begin
          (setq _verbose _verboseSave)
          (if (>= _verboseSynCount.STMTLIST _verboseSynIn.STMTLIST) (error "Count" "in Routine STMTLIST"))
       ))

   ;; Repeat or return to caller is handled here.
   _ret) ;; end _SYNRULE_STMTLIST
























;;**EXPORTKEY**:esm:selector:_SYNRULE_TERM
;; ************************************************
;; TERM user defined Syntax Rule implementation
;; Summary: This Lambda implements the TERM
;;          user defined rule. Each rule test is
;;          marked with a boxed comment line for
;;          ease of human understanding.
;; Note: This code was machine generated by ParseLib.
;; ************************************************
(defchild esm.selector _SYNRULE_TERM(...)
   ;; The token object _io is a persistent variable
   ;; The token pointer _ip is a persistent variable
   ;; All getToken logic increments _ip on each token fetch.
   ;; The return value from the rule is stored in _ret.
   ;; Remember that this rule may repeat depending on _repeatSW.
   vars:(_tk0 _tk1 _tk2 _tk3 _tk4  _tk5  _i
         _tk6 _tk7 _tk8 _tk9 _tkn  _repeatSW
         _ak0 _ak1 _ak2 _ak3 _ak4  _ak5  _tkthis
         _ak6 _ak7 _ak8 _ak9 _ret  _oldIp
         _ip0 _ip1 _ip2 _ip3 _ip4  _ip5
         _ip6 _ip7 _ip8 _ip9 _tkn  
         _verboseSave       
        ) ; end temporary variables
   ;; Collect any arguments which may have been passed to this rule
   (if (> (setq _tkn (argCount)) 0)
       (begin (setq _ak0 (argFetch 0))
          (if (> _tkn 1) 
              (begin (setq _ak1 (argFetch 1))
                 (if (> _tkn 2) 
                     (begin (setq _ak2 (argFetch 2))
                        (if (> _tkn 3) 
                            (begin (setq _ak3 (argFetch 3))
                               (if (> _tkn 4) 
                                   (begin (setq _ak4 (argFetch 4))
                                      (if (> _tkn 5) 
                                          (begin (setq _ak5 (argFetch 5))
                                             (if (> _tkn 6) 
                                                 (begin (setq _ak6 (argFetch 6))
                                                    (if (> _tkn 7) 
                                                        (begin (setq _ak7 (argFetch 7))
                                                           (if (> _tkn 8) 
                                                               (begin (setq _ak8 (argFetch 8))
                                                               (if (> _tkn 9) (setq _ak9 (argFetch 9))
                                                           ))))))))))))))))))) ; end argument collection
   ;; The default structure is given the named rule attribute
   (setq _tk0 (new Structure: TERM: true  ))
   (setq _tkn 0)
   (setq _oldIp _ip)
   (setq _repeatSW true)

   (if (= _verboseSynCount.TERM #void) 
          (setq _verboseSynCount.TERM 1) 
          (setq _verboseSynCount.TERM (iadd _verboseSynCount.TERM 1)))

   (if (and (<> _verboseSynIn.TERM #void) (> _verboseSynIn.TERM -1))
       (begin (setq _verboseSave _verbose) (setq _verbose true))) 

   ;; Save the old token pointer and test for user defined rules
   Skip::
     (setq _indent (iadd _indent 1))
     (if _verbose (_logLine "Attempting TERM Rule on: " source:))
     (setq _ip0 _ip)
     (++ _tkn)
     (setq _repeatSW false)

     ;; *****************************************
     ;; Begin testing for each user defined rule.
     ;; *****************************************

     ;; ====================
     ;; case: Boolean
     ;; ====================
     (if (if (<> (setq _tk1 (_getToken))[Boolean:] #void) true (setq _ip _ip0))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; *********************************************************
         ;; RULE: TERM: Boolean :: (setq (setq $0.Value $1.Boolean)[VType:] bool:) ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ret  (setq (setq _tk0.Value _tk1.Boolean)[VType:] bool:) )
            (if _verbose 
                (writeRule
                     {TERM: Boolean :: (setq (setq $0.Value $1.Boolean)[VType:] bool:) ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : Boolean
     ;; ====================
     ;; case: +
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (= (setq _tk1 (_getToken))[Value:] |+|:) true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: TERM
         ;; ====================
         (if (if (<> (setq _tk2 (_SYNRULE_TERM)) morphFail)true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; *********************************************************
             ;; RULE: TERM: + TERM :: (setVType $2) ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  (setVType _tk2) )
                (if _verbose 
                    (writeRule
                         {TERM: + TERM :: (setVType $2) ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : TERM
         ;; ====================
         ;; case: Term
         ;; ====================
         (if (begin (setq _ip _ip1)
          (if (<> (setq _tk2 (_getToken))[Term:] #void) true (setq _ip _ip1)))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; *********************************************************
             ;; RULE: TERM: + Term :: (setVType $2) ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  (setVType _tk2) )
                (if _verbose 
                    (writeRule
                         {TERM: + Term :: (setVType $2) ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : Term
       ) ; end begin
     ) ; end case : +
     ;; ====================
     ;; case: -
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (= (setq _tk1 (_getToken))[Value:] |-|:) true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: Number
         ;; ====================
         (if (if (<> (setq _tk2 (_getToken))[Number:] #void) true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; *********************************************************
             ;; RULE: TERM: - Number :: (foldConstants $0 |-|: 0 $2.Value) ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  (foldConstants _tk0 |-|: 0 _tk2.Value) )
                (if _verbose 
                    (writeRule
                         {TERM: - Number :: (foldConstants $0 |-|: 0 $2.Value) ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : Number
         ;; ====================
         ;; case: TERM
         ;; ====================
         (if (begin (setq _ip _ip1)
          (if (<> (setq _tk2 (_SYNRULE_TERM)) morphFail)true (setq _ip _ip1)))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; *********************************************************
             ;; RULE: TERM: - TERM :: (setq (setq $0.Value (list |-|: 0 $2.Value))[VType:] $2.VType) ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  (setq (setq _tk0.Value (list |-|: 0 _tk2.Value))[VType:] _tk2.VType) )
                (if _verbose 
                    (writeRule
                         {TERM: - TERM :: (setq (setq $0.Value (list |-|: 0 $2.Value))[VType:] $2.VType) ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : TERM
         ;; ====================
         ;; case: Term
         ;; ====================
         (if (begin (setq _ip _ip1)
          (if (<> (setq _tk2 (_getToken))[Term:] #void) true (setq _ip _ip1)))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; *********************************************************
             ;; RULE: TERM: - Term :: (setq (setq $0.Value (list |-|: 0 $2.Value))[VType:] $2.VType) ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  (setq (setq _tk0.Value (list |-|: 0 _tk2.Value))[VType:] _tk2.VType) )
                (if _verbose 
                    (writeRule
                         {TERM: - Term :: (setq (setq $0.Value (list |-|: 0 $2.Value))[VType:] $2.VType) ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : Term
       ) ; end begin
     ) ; end case : -
     ;; ====================
     ;; case: LeftParen
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_getToken))[LeftParen:] #void) true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: Type
         ;; ====================
         (if (if (<> (setq _tk2 (_getToken))[Type:] #void) true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: RightParen
             ;; ====================
             (if (if (<> (setq _tk3 (_getToken))[RightParen:] #void) true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; ====================
                 ;; case: SEXPRESSION
                 ;; ====================
                 (if (if (<> (setq _tk4 (_SYNRULE_SEXPRESSION)) morphFail)true (setq _ip _ip3))
                   (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 _ip) 
                     ;; *********************************************************
                     ;; RULE: TERM: LeftParen Type RightParen SEXPRESSION :: (setq (setq (setq $4.VType $2.Value)[TERM:] true)[Charpos:] $0.Charpos) ::
                     ;; *********************************************************
                     (if true
                      (begin
                        (setq _ret  (setq (setq (setq _tk4.VType _tk2.Value)[TERM:] true)[Charpos:] _tk0.Charpos) )
                        (if _verbose 
                            (writeRule
                                 {TERM: LeftParen Type RightParen SEXPRESSION :: (setq (setq (setq $4.VType $2.Value)[TERM:] true)[Charpos:] $0.Charpos) ::}
                                 _ret  _tk0 _tk1 _tk2 _tk3 _tk4 #void #void #void #void #void #void))
                        (setq _indent (isub _indent 1))
                        (return _ret))
                     ) ; end case : _default
                   ) ; end begin
                 ) ; end case : SEXPRESSION
               ) ; end begin
             ) ; end case : RightParen
           ) ; end begin
         ) ; end case : Type
         ;; ====================
         ;; case: SEXPRESSION
         ;; ====================
         (if (begin (setq _ip _ip1)
          (if (<> (setq _tk2 (_SYNRULE_SEXPRESSION)) morphFail)true (setq _ip _ip1)))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: RightParen
             ;; ====================
             (if (if (<> (setq _tk3 (_getToken))[RightParen:] #void) true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; *********************************************************
                 ;; RULE: TERM: LeftParen SEXPRESSION RightParen :: $2 ::
                 ;; *********************************************************
                 (if true
                  (begin
                    (setq _ret  _tk2 )
                    (if _verbose 
                        (writeRule
                             {TERM: LeftParen SEXPRESSION RightParen :: $2 ::}
                             _ret  _tk0 _tk1 _tk2 _tk3 #void #void #void #void #void #void #void))
                    (setq _indent (isub _indent 1))
                    (return _ret))
                 ) ; end case : _default
               ) ; end begin
             ) ; end case : RightParen
           ) ; end begin
         ) ; end case : SEXPRESSION
         ;; *********************************************************
         ;; RULE: TERM: LeftParen :: (_makeError "JS 130" $1.Charpos "Invalid expression") ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ip _ip1)
            (setq _ret  (_makeError "JS 130" _tk1.Charpos "Invalid expression") )
            (if _verbose 
                (writeRule
                     {TERM: LeftParen :: (_makeError "JS 130" $1.Charpos "Invalid expression") ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : LeftParen
     ;; ====================
     ;; case: Logical
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_getToken))[Logical:] #void) true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: SEXPRESSION
         ;; ====================
         (if (if (<> (setq _tk2 (_SYNRULE_SEXPRESSION)) morphFail)true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; *********************************************************
             ;; RULE: TERM: Logical SEXPRESSION :: (setq (setq $0.Value (list $1.Lisp $2.Value))[VType:] bool:) ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  (setq (setq _tk0.Value (list _tk1.Lisp _tk2.Value))[VType:] bool:) )
                (if _verbose 
                    (writeRule
                         {TERM: Logical SEXPRESSION :: (setq (setq $0.Value (list $1.Lisp $2.Value))[VType:] bool:) ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : SEXPRESSION
         ;; *********************************************************
         ;; RULE: TERM: Logical :: (_makeError "JS 131" $1.Charpos (append "Invalid " $1.Value " operator"))::
         ;; *********************************************************
         (if true
          (begin
            (setq _ip _ip1)
            (setq _ret  (_makeError "JS 131" _tk1.Charpos (append "Invalid " _tk1.Value " operator")))
            (if _verbose 
                (writeRule
                     {TERM: Logical :: (_makeError "JS 131" $1.Charpos (append "Invalid " $1.Value " operator"))::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : Logical
     ;; ====================
     ;; case: Increment
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_getToken))[Increment:] #void) true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: NAME
         ;; ====================
         (if (if (<> (setq _tk2 (_SYNRULE_NAME)) morphFail)true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; *********************************************************
             ;; RULE: TERM: Increment NAME :: (postfix $0 $2 $1 false) ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  (postfix _tk0 _tk2 _tk1 false) )
                (if _verbose 
                    (writeRule
                         {TERM: Increment NAME :: (postfix $0 $2 $1 false) ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : NAME
         ;; *********************************************************
         ;; RULE: TERM: Increment :: (_makeError "JS 131" $1.Charpos (append "Invalid " $1.Value " operator")) ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ip _ip1)
            (setq _ret  (_makeError "JS 131" _tk1.Charpos (append "Invalid " _tk1.Value " operator")) )
            (if _verbose 
                (writeRule
                     {TERM: Increment :: (_makeError "JS 131" $1.Charpos (append "Invalid " $1.Value " operator")) ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : Increment
     ;; ====================
     ;; case: Name
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_getToken))[Name:] #void) true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: CFCALL($1)
         ;; ====================
         (if (if (<> (setq _tk2 (_SYNRULE_CFCALL _tk1)) morphFail)true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; *********************************************************
             ;; RULE: TERM: Name CFCALL($1) :: $2 ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  _tk2 )
                (if _verbose 
                    (writeRule
                         {TERM: Name CFCALL($1) :: $2 ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : CFCALL($1)
         ;; ====================
         ;; case: LeftParen
         ;; ====================
         (if (begin (setq _ip _ip1)
          (if (<> (setq _tk2 (_getToken))[LeftParen:] #void) true (setq _ip _ip1)))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: ARGLIST
             ;; ====================
             (if (if (<> (setq _tk3 (_SYNRULE_ARGLIST)) morphFail)true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; *********************************************************
                 ;; RULE: TERM: Name LeftParen ARGLIST :: (argList $0 $1.Value $3.Value) ::
                 ;; *********************************************************
                 (if true
                  (begin
                    (setq _ret  (argList _tk0 _tk1.Value _tk3.Value) )
                    (if _verbose 
                        (writeRule
                             {TERM: Name LeftParen ARGLIST :: (argList $0 $1.Value $3.Value) ::}
                             _ret  _tk0 _tk1 _tk2 _tk3 #void #void #void #void #void #void #void))
                    (setq _indent (isub _indent 1))
                    (return _ret))
                 ) ; end case : _default
               ) ; end begin
             ) ; end case : ARGLIST
           ) ; end begin
         ) ; end case : LeftParen
       ) ; end begin
     ) ; end case : Name
     ;; ====================
     ;; case: NAME
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_SYNRULE_NAME)) morphFail)true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: LeftParen
         ;; ====================
         (if (if (<> (setq _tk2 (_getToken))[LeftParen:] #void) true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: ARGLIST
             ;; ====================
             (if (if (<> (setq _tk3 (_SYNRULE_ARGLIST)) morphFail)true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; *********************************************************
                 ;; RULE: TERM: NAME LeftParen ARGLIST :: (argList $0 $1.Value $3.Value) ::
                 ;; *********************************************************
                 (if true
                  (begin
                    (setq _ret  (argList _tk0 _tk1.Value _tk3.Value) )
                    (if _verbose 
                        (writeRule
                             {TERM: NAME LeftParen ARGLIST :: (argList $0 $1.Value $3.Value) ::}
                             _ret  _tk0 _tk1 _tk2 _tk3 #void #void #void #void #void #void #void))
                    (setq _indent (isub _indent 1))
                    (return _ret))
                 ) ; end case : _default
               ) ; end begin
             ) ; end case : ARGLIST
             ;; *********************************************************
             ;; RULE: TERM: NAME LeftParen :: (_makeError "JS 132" $1.Charpos "Invalid function call") ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ip _ip2)
                (setq _ret  (_makeError "JS 132" _tk1.Charpos "Invalid function call") )
                (if _verbose 
                    (writeRule
                         {TERM: NAME LeftParen :: (_makeError "JS 132" $1.Charpos "Invalid function call") ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : LeftParen
         ;; ====================
         ;; case: Increment
         ;; ====================
         (if (begin (setq _ip _ip1)
          (if (<> (setq _tk2 (_getToken))[Increment:] #void) true (setq _ip _ip1)))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; *********************************************************
             ;; RULE: TERM: NAME Increment :: (postfix $0 $1 $2 true) ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  (postfix _tk0 _tk1 _tk2 true) )
                (if _verbose 
                    (writeRule
                         {TERM: NAME Increment :: (postfix $0 $1 $2 true) ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : Increment
         ;; *********************************************************
         ;; RULE: TERM: NAME :: $1 ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ip _ip1)
            (setq _ret  _tk1 )
            (if _verbose 
                (writeRule
                     {TERM: NAME :: $1 ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : NAME
     ;; ====================
     ;; case: Number
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_getToken))[Number:] #void) true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: Operator
         ;; ====================
         (if (if (<> (setq _tk2 (_getToken))[Operator:] #void) true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: Number
             ;; ====================
             (if (if (<> (setq _tk3 (_getToken))[Number:] #void) true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; *********************************************************
                 ;; RULE: TERM: Number Operator Number :: (foldConstants $0 $2.Lisp $1.Value $3.Value) ::
                 ;; *********************************************************
                 (if true
                  (begin
                    (setq _ret  (foldConstants _tk0 _tk2.Lisp _tk1.Value _tk3.Value) )
                    (if _verbose 
                        (writeRule
                             {TERM: Number Operator Number :: (foldConstants $0 $2.Lisp $1.Value $3.Value) ::}
                             _ret  _tk0 _tk1 _tk2 _tk3 #void #void #void #void #void #void #void))
                    (setq _indent (isub _indent 1))
                    (return _ret))
                 ) ; end case : _default
               ) ; end begin
             ) ; end case : Number
           ) ; end begin
         ) ; end case : Operator
       ) ; end begin
     ) ; end case : Number
     ;; ====================
     ;; case: Term
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_getToken))[Term:] #void) true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; *********************************************************
         ;; RULE: TERM: Term :: (setVType $1) ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ret  (setVType _tk1) )
            (if _verbose 
                (writeRule
                     {TERM: Term :: (setVType $1) ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : Term
     ;; **************************************************
     ;; DEFAULT (if we get to here we failed to recognize)
     ;; Note:       (at least on this repetition)
     ;; **************************************************
     (begin
        (if (= _tkn 1)
            ;; If we have never recognized any token on previous repetitions 
            (begin
               (setq _ip _oldIp)     
               (if _verbose (_logLine "Rejecting Rule TERM on: " source:))
               (setq _indent (isub _indent 1))
               (setq _ret morphFail)
               ) ; end begin then
            else
            ;; If we have recognized some tokens on previous repetitions, then return $0
            ;; Note: In Syntax Rules, we do not fail the rule if it has 
            ;;       recognized a token in one or more repetitions of the rule. 
            (begin
               (setq _ip _ip0)
               (setq _ret _tk0)
               ) ; end begin else
            ) ; end if
        ) ; end of DEFAULT

   ;; End of tests for user defined rules.
   
   (if (> _verboseSynIn.TERM -1)
       (begin
          (setq _verbose _verboseSave)
          (if (>= _verboseSynCount.TERM _verboseSynIn.TERM) (error "Count" "in Routine TERM"))
       ))

   ;; Repeat or return to caller is handled here.
   _ret) ;; end _SYNRULE_TERM
























;;**EXPORTKEY**:esm:selector:_SYNRULE_VAR
;; ************************************************
;; VAR user defined Syntax Rule implementation
;; Summary: This Lambda implements the VAR
;;          user defined rule. Each rule test is
;;          marked with a boxed comment line for
;;          ease of human understanding.
;; Note: This code was machine generated by ParseLib.
;; ************************************************
(defchild esm.selector _SYNRULE_VAR(...)
   ;; The token object _io is a persistent variable
   ;; The token pointer _ip is a persistent variable
   ;; All getToken logic increments _ip on each token fetch.
   ;; The return value from the rule is stored in _ret.
   ;; Remember that this rule may repeat depending on _repeatSW.
   vars:(_tk0 _tk1 _tk2 _tk3 _tk4  _tk5  _i
         _tk6 _tk7 _tk8 _tk9 _tkn  _repeatSW
         _ak0 _ak1 _ak2 _ak3 _ak4  _ak5  _tkthis
         _ak6 _ak7 _ak8 _ak9 _ret  _oldIp
         _ip0 _ip1 _ip2 _ip3 _ip4  _ip5
         _ip6 _ip7 _ip8 _ip9 _tkn  
         _verboseSave       
        ) ; end temporary variables
   ;; Collect any arguments which may have been passed to this rule
   (if (> (setq _tkn (argCount)) 0)
       (begin (setq _ak0 (argFetch 0))
          (if (> _tkn 1) 
              (begin (setq _ak1 (argFetch 1))
                 (if (> _tkn 2) 
                     (begin (setq _ak2 (argFetch 2))
                        (if (> _tkn 3) 
                            (begin (setq _ak3 (argFetch 3))
                               (if (> _tkn 4) 
                                   (begin (setq _ak4 (argFetch 4))
                                      (if (> _tkn 5) 
                                          (begin (setq _ak5 (argFetch 5))
                                             (if (> _tkn 6) 
                                                 (begin (setq _ak6 (argFetch 6))
                                                    (if (> _tkn 7) 
                                                        (begin (setq _ak7 (argFetch 7))
                                                           (if (> _tkn 8) 
                                                               (begin (setq _ak8 (argFetch 8))
                                                               (if (> _tkn 9) (setq _ak9 (argFetch 9))
                                                           ))))))))))))))))))) ; end argument collection
   ;; The default structure is given the named rule attribute
   (setq _tk0 (new Structure: VAR: true  ))
   (setq _tkn 0)
   (setq _oldIp _ip)
   (setq _repeatSW true)

   (if (= _verboseSynCount.VAR #void) 
          (setq _verboseSynCount.VAR 1) 
          (setq _verboseSynCount.VAR (iadd _verboseSynCount.VAR 1)))

   (if (and (<> _verboseSynIn.VAR #void) (> _verboseSynIn.VAR -1))
       (begin (setq _verboseSave _verbose) (setq _verbose true))) 

   ;; Save the old token pointer and test for user defined rules
   Skip::
     (setq _indent (iadd _indent 1))
     (if _verbose (_logLine "Attempting VAR Rule on: " source:))
     (setq _ip0 _ip)
     (++ _tkn)
     (setq _repeatSW false)

     ;; *****************************************
     ;; Begin testing for each user defined rule.
     ;; *****************************************

     ;; ====================
     ;; case: Type
     ;; ====================
     (if (if (<> (setq _tk1 (_getToken))[Type:] #void) true (setq _ip _ip0))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: Name
         ;; ====================
         (if (if (<> (setq _tk2 (_getToken))[Name:] #void) true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: InitializeOperator
             ;; ====================
             (if (if (<> (setq _tk3 (_getToken))[InitializeOperator:] #void) true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; ====================
                 ;; case: SEXPRESSION
                 ;; ====================
                 (if (if (<> (setq _tk4 (_SYNRULE_SEXPRESSION)) morphFail)true (setq _ip _ip3))
                   (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 _ip) 
                     ;; ====================
                     ;; case: Comma
                     ;; ====================
                     (if (if (<> (setq _tk5 (_getToken))[Comma:] #void) true (setq _ip _ip4))
                       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip5 _ip) 
                         ;; ====================
                         ;; case: VAR(%0 %1)
                         ;; ====================
                         (if (if (<> (setq _tk6 (_SYNRULE_VAR _ak0 _ak1)) morphFail)true (setq _ip _ip5))
                           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip6 _ip) 
                             ;; *********************************************************
                             ;; RULE: VAR: Type Name InitializeOperator SEXPRESSION Comma VAR(%0 %1) :: (addVar %0 $1.Value $2.Value (addToBeginList $0 (appendList %1 $2.Value (list $4.Value))) $6) ::
                             ;; *********************************************************
                             (if true
                              (begin
                                (setq _ret  (addVar _ak0 _tk1.Value _tk2.Value (addToBeginList _tk0 (appendList _ak1 _tk2.Value (list _tk4.Value))) _tk6) )
                                (if _verbose 
                                    (writeRule
                                         {VAR: Type Name InitializeOperator SEXPRESSION Comma VAR(%0 %1) :: (addVar %0 $1.Value $2.Value (addToBeginList $0 (appendList %1 $2.Value (list $4.Value))) $6) ::}
                                         _ret  _tk0 _tk1 _tk2 _tk3 _tk4 _tk5 _tk6 #void #void #void #void))
                                (setq _indent (isub _indent 1))
                                (return _ret))
                             ) ; end case : _default
                           ) ; end begin
                         ) ; end case : VAR(%0 %1)
                         ;; *********************************************************
                         ;; RULE: VAR: Type Name InitializeOperator SEXPRESSION Comma :: (_makeError "JS 135" $5.Charpos "Invalid var statement") ::
                         ;; *********************************************************
                         (if true
                          (begin
                            (setq _ip _ip5)
                            (setq _ret  (_makeError "JS 135" _tk5.Charpos "Invalid var statement") )
                            (if _verbose 
                                (writeRule
                                     {VAR: Type Name InitializeOperator SEXPRESSION Comma :: (_makeError "JS 135" $5.Charpos "Invalid var statement") ::}
                                     _ret  _tk0 _tk1 _tk2 _tk3 _tk4 _tk5 #void #void #void #void #void))
                            (setq _indent (isub _indent 1))
                            (return _ret))
                         ) ; end case : _default
                       ) ; end begin
                     ) ; end case : Comma
                     ;; *********************************************************
                     ;; RULE: VAR: Type Name InitializeOperator SEXPRESSION :: (addVar %0 $1.Value $2.Value (addToBeginList $0 (appendList %1 $2.Value (list $4.Value))) #void) ::
                     ;; *********************************************************
                     (if true
                      (begin
                        (setq _ip _ip4)
                        (setq _ret  (addVar _ak0 _tk1.Value _tk2.Value (addToBeginList _tk0 (appendList _ak1 _tk2.Value (list _tk4.Value))) #void) )
                        (if _verbose 
                            (writeRule
                                 {VAR: Type Name InitializeOperator SEXPRESSION :: (addVar %0 $1.Value $2.Value (addToBeginList $0 (appendList %1 $2.Value (list $4.Value))) #void) ::}
                                 _ret  _tk0 _tk1 _tk2 _tk3 _tk4 #void #void #void #void #void #void))
                        (setq _indent (isub _indent 1))
                        (return _ret))
                     ) ; end case : _default
                   ) ; end begin
                 ) ; end case : SEXPRESSION
               ) ; end begin
             ) ; end case : InitializeOperator
             ;; ====================
             ;; case: Comma
             ;; ====================
             (if (begin (setq _ip _ip2)
              (if (<> (setq _tk3 (_getToken))[Comma:] #void) true (setq _ip _ip2)))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; ====================
                 ;; case: VAR(%0 %1)
                 ;; ====================
                 (if (if (<> (setq _tk4 (_SYNRULE_VAR _ak0 _ak1)) morphFail)true (setq _ip _ip3))
                   (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 _ip) 
                     ;; *********************************************************
                     ;; RULE: VAR: Type Name Comma VAR(%0 %1) :: (addVar %0 $1.Value $2.Value $0 $4) ::
                     ;; *********************************************************
                     (if true
                      (begin
                        (setq _ret  (addVar _ak0 _tk1.Value _tk2.Value _tk0 _tk4) )
                        (if _verbose 
                            (writeRule
                                 {VAR: Type Name Comma VAR(%0 %1) :: (addVar %0 $1.Value $2.Value $0 $4) ::}
                                 _ret  _tk0 _tk1 _tk2 _tk3 _tk4 #void #void #void #void #void #void))
                        (setq _indent (isub _indent 1))
                        (return _ret))
                     ) ; end case : _default
                   ) ; end begin
                 ) ; end case : VAR(%0 %1)
                 ;; *********************************************************
                 ;; RULE: VAR: Type Name Comma :: (_makeError "JS 136" $3.Charpos "Invalid var statement") ::
                 ;; *********************************************************
                 (if true
                  (begin
                    (setq _ip _ip3)
                    (setq _ret  (_makeError "JS 136" _tk3.Charpos "Invalid var statement") )
                    (if _verbose 
                        (writeRule
                             {VAR: Type Name Comma :: (_makeError "JS 136" $3.Charpos "Invalid var statement") ::}
                             _ret  _tk0 _tk1 _tk2 _tk3 #void #void #void #void #void #void #void))
                    (setq _indent (isub _indent 1))
                    (return _ret))
                 ) ; end case : _default
               ) ; end begin
             ) ; end case : Comma
             ;; *********************************************************
             ;; RULE: VAR: Type Name :: (addVar %0 $1.Value $2.Value $0 #void) ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ip _ip2)
                (setq _ret  (addVar _ak0 _tk1.Value _tk2.Value _tk0 #void) )
                (if _verbose 
                    (writeRule
                         {VAR: Type Name :: (addVar %0 $1.Value $2.Value $0 #void) ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : Name
       ) ; end begin
     ) ; end case : Type
     ;; ====================
     ;; case: Name
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_getToken))[Name:] #void) true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: InitializeOperator
         ;; ====================
         (if (if (<> (setq _tk2 (_getToken))[InitializeOperator:] #void) true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: SEXPRESSION
             ;; ====================
             (if (if (<> (setq _tk3 (_SYNRULE_SEXPRESSION)) morphFail)true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; ====================
                 ;; case: Comma
                 ;; ====================
                 (if (if (<> (setq _tk4 (_getToken))[Comma:] #void) true (setq _ip _ip3))
                   (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 _ip) 
                     ;; ====================
                     ;; case: VAR(%0 %1)
                     ;; ====================
                     (if (if (<> (setq _tk5 (_SYNRULE_VAR _ak0 _ak1)) morphFail)true (setq _ip _ip4))
                       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip5 _ip) 
                         ;; *********************************************************
                         ;; RULE: VAR: Name InitializeOperator SEXPRESSION Comma VAR(%0 %1) :: (addVar %0 obj: $1.Value (addToBeginList $0 (appendList %1 $1.Value (list $3.Value))) $5) ::
                         ;; *********************************************************
                         (if true
                          (begin
                            (setq _ret  (addVar _ak0 obj: _tk1.Value (addToBeginList _tk0 (appendList _ak1 _tk1.Value (list _tk3.Value))) _tk5) )
                            (if _verbose 
                                (writeRule
                                     {VAR: Name InitializeOperator SEXPRESSION Comma VAR(%0 %1) :: (addVar %0 obj: $1.Value (addToBeginList $0 (appendList %1 $1.Value (list $3.Value))) $5) ::}
                                     _ret  _tk0 _tk1 _tk2 _tk3 _tk4 _tk5 #void #void #void #void #void))
                            (setq _indent (isub _indent 1))
                            (return _ret))
                         ) ; end case : _default
                       ) ; end begin
                     ) ; end case : VAR(%0 %1)
                     ;; *********************************************************
                     ;; RULE: VAR: Name InitializeOperator SEXPRESSION Comma :: (_makeError "JS 133" $4.Charpos "Invalid var statement") ::
                     ;; *********************************************************
                     (if true
                      (begin
                        (setq _ip _ip4)
                        (setq _ret  (_makeError "JS 133" _tk4.Charpos "Invalid var statement") )
                        (if _verbose 
                            (writeRule
                                 {VAR: Name InitializeOperator SEXPRESSION Comma :: (_makeError "JS 133" $4.Charpos "Invalid var statement") ::}
                                 _ret  _tk0 _tk1 _tk2 _tk3 _tk4 #void #void #void #void #void #void))
                        (setq _indent (isub _indent 1))
                        (return _ret))
                     ) ; end case : _default
                   ) ; end begin
                 ) ; end case : Comma
                 ;; *********************************************************
                 ;; RULE: VAR: Name InitializeOperator SEXPRESSION :: (addVar %0 obj: $1.Value (addToBeginList $0 (appendList %1 $1.Value (list $3.Value))) #void) ::
                 ;; *********************************************************
                 (if true
                  (begin
                    (setq _ip _ip3)
                    (setq _ret  (addVar _ak0 obj: _tk1.Value (addToBeginList _tk0 (appendList _ak1 _tk1.Value (list _tk3.Value))) #void) )
                    (if _verbose 
                        (writeRule
                             {VAR: Name InitializeOperator SEXPRESSION :: (addVar %0 obj: $1.Value (addToBeginList $0 (appendList %1 $1.Value (list $3.Value))) #void) ::}
                             _ret  _tk0 _tk1 _tk2 _tk3 #void #void #void #void #void #void #void))
                    (setq _indent (isub _indent 1))
                    (return _ret))
                 ) ; end case : _default
               ) ; end begin
             ) ; end case : SEXPRESSION
           ) ; end begin
         ) ; end case : InitializeOperator
         ;; ====================
         ;; case: Comma
         ;; ====================
         (if (begin (setq _ip _ip1)
          (if (<> (setq _tk2 (_getToken))[Comma:] #void) true (setq _ip _ip1)))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: VAR(%0 %1)
             ;; ====================
             (if (if (<> (setq _tk3 (_SYNRULE_VAR _ak0 _ak1)) morphFail)true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; *********************************************************
                 ;; RULE: VAR: Name Comma VAR(%0 %1) :: (addVar %0 obj: $1.Value $0 $3) ::
                 ;; *********************************************************
                 (if true
                  (begin
                    (setq _ret  (addVar _ak0 obj: _tk1.Value _tk0 _tk3) )
                    (if _verbose 
                        (writeRule
                             {VAR: Name Comma VAR(%0 %1) :: (addVar %0 obj: $1.Value $0 $3) ::}
                             _ret  _tk0 _tk1 _tk2 _tk3 #void #void #void #void #void #void #void))
                    (setq _indent (isub _indent 1))
                    (return _ret))
                 ) ; end case : _default
               ) ; end begin
             ) ; end case : VAR(%0 %1)
             ;; *********************************************************
             ;; RULE: VAR: Name Comma :: (_makeError "JS 134" $2.Charpos "Invalid var statement") ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ip _ip2)
                (setq _ret  (_makeError "JS 134" _tk2.Charpos "Invalid var statement") )
                (if _verbose 
                    (writeRule
                         {VAR: Name Comma :: (_makeError "JS 134" $2.Charpos "Invalid var statement") ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : Comma
         ;; ====================
         ;; case: Name
         ;; ====================
         (if (begin (setq _ip _ip1)
          (if (<> (setq _tk2 (_getToken))[Name:] #void) true (setq _ip _ip1)))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; *********************************************************
             ;; RULE: VAR: Name Name :: (_makeError "JS 136" $1.Charpos "Invalid var type") ::
             ;; *********************************************************
             (if true
              (begin
                (setq _ret  (_makeError "JS 136" _tk1.Charpos "Invalid var type") )
                (if _verbose 
                    (writeRule
                         {VAR: Name Name :: (_makeError "JS 136" $1.Charpos "Invalid var type") ::}
                         _ret  _tk0 _tk1 _tk2 #void #void #void #void #void #void #void #void))
                (setq _indent (isub _indent 1))
                (return _ret))
             ) ; end case : _default
           ) ; end begin
         ) ; end case : Name
         ;; *********************************************************
         ;; RULE: VAR: Name :: (addVar %0 obj: $1.Value $0 #void) ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ip _ip1)
            (setq _ret  (addVar _ak0 obj: _tk1.Value _tk0 #void) )
            (if _verbose 
                (writeRule
                     {VAR: Name :: (addVar %0 obj: $1.Value $0 #void) ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : Name
     ;; **************************************************
     ;; DEFAULT (if we get to here we failed to recognize)
     ;; Note:       (at least on this repetition)
     ;; **************************************************
     (begin
        (if (= _tkn 1)
            ;; If we have never recognized any token on previous repetitions 
            (begin
               (setq _ip _oldIp)     
               (if _verbose (_logLine "Rejecting Rule VAR on: " source:))
               (setq _indent (isub _indent 1))
               (setq _ret morphFail)
               ) ; end begin then
            else
            ;; If we have recognized some tokens on previous repetitions, then return $0
            ;; Note: In Syntax Rules, we do not fail the rule if it has 
            ;;       recognized a token in one or more repetitions of the rule. 
            (begin
               (setq _ip _ip0)
               (setq _ret _tk0)
               ) ; end begin else
            ) ; end if
        ) ; end of DEFAULT

   ;; End of tests for user defined rules.
   
   (if (> _verboseSynIn.VAR -1)
       (begin
          (setq _verbose _verboseSave)
          (if (>= _verboseSynCount.VAR _verboseSynIn.VAR) (error "Count" "in Routine VAR"))
       ))

   ;; Repeat or return to caller is handled here.
   _ret) ;; end _SYNRULE_VAR
























;;**EXPORTKEY**:esm:selector:_SYNRULE_WEIGHTLIST
;; ************************************************
;; WEIGHTLIST user defined Syntax Rule implementation
;; Summary: This Lambda implements the WEIGHTLIST
;;          user defined rule. Each rule test is
;;          marked with a boxed comment line for
;;          ease of human understanding.
;; Note: This code was machine generated by ParseLib.
;; ************************************************
(defchild esm.selector _SYNRULE_WEIGHTLIST(...)
   ;; The token object _io is a persistent variable
   ;; The token pointer _ip is a persistent variable
   ;; All getToken logic increments _ip on each token fetch.
   ;; The return value from the rule is stored in _ret.
   ;; Remember that this rule may repeat depending on _repeatSW.
   vars:(_tk0 _tk1 _tk2 _tk3 _tk4  _tk5  _i
         _tk6 _tk7 _tk8 _tk9 _tkn  _repeatSW
         _ak0 _ak1 _ak2 _ak3 _ak4  _ak5  _tkthis
         _ak6 _ak7 _ak8 _ak9 _ret  _oldIp
         _ip0 _ip1 _ip2 _ip3 _ip4  _ip5
         _ip6 _ip7 _ip8 _ip9 _tkn  
         _verboseSave       
        ) ; end temporary variables
   ;; Collect any arguments which may have been passed to this rule
   (if (> (setq _tkn (argCount)) 0)
       (begin (setq _ak0 (argFetch 0))
          (if (> _tkn 1) 
              (begin (setq _ak1 (argFetch 1))
                 (if (> _tkn 2) 
                     (begin (setq _ak2 (argFetch 2))
                        (if (> _tkn 3) 
                            (begin (setq _ak3 (argFetch 3))
                               (if (> _tkn 4) 
                                   (begin (setq _ak4 (argFetch 4))
                                      (if (> _tkn 5) 
                                          (begin (setq _ak5 (argFetch 5))
                                             (if (> _tkn 6) 
                                                 (begin (setq _ak6 (argFetch 6))
                                                    (if (> _tkn 7) 
                                                        (begin (setq _ak7 (argFetch 7))
                                                           (if (> _tkn 8) 
                                                               (begin (setq _ak8 (argFetch 8))
                                                               (if (> _tkn 9) (setq _ak9 (argFetch 9))
                                                           ))))))))))))))))))) ; end argument collection
   ;; The default structure is given the named rule attribute
   (setq _tk0 (new Structure: WEIGHTLIST: true  ))
   (setq _tkn 0)
   (setq _oldIp _ip)
   (setq _repeatSW true)

   (if (= _verboseSynCount.WEIGHTLIST #void) 
          (setq _verboseSynCount.WEIGHTLIST 1) 
          (setq _verboseSynCount.WEIGHTLIST (iadd _verboseSynCount.WEIGHTLIST 1)))

   (if (and (<> _verboseSynIn.WEIGHTLIST #void) (> _verboseSynIn.WEIGHTLIST -1))
       (begin (setq _verboseSave _verbose) (setq _verbose true))) 

   ;; Save the old token pointer and test for user defined rules
   Skip::
     (setq _indent (iadd _indent 1))
     (if _verbose (_logLine "Attempting WEIGHTLIST Rule on: " source:))
     (setq _ip0 _ip)
     (++ _tkn)
     (setq _repeatSW false)

     ;; *****************************************
     ;; Begin testing for each user defined rule.
     ;; *****************************************

     ;; ====================
     ;; case: RightParen
     ;; ====================
     (if (if (<> (setq _tk1 (_getToken))[RightParen:] #void) true (setq _ip _ip0))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; *********************************************************
         ;; RULE: WEIGHTLIST: RightParen :: $0 ::
         ;; *********************************************************
         (if true
          (begin
            (setq _ret  _tk0 )
            (if _verbose 
                (writeRule
                     {WEIGHTLIST: RightParen :: $0 ::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : RightParen
     ;; ====================
     ;; case: Pound
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_getToken))[Pound:] #void) true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; ====================
         ;; case: LeftParen
         ;; ====================
         (if (if (<> (setq _tk2 (_getToken))[LeftParen:] #void) true (setq _ip _ip1))
           (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip2 _ip) 
             ;; ====================
             ;; case: "num"
             ;; ====================
             (if (if (= (setq _tk3 (_getToken))[Value:] "num") true (setq _ip _ip2))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; ====================
                 ;; case: Bar
                 ;; ====================
                 (if (if (<> (setq _tk4 (_getToken))[Bar:] #void) true (setq _ip _ip3))
                   (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 _ip) 
                     ;; ====================
                     ;; case: NUMLIST
                     ;; ====================
                     (if (if (<> (setq _tk5 (_SYNRULE_NUMLIST)) morphFail)true (setq _ip _ip4))
                       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip5 _ip) 
                         ;; *********************************************************
                         ;; RULE: WEIGHTLIST: Pound LeftParen "num" Bar NUMLIST << (setq $0.Value (append $0.Value " #(num| " $5.Value)) >>
                         ;; *********************************************************
                         (if true
                          (begin
                            (setq _ret  (setq _tk0.Value (append _tk0.Value " #(num| " _tk5.Value)) )
                            (if _verbose 
                                (writeRule
                                     {WEIGHTLIST: Pound LeftParen "num" Bar NUMLIST << (setq $0.Value (append $0.Value " #(num| " $5.Value)) >>}
                                     _ret  _tk0 _tk1 _tk2 _tk3 _tk4 _tk5 #void #void #void #void #void))
                            (setq _indent (isub _indent 1))
                            (goto Skip:))

                         ) ; end case : _default
                       ) ; end begin
                     ) ; end case : NUMLIST
                   ) ; end begin
                 ) ; end case : Bar
               ) ; end begin
             ) ; end case : "num"
             ;; ====================
             ;; case: "obj"
             ;; ====================
             (if (begin (setq _ip _ip2)
              (if (= (setq _tk3 (_getToken))[Value:] "obj") true (setq _ip _ip2)))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; ====================
                 ;; case: Bar
                 ;; ====================
                 (if (if (<> (setq _tk4 (_getToken))[Bar:] #void) true (setq _ip _ip3))
                   (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip4 _ip) 
                     ;; ====================
                     ;; case: WEIGHTLIST
                     ;; ====================
                     (if (if (<> (setq _tk5 (_SYNRULE_WEIGHTLIST)) morphFail)true (setq _ip _ip4))
                       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip5 _ip) 
                         ;; *********************************************************
                         ;; RULE: WEIGHTLIST: Pound LeftParen "obj" Bar WEIGHTLIST << (setq $0.Value (append $0.Value " #(obj| " $5.Value " )")) >>
                         ;; *********************************************************
                         (if true
                          (begin
                            (setq _ret  (setq _tk0.Value (append _tk0.Value " #(obj| " _tk5.Value " )")) )
                            (if _verbose 
                                (writeRule
                                     {WEIGHTLIST: Pound LeftParen "obj" Bar WEIGHTLIST << (setq $0.Value (append $0.Value " #(obj| " $5.Value " )")) >>}
                                     _ret  _tk0 _tk1 _tk2 _tk3 _tk4 _tk5 #void #void #void #void #void))
                            (setq _indent (isub _indent 1))
                            (goto Skip:))

                         ) ; end case : _default
                       ) ; end begin
                     ) ; end case : WEIGHTLIST
                   ) ; end begin
                 ) ; end case : Bar
               ) ; end begin
             ) ; end case : "obj"
             ;; ====================
             ;; case: WEIGHTLIST
             ;; ====================
             (if (begin (setq _ip _ip2)
              (if (<> (setq _tk3 (_SYNRULE_WEIGHTLIST)) morphFail)true (setq _ip _ip2)))
               (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip3 _ip) 
                 ;; *********************************************************
                 ;; RULE: WEIGHTLIST: Pound LeftParen WEIGHTLIST << (setq $0.Value (append $0.Value " #( " $3.Value " )")) >>
                 ;; *********************************************************
                 (if true
                  (begin
                    (setq _ret  (setq _tk0.Value (append _tk0.Value " #( " _tk3.Value " )")) )
                    (if _verbose 
                        (writeRule
                             {WEIGHTLIST: Pound LeftParen WEIGHTLIST << (setq $0.Value (append $0.Value " #( " $3.Value " )")) >>}
                             _ret  _tk0 _tk1 _tk2 _tk3 #void #void #void #void #void #void #void))
                    (setq _indent (isub _indent 1))
                    (goto Skip:))

                 ) ; end case : _default
               ) ; end begin
             ) ; end case : WEIGHTLIST
           ) ; end begin
         ) ; end case : LeftParen
       ) ; end begin
     ) ; end case : Pound
     ;; ====================
     ;; case: Value
     ;; ====================
     (if (begin (setq _ip _ip0)
      (if (<> (setq _tk1 (_getToken))[Value:] #void) true (setq _ip _ip0)))
       (begin (++ _ruleCount) (if (and (> _verboseTrigger 0) (> _ruleCount _verboseTrigger) (not _verbose)) (_startLog)) (setq _ip1 _ip) 
         ;; *********************************************************
         ;; RULE: WEIGHTLIST: Value :: (_makeError "JS 137" $1.Charpos "Invalid weight list")::
         ;; *********************************************************
         (if true
          (begin
            (setq _ret  (_makeError "JS 137" _tk1.Charpos "Invalid weight list"))
            (if _verbose 
                (writeRule
                     {WEIGHTLIST: Value :: (_makeError "JS 137" $1.Charpos "Invalid weight list")::}
                     _ret  _tk0 _tk1 #void #void #void #void #void #void #void #void #void))
            (setq _indent (isub _indent 1))
            (return _ret))
         ) ; end case : _default
       ) ; end begin
     ) ; end case : Value
     ;; **************************************************
     ;; DEFAULT (if we get to here we failed to recognize)
     ;; Note:       (at least on this repetition)
     ;; **************************************************
     (begin
        (if (= _tkn 1)
            ;; If we have never recognized any token on previous repetitions 
            (begin
               (setq _ip _oldIp)     
               (if _verbose (_logLine "Rejecting Rule WEIGHTLIST on: " source:))
               (setq _indent (isub _indent 1))
               (setq _ret morphFail)
               ) ; end begin then
            else
            ;; If we have recognized some tokens on previous repetitions, then return $0
            ;; Note: In Syntax Rules, we do not fail the rule if it has 
            ;;       recognized a token in one or more repetitions of the rule. 
            (begin
               (setq _ip _ip0)
               (setq _ret _tk0)
               ) ; end begin else
            ) ; end if
        ) ; end of DEFAULT

   ;; End of tests for user defined rules.
   
   (if (> _verboseSynIn.WEIGHTLIST -1)
       (begin
          (setq _verbose _verboseSave)
          (if (>= _verboseSynCount.WEIGHTLIST _verboseSynIn.WEIGHTLIST) (error "Count" "in Routine WEIGHTLIST"))
       ))

   ;; Repeat or return to caller is handled here.
   _ret) ;; end _SYNRULE_WEIGHTLIST
























;;**EXPORTKEY**:esm:simplify
(deforphan esm:simplify(inFormula)
;; *******************************************************************
;; summary:  Returns the normalized form of any basic Selector expression.
;;
;; Args:     inFormula    The basic Selector expression to be simplified.    
;; Return:   result       The simplified basic Selector expression.
;;
;; Depends:  browseLib 
;;           math 
;;           rulesLib 
;;
;; *******************************************************************
    vars:(name s i n commands keys)
    pvars:(;; Persistent Variables
           myVariables             ;; The memory for symbolic math variables.
           mathKB                  ;; The knowledgebase of symbolic math rules.
           myExplanation           ;; The explanation of symbolic math rules.
           myInFormulaString       ;; The input Selector grammar WFF.
           ;; Child Lambdas
           errorHandler            ;; Called with user input from HTML form.
           initialize              ;; Initialize the this Lambda.
           showVariables           ;; Show the variables currently stored in symbolic math.
           ) ;; end of pvars
    ;; *******************
    ;; Define child Lambdas
    ;; *******************
    ;; Manages any unforseen errors which occur.
    (defun errorHandler(errMsg) myInFormulaString)
    ;; Initialize the this Lambda.
    (defun initialize()
        vars:(temp)
        (onError errorHandler)

	    ;;************************************************************************
	    ;;  Create a new copy of the rules Lambda and create the rule database
	    ;;  to be used for this symbolic math demonstration application.
	    ;;************************************************************************
	    (setq myVariables (new Dictionary:))
	    (setq mathKB (new |Gv:rulesLib|))
	    (mathKB.setSinglePass false)
	    (mathKB.setVerbose false)
	    ;; ****************************************************
	    ;; MATH RULE DECLARATIONS
	    ;; ****************************************************
	    ;; Declare user defined function for use in rules
	    (mathKB.assert  $VAR:(lambda(s) (if (and (isSymbol s) (isCharAlphabetic s[0]) (isMember s myVariables)) myVariables[s])))
	    (mathKB.assert  $SYM:(lambda(s) (if (and (isSymbol s) (isCharAlphabetic s[0])) s)))
	    (mathKB.assert  $SET:(lambda(s v) (setq myVariables[s] (list v)) (list v)))
	    (mathKB.assert  $SUB:(lambda(a v b) 
	                           vars:(p) 
	                           (setq p v) 
	                           (if (<> b #void) 
	                               (setq p (append (list p) b))) 
	                           (if (<> a #void) 
	                               (setq p (append a p))) p))
	    (mathKB.assert  $AOP:(lambda(x) vars:((d #{ruleAdd ruleAdd ruleAvg ruleAvg ruleDiv ruleDiv ruleExpt ruleExpt ruleMax ruleMax ruleMin ruleMin ruleMod ruleMod ruleMul ruleMul ruleSign ruleSign ruleSub ruleSub})) (if (isMember x d) d[x])))
	    (mathKB.assert  $EXP:(lambda(x) vars:((op #{ruleAdd + ruleAvg avg ruleDiv / ruleExpt expt ruleMax max ruleMin min ruleMod mod ruleMul * ruleSign sign ruleSub -})
	                                          (fn #{ruleAbs abs ruleCos cos ruleExp exp ruleInt integer ruleLog log ruleSin sin ruleSqrt sqrt ruleTan tan ruleTanh tanh}))
	                                   (if (or (isPair x) 
	                                           (isNumber x)
	                                           (and (isSymbol x)
	                                                (not (isMember x op))
	                                                (not (isMember x fn))
	                                                (isCharAlphabetic x)))
	                                            x)))
	    (mathKB.assert  $PFN:(lambda(a x fn1 y fn2 z b) 
	                             vars:(p) 
	                             (setq p (list (list x fn1 y) fn2 z)) 
	                             (if (<> b #void) 
	                                 (setq p (append (list p) b))) 
	                             (if (<> a #void) 
	                                 (setq p (append a p))) p))
	    (mathKB.assert  $NAME:(lambda(n) (esm.ruleExp.ruleName n)))
	    (mathKB.assert  $FOLD:(lambda(op x y) vars:(f) (setq f (getGlobalValue (symbol op))) (f x y)))
	    (mathKB.assert  $IFFOLD:(lambda(x op y a b) vars:(f) (setq f (getGlobalValue (symbol op))) (if (f x y) a b)))
	    (mathKB.assert  $UFOLD:(lambda(fn x) vars:(f) (setq f (getGlobalValue (symbol (downcase (string fn))))) (f x)))
	    (mathKB.assert  $NUM:(lambda(x) (if (isNumber x) x)))
	    (mathKB.assert  $NUMPOS:(lambda(x) (if (isPositive x) x)))
	    (mathKB.assert  $NUMNEG:(lambda(x) (if (isNegative x) x)))
	    (mathKB.assert  $OP:(lambda(x) vars:((d #{ruleAdd + ruleAvg avg ruleDiv / ruleExpt expt ruleMax max ruleMin min ruleMod mod ruleMul * ruleSign sign ruleSub -})) (if (isMember x d) d[x])))
	    (mathKB.assert  $UNARY:(lambda(x) vars:((d #{ruleAbs abs ruleCos cos ruleExp exp ruleInt integer ruleLog log ruleSin sin ruleSqrt sqrt ruleTan tan ruleTanh tanh})) (if (isMember x d) d[x])))
	    (mathKB.assert  $TERM:(lambda(x) vars:((d #{ruleAbs abs ruleCos cos ruleExp exp ruleInt integer ruleLog log ruleSin sin ruleSqrt sqrt ruleTan tan ruleTanh tanh}))
	                                 (if (or (isPair x) 
	                                         (and (isSymbol x)
	                                              (not (isMember x d))
	                                              (isCharAlphabetic x)))
	                                          x)))
	    (mathKB.assert  $FACT:(lambda(a x op y b) 
	                             vars:(p) 
	                             (setq p (list x op y)) 
	                             (if (<> b #void) 
	                                 (setq p (append (list p) b))) 
	                             (if (<> a #void) 
	                                 (setq p (append a p))) p))
	    ;; Constant folding rules
	    (mathKB.assert  '(<$F=$OP> <$X=$NUM> <$Z=$NUM>) '(<$FOLD> $F $X $Z))
	    (mathKB.assert  '(<$F=$BINARY> <$X=$NUM> <$Z=$NUM>) '(<$FOLD> $F $X $Z))
	    (mathKB.assert  '(<$F=$UNARY> <$X=$NUM>) '(<$UFOLD> $F $X))
	    (mathKB.assert  '(ruleName $N) '(<$NAME> $N))
	    ;; Addition expression reduction rules
	    (mathKB.assert  '(ruleAdd (ruleAdd <$X=$TERM> <$Y=$NUM>) <$Z=$NUM>) '(ruleAdd $X (<$FOLD> + $Y $Z)))
	    (mathKB.assert  '(ruleAdd (ruleAdd <$X=$NUM> <$Y=$TERM>) <$Z=$NUM>) '(ruleAdd (<$FOLD> + $X $Z) $Y))
	    (mathKB.assert  '(ruleAdd (ruleSub <$X=$TERM> <$Y=$NUM>) <$Z=$NUM>) '(ruleAdd $X (<$FOLD> - $Z $Y)))
	    (mathKB.assert  '(ruleAdd (ruleSub <$X=$NUM> <$Y=$TERM>) <$Z=$NUM>) '(ruleSub (<$FOLD> + $X $Z) $Y))
	    (mathKB.assert  '(ruleAdd (ruleSub $X $Y) $X) '(ruleSub (ruleMul 2.0 $X) $Y))
	    (mathKB.assert  '(ruleAdd (ruleSub $X $Y) $Y) '$X)
	    (mathKB.assert  '(ruleAdd $X $X) '(ruleMul 2.0 $X))
	    (mathKB.assert  '(ruleAdd $X 0.0) '$X)
	    (mathKB.assert  '(ruleAdd 0.0 $X) '$X)
	    ;; Subtraction expression reduction rules
	    (mathKB.assert  '(ruleSub (ruleAdd <$X=$TERM> <$Y=$NUM>) <$Z=$NUM>) '(ruleAdd $X (<$FOLD> - $Y $Z)))
	    (mathKB.assert  '(ruleSub (ruleAdd <$X=$NUM> <$Y=$TERM>) <$Z=$NUM>) '(ruleAdd (<$FOLD> - $X $Z) $Y))
	    (mathKB.assert  '(ruleSub (ruleSub <$X=$TERM> <$Y=$NUM>) <$Z=$NUM>) '(ruleSub $X (<$FOLD> + $Y $Z)))
	    (mathKB.assert  '(ruleSub (ruleSub <$X=$NUM> <$Y=$TERM>) <$Z=$NUM>) '(ruleSub (<$FOLD> - $X $Z) $Y))
	    (mathKB.assert  '(ruleSub (ruleAdd $X $Y) $Y) '$X)
	    (mathKB.assert  '(ruleSub (ruleAdd $X $Y) $X) '$Y)
	    (mathKB.assert  '(ruleSub $X 0.0) '$X)
	    (mathKB.assert  '(ruleSub $X $X) 0.0)
	    ;; Division expression reduction rules
	    (mathKB.assert  '(ruleDiv (ruleMul <$X=$TERM> <$Y=$NUM>) <$Z=$NUM>) '(ruleMul $X (<$FOLD> / $Y $Z)))
	    (mathKB.assert  '(ruleDiv (ruleMul <$X=$NUM> <$Y=$TERM>) <$Z=$NUM>) '(ruleMul (<$FOLD> / $X $Z) $Y))
	    (mathKB.assert  '(ruleDiv (ruleDiv <$X=$TERM> <$Y=$NUM>) <$Z=$NUM>) '(ruleDiv $X (<$FOLD> * $Y $Z)))
	    (mathKB.assert  '(ruleDiv (ruleDiv <$X=$NUM> <$Y=$TERM>) <$Z=$NUM>) '(ruleDiv (<$FOLD> / $X $Z) $Y))
	    (mathKB.assert  '(ruleDiv (ruleMul $Y $X) (ruleMul $Z $X)) '(ruleMul (ruleDiv $Y $Z) $X))
	    (mathKB.assert  '(ruleDiv (ruleMul $Y $X) $X) '$Y)
	    (mathKB.assert  '(ruleDiv (ruleMul $Y $X) $Y) '$X)
	    (mathKB.assert  '(ruleDiv $X $X) 1.0)
	    (mathKB.assert  '(ruleDiv $X 1.0) '$X)
	    (mathKB.assert  '(ruleDiv 0.0 $X) 0.0)
	    (mathKB.assert  '(ruleDiv $X $X) 1.0)
	    ;; Multiplication expression reduction rules
	    (mathKB.assert  '(ruleMul (ruleSub <$X=$TERM> <$Y=$NUM>) <$Z=$NUM>) '(ruleSub (ruleMul $X $Z) (<$FOLD> * $Y $Z)))
	    (mathKB.assert  '(ruleMul (ruleSub <$X=$NUM> <$Y=$TERM>) <$Z=$NUM>) '(ruleSub (<$FOLD> * $X $Z) (ruleMul $Y $Z)))
	    (mathKB.assert  '(ruleMul (ruleMul <$X=$TERM> <$Y=$NUM>) <$Z=$NUM>) '(ruleMul $X (<$FOLD> * $Y $Z)))
	    (mathKB.assert  '(ruleMul (ruleMul <$X=$NUM> <$Y=$TERM>) <$Z=$NUM>) '(ruleMul (<$FOLD> * $X $Z) $Y))
	    (mathKB.assert  '(ruleMul (ruleDiv <$X=$TERM> <$Y=$NUM>) <$Z=$NUM>) '(ruleMul $X (<$FOLD> / $Z $Y)))
	    (mathKB.assert  '(ruleMul (ruleDiv <$X=$NUM> <$Y=$TERM>) <$Z=$NUM>) '(ruleDiv (<$FOLD> * $X $Z) $Y))
	    (mathKB.assert  '(ruleMul (ruleAdd <$X=$TERM> <$Y=$NUM>) <$Z=$NUM>) '(ruleAdd (ruleMul $X $Z) (<$FOLD> * $Y $Z)))
	    (mathKB.assert  '(ruleMul (ruleAdd <$X=$NUM> <$Y=$TERM>) <$Z=$NUM>) '(ruleAdd (<$FOLD> - $X $Z) (ruleMul $Y $Z)))
	    (mathKB.assert  '(ruleMul (ruleDiv $Y $X) $X) '$Y)
	    (mathKB.assert  '(ruleMul $X 0.0) 0.0)
	    (mathKB.assert  '(ruleMul 0.0 $X) 0.0)
	    (mathKB.assert  '(ruleMul $X 1.0) '$X)
	    (mathKB.assert  '(ruleMul 1.0 $X) '$X)
	    ;; Abs function reduction rules
	    (mathKB.assert  '(ruleAbs (ruleIf $X $Y $Z <$C1=$NUMPOS> <$C2=$NUMPOS>)) '(ruleIf $X $Y $Z $C1 $C2))
	    (mathKB.assert  '(ruleAbs (ruleAbs $X)) '(ruleAbs $X))
	    (mathKB.assert  '(ruleAbs (ruleLog $X)) '(ruleLog $X))
	    (mathKB.assert  '(ruleAbs (ruleNeg $X)) '(ruleAbs $X))
	    (mathKB.assert  '(ruleAbs (ruleSign $X)) '1.0)
	    ;; Exp function reduction rules
	    (mathKB.assert  '(ruleExp (ruleLog $X)) '$X)
	    ;; Expt function reduction rules
	    (mathKB.assert  '(ruleExpt $X 1.0) '$X)
	    (mathKB.assert  '(ruleExpt $X 0.0) 1.0)
	    ;; If function reduction rules
	    (mathKB.assert  '(ruleIf $X $Y $Z $L $L) '$L)
	    (mathKB.assert  '(ruleIf $X < $X $A $B) '$B)
	    (mathKB.assert  '(ruleIf $X <= $X $A $B) '$A)
	    (mathKB.assert  '(ruleIf $X >= $X $A $B) '$A)
	    (mathKB.assert  '(ruleIf $X > $X $A $B) '$B)
	    (mathKB.assert  '(ruleIf <$X=$NUM> $OP <$Y=$NUM> $A $B) '(<$IFFOLD> $X $OP $Y $A $B))
	    ;; Int function reduction rules
	    (mathKB.assert  '(ruleInt (ruleInt $X)) '(ruleInt $X))
	    ;; Log function reduction rules
	    (mathKB.assert  '(ruleLog (ruleExp $X)) '$X)
	    (mathKB.assert  '(ruleLogb (ruleExpt $X $Y) $X) '$Y)
	    (mathKB.assert  '(ruleLogb $X $X) '$X)
	    ;; Max function reduction rules
	    (mathKB.assert  '(ruleMax (ruleAbs $X) $X) '(ruleAbs $X))
	    (mathKB.assert  '(ruleMax $X (ruleAbs $X)) '(ruleAbs $X))
	    (mathKB.assert  '(ruleMax $X (ruleMax $X $Y)) '(ruleMax $X $Y))
	    (mathKB.assert  '(ruleMax $Y (ruleMax $X $Y)) '(ruleMax $X $Y))
	    (mathKB.assert  '(ruleMax $X $X) '$X)
	    ;; Min function reduction rules
	    (mathKB.assert  '(ruleMin (ruleNeg $X) $X) '(ruleNeg $X))
	    (mathKB.assert  '(ruleMin $X (ruleNeg $X)) '(ruleNeg $X))
	    (mathKB.assert  '(ruleMin $X (ruleMin $X $Y)) '(ruleMin $X $Y))
	    (mathKB.assert  '(ruleMin $Y (ruleMin $X $Y)) '(ruleMin $X $Y))
	    (mathKB.assert  '(ruleMin $X $X) '$X)
	    ;; Sign function reduction rules
	    (mathKB.assert  '(ruleSign (ruleIf $X $Y $Z <$C1=$NUMNEG> <$C2=$NUMNEG>)) '-1.0)
	    (mathKB.assert  '(ruleSign (ruleIf $X $Y $Z <$C1=$NUMPOS> <$C2=$NUMNEG>)) '(ruleIf $X $Y $Z 1.0 -1.0))
	    (mathKB.assert  '(ruleSign (ruleIf $X $Y $Z <$C1=$NUMNEG> <$C2=$NUMPOS>)) '(ruleIf $X $Y $Z -1.0 1.0))
	    (mathKB.assert  '(ruleSign (ruleIf $X $Y $Z <$C1=$NUMPOS> <$C2=$NUMPOS>)) '1.0)
	    (mathKB.assert  '(ruleSign (ruleSign $X)) '(ruleSign $X))
	    (mathKB.assert  '(ruleSign (ruleAbs $X)) '1.0)

        true) ;; end initialize
    ;; Show the variables currently stored in symbolic math.
    (defun showVariables()
        vars:(n N answerString)
        (setq answerString _eol)
        (setq N (length myVariables))
        (loop for n from 0 until N do
          ;; Display all atoms as is.
          (setq answerString (append answerString myVariables[n 0] " = " (prettyPrint myVariables[n 1] false) _eol))
          ) ; end loop
        answerString) ;; end showVariables

    ;; **********************************
    ;; Begin MAIN logic section.
    ;; **********************************
    vars:(stemp outFormula (stringOUT false))
    (onError errorHandler)
    (if (= mathKB #void) (initialize))

    ;; If the formula is empty, return an empty Selector WFF.
    (if (= inFormula "") (return ""))

    ;; Simplify basic Selector WFF expression and return a simplified Selector WFF.
    (setq myInFormulaString inFormula)
    (if (isString inFormula) (begin (setq stringOUT true) (setq inFormula (esm.listWff inFormula))))
    (setq outFormula (mathKB.apply inFormula))

    ;; Perform parsing validity check of simplified Selector WFF.
    (setq stemp (string outFormula true))
    (if (isNumber (find "1.#IND" stemp)) (return myInFormulaString))
    (if (isNumber (find "1.#INF" stemp)) (return myInFormulaString))
    (lisp stemp) 

    ;; Return simplified Selector WFF in same format as input.
    (if (= stringOUT true) (setq outFormula stemp))

    outFormula) ;; end simplify

























































;;**EXPORTKEY**:esm:svmRegress
(deforphan esm:svmRegress(x y ...)
;; *******************************************************************
;; name:     svmRegress
;; 
;; summary:  Trains a support vector machine regression Lambda, using the
;;           Sequential Error Estimation (SEE) algorithm and returns the 
;;           trained estimator Lambda. (multiple regression network)
;;
;;           The Sequential Error Estimation (SEE) algorithm, allows a
;;           single kernel function or a vector of kernel functions to 
;;           build a support vector regression network linked with a
;;           multiple regression model.
;;            
;;           Sequential sample sets of the training examples are used
;;           to build multiple support vector regression models of the
;;           training data. The best of these models are linked together,
;;           via multiple regression, to form a composite model. This
;;           sequential sampling method automatically supports out-of-sample
;;           testing during the training phase; and, allows larger training
;;           data sets to be estimated in a fraction of the time required
;;           for an exhaustive Gramm matrix regression.
;;
;;           The model error is calculated based upon the absolute error of
;;           each estimate (calculated as a percent of the target variable).
;;           An error estimation grid can be constructed which penializes the
;;           model error for each grid slot whose average Y values are not
;;           sequentially increasing. This method enhances model fit in cases
;;           with low signal-to-noise ratios.  
;;
;; Parms:    x:         The N by M vector array representing the original observations
;;                      in the form of:    x x ... x
;;                                         x x ... x
;;                                             ... 
;;                                         x x ... x
;;           y   		The N vector of dependent variables.
;;           kernelID   (Optional)The kernel identifier to be used for support vector machine training (may also be a vector of kernel functions).
;;                        "all"			A composite vector of svm regression kernel functions.  
;;                        "linear"		A linear dot product kernel.  
;;                        "square"		A squared linear dot product kernel.  
;;                        "cube"		A cubed linear dot product kernel.  
;;                        Lambda		    Any user supplied modified dot product kernel.  
;;                        function		Any user supplied modified dot product kernel.
;;           properties (Optional) The Structure of SVM training settings.
;;             ETollerance  (Property) The regression error tollerance as a percent of Y.
;;             GridErr      (Property) Size of the sequential error estimation grid.
;;             MaxErr  	    (Property) The maximum error before halting training as a percent of Y.
;;             MaxGen  	    (Property) The maximum generation count before halting training.
;;             MaxLayers    (Property) The maximum number of svm layers before halting training.
;;             ModelCount  	(Property) The maximum number of svm regression models to link via multiple regression.
;;             OverrideSW   (Property) True iff we are to use the user specified sample size without override.
;;             UserSVSize   (Property) The Gaussian sample size (maximum number of support vectors to use during Gaussian initialization).
;;             VerboseSW    (Property) True iff we are to set verbose mode on.
;; Return:   result: 	The result structure containing the trained SEE regression model results, where
;;		  	    			result.Error		Contains the final tollerant error (expressed as a percent of each target value)
;;		  	    			result.Generations	Contains the number of generations used during training
;;							result.Weights 		Contains the weight number vector after training
;;							result.Support 		Contains the support vectors after training
;;
;;			 Note1: Support Vector Machine regression can be highly accurate
;;				    when solving non-linear regression problems. However, the
;;                  accuracy varies from excellent to poor depending upon
;;                  the ratio of: the chosen Gaussian Sample Size (mySampleSize);
;;                  the number of regression variables (M); and the THEORETICAL
;;                  number of variables created by the kernel function to
;;                  make the non-linear problem linearly solvable. A simplified
;;                  example would be as follows.
;;
;;					Solving a quadratic regression problem with variableCount == 3,
;;                  y = sum{m from 0 until variableCount}(Cm*Xm*Xm), and 
;;                  a kernel function of vectorSquareInnerProduct, is very
;;                  accurate with a Gaussian Sample Size (mySampleSize) of 10.
;;                  However, if the variableCount is increased to 10, then the
;;                  accuracy completely breaks down and is not restored until
;;                  the Gaussian Sample Size is increased to around 100. An
;;                  explanation is as follows.
;;
;;					In order to make the quadratic regression linearly tractable,
;;                  the vectorSquareInnerProduct performs an on-the-fly squaring
;;                  of each training point vector. Thus, with a training point
;;                  vector of size three, the vectorSquareInnerProduct creates
;;                  the following on-the-fly THEORETICAL new training point:
;;					kernel(X1,X2,X3) => (X1,X2,X3,X1*X1,X2*X2,X3*X3,X1*X2,X1*X3,X2*X3).
;;                  Clearly the problem is now linearly solvable because the squared
;;                  variables are now terms in the THEORETICAL on-the-fly linear regression
;;                  created by the kernel. NOTICE however, that the THEORETICAL linear
;;                  regression, created on-the-fly by the kernel function, has nine variables
;;                  not three variables as in the original quadratic problem. Unless the
;;                  number of training points is greater than nine, and the Gaussian 
;;                  sample size is greater than nine, the on-the-fly linear regression
;;                  will not have enough data to get accurate results.
;; 
;;           Note2: See Cristianini, "Support Vector Machines", page 169.
;;           Note3: This SVM regression Lambda uses the Sequential Error Estimation (SEE)
;;                  algorithm.
;; *******************************************************************
    pvars:(;; Public variables
           Number:Error                 ;; The current absolute error for the SEE regression model (in percent of target). 
           Number:ErrorMax           	;; The maximum absolute error for the SEE regression model (in percent of target). 
           Number:ETollerance           ;; The current forgiving absolute error tollerance for the SEE regression model (in percent of target). 
           Integer:Generations          ;; The number of training cycles used to train the current SEE regression model. 
           Integer:GenerationMax        ;; The maximum number of training cycles before training is halted at any error rate. 
           kernel   		       		;; The current support vector machine kernel (user specified).
           kernelChoices                ;; The current support vector machine kernel choices (automatically assigned).
           Vector:KX                    ;; The kernel vector for the SEE regression model.
           Integer:myErrorGridSize      ;; The size of the sequential error estimation grid. 
           NumVector:myErrorGrid        ;; The sequential error estimation grid for the SEE regression model.
           Integer:myLayers     	    ;; The current number of svm regression network layers.
           Integer:myMaxLayers     	    ;; The maximum number of svm regression network layers.
           Integer:myModelCount     	;; The maximum number of svm regression models to link via multiple regression.
           myOverrideSW     			;; True iff we are to use the user specified sample size without override. 
           Integer:mySampleSize     	;; Size of the training sample sets, selected from the total training examples, during each iterative regression. 
           Integer:mySampleCount        ;; Count of the training sample sets, selected from the total training examples, during each iterative regression. 
           mySampleHistory              ;; History of the training sample sets, selected from the total training examples, during each iterative regression. 
           mySVMParent                  ;; The parent Lambda of this SVM regression Lambda community. 
           myVerboseSW      			;; True iff we are to display progress on the console. 
           Integer:M					;; The number of elements in each training example (independent variables). 
           Integer:N					;; The number of training examples (independent variables). 
           IntVector:sortedY            ;; The N vector of target points, dependent variable, sorted in ascending order. 
           NumVector:W                  ;; The weight coefficient vector for the SEE regression model.
           ObjVector:WX                 ;; The object vector of support vectors for the SEE regression model.
           X                       		;; The N x M matrix of training examples, independent variables, for training the SEE regression model. 
           NumVector:Y                  ;; The N vector of training examples (dependent variables). 
           ;; Public child methods
           clear			       		;; Clear the current support vector machine.
           compositeKernel              ;; A built in composite kernel used for machine learning.
           computeError	           		;; Return the average percent error for all of the training examples.
           createCompositeModel	        ;; Create the best composite regression model from the individual support vector regressions from each training generation.
           createHiddenLayerArray       ;; Create the hidden layer training array for training the next support vector layer.
           kernel   		       		;; Return the svm kernel output for the SEE regression model.
	       multipleRegression           ;; Performs a Gaussian multiple regression on the N x M+1 matrix
           svmLambda			       		;; Return an Lambda ready to compute the svm output for specified input vector.
           svmMultipleLayerLambda		;; Return a multiple layer Lambda ready to compute the svm output for specified input vector.
           svmOutput		       		;; Return the svm output for the specified input vector.
           svmTraining		       		;; Train the svm machine on the specified training examples.
           trainRegressionModel		    ;; Incrementally train the SEE regression model.
           ;; Private maintenance child methods
           selfTest                		;; The self test method for this Lambda. 
           ;; Private maintenance templates
           (myHistoryTemplate #{Step: #void Layer: #void Error: #void KX: #void W: #void WX: #void Ey: #void Py: #void Composite: #void}) 
           ) ;; end of persistent variables
    ;; ***************************
    ;; Define Public Child Lambdas.
    ;; ***************************
    ;; Clear the current support vector machine.
    (defun clear()
       (setq kernel (lambda(k) k))
       (setq Error 0.0)
       (setq M 0)
       (setq N 0)
       (setq Generations 0) 
       (setq sortedY #void)
       (setq W #void)
       (setq X #void)
       (setq Y #void)
       (setq mySampleHistory #void)
       true) ; end clear
    ;; A built in composite kernel used for machine learning.
    (defun compositeKernel(NumVector:x NumVector:y)
       regs:( Number:kx (Number:xx 1.0) Number:ex Number:lx)
       (setq kx (|Gv:vectorInnerProduct| x y))
       (setq ex (- (/ 2.0 (+ 1.0 (exp (- kx)))) 1.0))
       (setq lx (log (+ 1.0 (abs kx)))) 
       (+= xx kx) ;; linear
       (+= xx (if (> ex 0.0) 1.0 0.0)) ;; binary
       (+= xx (if (> ex 0.0) 1.0 -1.0)) ;; bipolar
       (+= xx (cos kx)) ;; cosine
       (+= xx ex) ;; exponent
       (+= xx lx) ;; log
       (+= xx (/ 1.0 (+ 1.0 (exp (- kx))))) ;; sigmoid  
       (+= xx (/ 1.0 (+ 1.0 (exp (- kx))))) ;; sigmoid  
       (+= xx (sin kx)) ;; sine
       (+= xx (tan kx)) ;; tangent
       (+= xx (tanh kx)) ;; hypertangent
       (setq xx (* xx xx xx))
       xx) ; end compositeKernel
    ;; Return the average percent error for all of the training examples.
    ;; Note1: The regression error is computed as a percent of the target (dependent variable).
    ;; Note2: Any regression error within the tollerance limit, ETollerance, is treated as NO error.
    (defun computeError(Structure:HRecord)
       regs:(k m n NN)
       regs:(Number:etol Number:pct Number:dy Number:ey Number:Yn)
       regs:(Number:Pyk Number:Pym)
       regs:((Number:err 0.0) (Number:RZero 0.0) Number:QAdj (Number:QSum 0.0))
       regs:(NumPointer:pY NumPointer:pEy NumPointer:pPy)
       vars:(NumVector:Ey NumVector:Py IntVector:sortedEy)
       ;; Initialize (if necessary)
	   (setq etol ETollerance)
	   (setq NN N)
       (setq pY Y)
       (setq HRecord.Ey (setq Ey (new Vector: Number: NN)))
       (setq HRecord.Py #void)
       (setq pEy Ey)
       ;; We recompute the average absolute percent error
       ;; Note: This is a mission critical Lambda and MUST run extra fast.
       (vmregRunInHardware start:)
	   (loop for n from 0 until NN do
          (setq ey (svmOutput X[n] HRecord))
          (setq pEy[n] ey) 
          (setq Yn pY[n]) 
          (setq dy (- ey Yn))
	      (if (<> Yn RZero) then (/= dy Yn))
          (setq pct (|Gv:abs| dy))
          (if (> pct etol) (-= pct etol) (setq pct 0.0))
	      (+= err pct)
	      ) ; end error loop
       (vmregRunInHardware stop:)
	   (/= err NN)
       (setq HRecord.Error err)
       (setq HRecord.ETollerance ETollerance)
       ;; Compute error estimation grid and adjust percent error score accordingly.
       ;; Note: We compute the average Y value for each slot in the error estimation
       ;;       grid. Ideally sequentially increasing grid slots should have 
       ;;       sequentially increasing average Y values. For every grid slot where
       ;;       this ideal condition is NOT true, we penalize the error score.
       (if (> myErrorGridSize 1)
           (begin
		      (setq sortedEy (sort Ey < true))
              (setq HRecord.Py (setq Py (new Vector: Number: myErrorGridSize)))
              (setq pY Y)
		      (setq pPy Py)
              (setq pEy Ey)
		      (setq QAdj (/ myErrorGridSize NN))
		      (vmregRunInHardware start:)
			  (loop for n from 0 until NN do
		         (setq m sortedEy[n])
		         (setq Yn pY[m])
		         (*= Yn QAdj)
		         (setq k QSum)
                 (setq Pyk pPy[k])
		         (+= Pyk Yn) 
		         (setq pPy[k] Pyk) 
		         (+= QSum QAdj) 
			     ) ; end error loop
		      (vmregRunInHardware stop:)
		      ;; Penalize the final error score for every grid slot
              ;; which fails a simple sequential increase test.
		      (vmregRunInHardware start:)
              (setq m 0)
              (loop for k from 1 until myErrorGridSize do 
                (setq Pym pPy[m])
                (setq Pyk pPy[k])
		        (if (<= Pyk Pym) then (*= err 10.0))
                (++ m)
		        ) ; end K loop  
		      (vmregRunInHardware stop:)
              (setq HRecord.Error err)
           )) ;; end compute error esitmation grid
       err) ; end computeError
    ;; Create the best composite regression model from the individual support vector regressions from each training generation.
    (defun createCompositeModel()
       regs:(k K m MM n NN rm (KC 2))
       regs:(Number:wk Number:cc)
       regs:(NumPointer:pRM NumPointer:pEy NumPointer:pEy NumPointer:pY)
       vars:(Structure:HRecord Structure:HR)
       vars:(NumVector:CS NumVector:WK NumMatrix:RM NumVector:Ey)
       ;; Do NOT create a composite model if there are less than two sampled regression models.
       (setq K (length mySampleHistory))
       (if (< K KC) (return mySampleHistory[0]))
       (sort mySampleHistory (lambda(x y) (< x.Error y.Error)))
       ;; Form a composite model which combines the previous sampled regressions.
       ;; Note: We use another multiple regression to compute the optimal
       ;;       coefficients for merging the previous support vector regression models.
       (setq NN N)
       (setq RM (new Matrix: Number: 2 NN (addi KC 1)))
       (vmregRunInHardware start:)
       (setq pY Y)
       (setq pRM RM)
       (setq rm -1)
       (loop for n from 0 until NN do
          (loop for k from 0 until KC do
            (setq HR mySampleHistory[k])
            (setq Ey HR.Ey)
            (setq pEy Ey)
            (++ rm)(setq pRM[rm] pEy[n])
            ) ; end K loop
          (++ rm)(setq pRM[rm] pY[n])
          ) ; end NN loop
       (vmregRunInHardware stop:)
       ;; Solve for the best Gaussian coefficients for merging the support vector models.
       (setq CS (multipleRegression RM))
       (setq mySampleHistory[K] (setq HRecord (new myHistoryTemplate)))
       (setq HRecord.Step Composite:)
       (setq HRecord.W (new Vector: Number:))
       (setq HRecord.WX (new Vector: Object:))
       (setq HRecord.KX (new Vector:))
       (vmregRunInHardware start:)
       (loop for k from 0 until KC do
          (setq HR mySampleHistory[k])
          (setq WK (copy HR.W))
          (setq cc CS[k])
          (setq MM (length WK))
          (loop for m from 0 until MM do
             (setq wk WK[m])
             (*= wk cc)
             (setq WK[m] wk)
             ) ; end m
          (setq HRecord.W (append HRecord.W WK))
          (setq HRecord.WX (append HRecord.WX HR.WX))
          (setq HRecord.KX (append HRecord.KX HR.KX))
          ) ; end K loop
       (vmregRunInHardware stop:)
       (computeError HRecord)
       (if (> Error HRecord.Error)
           (begin
             (setq Error HRecord.Error)
             (setq W HRecord.W)
             (setq KX HRecord.KX)
             (setq WX HRecord.WX)
             (setq myErrorGrid HRecord.Py)
           )) ; end if
       HRecord) ; end createCompositeModel
    ;; Create the hidden layer training array for training the next support vector layer.
    (defun createHiddenLayerArray(Lambda Integer:modelCount)
       regs:(k K n NN)
       vars:(tX TX)
       ;; Do NOT create a hidden layer if there are less than two sampled regression models.
       (setq K (length mySampleHistory))
       (if (or (< modelCount 2) (< K modelCount)) (return #void))
       (sort mySampleHistory (lambda(x y) (< x.Error y.Error)))
       ;; Form a composite model which combines the previous sampled regressions.
       ;; Note: We use another multiple regression to compute the optimal
       ;;       coefficients for merging the previous support vector regression models.
       (setq NN N)
       (setq TX (new Vector: Object: NN))
       (setq Lambda.hiddenLambdas (new Vector: Object: K))
       (loop for k from 0 until modelCount do (setq Lambda.hiddenLambdas[k] (svmLambda mySampleHistory[k])))
       (loop for n from 0 until NN do
          (setq TX[n] (setq tX (new Vector: Number: modelCount)))
          (loop for k from 0 until modelCount do
            (setq tX[k] mySampleHistory[k].Ey[n])
            ) ; end K loop
          ) ; end NN loop
       TX) ; end createHiddenLayerArray
	;; summary:  Performs a Gaussian multiple regression on the N by M+1 matrix
	;; Parms:    MXY:     The N by M+1 matrix representing the original observations
	;;                    in the form of:    x x ... x y
	;;                                       x x ... x y
	;;                                           ... 
	;;                                       x x ... x y
	;; Return:   C:       The M coefficient vector for the regression.
	;; Note1:    Average error statistics are computed as a percent of the target (dependent variable).
	;; Note2:    See Sedgewick[2] chap 37.
	(defun multipleRegression(NumMatrix:MXY)
	    vars:(NumMatrix:Xt NumVector:C)
	    ;; Perform a least squares regression on all the factors.
	    (setq Xt (|Gv:makeGaussianMatrix| MXY))
	    (setq Xt (|Gv:matrixGaussianEliminate| Xt true))
	    (setq C (|Gv:matrixGaussianSubstitute| Xt))
	    ;; Return the coefficient vector for the regression.
	    C) ; end multipleRegression
    ; Return an Lambda ready to compute the svm output for specified input points.
    (defun svmLambda(...)
       regs:(n m NW)
       vars:(NumVector:hW Vector:hKX ObjVector:hWX)
       vars:(Lambda HRecord)
       (setq Lambda (eval "(lambda(x) pvars:(Number:Error NumVector:ErrorGrid Number:ETollerance Expressions Integer:M Integer:N Strategy ObjVector:WX Vector:KX NumVector:W) regs:(n NN Number:xn Number:wn Number:ey NumPointer:pW) (setq NN (length W)) (setq pW W) (loop for n from 0 until NN do (setq wn pW[n]) (if (<> wn 0.0) then (begin (setq xn (KX[n] x WX[n])) (*= xn wn) (+= ey xn)  )) ) ey)"))
       (setq Lambda.Strategy svmSEE:)
       (if (>= (argCount) 1) (setq HRecord (argFetch 0)) (setq HRecord mySampleHistory[0]))
       ;; Drop all but the support vectors for the trained model.
       (setq hW HRecord.W)
       (setq hKX HRecord.KX)
       (setq hWX HRecord.WX)
       (setq NW (length hW))
       (setq m 0)(loop for n from 0 until NW do (if (<> hW[n] 0.0) (++ m)))
       (setq Lambda.N m)
       (setq Lambda.W (new Vector: Number: m))
       (setq Lambda.KX (new Vector: m))
       (setq Lambda.WX (new Vector: Object: m))
       (setq m 0)
       (loop for n from 0 until NW do 
          (if (<> hW[n] 0.0)
              (begin
                 (setq Lambda.W[m] hW[n])
                 (setq Lambda.KX[m] hKX[n])
                 (setq Lambda.WX[m] hWX[n])
                 (++ m)
              )) ; end if
          ) ; end loop
       (setq Lambda.M M)
       (setq Lambda.ETollerance HRecord.ETollerance)
       (setq Lambda.ErrorGrid HRecord.Py)
       (setq Lambda.Error HRecord.Error)
       Lambda) ; end svmLambda
    ; Return a multiple layer Lambda ready to compute the svm output for specified input points.
    (defun svmMultipleLayerLambda(kernelID)
       regs:(n m modelCount)
       vars:(NumVector:hW Vector:hKX ObjVector:hWX)
       vars:(TX Lambda nextLambda HRecord properties)
       (setq Lambda (eval "(lambda(x) pvars:(Number:Error NumVector:ErrorGrid  Number:ETollerance Integer:M Integer:N Strategy Lambda:outputLambda ObjVector:hiddenLambdas Integer:N Integer:eyM) regs:(m MM Number:xn Number:wn Number:ey) vars:(NumVector:eyX) (setq MM eyM) (setq eyX (new Vector: Number: eyM)) (loop for m from 0 until MM do (setq eyX[m] (hiddenLambdas[m] x)) (setq ey (outputLambda eyX))) ey)"))
       (setq Lambda.Strategy svmSEE:)
	   ;; Setup the properties for the next layer of svm training.
       (setq modelCount myModelCount)
       (setq properties (new Structure: GridErr: myErrorGridSize
                                        MaxGen: GenerationMax
                                        MaxLayers: myMaxLayers
                                        CurrentLayer: myLayers
                                        ModelCount: modelCount
                                        OverrideSW: myOverrideSW
                                        UserSVSize: mySampleSize
                                        VerboseSW: myVerboseSW
                                        ETollerance: ETollerance
                                        MaxErr: ErrorMax
                               )) ; end new properties
       ;; Train the output layer support vector Lambda, using the hidden layer Y estimates as inputs.
       (setq Lambda.eyM modelCount)
       (setq Lambda.M M)
       (setq Lambda.N N)
       (setq TX (createHiddenLayerArray Lambda modelCount))
       (setq nextLambda (mySVMParent TX Y kernelID properties))
       ;; Place together the support vectors layers for the final trained model.
       (setq Lambda.outputLambda nextLambda)
       (setq Lambda.ETollerance nextLambda.ETollerance)
       (setq Lambda.ErrorGrid nextLambda.ErrorGrid)
       (setq Lambda.Error nextLambda.Error)
       ;; Never return a layered model with greater error than the current best model
       (if (>= Lambda Error) (setq Lambda (svmLambda)))
       Lambda) ; end svmMultipleLayerLambda
    ;; Return the svm output for the specified input point.
    ;; Note1: x must be a vector of length M
    ;; Note2: The SEE model says: y ~= sum{n=0-N,W[n]*Y[n]*Kernel(X[n],x)}
    (defun svmOutput(NumVector:x Structure:HRecord)
       regs:(n NN)
       regs:(Number:xn Number:wn Number:ey)
       regs:(NumPointer:pW)
       vars:(NumVector:tW ObjVector:tWX Vector:tKX)
       (setq tW HRecord.W)
       (setq tWX HRecord.WX)
       (setq tKX HRecord.KX)
       (setq NN (length tW))
       (setq pW tW)
       (loop for n from 0 until NN do
          (setq wn pW[n])
          (if (<> wn 0.0)
              then
              (begin 
                (setq xn (tKX[n] x tWX[n]))
                (*= xn wn)
                (+= ey xn)
              )) ; end if        
          ) ; end NN model loop
       ey) ; end svmOutput
    ;; Train the svm machine on the specified inputs and model.
	;; Parms:    x:         The N by M vector array representing the original observations
	;;                      in the form of:    x x ... x
	;;                                         x x ... x
	;;                                             ... 
	;;                                         x x ... x
	;;           y   		The N vector of dependent variables.
	;;           kernelID   The kernel identifier to be used for support vector machine training.
	;;                        #void			A linear dot product kernel.  
	;;                        "linear"		A linear dot product kernel.  
	;;                        "square"		A squared linear dot product kernel.  
	;;                        "cube"		A cubed linear dot product kernel.  
	;;                        Lambda		    Any user supplied modified dot product kernel.  
	;;                        function		Any user supplied modified dot product kernel.  
    ;;           properties The Structure of SVM training settings.
    ;;             ETollerance  (Property) The regression error tollerance as a percent of Y.
    ;;             GridErr      (Property) Size of the sequential error estimation grid.
    ;;             MaxErr  	    (Property) The maximum error before halting training as a percent of Y.
    ;;             MaxGen  	    (Property) The maximum generation count before halting training.
    ;;             MaxLayers    (Property) The maximum number of svm layers before halting training.
    ;;             ModelCount  	(Property) The maximum number of svm regression models to link via multiple regression.
    ;;             OverrideSW   (Property) True iff we are to use the user specified sample size without override.
    ;;             UserSVSize   (Property) The Gaussian sample size (maximum number of support vectors to use during Gaussian initialization).
    ;;             VerboseSW    (Property) True iff we are to set verbose mode on.
	;; Return:   result: 	The result structure containing the trained SEE regression model results, where
	;;		  	    			result.Error		Contains the final tollerant error (expressed as a percent of each target value)
	;;		  	    			result.Generations	Contains the number of generations used during training
	;;							result.Weights 		Contains the weight number vector after training
    ;;							result.Support 		Contains the support vectors after training
    (defun svmTraining(x y kernelID properties)
        regs:(k K m n NN SN minSV begK)
        vars:(result HRecord userSVSize)
	    ;; Clear support vector machine for retraining.
	    (clear)
	    ;; Retrieve any optional arguments and perform setup.
	    (setq myErrorGridSize properties.GridErr)
	    (setq GenerationMax properties.MaxGen)
	    (setq myMaxLayers properties.MaxLayers)
	    (setq myLayers properties.CurrentLayer)
	    (setq myModelCount properties.ModelCount)
	    (setq myOverrideSW properties.OverrideSW)
	    (setq userSVSize properties.UserSVSize)
	    (setq myVerboseSW properties.VerboseSW)
		(setq ETollerance properties.ETollerance)
	    (setq ErrorMax properties.MaxErr)
	    (if (or (<> (isVector x) true) (<> (isVector x[0]) true)) (error "svmRegress: X argument must be a Vector Array of rank 2"))
	    (setq m (length x[0]))
	    (setq n (length x))
	    (if (or (<> (isVector y) true) (<> (length y) n)) (error "svmRegress: Y argument must be a Vector of length the same as X"))
	    ;; Initialize the untrained svm SEE model.
	    (setq X x)
	    (setq Y y)
	    (setq M m)
	    (setq N n)
	    ;; Initialize the user specified support vector machine kernel.
	    (cond
	      ((or (= kernelID "all") (= kernelID "default") (= kernelID #void)) (begin (setq kernelChoices (new Vector: 13 |Gv:vectorInnerProduct| |Gv:vectorSquareInnerProduct| |Gv:vectorCubeInnerProduct| |Gv:vectorSigmoidInnerProduct| |Gv:vectorExpInnerProduct| |Gv:vectorLogInnerProduct| |Gv:vectorTanInnerProduct| |Gv:vectorTanhInnerProduct| |Gv:vectorSineInnerProduct| |Gv:vectorCosineInnerProduct| |Gv:vectorBinaryInnerProduct| |Gv:vectorBipolarInnerProduct| compositeKernel)) (setq minSV (* M M M))))
	      ((or (= kernelID "binary") (= kernelID |Gv:vectorBinaryInnerProduct|)) (begin (setq kernelChoices (new Vector: 1 |Gv:vectorBinaryInnerProduct|)) (setq minSV (* M M M))))
	      ((or (= kernelID "bipolar") (= kernelID |Gv:vectorBipolarInnerProduct|)) (begin (setq kernelChoices (new Vector: 1 |Gv:vectorBipolarInnerProduct|)) (setq minSV (* M M M))))
	      ((= kernelID "composite") (begin (setq kernelChoices (new Vector: 1 compositeKernel)) (setq minSV (* M M M))))
	      ((or (= kernelID "cosine") (= kernelID |Gv:vectorCosineInnerProduct|)) (begin (setq kernelChoices (new Vector: 1 |Gv:vectorCosineInnerProduct|)) (setq minSV (* M M M))))
	      ((or (= kernelID "cube") (= kernelID |Gv:vectorCubeInnerProduct|)) (begin (setq kernelChoices (new Vector: 1 |Gv:vectorCubeInnerProduct|)) (setq minSV (* M M M))))
	      ((or (= kernelID "exp") (= kernelID |Gv:vectorExpInnerProduct|)) (begin (setq kernelChoices (new Vector: 1 |Gv:vectorExpInnerProduct|)) (setq minSV (* M M M))))
	      ((or (= kernelID "linear") (= kernelID |Gv:vectorInnerProduct|)) (begin (setq kernelChoices (new Vector: 1 |Gv:vectorInnerProduct|)) (setq minSV M)))
	      ((or (= kernelID "log") (= kernelID |Gv:vectorLogInnerProduct|)) (begin (setq kernelChoices (new Vector: 1 |Gv:vectorLogInnerProduct|)) (setq minSV (* M M M))))
	      ((= kernelID "poly") (begin (setq kernelChoices (new Vector: 3 |Gv:vectorInnerProduct| |Gv:vectorSquareInnerProduct| |Gv:vectorCubeInnerProduct|)) (setq minSV (* M M M))))
	      ((or (= kernelID "quart") (= kernelID |Gv:vectorQuartInnerProduct|)) (begin (setq kernelChoices (new Vector: 1 |Gv:vectorQuartInnerProduct|)) (setq minSV (* M M M M))))
	      ((or (= kernelID "quint") (= kernelID |Gv:vectorQuintInnerProduct|)) (begin (setq kernelChoices (new Vector: 1 |Gv:vectorQuintInnerProduct|)) (setq minSV (* M M M M M))))
	      ((or (= kernelID "sigmoid") (= kernelID |Gv:vectorSigmoidInnerProduct|)) (begin (setq kernelChoices (new Vector: 1 |Gv:vectorSigmoidInnerProduct|)) (setq minSV (* M M M))))
	      ((or (= kernelID "sine") (= kernelID |Gv:vectorSineInnerProduct|)) (begin (setq kernelChoices (new Vector: 1 |Gv:vectorSineInnerProduct|)) (setq minSV (* M M M))))
	      ((or (= kernelID "square") (= kernelID |Gv:vectorSquareInnerProduct|)) (begin (setq kernelChoices (new Vector: 1 |Gv:vectorSquareInnerProduct|)) (setq minSV (* M M))))
	      ((or (= kernelID "tan") (= kernelID |Gv:vectorTanInnerProduct|)) (begin (setq kernelChoices (new Vector: 1 |Gv:vectorTanInnerProduct|)) (setq minSV (* M M M))))
	      ((or (= kernelID "tanh") (= kernelID |Gv:vectorTanhInnerProduct|)) (begin (setq kernelChoices (new Vector: 1 |Gv:vectorTanhInnerProduct|)) (setq minSV (* M M M))))
	      ((isVector kernelID) (begin (setq kernelChoices kernelID) (setq minSV M)))           
	      (else  (begin (setq kernelChoices (new Vector: 1 kernelID)) (setq minSV M)))
	      ) ; end cond 
		;; Initialize the persistent variables before proceeding with training.
        (setq sortedY (sort Y < true))
		(setq Error 1.0e300)
        (setq W #void)
	    ;; Run multiple generations taking the first one to achieve success.
	    ;; Note: If no generation achieves success, we take the best so far.
        (if (and (= myOverrideSW true) (< userSVSize minSV)) (setq mySampleSize minSV) (setq mySampleSize userSVSize))      
        (if (< N mySampleSize) (setq mySampleSize N))
        (setq mySampleCount (/ N mySampleSize))
        (if (< mySampleCount 1) then (begin (setq mySampleSize N) (setq mySampleCount 1)))
        (setq mySampleHistory (new Vector:))
	    (setq Generations 0)
	    ;; Train the next generation.
        (setq K (length kernelChoices))
	    (while (and (>= Error ErrorMax) (< Generations GenerationMax)) do
           (loop for k from 0 until K do
              (setq kernel kernelChoices[k])
	          (trainRegressionModel)
              ) ; end kernel loop
	       (++ Generations)
	       ) ; end while
        ;; Create the best possible composite regression model.
        (createCompositeModel)
        ;; Create training result structure, and clean up after learning.
        (++ myLayers)
        (if (>= myLayers myMaxLayers) 
	        (setq result (svmLambda))
	        (setq result (svmMultipleLayerLambda kernelID))
	        ) ; end if
	    ;; Generate the final absolute error score after training.
	    HaltTraining::
        (if (= result #void) (setq result (svmLambda)))
	    (if myVerboseSW (writeln "svmRegress: Final Model Generations = [" Generations "], SVMs = [" (length W) "], ETollerance = [" ETollerance "], Error = [" Error "]"))       
        result) ; end svmTraining
    ;; Incrementally train the SEE model on the two training points.
    ;; Note1: The regression error is computed as a percent of the target.
    ;; Note2: Any regression error within the tollerance limit, ETollerance, is treated as NO error.
    (defun trainRegressionModel()
       regs:(k K m n rm sn)
       regs:(Number:dotProduct Number:y Number:xk Number:xn)
       regs:(IntPointer:pSV IntPointer:pSortedY NumPointer:pXn NumPointer:pXk NumPointer:pRM)
       regs:(Number:avgY Number:avgErr)
       regs:(Integer:NN                  ;; Number of total training examples.
             Integer:SN                  ;; Number of seed support vectors to use in Gaussian initialization.
             Integer:firstStep           ;; Example first step size for use in support vector selection.
             Integer:stepInc             ;; Example selection step size for use in support vector selection.
             Number:weightFactor         ;; Weight factor for use in support vector merging.
             ) ; end register variables
       vars:(NumVector:CS                ;; The Gaussian regression coefficients for the support vectors.
             ObjVector:HRecord  		 ;; The history record for the nth sample set.
             NumMatrix:RM 				 ;; The regression matrix regressing the support vectors against all of the training points in X.
             IntVector:sortedHistories	 ;; The Integer Vector of the sorted sample set histories.
             IntVector:SV				 ;; The indices of the current support vector set. 
             NumVector:Xk 				 ;; The Number Vector, of the independent variables, for the kth training example.
             NumVector:Xn 				 ;; The Number Vector, of the independent variables, for the nth training example.
             Vector:tKX 				 ;; The Vector, of the kernel functions, for the nth sample set.
             ObjVector:tWX 				 ;; The Object Vector, of the support vectors, for the nth sample set.
             ) ; end temporary variables
       ;; Zero the errors where there are already support vectors.
       (setq firstStep (integer (+ Generations (* myLayers GenerationMax))))
       (if (>= firstStep mySampleCount) (goto TrainingCompleted:))
       (setq K (length SV))
       (setq NN N)
       ;; Extend the support vectors with the examples with the worst percent errors
       (setq SN mySampleSize)
       (setq SV (new Vector: Integer: SN))
       (setq stepInc mySampleCount)
       (setq pSV SV)
       (setq pSortedY sortedY)
       (setq n firstStep)
       (vmregRunInHardware start:)
       (loop for sn from 0 until SN do
		  (setq pSV[sn] pSortedY[n])
          (+= n stepInc)
          ) ; end loop 
       (vmregRunInHardware stop:)
       (setq SN (length SV))
       ;; Construct the support vectors regression matrix.
       ;; Note: We attempt to regress a linear model of the support vectors
       ;;       against all of the training points in X. The Regression
       ;;       Matrix is of the following form:
       ;;          kernel(X[0],X[SV[0]]) ... kernel(X[0],X[SV[SN]]) Y[0]
       ;;          kernel(X[1],X[SV[0]]) ... kernel(X[1],X[SV[SN]]) Y[1]
       ;;                  ...           ...              ...
       ;;          kernel(X[N],X[SV[0]]) ... kernel(X[N],X[SV[SN]]) Y[N]
       (setq RM #void)
       (setq RM (new Matrix: number: 2 NN (addi SN 1)))
       (vmregRunInHardware start:)
       (setq pSV SV)
       (setq pRM RM)
       (setq rm -1)
       (loop for n from 0 until NN do
           (loop for m from 0 until SN do
              (setq k pSV[m])
              (setq Xk X[k])
              (setq Xn X[n])
              (setq dotProduct (kernel Xk Xn))
              (++ rm)
              (setq pRM[rm] dotProduct)
              ) ; end SN loop
           (++ rm)
           (setq pRM[rm] Y[n])
           ) ; end NN loop
       (vmregRunInHardware stop:)
       ;; Solve for the Gaussian coefficients of the support vectors.
       ;; Note: We perform a Gaussian linear regression on the Regression
       ;;       Matrix, returning an SN Vector of coefficients, which we
       ;;       set as the weights, in the SVM  model, of each of the 
       ;;       support vectors respectively. All other non-support-vector
       ;;       weights, in the initial SVM model, are set to zero.
       (setq CS (multipleRegression RM))
       (setq K (length mySampleHistory))
       (setq mySampleHistory[K] (setq HRecord (new myHistoryTemplate)))
       (setq HRecord.Step Generations)
       (setq HRecord.W CS)
       (setq HRecord.WX (setq tWX (new Vector: Object: SN)))
       (setq HRecord.KX (setq tKX (new Vector: SN)))
       (vmregRunInHardware start:)
       (loop for m from 0 until SN do
          (setq k SV[m])
          (setq tKX[m] kernel)
          (setq tWX[m] X[k])
          ) ; end loop
       (vmregRunInHardware stop:)
       ;; Score the current Generation's regression model.
       (computeError HRecord)
       (if (> Error HRecord.Error)
           (begin
              (setq Error HRecord.Error)
              (setq W HRecord.W)
              (setq KX HRecord.KX)
              (setq WX HRecord.WX)
              (setq myErrorGrid HRecord.Py)
           )) ; end if
       ;; This generation of SVM model training is complete.
       TrainingCompleted::
       (if myVerboseSW (writeln "svmRegress: next generation complete.")) 
       ;; Return success
       true) ; end trainRegressionModel
    ;; ****************************************
    ;; Define Private Maintenance Child Lambdas.
    ;; ****************************************
    ;; The self test method for this Lambda.
    (defun selfTest(Test kernelID Ms Ns Gs Lc Mc Ss Eg Os)
       vars:(k m n g G y ey C c X Y Yv avgY avgTopEy topEyCnt
             Lambda err Net pct properties 
             startTime endTime startTimeT endTimeT
             (checkResults true)
             (tol 0.0) (errStop 0.01) (Cs 1.0)
             ) ; end temporary variables
       (clear)
       (setq properties (new Structure: ETollerance: 0.00 GridErr: Eg MaxErr: 0.01 MaxGen: Gs MaxLayers: Lc ModelCount: Mc OverrideSW: Os UserSVSize: Ss VerboseSW: (setq myVerboseSW false)))
       (setq startTimeT (getTickCount 0))
       (setq srandom.seed 8192.0)      
       ;; Select the requested test case
       ;; Test Case linear 
       (if (or (= Test all:) (= Test linear:))
           (begin
		       ;; Create a test polynomial regression where y = C[0]*X[0] + C[1]*X[1] - C[2]*X[2] + C[3]*X[3] - C[4]*X[4] ...
		       ;; Note1: We support a bias by having X[0] == 1 for all N.
		       ;; Note2: This algorithm seems to work well when N is at least 25 times M.
		       (setq c Cs)
		       (setq M Ms)
		       (setq N Ns)
		       (setq X (new Vector: object: N))
		       (setq Y (new Vector: Number: N))
		       (setq C (new Vector: Number: M))
		       (setq C[0] c)
		       (loop for m from 1 until M do
		          (setq C[m] (- (srandom 100.0) 50.0))
		          ) ; end C loop
		       (loop for n from 0 until N do
		          (setq X[n] (new Vector: Number: M))
		          (setq X[n][0] 1)
		          (setq y (* C[0] X[n][0]))
		          (loop for m from 1 until M do 
		          	 (setq X[n][m] (- (srandom 100.0) 50.0))
		             (setq y (+ y (* X[n][m] C[m])))
		             ) ; end M loop
		          (setq Y[n] y)
		          ) ; end N loop
		       ;; Train on the test case.
		       (writeln _eol "Starting test case: linear")
		       (setq Lambda (setq Lambda (esm.svmRegress.svmTraining X Y kernelID properties)))
		       (if (= myVerboseSW false) (writeln "svmRegress: N = [" Ns "], M = [" Lambda.M "], Generations = [" Generations  "], Layers = [" myMaxLayers  "], SVM's = [" Lambda.N "], ETollerance=[" Lambda.ETollerance "], Error=[" Lambda.Error "], ErrorGrid=[" (string Lambda.ErrorGrid true) "]")) 
               (if (= checkResults true)
                   (begin
                      (setq err 0.0)
                      (setq avgTopEy 0.0)
                      (setq topEyCnt 0)
                      (setq avgY (avg Y))
		              (loop for n from 0 until N do
		                 (setq y (Lambda X[n]))
                         (setq pct (- Y[n] y))
                         (if (<> Y[n] 0.0) (/= pct Y[n])) 
                         (if (< pct 0.0) (setq pct (- 0.0 pct)))
                         (setq pct (- pct tol))
                         (if (< pct 0.0) (setq pct 0.0))
		                 (+= err pct)
                         (if (= (modi n (divi N 10)) 0) (writeln "[" n "] ey=[" y "] y=[" Y[n] "] err=[" (- Y[n] y) "] err%=[" pct "]"))
		                 ) ; end N loop
                      (/= err N) 
		              (writeln "svmRegress: err=[" err "], avgY=[" avgY "]")
		           )) ; end if
          )) ; end Test Case linear
       ;; Test Case linearSigmoid 
       (if (or (= Test all:) (= Test linearSigmoid:))
           (begin
		       ;; Create a test polynomial regression where y = C[0]*X[0] + C[1]*X[1] - C[2]*X[2] + C[3]*X[3] - C[4]*X[4] ...
		       ;; Note1: The inputs, X, are restricted to the sigmoid domain.
		       ;; Note2: We support a bias by having X[0] == 1 for all N.
		       ;; Note3: This algorithm seems to work well when N is at least 25 times M.
		       (setq c Cs)
		       (setq M Ms)
		       (setq N Ns)
		       (setq X (new Vector: object: N))
		       (setq Y (new Vector: Number: N))
		       (setq C (new Vector: Number: M))
		       (setq C[0] c)
		       (loop for m from 1 until M do
		          (setq C[m] (- (srandom 1) .5))
		          ) ; end C loop
		       (loop for n from 0 until N do
		          (setq X[n] (new Vector: Number: M))
		          (setq X[n][0] 1)
		          (setq y (* C[0] X[n][0]))
		          (loop for m from 1 until M do 
		          	 (setq X[n][m] (- (srandom .999999999) 0))
		             (setq y (+ y (* X[n][m] C[m])))
		             ) ; end M loop
		          (setq Y[n] y)
		          ) ; end N loop
		       ;; Train on the test case.
		       (writeln _eol "Starting test case: linearSigmoid")
		       (setq Lambda (setq Lambda (esm.svmRegress.svmTraining X Y kernelID properties)))
		       (if (= myVerboseSW false) (writeln "svmRegress: N = [" Ns "], M = [" Lambda.M "], Generations = [" Generations  "], Layers = [" myMaxLayers  "], SVM's = [" Lambda.N "], ETollerance=[" Lambda.ETollerance "], Error=[" Lambda.Error "], ErrorGrid=[" (string Lambda.ErrorGrid true) "]")) 
               (if (= checkResults true)
                   (begin
                      (setq err 0.0)
                      (setq avgTopEy 0.0)
                      (setq topEyCnt 0)
                      (setq avgY (avg Y))
		              (loop for n from 0 until N do
		                 (setq y (Lambda X[n]))
                         (if (> y avgY) then (begin (++ topEyCnt) (+= avgTopEy Y[n])))
                         (setq pct (- Y[n] y))
                         (if (<> Y[n] 0.0) (/= pct Y[n])) 
                         (if (< pct 0.0) (setq pct (- 0.0 pct)))
                         (setq pct (- pct tol))
                         (if (< pct 0.0) (setq pct 0.0))
		                 (+= err pct)
                         (if (= (modi n (divi N 10)) 0) (writeln "[" n "] ey=[" y "] y=[" Y[n] "] err=[" (- Y[n] y) "] err%=[" pct "]"))
		                 ) ; end N loop
                      (/= err N) 
		              (writeln "svmRegress: err=[" err "], avgY=[" avgY "]")
		           )) ; end if
          )) ; end Test Case linearSigmoid
       ;; Test Case srandom 
       (if (or (= Test all:) (= Test srandom:))
           (begin
		       ;; Create a test polynomial regression where y = C[0]*X[0] + C[1]*X[1] - C[2]*X[2] + C[3]*X[3] - C[4]*X[4] ...
		       ;; Note1: We support a bias by having X[0] == 1 for all N.
		       ;; Note2: This algorithm seems to work well when N is at least 25 times M.
		       (setq c Cs)
		       (setq M Ms)
		       (setq N Ns)
		       (setq X (new Vector: object: N))
		       (setq Y (new Vector: Number: N))
		       (setq C (new Vector: Number: M))
		       (setq C[0] c)
		       (loop for m from 1 until M do
		          (setq C[m] (- (srandom 100.0) 50.0))
		          ) ; end C loop
		       (loop for n from 0 until N do
		          (setq X[n] (new Vector: Number: M))
		          (setq X[n][0] 1)
		          (setq y (* C[0] X[n][0]))
		          (loop for m from 1 until M do
		          	 (setq X[n][m] (- (srandom 100.0) 50.0))
		             (setq y (+ y (* X[n][m] C[m])))
		             ) ; end M loop
		          (setq Y[n] (+ (* y .8) (* y (srandom .4))))
		          ) ; end N loop
		       ;; Train on the test case.
		       (writeln _eol "Starting test case: srandom")
		       (setq Lambda (setq Lambda (esm.svmRegress.svmTraining X Y kernelID properties)))
		       (if (= myVerboseSW false) (writeln "svmRegress: N = [" Ns "], M = [" Lambda.M "], Generations = [" Generations  "], Layers = [" myMaxLayers  "], SVM's = [" Lambda.N "], ETollerance=[" Lambda.ETollerance "], Error=[" Lambda.Error "], ErrorGrid=[" (string Lambda.ErrorGrid true) "]")) 
               (if (= checkResults true)
                   (begin
                      (setq err 0.0)
                      (setq avgTopEy 0.0)
                      (setq topEyCnt 0)
                      (setq avgY (avg Y))
		              (loop for n from 0 until N do
		                 (setq y (Lambda X[n]))
                         (if (> y avgY) then (begin (++ topEyCnt) (+= avgTopEy Y[n])))
                         (setq pct (- Y[n] y))
                         (if (<> Y[n] 0.0) (/= pct Y[n])) 
                         (if (< pct 0.0) (setq pct (- 0.0 pct)))
                         (setq pct (- pct tol))
                         (if (< pct 0.0) (setq pct 0.0))
		                 (+= err pct)
                         (if (= (modi n (divi N 10)) 0) (writeln "[" n "] ey=[" y "] y=[" Y[n] "] err=[" (- Y[n] y) "] err%=[" pct "]"))
		                 ) ; end N loop
                      (/= err N) 
		              (writeln "svmRegress: err=[" err "], avgY=[" avgY "]")
		           )) ; end if
          )) ; end Test Case srandom
       ;; Test Case mixedRandom 
       (if (or (= Test all:) (= Test mixedRandom:))
           (begin
		       ;; Create a test polynomial linear model where y = C[0]*X[0] + C[1]*X[1] + C[2]*X[2] ...
		       ;; Create a test polynomial square model where y = C[0]*X[0]*X[0] + C[1]*X[1]*X[1] + C[2]*X[2]*X[2] ...
		       ;; Create a test polynomial sin model where y = C[0]*sin(X[0]) + C[1]*sin(X[1]) + C[2]*sin(X[2]) ...
		       ;; Create a test polynomial log model where y = C[0]*log(abs(X[0])+.000001) + C[1]*log(abs(X[1])+.000001) + C[1]*log(abs(X[2])+.000001) ...
               ;; These four models are mixed together and random noise is added.
		       ;; Note1: We support a bias by having X[0] == 1 for all N.
		       ;; Note2: This algorithm seems to work well when N is at least 25 times M.
		       (setq c Cs)
		       (setq M Ms)
		       (setq N Ns)
		       (setq X (new Vector: object: N))
		       (setq Y (new Vector: Number: N))
		       (setq C (new Vector: Number: M))
		       (setq C[0] c)
		       (loop for m from 1 until M do
		          (setq C[m] (- (srandom 100.0) 50.0))
		          ) ; end C loop
		       (loop for n from 0 until N do
		          (setq X[n] (new Vector: Number: M))
                  (setq k (modi n 4)) 
 		          (setq X[n][0] 1.0)
 		          (setq X[n][1] (number k))
		          (setq y (* C[0] X[n][0]))
		          (loop for m from 2 until M do
		          	 (setq X[n][m] (- (srandom 100.0) 50.0))
                     ;; Mix the four models together
                     (cond
                        ;; Linear model
                        ((= k 0) (setq y (+ y (* X[n][m] C[m]))))
                        ;; Square model
                        ((= k 1) (setq y (+ y (* X[n][m] X[n][m] C[m]))))
                        ;; Sine model
                        ((= k 2) (setq y (+ y (* (|Gv:sin| X[n][m]) C[m]))))
                        ;; Log model
                        (else (setq y (+ y (* (|Gv:log| (+ .000001 (|Gv:abs| X[n][m]))) C[m]))))
                        ) ; end cond
		             ) ; end M loop
		          (setq Y[n] (+ (* y .8) (* y (srandom .4))))
		          ) ; end N loop
		       ;; Train on the test case.
		       (writeln _eol "Starting test case: mixedRandom")
		       (setq Lambda (esm.svmRegress.svmTraining X Y kernelID properties))
		       (if (= myVerboseSW false) (writeln "svmRegress: N = [" Ns "], M = [" Lambda.M "], Generations = [" Generations  "], Layers = [" myMaxLayers  "], SVM's = [" Lambda.N "], ETollerance=[" Lambda.ETollerance "], Error=[" Lambda.Error "], ErrorGrid=[" (string Lambda.ErrorGrid true) "]")) 
               (if (= checkResults true)
                   (begin
                      (setq err 0.0)
                      (setq avgTopEy 0.0)
                      (setq topEyCnt 0)
                      (setq avgY (avg Y))
		              (loop for n from 0 until N do
		                 (setq y (Lambda X[n]))
                         (if (> y avgY) then (begin (++ topEyCnt) (+= avgTopEy Y[n])))
                         (setq pct (- Y[n] y))
                         (if (<> Y[n] 0.0) (/= pct Y[n])) 
                         (if (< pct 0.0) (setq pct (- 0.0 pct)))
                         (setq pct (- pct tol))
                         (if (< pct 0.0) (setq pct 0.0))
		                 (+= err pct)
                         (if (= (modi n (divi N 10)) 0) (writeln "[" n "] ey=[" y "] y=[" Y[n] "] err=[" (- Y[n] y) "] err%=[" pct "]"))
		                 ) ; end N loop
                      (/= err N) 
		              (writeln "svmRegress: err=[" err "], avgY=[" avgY "]")
		           )) ; end if
          )) ; end Test Case mixedRandom
       ;; Test Case randomSigmoid 
       (if (or (= Test all:) (= Test randomSigmoid:))
           (begin
		       ;; Create a test polynomial regression where y = C[0]*X[0] + C[1]*X[1] - C[2]*X[2] + C[3]*X[3] - C[4]*X[4] ...
		       ;; Note1: The inputs, X, are restricted to the sigmoid domain.
		       ;; Note2: We support a bias by having X[0] == 1 for all N.
		       ;; Note3: This algorithm seems to work well when N is at least 25 times M.
		       (setq c Cs)
		       (setq M Ms)
		       (setq N Ns)
		       (setq X (new Vector: object: N))
		       (setq Y (new Vector: Number: N))
		       (setq C (new Vector: Number: M))
		       (setq C[0] c)
		       (loop for m from 1 until M do
		          (setq C[m] (- (srandom 1) .5))
		          ) ; end C loop
		       (loop for n from 0 until N do
		          (setq X[n] (new Vector: Number: M))
		          (setq X[n][0] 1)
		          (setq y (* C[0] X[n][0]))
		          (loop for m from 1 until M do 
		          	 (setq X[n][m] (- (srandom .999999999) 0))
		             (setq y (+ y (* X[n][m] C[m])))
		             ) ; end M loop
		          (setq Y[n] (+ (* y .8) (* y (srandom .4))))
		          ) ; end N loop
		       ;; Train on the test case.
		       (writeln _eol "Starting test case: randomSigmoid")
		       (setq Lambda (esm.svmRegress.svmTraining X Y kernelID properties))
		       (if (= myVerboseSW false) (writeln "svmRegress: N = [" Ns "], M = [" Lambda.M "], Generations = [" Generations  "], Layers = [" myMaxLayers  "], SVM's = [" Lambda.N "], ETollerance=[" Lambda.ETollerance "], Error=[" Lambda.Error "], ErrorGrid=[" (string Lambda.ErrorGrid true) "]")) 
               (if (= checkResults true)
                   (begin
                      (setq err 0.0)
                      (setq avgTopEy 0.0)
                      (setq topEyCnt 0)
                      (setq avgY (avg Y))
		              (loop for n from 0 until N do
		                 (setq y (Lambda X[n]))
                         (if (> y avgY) then (begin (++ topEyCnt) (+= avgTopEy Y[n])))
                         (setq pct (- Y[n] y))
                         (if (<> Y[n] 0.0) (/= pct Y[n])) 
                         (if (< pct 0.0) (setq pct (- 0.0 pct)))
                         (setq pct (- pct tol))
                         (if (< pct 0.0) (setq pct 0.0))
		                 (+= err pct)
                         (if (= (modi n (divi N 10)) 0) (writeln "[" n "] ey=[" y "] y=[" Y[n] "] err=[" (- Y[n] y) "] err%=[" pct "]"))
		                 ) ; end N loop
                      (/= err N) 
		              (writeln "svmRegress: err=[" err "], avgY=[" avgY "]")
		           )) ; end if
          )) ; end Test Case randomSigmoid
       ;; Test Case square 
       (if (or (= Test all:) (= Test square:))
           (begin
		       ;; Create a test polynomial regression where y = -11.2 + C[0]*X[0] - C[1]*(X[1]**2) + C[2]*X[2] - C[3]*(X[3]**2) ...
		       ;; Note: We support a bias by having X[0] == 1 for all N.
		       (setq c Cs)
		       (setq M Ms)
		       (setq N Ns)
		       (setq X (new Vector: object: N))
		       (setq Y (new Vector: Number: N))
		       (setq C (new Vector: Number: M))
		       (setq C[0] c)
		       (loop for m from 1 until M do
		          (setq C[m] (- (srandom 100.0) 50.0))
		          ) ; end C loop
		       (loop for n from 0 until N do
		          (setq X[n] (new Vector: Number: M))
		          (setq X[n][0] 1)
		          (setq y (* C[0] X[n][0]))
		          (loop for m from 1 until M do
		          	 (setq X[n][m] (- (srandom 100.0) 50.0))
		             (if (isOdd m)
			             (setq y (+ y (* X[n][m] C[m])))
			             (setq y (+ y (* X[n][m] X[n][m] C[m])))
		                 ) ; end if
		             ) ; end M loop
		          (setq Y[n] y)
		          ) ; end N loop
		       ;; Train on the test case.
		       (writeln _eol "Starting test case: square")
		       (setq Lambda (esm.svmRegress.svmTraining X Y kernelID properties))
		       (if (= myVerboseSW false) (writeln "svmRegress: N = [" Ns "], M = [" Lambda.M "], Generations = [" Generations  "], Layers = [" myMaxLayers  "], SVM's = [" Lambda.N "], ETollerance=[" Lambda.ETollerance "], Error=[" Lambda.Error "], ErrorGrid=[" (string Lambda.ErrorGrid true) "]")) 
               (if (= checkResults true)
                   (begin
                      (setq err 0.0)
                      (setq avgTopEy 0.0)
                      (setq topEyCnt 0)
                      (setq avgY (avg Y))
		              (loop for n from 0 until N do
		                 (setq y (Lambda X[n]))
                         (if (> y avgY) then (begin (++ topEyCnt) (+= avgTopEy Y[n])))
                         (setq pct (- Y[n] y))
                         (if (<> Y[n] 0.0) (/= pct Y[n])) 
                         (if (< pct 0.0) (setq pct (- 0.0 pct)))
                         (setq pct (- pct tol))
                         (if (< pct 0.0) (setq pct 0.0))
		                 (+= err pct)
                         (if (= (modi n (divi N 10)) 0) (writeln "[" n "] ey=[" y "] y=[" Y[n] "] err=[" (- Y[n] y) "] err%=[" pct "]"))
		                 ) ; end N loop
                      (/= err N) 
		              (writeln "svmRegress: err=[" err "], avgY=[" avgY "]")
		           )) ; end if
          )) ; end Test Case square
       ;; Test Case tan 
       (if (or (= Test all:) (= Test tan:))
           (begin
		       ;; Create a test polynomial regression where y = -11.2 + C[0]*X[0] - C[1]*(X[1]**2) + C[2]*X[2] - C[3]*(X[3]**2) ...
		       ;; Note: We support a bias by having X[0] == 1 for all N.
		       (setq c Cs)
		       (setq M Ms)
		       (setq N Ns)
		       (setq X (new Vector: object: N))
		       (setq Y (new Vector: Number: N))
		       (setq C (new Vector: Number: M))
		       (setq C[0] c)
		       (loop for m from 1 until M do
		          (setq C[m] (- (srandom 100.0) 50.0))
		          ) ; end C loop
		       (loop for n from 0 until N do
		          (setq X[n] (new Vector: Number: M))
		          (setq X[n][0] 1)
		          (setq y (* C[0] X[n][0]))
		          (loop for m from 1 until M do
		          	 (setq X[n][m] (- (srandom 100.0) 50.0))
		             (setq y (+ y (* (tan X[n][m]) C[m])))
		             ) ; end M loop
		          (setq Y[n] y)
		          ) ; end N loop
		       ;; Train on the test case.
		       (writeln _eol "Starting test case: tan")
		       (setq Lambda (esm.svmRegress.svmTraining X Y kernelID properties))
		       (if (= myVerboseSW false) (writeln "svmRegress: N = [" Ns "], M = [" Lambda.M "], Generations = [" Generations  "], Layers = [" myMaxLayers  "], SVM's = [" Lambda.N "], ETollerance=[" Lambda.ETollerance "], Error=[" Lambda.Error "], ErrorGrid=[" (string Lambda.ErrorGrid true) "]")) 
               (if (= checkResults true)
                   (begin
                      (setq err 0.0)
                      (setq avgTopEy 0.0)
                      (setq topEyCnt 0)
                      (setq avgY (avg Y))
		              (loop for n from 0 until N do
		                 (setq y (Lambda X[n]))
                         (if (> y avgY) then (begin (++ topEyCnt) (+= avgTopEy Y[n])))
                         (setq pct (- Y[n] y))
                         (if (<> Y[n] 0.0) (/= pct Y[n])) 
                         (if (< pct 0.0) (setq pct (- 0.0 pct)))
                         (setq pct (- pct tol))
                         (if (< pct 0.0) (setq pct 0.0))
		                 (+= err pct)
                         (if (= (modi n (divi N 10)) 0) (writeln "[" n "] ey=[" y "] y=[" Y[n] "] err=[" (- Y[n] y) "] err%=[" pct "]"))
		                 ) ; end N loop
                      (/= err N) 
		              (writeln "svmRegress: err=[" err "], avgY=[" avgY "]")
		           )) ; end if
          )) ; end Test Case tan
       ;; Test Case log 
       (if (or (= Test all:) (= Test log:))
           (begin
		       ;; Create a test polynomial regression where y = -11.2 + C[0]*X[0] - C[1]*(X[1]**2) + C[2]*X[2] - C[3]*(X[3]**2) ...
		       ;; Note: We support a bias by having X[0] == 1 for all N.
		       (setq c Cs)
		       (setq M Ms)
		       (setq N Ns)
		       (setq X (new Vector: object: N))
		       (setq Y (new Vector: Number: N))
		       (setq C (new Vector: Number: M))
		       (setq C[0] c)
		       (loop for m from 1 until M do
		          (setq C[m] (- (srandom 100.0) 50.0))
		          ) ; end C loop
		       (loop for n from 0 until N do
		          (setq X[n] (new Vector: Number: M))
		          (setq X[n][0] 1)
		          (setq y (* C[0] X[n][0]))
		          (loop for m from 1 until M do
		          	 (setq X[n][m] (- (srandom 100.0) 50.0))
		             (setq y (+ y (* (log (+ 1.0 (abs X[n][m]))) C[m])))
		             ) ; end M loop
		          (setq Y[n] y)
		          ) ; end N loop
		       ;; Train on the test case.
		       (writeln _eol "Starting test case: log")
		       (setq Lambda (esm.svmRegress.svmTraining X Y kernelID properties))
		       (if (= myVerboseSW false) (writeln "svmRegress: N = [" Ns "], M = [" Lambda.M "], Generations = [" Generations  "], Layers = [" myMaxLayers  "], SVM's = [" Lambda.N "], ETollerance=[" Lambda.ETollerance "], Error=[" Lambda.Error "], ErrorGrid=[" (string Lambda.ErrorGrid true) "]")) 
               (if (= checkResults true)
                   (begin
                      (setq err 0.0)
                      (setq avgTopEy 0.0)
                      (setq topEyCnt 0)
                      (setq avgY (avg Y))
		              (loop for n from 0 until N do
		                 (setq y (Lambda X[n]))
                         (if (> y avgY) then (begin (++ topEyCnt) (+= avgTopEy Y[n])))
                         (setq pct (- Y[n] y))
                         (if (<> Y[n] 0.0) (/= pct Y[n])) 
                         (if (< pct 0.0) (setq pct (- 0.0 pct)))
                         (setq pct (- pct tol))
                         (if (< pct 0.0) (setq pct 0.0))
		                 (+= err pct)
                         (if (= (modi n (divi N 10)) 0) (writeln "[" n "] ey=[" y "] y=[" Y[n] "] err=[" (- Y[n] y) "] err%=[" pct "]"))
		                 ) ; end N loop
                      (/= err N) 
		              (writeln "svmRegress: err=[" err "], avgY=[" avgY "]")
		           )) ; end if
          )) ; end Test Case log
       (writeln "svmRegress.selfTest: completed in [" (/ (setq endTimeT (getTickCount startTimeT)) 60.0) "] minutes.")       
       Lambda) ; end selfTest
    ;; *****************
    ;; Begin main logic.
    ;; ***************** 
    vars:(Lambda properties svmLambda kernelID sampleSize)

    ;; Create and train a new support vector machine.
    ;; Note: Retrieve any optional arguments.
    (setq kernelID (if (>= (argCount) 3) (argFetch 2) esm.mySvmKernelID))
    ;; Always imit SVM sample size to 10000 rows or less for proper memory usage.
    (setq properties (if (>= (argCount) 4) (argFetch 3) (new Structure: ETollerance: 0.0 GridErr: 0 MaxErr: 0.001 MaxGen: esm.mySvmMaxGen MaxLayers: esm.mySvmMaxLayers ModelCount: esm.mySvmModelCount OverrideSW: false UserSVSize: esm.mySvmSampleSize VerboseSW: false))) 
    ;; Train a new support vector machine Lambda.
    (setq svmLambda (new (myself)))
    (setq svmLambda.mySVMParent svmLambda)
    (setq Lambda (svmLambda.svmTraining x y kernelID properties))
    Lambda) ; end svmRegress















































